{
  "id": 20,
  "repo": "neon",
  "issue_url": "https://github.com/neondatabase/neon/issues/559",
  "pr_url": "https://github.com/neondatabase/neon/pull/6712",
  "issue_description": "Found in main\r\n```\r\nAssertionError: assert (['pg_xact/0000'], []) == ([], [])\r\n  At index 0 diff: ['pg_xact/0000'] != []\r\n  Full diff:\r\n  - ([], [])\r\n  + (['pg_xact/0000'], [])\r\nbatch_pg_regress/test_pg_regress.py:58: in test_pg_regress\r\n    check_restored_datadir_content(zenith_cli, test_output_dir, pg)\r\nfixtures/zenith_fixtures.py:1011: in check_restored_datadir_content\r\n    assert (mismatch, error) == ([], [])\r\nE   AssertionError: assert (['pg_xact/0000'], []) == ([], [])\r\nE     At index 0 diff: ['pg_xact/0000'] != []\r\nE     Full diff:\r\nE     - ([], [])\r\nE     + (['pg_xact/0000'], [])\r\n```\r\n\r\n@lubennikovaav is it because of the recent changes or this is a spurious error?",
  "files_changed": [
    {
      "filename": "libs/walproposer/src/api_bindings.rs",
      "status": "modified",
      "patch": "@@ -324,11 +324,11 @@ extern \"C\" fn finish_sync_safekeepers(wp: *mut WalProposer, lsn: XLogRecPtr) {\n     }\n }\n \n-extern \"C\" fn process_safekeeper_feedback(wp: *mut WalProposer, commit_lsn: XLogRecPtr) {\n+extern \"C\" fn process_safekeeper_feedback(wp: *mut WalProposer) {\n     unsafe {\n         let callback_data = (*(*wp).config).callback_data;\n         let api = callback_data as *mut Box<dyn ApiImpl>;\n-        (*api).process_safekeeper_feedback(&mut (*wp), commit_lsn)\n+        (*api).process_safekeeper_feedback(&mut (*wp))\n     }\n }\n "
    },
    {
      "filename": "libs/walproposer/src/walproposer.rs",
      "status": "modified",
      "patch": "@@ -142,7 +142,7 @@ pub trait ApiImpl {\n         todo!()\n     }\n \n-    fn process_safekeeper_feedback(&self, _wp: &mut WalProposer, _commit_lsn: u64) {\n+    fn process_safekeeper_feedback(&mut self, _wp: &mut WalProposer) {\n         todo!()\n     }\n "
    },
    {
      "filename": "pgxn/neon/walproposer.c",
      "status": "modified",
      "patch": "@@ -1220,7 +1220,7 @@ PrepareAppendRequest(WalProposer *wp, AppendRequestHeader *req, XLogRecPtr begin\n \treq->epochStartLsn = wp->propEpochStartLsn;\n \treq->beginLsn = beginLsn;\n \treq->endLsn = endLsn;\n-\treq->commitLsn = GetAcknowledgedByQuorumWALPosition(wp);\n+\treq->commitLsn = wp->commitLsn;\n \treq->truncateLsn = wp->truncateLsn;\n \treq->proposerId = wp->greetRequest.proposerId;\n }\n@@ -1405,7 +1405,7 @@ static bool\n RecvAppendResponses(Safekeeper *sk)\n {\n \tWalProposer *wp = sk->wp;\n-\tXLogRecPtr\tminQuorumLsn;\n+\tXLogRecPtr\tnewCommitLsn;\n \tbool\t\treadAnything = false;\n \n \twhile (true)\n@@ -1444,18 +1444,19 @@ RecvAppendResponses(Safekeeper *sk)\n \tif (!readAnything)\n \t\treturn sk->state == SS_ACTIVE;\n \n-\tHandleSafekeeperResponse(wp);\n-\n+\t/* update commit_lsn */\n+\tnewCommitLsn = GetAcknowledgedByQuorumWALPosition(wp);\n \t/*\n-\t * Also send the new commit lsn to all the safekeepers.\n+\t * Send the new value to all safekeepers.\n \t */\n-\tminQuorumLsn = GetAcknowledgedByQuorumWALPosition(wp);\n-\tif (minQuorumLsn > wp->lastSentCommitLsn)\n+\tif (newCommitLsn > wp->commitLsn)\n \t{\n+\t\twp->commitLsn = newCommitLsn;\n \t\tBroadcastAppendRequest(wp);\n-\t\twp->lastSentCommitLsn = minQuorumLsn;\n \t}\n \n+\tHandleSafekeeperResponse(wp);\n+\n \treturn sk->state == SS_ACTIVE;\n }\n \n@@ -1632,11 +1633,9 @@ GetDonor(WalProposer *wp, XLogRecPtr *donor_lsn)\n static void\n HandleSafekeeperResponse(WalProposer *wp)\n {\n-\tXLogRecPtr\tminQuorumLsn;\n \tXLogRecPtr\tcandidateTruncateLsn;\n \n-\tminQuorumLsn = GetAcknowledgedByQuorumWALPosition(wp);\n-\twp->api.process_safekeeper_feedback(wp, minQuorumLsn);\n+\twp->api.process_safekeeper_feedback(wp);\n \n \t/*\n \t * Try to advance truncateLsn -- the last record flushed to all\n@@ -1649,7 +1648,7 @@ HandleSafekeeperResponse(WalProposer *wp)\n \t * can't commit entries from previous term' in Raft); 2)\n \t */\n \tcandidateTruncateLsn = CalculateMinFlushLsn(wp);\n-\tcandidateTruncateLsn = Min(candidateTruncateLsn, minQuorumLsn);\n+\tcandidateTruncateLsn = Min(candidateTruncateLsn, wp->commitLsn);\n \tif (candidateTruncateLsn > wp->truncateLsn)\n \t{\n \t\twp->truncateLsn = candidateTruncateLsn;"
    },
    {
      "filename": "pgxn/neon/walproposer.h",
      "status": "modified",
      "patch": "@@ -564,7 +564,7 @@ typedef struct walproposer_api\n \t * backpressure feedback and to confirm WAL persistence (has been commited\n \t * on the quorum of safekeepers).\n \t */\n-\tvoid\t\t(*process_safekeeper_feedback) (WalProposer *wp, XLogRecPtr commitLsn);\n+\tvoid\t\t(*process_safekeeper_feedback) (WalProposer *wp);\n \n \t/*\n \t * Write a log message to the internal log processor. This is used only\n@@ -646,8 +646,8 @@ typedef struct WalProposer\n \t/* WAL has been generated up to this point */\n \tXLogRecPtr\tavailableLsn;\n \n-\t/* last commitLsn broadcasted to safekeepers */\n-\tXLogRecPtr\tlastSentCommitLsn;\n+\t/* cached GetAcknowledgedByQuorumWALPosition result */\n+\tXLogRecPtr\tcommitLsn;\n \n \tProposerGreeting greetRequest;\n "
    },
    {
      "filename": "pgxn/neon/walproposer_pg.c",
      "status": "modified",
      "patch": "@@ -68,6 +68,8 @@ static WalproposerShmemState *walprop_shared;\n static WalProposerConfig walprop_config;\n static XLogRecPtr sentPtr = InvalidXLogRecPtr;\n static const walproposer_api walprop_pg;\n+static volatile sig_atomic_t got_SIGUSR2 = false;\n+static bool reported_sigusr2 = false;\n \n static void nwp_shmem_startup_hook(void);\n static void nwp_register_gucs(void);\n@@ -101,6 +103,8 @@ static void add_nwr_event_set(Safekeeper *sk, uint32 events);\n static void update_nwr_event_set(Safekeeper *sk, uint32 events);\n static void rm_safekeeper_event_set(Safekeeper *to_remove, bool is_sk);\n \n+static void CheckGracefulShutdown(WalProposer *wp);\n+\n static XLogRecPtr GetLogRepRestartLSN(WalProposer *wp);\n \n static void\n@@ -492,6 +496,24 @@ walprop_pg_init_standalone_sync_safekeepers(void)\n \tBackgroundWorkerUnblockSignals();\n }\n \n+/*\n+ * We pretend to be a walsender process, and the lifecycle of a walsender is\n+ * slightly different than other procesess. At shutdown, walsender processes\n+ * stay alive until the very end, after the checkpointer has written the\n+ * shutdown checkpoint. When the checkpointer exits, the postmaster sends all\n+ * remaining walsender processes SIGUSR2. On receiving SIGUSR2, we try to send\n+ * the remaining WAL, and then exit. This ensures that the checkpoint record\n+ * reaches durable storage (in safekeepers), before the server shuts down\n+ * completely.\n+ */\n+static void\n+walprop_sigusr2(SIGNAL_ARGS)\n+{\n+\tgot_SIGUSR2 = true;\n+\n+\tSetLatch(MyLatch);\n+}\n+\n static void\n walprop_pg_init_bgworker(void)\n {\n@@ -503,6 +525,7 @@ walprop_pg_init_bgworker(void)\n \tpqsignal(SIGUSR1, procsignal_sigusr1_handler);\n \tpqsignal(SIGHUP, SignalHandlerForConfigReload);\n \tpqsignal(SIGTERM, die);\n+\tpqsignal(SIGUSR2, walprop_sigusr2);\n \n \tBackgroundWorkerUnblockSignals();\n \n@@ -1075,14 +1098,26 @@ StartProposerReplication(WalProposer *wp, StartReplicationCmd *cmd)\n #endif\n \n \t/*\n-\t * When we first start replication the standby will be behind the primary.\n-\t * For some applications, for example synchronous replication, it is\n-\t * important to have a clear state for this initial catchup mode, so we\n-\t * can trigger actions when we change streaming state later. We may stay\n-\t * in this state for a long time, which is exactly why we want to be able\n-\t * to monitor whether or not we are still here.\n+\t * XXX: Move straight to STOPPING state, skipping the STREAMING state.\n+\t *\n+\t * This is a bit weird. Normal walsenders stay in STREAMING state, until\n+\t * the checkpointer signals them that it is about to start writing the\n+\t * shutdown checkpoint. The walsenders acknowledge that they have received\n+\t * that signal by switching to STOPPING state. That tells the walsenders\n+\t * that they must not write any new WAL.\n+\t *\n+\t * However, we cannot easily intercept that signal from the checkpointer.\n+\t * It's sent by WalSndInitStopping(), using\n+\t * SendProcSignal(PROCSIGNAL_WALSND_INIT_STOPPING). It's received by\n+\t * HandleWalSndInitStopping, which sets a process-local got_STOPPING flag.\n+\t * However, that's all private to walsender.c.\n+\t *\n+\t * We don't need to do anything special upon receiving the signal, the\n+\t * walproposer doesn't write any WAL anyway, so we skip the STREAMING\n+\t * state and go directly to STOPPING mode. That way, the checkpointer\n+\t * won't wait for us.\n \t */\n-\tWalSndSetState(WALSNDSTATE_CATCHUP);\n+\tWalSndSetState(WALSNDSTATE_STOPPING);\n \n \t/*\n \t * Don't allow a request to stream from a future point in WAL that hasn't\n@@ -1122,6 +1157,8 @@ StartProposerReplication(WalProposer *wp, StartReplicationCmd *cmd)\n static void\n WalSndLoop(WalProposer *wp)\n {\n+\tXLogRecPtr\tflushPtr;\n+\n \t/* Clear any already-pending wakeups */\n \tResetLatch(MyLatch);\n \n@@ -1130,9 +1167,6 @@ WalSndLoop(WalProposer *wp)\n \t\tCHECK_FOR_INTERRUPTS();\n \n \t\tXLogBroadcastWalProposer(wp);\n-\n-\t\tif (MyWalSnd->state == WALSNDSTATE_CATCHUP)\n-\t\t\tWalSndSetState(WALSNDSTATE_STREAMING);\n \t\tWalProposerPoll(wp);\n \t}\n }\n@@ -1745,6 +1779,9 @@ walprop_pg_wait_event_set(WalProposer *wp, long timeout, Safekeeper **sk, uint32\n \t{\n \t\tConditionVariableCancelSleep();\n \t\tResetLatch(MyLatch);\n+\n+\t\tCheckGracefulShutdown(wp);\n+\n \t\t*events = WL_LATCH_SET;\n \t\treturn 1;\n \t}\n@@ -1798,6 +1835,41 @@ walprop_pg_finish_sync_safekeepers(WalProposer *wp, XLogRecPtr lsn)\n \texit(0);\n }\n \n+/*\n+ * Like vanilla walsender, on sigusr2 send all remaining WAL and exit.\n+ *\n+ * Note that unlike sync-safekeepers waiting here is not reliable: we\n+ * don't check that majority of safekeepers received and persisted\n+ * commit_lsn -- only that walproposer reached it (which immediately\n+ * broadcasts new value). Doing that without incurring redundant control\n+ * file syncing would need wp -> sk protocol change. OTOH unlike\n+ * sync-safekeepers which must bump commit_lsn or basebackup will fail,\n+ * this catchup is important only for tests where safekeepers/network\n+ * don't crash on their own.\n+ */\n+static void\n+CheckGracefulShutdown(WalProposer *wp)\n+{\n+\tif (got_SIGUSR2)\n+\t{\n+\t\tif (!reported_sigusr2)\n+\t\t{\n+\t\t\tXLogRecPtr\tflushPtr = walprop_pg_get_flush_rec_ptr(wp);\n+\n+\t\t\twpg_log(LOG, \"walproposer will send and wait for remaining WAL between %X/%X and %X/%X\",\n+\t\t\t\t\tLSN_FORMAT_ARGS(wp->commitLsn), LSN_FORMAT_ARGS(flushPtr));\n+\t\t\treported_sigusr2 = true;\n+\t\t}\n+\n+\t\tif (wp->commitLsn >= walprop_pg_get_flush_rec_ptr(wp))\n+\t\t{\n+\t\t\twpg_log(LOG, \"walproposer sent all WAL up to %X/%X, exiting\",\n+\t\t\t\t\tLSN_FORMAT_ARGS(wp->commitLsn));\n+\t\t\tproc_exit(0);\n+\t\t}\n+\t}\n+}\n+\n /*\n  * Choose most advanced PageserverFeedback and set it to *rf.\n  */\n@@ -1878,7 +1950,7 @@ CombineHotStanbyFeedbacks(HotStandbyFeedback *hs, WalProposer *wp)\n  * None of that is functional in sync-safekeepers.\n  */\n static void\n-walprop_pg_process_safekeeper_feedback(WalProposer *wp, XLogRecPtr commitLsn)\n+walprop_pg_process_safekeeper_feedback(WalProposer *wp)\n {\n \tHotStandbyFeedback hsFeedback;\n \tXLogRecPtr\toldDiskConsistentLsn;\n@@ -1893,10 +1965,10 @@ walprop_pg_process_safekeeper_feedback(WalProposer *wp, XLogRecPtr commitLsn)\n \treplication_feedback_set(&quorumFeedback.rf);\n \tSetZenithCurrentClusterSize(quorumFeedback.rf.currentClusterSize);\n \n-\tif (commitLsn > quorumFeedback.flushLsn || oldDiskConsistentLsn != quorumFeedback.rf.disk_consistent_lsn)\n+\tif (wp->commitLsn > quorumFeedback.flushLsn || oldDiskConsistentLsn != quorumFeedback.rf.disk_consistent_lsn)\n \t{\n-\t\tif (commitLsn > quorumFeedback.flushLsn)\n-\t\t\tquorumFeedback.flushLsn = commitLsn;\n+\t\tif (wp->commitLsn > quorumFeedback.flushLsn)\n+\t\t\tquorumFeedback.flushLsn = wp->commitLsn;\n \n \t\t/*\n \t\t * Advance the replication slot to commitLsn. WAL before it is\n@@ -1929,6 +2001,8 @@ walprop_pg_process_safekeeper_feedback(WalProposer *wp, XLogRecPtr commitLsn)\n \t\t\t\t\t\t\t\t XidFromFullTransactionId(hsFeedback.catalog_xmin),\n \t\t\t\t\t\t\t\t EpochFromFullTransactionId(hsFeedback.catalog_xmin));\n \t}\n+\n+\tCheckGracefulShutdown(wp);\n }\n \n static XLogRecPtr"
    },
    {
      "filename": "safekeeper/tests/walproposer_sim/walproposer_api.rs",
      "status": "modified",
      "patch": "@@ -196,6 +196,7 @@ pub struct SimulationApi {\n     safekeepers: RefCell<Vec<SafekeeperConn>>,\n     disk: Arc<DiskWalProposer>,\n     redo_start_lsn: Option<Lsn>,\n+    last_logged_commit_lsn: u64,\n     shmem: UnsafeCell<walproposer::bindings::WalproposerShmemState>,\n     config: Config,\n     event_set: RefCell<Option<EventSet>>,\n@@ -228,6 +229,7 @@ impl SimulationApi {\n             safekeepers: RefCell::new(sk_conns),\n             disk: args.disk,\n             redo_start_lsn: args.redo_start_lsn,\n+            last_logged_commit_lsn: 0,\n             shmem: UnsafeCell::new(walproposer::bindings::WalproposerShmemState {\n                 mutex: 0,\n                 feedback: PageserverFeedback {\n@@ -596,14 +598,11 @@ impl ApiImpl for SimulationApi {\n         }\n     }\n \n-    fn process_safekeeper_feedback(\n-        &self,\n-        wp: &mut walproposer::bindings::WalProposer,\n-        commit_lsn: u64,\n-    ) {\n-        debug!(\"process_safekeeper_feedback, commit_lsn={}\", commit_lsn);\n-        if commit_lsn > wp.lastSentCommitLsn {\n-            self.os.log_event(format!(\"commit_lsn;{}\", commit_lsn));\n+    fn process_safekeeper_feedback(&mut self, wp: &mut walproposer::bindings::WalProposer) {\n+        debug!(\"process_safekeeper_feedback, commit_lsn={}\", wp.commitLsn);\n+        if wp.commitLsn > self.last_logged_commit_lsn {\n+            self.os.log_event(format!(\"commit_lsn;{}\", wp.commitLsn));\n+            self.last_logged_commit_lsn = wp.commitLsn;\n         }\n     }\n "
    },
    {
      "filename": "test_runner/fixtures/neon_fixtures.py",
      "status": "modified",
      "patch": "@@ -15,11 +15,11 @@\n import time\n import uuid\n from contextlib import closing, contextmanager\n-from dataclasses import dataclass, field\n+from dataclasses import dataclass\n from datetime import datetime\n from enum import Enum\n from fcntl import LOCK_EX, LOCK_UN, flock\n-from functools import cached_property\n+from functools import cached_property, partial\n from itertools import chain, product\n from pathlib import Path\n from types import TracebackType\n@@ -70,6 +70,8 @@\n     default_remote_storage,\n     remote_storage_to_toml_inline_table,\n )\n+from fixtures.safekeeper.http import SafekeeperHttpClient\n+from fixtures.safekeeper.utils import are_walreceivers_absent\n from fixtures.types import Lsn, TenantId, TenantShardId, TimelineId\n from fixtures.utils import (\n     ATTACHMENT_NAME_REGEX,\n@@ -2547,6 +2549,20 @@ def run_capture(\n         )\n         return base_path\n \n+    def get_pg_controldata_checkpoint_lsn(self, pgdata: str) -> Lsn:\n+        \"\"\"\n+        Run pg_controldata on given datadir and extract checkpoint lsn.\n+        \"\"\"\n+\n+        pg_controldata_path = os.path.join(self.pg_bin_path, \"pg_controldata\")\n+        cmd = f\"{pg_controldata_path} -D {pgdata}\"\n+        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n+        checkpoint_lsn = re.findall(\n+            \"Latest checkpoint location:\\\\s+([0-9A-F]+/[0-9A-F]+)\", result.stdout\n+        )[0]\n+        log.info(f\"last checkpoint at {checkpoint_lsn}\")\n+        return Lsn(checkpoint_lsn)\n+\n \n @pytest.fixture(scope=\"function\")\n def pg_bin(test_output_dir: Path, pg_distrib_dir: Path, pg_version: PgVersion) -> PgBin:\n@@ -3565,220 +3581,6 @@ def list_segments(self, tenant_id, timeline_id) -> List[str]:\n         return segments\n \n \n-# Walreceiver as returned by sk's timeline status endpoint.\n-@dataclass\n-class Walreceiver:\n-    conn_id: int\n-    state: str\n-\n-\n-@dataclass\n-class SafekeeperTimelineStatus:\n-    acceptor_epoch: int\n-    pg_version: int  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n-    flush_lsn: Lsn\n-    commit_lsn: Lsn\n-    timeline_start_lsn: Lsn\n-    backup_lsn: Lsn\n-    peer_horizon_lsn: Lsn\n-    remote_consistent_lsn: Lsn\n-    walreceivers: List[Walreceiver]\n-\n-\n-@dataclass\n-class SafekeeperMetrics:\n-    # These are metrics from Prometheus which uses float64 internally.\n-    # As a consequence, values may differ from real original int64s.\n-    flush_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n-    commit_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n-\n-\n-class SafekeeperHttpClient(requests.Session):\n-    HTTPError = requests.HTTPError\n-\n-    def __init__(self, port: int, auth_token: Optional[str] = None, is_testing_enabled=False):\n-        super().__init__()\n-        self.port = port\n-        self.auth_token = auth_token\n-        self.is_testing_enabled = is_testing_enabled\n-\n-        if auth_token is not None:\n-            self.headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n-\n-    def check_status(self):\n-        self.get(f\"http://localhost:{self.port}/v1/status\").raise_for_status()\n-\n-    def is_testing_enabled_or_skip(self):\n-        if not self.is_testing_enabled:\n-            pytest.skip(\"safekeeper was built without 'testing' feature\")\n-\n-    def configure_failpoints(self, config_strings: Tuple[str, str] | List[Tuple[str, str]]):\n-        self.is_testing_enabled_or_skip()\n-\n-        if isinstance(config_strings, tuple):\n-            pairs = [config_strings]\n-        else:\n-            pairs = config_strings\n-\n-        log.info(f\"Requesting config failpoints: {repr(pairs)}\")\n-\n-        res = self.put(\n-            f\"http://localhost:{self.port}/v1/failpoints\",\n-            json=[{\"name\": name, \"actions\": actions} for name, actions in pairs],\n-        )\n-        log.info(f\"Got failpoints request response code {res.status_code}\")\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert res_json is None\n-        return res_json\n-\n-    def debug_dump(self, params: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n-        params = params or {}\n-        res = self.get(f\"http://localhost:{self.port}/v1/debug_dump\", params=params)\n-        res.raise_for_status()\n-        res_json = json.loads(res.text)\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def patch_control_file(\n-        self,\n-        tenant_id: TenantId,\n-        timeline_id: TimelineId,\n-        patch: Dict[str, Any],\n-    ) -> Dict[str, Any]:\n-        res = self.patch(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/control_file\",\n-            json={\n-                \"updates\": patch,\n-                \"apply_fields\": list(patch.keys()),\n-            },\n-        )\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def pull_timeline(self, body: Dict[str, Any]) -> Dict[str, Any]:\n-        res = self.post(f\"http://localhost:{self.port}/v1/pull_timeline\", json=body)\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def copy_timeline(self, tenant_id: TenantId, timeline_id: TimelineId, body: Dict[str, Any]):\n-        res = self.post(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/copy\",\n-            json=body,\n-        )\n-        res.raise_for_status()\n-\n-    def timeline_digest(\n-        self, tenant_id: TenantId, timeline_id: TimelineId, from_lsn: Lsn, until_lsn: Lsn\n-    ) -> Dict[str, Any]:\n-        res = self.get(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/digest\",\n-            params={\n-                \"from_lsn\": str(from_lsn),\n-                \"until_lsn\": str(until_lsn),\n-            },\n-        )\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def timeline_create(\n-        self,\n-        tenant_id: TenantId,\n-        timeline_id: TimelineId,\n-        pg_version: int,  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n-        commit_lsn: Lsn,\n-    ):\n-        body = {\n-            \"tenant_id\": str(tenant_id),\n-            \"timeline_id\": str(timeline_id),\n-            \"pg_version\": pg_version,\n-            \"commit_lsn\": str(commit_lsn),\n-        }\n-        res = self.post(f\"http://localhost:{self.port}/v1/tenant/timeline\", json=body)\n-        res.raise_for_status()\n-\n-    def timeline_status(\n-        self, tenant_id: TenantId, timeline_id: TimelineId\n-    ) -> SafekeeperTimelineStatus:\n-        res = self.get(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\")\n-        res.raise_for_status()\n-        resj = res.json()\n-        walreceivers = [Walreceiver(wr[\"conn_id\"], wr[\"status\"]) for wr in resj[\"walreceivers\"]]\n-        return SafekeeperTimelineStatus(\n-            acceptor_epoch=resj[\"acceptor_state\"][\"epoch\"],\n-            pg_version=resj[\"pg_info\"][\"pg_version\"],\n-            flush_lsn=Lsn(resj[\"flush_lsn\"]),\n-            commit_lsn=Lsn(resj[\"commit_lsn\"]),\n-            timeline_start_lsn=Lsn(resj[\"timeline_start_lsn\"]),\n-            backup_lsn=Lsn(resj[\"backup_lsn\"]),\n-            peer_horizon_lsn=Lsn(resj[\"peer_horizon_lsn\"]),\n-            remote_consistent_lsn=Lsn(resj[\"remote_consistent_lsn\"]),\n-            walreceivers=walreceivers,\n-        )\n-\n-    def record_safekeeper_info(self, tenant_id: TenantId, timeline_id: TimelineId, body):\n-        res = self.post(\n-            f\"http://localhost:{self.port}/v1/record_safekeeper_info/{tenant_id}/{timeline_id}\",\n-            json=body,\n-        )\n-        res.raise_for_status()\n-\n-    # only_local doesn't remove segments in the remote storage.\n-    def timeline_delete(\n-        self, tenant_id: TenantId, timeline_id: TimelineId, only_local: bool = False\n-    ) -> Dict[Any, Any]:\n-        res = self.delete(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\",\n-            params={\n-                \"only_local\": str(only_local).lower(),\n-            },\n-        )\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def tenant_delete_force(self, tenant_id: TenantId) -> Dict[Any, Any]:\n-        res = self.delete(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}\")\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def get_metrics_str(self) -> str:\n-        request_result = self.get(f\"http://localhost:{self.port}/metrics\")\n-        request_result.raise_for_status()\n-        return request_result.text\n-\n-    def get_metrics(self) -> SafekeeperMetrics:\n-        all_metrics_text = self.get_metrics_str()\n-\n-        metrics = SafekeeperMetrics()\n-        for match in re.finditer(\n-            r'^safekeeper_flush_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n-            all_metrics_text,\n-            re.MULTILINE,\n-        ):\n-            metrics.flush_lsn_inexact[(TenantId(match.group(1)), TimelineId(match.group(2)))] = int(\n-                match.group(3)\n-            )\n-        for match in re.finditer(\n-            r'^safekeeper_commit_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n-            all_metrics_text,\n-            re.MULTILINE,\n-        ):\n-            metrics.commit_lsn_inexact[\n-                (TenantId(match.group(1)), TimelineId(match.group(2)))\n-            ] = int(match.group(3))\n-        return metrics\n-\n-\n class S3Scrubber:\n     def __init__(self, env: NeonEnvBuilder, log_dir: Optional[Path] = None):\n         self.env = env\n@@ -4088,32 +3890,29 @@ def list_files_to_compare(pgdata_dir: Path) -> List[str]:\n \n # pg is the existing and running compute node, that we want to compare with a basebackup\n def check_restored_datadir_content(test_output_dir: Path, env: NeonEnv, endpoint: Endpoint):\n+    pg_bin = PgBin(test_output_dir, env.pg_distrib_dir, env.pg_version)\n+\n     # Get the timeline ID. We need it for the 'basebackup' command\n     timeline_id = TimelineId(endpoint.safe_psql(\"SHOW neon.timeline_id\")[0][0])\n \n-    # many tests already checkpoint, but do it just in case\n-    with closing(endpoint.connect()) as conn:\n-        with conn.cursor() as cur:\n-            cur.execute(\"CHECKPOINT\")\n-\n-    # wait for pageserver to catch up\n-    wait_for_last_flush_lsn(env, endpoint, endpoint.tenant_id, timeline_id)\n     # stop postgres to ensure that files won't change\n     endpoint.stop()\n \n+    # Read the shutdown checkpoint's LSN\n+    checkpoint_lsn = pg_bin.get_pg_controldata_checkpoint_lsn(endpoint.pg_data_dir_path())\n+\n     # Take a basebackup from pageserver\n     restored_dir_path = env.repo_dir / f\"{endpoint.endpoint_id}_restored_datadir\"\n     restored_dir_path.mkdir(exist_ok=True)\n \n-    pg_bin = PgBin(test_output_dir, env.pg_distrib_dir, env.pg_version)\n     psql_path = os.path.join(pg_bin.pg_bin_path, \"psql\")\n \n     pageserver_id = env.attachment_service.locate(endpoint.tenant_id)[0][\"node_id\"]\n     cmd = rf\"\"\"\n         {psql_path}                                    \\\n             --no-psqlrc                                \\\n             postgres://localhost:{env.get_pageserver(pageserver_id).service_port.pg}  \\\n-            -c 'basebackup {endpoint.tenant_id} {timeline_id}'  \\\n+            -c 'basebackup {endpoint.tenant_id} {timeline_id} {checkpoint_lsn}'  \\\n          | tar -x -C {restored_dir_path}\n     \"\"\"\n \n@@ -4262,6 +4061,49 @@ def wait_for_last_flush_lsn(\n     return min(results)\n \n \n+def flush_ep_to_pageserver(\n+    env: NeonEnv,\n+    ep: Endpoint,\n+    tenant: TenantId,\n+    timeline: TimelineId,\n+    pageserver_id: Optional[int] = None,\n+) -> Lsn:\n+    \"\"\"\n+    Stop endpoint and wait until all committed WAL reaches the pageserver\n+    (last_record_lsn). This is for use by tests which want everything written so\n+    far to reach pageserver *and* expecting that no more data will arrive until\n+    endpoint starts again, so unlike wait_for_last_flush_lsn it polls\n+    safekeepers instead of compute to learn LSN.\n+\n+    Returns the catch up LSN.\n+    \"\"\"\n+    ep.stop()\n+\n+    commit_lsn: Lsn = Lsn(0)\n+    # In principle in the absense of failures polling single sk would be enough.\n+    for sk in env.safekeepers:\n+        cli = sk.http_client()\n+        # wait until compute connections are gone\n+        wait_until(30, 0.5, partial(are_walreceivers_absent, cli, tenant, timeline))\n+        commit_lsn = max(cli.get_commit_lsn(tenant, timeline), commit_lsn)\n+\n+    # Note: depending on WAL filtering implementation, probably most shards\n+    # won't be able to reach commit_lsn (unless gaps are also ack'ed), so this\n+    # is broken in sharded case.\n+    shards = tenant_get_shards(env, tenant, pageserver_id)\n+    for tenant_shard_id, pageserver in shards:\n+        log.info(\n+            f\"flush_ep_to_pageserver: waiting for {commit_lsn} on shard {tenant_shard_id} on pageserver {pageserver.id})\"\n+        )\n+        waited = wait_for_last_record_lsn(\n+            pageserver.http_client(), tenant_shard_id, timeline, commit_lsn\n+        )\n+\n+        assert waited >= commit_lsn\n+\n+    return commit_lsn\n+\n+\n def wait_for_wal_insert_lsn(\n     env: NeonEnv,\n     endpoint: Endpoint,"
    },
    {
      "filename": "test_runner/fixtures/safekeeper/__init__.py",
      "status": "added",
      "patch": ""
    },
    {
      "filename": "test_runner/fixtures/safekeeper/http.py",
      "status": "added",
      "patch": "@@ -0,0 +1,227 @@\n+import json\n+import re\n+from dataclasses import dataclass, field\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import pytest\n+import requests\n+\n+from fixtures.log_helper import log\n+from fixtures.types import Lsn, TenantId, TimelineId\n+\n+\n+# Walreceiver as returned by sk's timeline status endpoint.\n+@dataclass\n+class Walreceiver:\n+    conn_id: int\n+    state: str\n+\n+\n+@dataclass\n+class SafekeeperTimelineStatus:\n+    acceptor_epoch: int\n+    pg_version: int  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n+    flush_lsn: Lsn\n+    commit_lsn: Lsn\n+    timeline_start_lsn: Lsn\n+    backup_lsn: Lsn\n+    peer_horizon_lsn: Lsn\n+    remote_consistent_lsn: Lsn\n+    walreceivers: List[Walreceiver]\n+\n+\n+@dataclass\n+class SafekeeperMetrics:\n+    # These are metrics from Prometheus which uses float64 internally.\n+    # As a consequence, values may differ from real original int64s.\n+    flush_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n+    commit_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n+\n+\n+class SafekeeperHttpClient(requests.Session):\n+    HTTPError = requests.HTTPError\n+\n+    def __init__(self, port: int, auth_token: Optional[str] = None, is_testing_enabled=False):\n+        super().__init__()\n+        self.port = port\n+        self.auth_token = auth_token\n+        self.is_testing_enabled = is_testing_enabled\n+\n+        if auth_token is not None:\n+            self.headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n+\n+    def check_status(self):\n+        self.get(f\"http://localhost:{self.port}/v1/status\").raise_for_status()\n+\n+    def is_testing_enabled_or_skip(self):\n+        if not self.is_testing_enabled:\n+            pytest.skip(\"safekeeper was built without 'testing' feature\")\n+\n+    def configure_failpoints(self, config_strings: Union[Tuple[str, str], List[Tuple[str, str]]]):\n+        self.is_testing_enabled_or_skip()\n+\n+        if isinstance(config_strings, tuple):\n+            pairs = [config_strings]\n+        else:\n+            pairs = config_strings\n+\n+        log.info(f\"Requesting config failpoints: {repr(pairs)}\")\n+\n+        res = self.put(\n+            f\"http://localhost:{self.port}/v1/failpoints\",\n+            json=[{\"name\": name, \"actions\": actions} for name, actions in pairs],\n+        )\n+        log.info(f\"Got failpoints request response code {res.status_code}\")\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert res_json is None\n+        return res_json\n+\n+    def debug_dump(self, params: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n+        params = params or {}\n+        res = self.get(f\"http://localhost:{self.port}/v1/debug_dump\", params=params)\n+        res.raise_for_status()\n+        res_json = json.loads(res.text)\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def patch_control_file(\n+        self,\n+        tenant_id: TenantId,\n+        timeline_id: TimelineId,\n+        patch: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n+        res = self.patch(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/control_file\",\n+            json={\n+                \"updates\": patch,\n+                \"apply_fields\": list(patch.keys()),\n+            },\n+        )\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def pull_timeline(self, body: Dict[str, Any]) -> Dict[str, Any]:\n+        res = self.post(f\"http://localhost:{self.port}/v1/pull_timeline\", json=body)\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def copy_timeline(self, tenant_id: TenantId, timeline_id: TimelineId, body: Dict[str, Any]):\n+        res = self.post(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/copy\",\n+            json=body,\n+        )\n+        res.raise_for_status()\n+\n+    def timeline_digest(\n+        self, tenant_id: TenantId, timeline_id: TimelineId, from_lsn: Lsn, until_lsn: Lsn\n+    ) -> Dict[str, Any]:\n+        res = self.get(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/digest\",\n+            params={\n+                \"from_lsn\": str(from_lsn),\n+                \"until_lsn\": str(until_lsn),\n+            },\n+        )\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def timeline_create(\n+        self,\n+        tenant_id: TenantId,\n+        timeline_id: TimelineId,\n+        pg_version: int,  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n+        commit_lsn: Lsn,\n+    ):\n+        body = {\n+            \"tenant_id\": str(tenant_id),\n+            \"timeline_id\": str(timeline_id),\n+            \"pg_version\": pg_version,\n+            \"commit_lsn\": str(commit_lsn),\n+        }\n+        res = self.post(f\"http://localhost:{self.port}/v1/tenant/timeline\", json=body)\n+        res.raise_for_status()\n+\n+    def timeline_status(\n+        self, tenant_id: TenantId, timeline_id: TimelineId\n+    ) -> SafekeeperTimelineStatus:\n+        res = self.get(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\")\n+        res.raise_for_status()\n+        resj = res.json()\n+        walreceivers = [Walreceiver(wr[\"conn_id\"], wr[\"status\"]) for wr in resj[\"walreceivers\"]]\n+        return SafekeeperTimelineStatus(\n+            acceptor_epoch=resj[\"acceptor_state\"][\"epoch\"],\n+            pg_version=resj[\"pg_info\"][\"pg_version\"],\n+            flush_lsn=Lsn(resj[\"flush_lsn\"]),\n+            commit_lsn=Lsn(resj[\"commit_lsn\"]),\n+            timeline_start_lsn=Lsn(resj[\"timeline_start_lsn\"]),\n+            backup_lsn=Lsn(resj[\"backup_lsn\"]),\n+            peer_horizon_lsn=Lsn(resj[\"peer_horizon_lsn\"]),\n+            remote_consistent_lsn=Lsn(resj[\"remote_consistent_lsn\"]),\n+            walreceivers=walreceivers,\n+        )\n+\n+    def get_commit_lsn(self, tenant_id: TenantId, timeline_id: TimelineId) -> Lsn:\n+        return self.timeline_status(tenant_id, timeline_id).commit_lsn\n+\n+    def record_safekeeper_info(self, tenant_id: TenantId, timeline_id: TimelineId, body):\n+        res = self.post(\n+            f\"http://localhost:{self.port}/v1/record_safekeeper_info/{tenant_id}/{timeline_id}\",\n+            json=body,\n+        )\n+        res.raise_for_status()\n+\n+    # only_local doesn't remove segments in the remote storage.\n+    def timeline_delete(\n+        self, tenant_id: TenantId, timeline_id: TimelineId, only_local: bool = False\n+    ) -> Dict[Any, Any]:\n+        res = self.delete(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\",\n+            params={\n+                \"only_local\": str(only_local).lower(),\n+            },\n+        )\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def tenant_delete_force(self, tenant_id: TenantId) -> Dict[Any, Any]:\n+        res = self.delete(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}\")\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def get_metrics_str(self) -> str:\n+        request_result = self.get(f\"http://localhost:{self.port}/metrics\")\n+        request_result.raise_for_status()\n+        return request_result.text\n+\n+    def get_metrics(self) -> SafekeeperMetrics:\n+        all_metrics_text = self.get_metrics_str()\n+\n+        metrics = SafekeeperMetrics()\n+        for match in re.finditer(\n+            r'^safekeeper_flush_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n+            all_metrics_text,\n+            re.MULTILINE,\n+        ):\n+            metrics.flush_lsn_inexact[(TenantId(match.group(1)), TimelineId(match.group(2)))] = int(\n+                match.group(3)\n+            )\n+        for match in re.finditer(\n+            r'^safekeeper_commit_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n+            all_metrics_text,\n+            re.MULTILINE,\n+        ):\n+            metrics.commit_lsn_inexact[\n+                (TenantId(match.group(1)), TimelineId(match.group(2)))\n+            ] = int(match.group(3))\n+        return metrics"
    },
    {
      "filename": "test_runner/fixtures/safekeeper/utils.py",
      "status": "added",
      "patch": "@@ -0,0 +1,11 @@\n+from fixtures.log_helper import log\n+from fixtures.safekeeper.http import SafekeeperHttpClient\n+from fixtures.types import TenantId, TimelineId\n+\n+\n+def are_walreceivers_absent(\n+    sk_http_cli: SafekeeperHttpClient, tenant_id: TenantId, timeline_id: TimelineId\n+):\n+    status = sk_http_cli.timeline_status(tenant_id, timeline_id)\n+    log.info(f\"waiting for walreceivers to be gone, currently {status.walreceivers}\")\n+    return len(status.walreceivers) == 0"
    },
    {
      "filename": "test_runner/regress/test_layer_eviction.py",
      "status": "modified",
      "patch": "@@ -4,12 +4,11 @@\n from fixtures.log_helper import log\n from fixtures.neon_fixtures import (\n     NeonEnvBuilder,\n+    flush_ep_to_pageserver,\n     wait_for_last_flush_lsn,\n )\n-from fixtures.pageserver.utils import wait_for_last_record_lsn, wait_for_upload\n+from fixtures.pageserver.utils import wait_for_upload\n from fixtures.remote_storage import RemoteStorageKind\n-from fixtures.types import Lsn\n-from fixtures.utils import query_scalar\n \n \n # Crates a few layers, ensures that we can evict them (removing locally but keeping track of them anyway)\n@@ -46,14 +45,15 @@ def test_basic_eviction(\n             FROM generate_series(1, 5000000) g\n             \"\"\"\n         )\n-        current_lsn = Lsn(query_scalar(cur, \"SELECT pg_current_wal_flush_lsn()\"))\n \n-    wait_for_last_record_lsn(client, tenant_id, timeline_id, current_lsn)\n+    # stops the endpoint\n+    current_lsn = flush_ep_to_pageserver(env, endpoint, tenant_id, timeline_id)\n+\n     client.timeline_checkpoint(tenant_id, timeline_id)\n     wait_for_upload(client, tenant_id, timeline_id, current_lsn)\n \n-    # disable compute & sks to avoid on-demand downloads by walreceiver / getpage\n-    endpoint.stop()\n+    # stop sks to avoid on-demand downloads by walreceiver / getpage; endpoint\n+    # has already been stopped by flush_ep_to_pageserver\n     for sk in env.safekeepers:\n         sk.stop()\n "
    },
    {
      "filename": "test_runner/regress/test_layers_from_future.py",
      "status": "modified",
      "patch": "@@ -1,7 +1,7 @@\n import time\n \n from fixtures.log_helper import log\n-from fixtures.neon_fixtures import NeonEnvBuilder\n+from fixtures.neon_fixtures import NeonEnvBuilder, flush_ep_to_pageserver\n from fixtures.pageserver.types import (\n     DeltaLayerFileName,\n     ImageLayerFileName,\n@@ -115,8 +115,7 @@ def get_future_layers():\n                     )\n                     == 0\n                 )\n-\n-    endpoint.stop()\n+    last_record_lsn = flush_ep_to_pageserver(env, endpoint, tenant_id, timeline_id)\n \n     wait_for_upload_queue_empty(ps_http, tenant_id, timeline_id)\n "
    },
    {
      "filename": "test_runner/regress/test_ondemand_download.py",
      "status": "modified",
      "patch": "@@ -8,6 +8,7 @@\n from fixtures.log_helper import log\n from fixtures.neon_fixtures import (\n     NeonEnvBuilder,\n+    flush_ep_to_pageserver,\n     last_flush_lsn_upload,\n     wait_for_last_flush_lsn,\n )\n@@ -517,7 +518,7 @@ def downloaded_bytes_and_count(pageserver_http: PageserverHttpClient) -> Tuple[i\n \n         with endpoint.cursor() as cur:\n             cur.execute(\"update a set id = -id\")\n-        wait_for_last_flush_lsn(env, endpoint, tenant_id, timeline_id)\n+        flush_ep_to_pageserver(env, endpoint, tenant_id, timeline_id)\n         pageserver_http.timeline_checkpoint(tenant_id, timeline_id)\n \n     layers = pageserver_http.layer_map_info(tenant_id, timeline_id)"
    },
    {
      "filename": "test_runner/regress/test_wal_acceptor.py",
      "status": "modified",
      "patch": "@@ -28,7 +28,6 @@\n     PgBin,\n     PgProtocol,\n     Safekeeper,\n-    SafekeeperHttpClient,\n     SafekeeperPort,\n     last_flush_lsn_upload,\n )\n@@ -46,6 +45,8 @@\n     default_remote_storage,\n     s3_storage,\n )\n+from fixtures.safekeeper.http import SafekeeperHttpClient\n+from fixtures.safekeeper.utils import are_walreceivers_absent\n from fixtures.types import Lsn, TenantId, TimelineId\n from fixtures.utils import get_dir_size, query_scalar, start_in_background\n \n@@ -1097,12 +1098,6 @@ def is_flush_lsn_aligned(sk_http_clis, tenant_id, timeline_id):\n     return all([flush_lsns[0] == flsn for flsn in flush_lsns])\n \n \n-def are_walreceivers_absent(sk_http_cli, tenant_id: TenantId, timeline_id: TimelineId):\n-    status = sk_http_cli.timeline_status(tenant_id, timeline_id)\n-    log.info(f\"waiting for walreceivers to be gone, currently {status.walreceivers}\")\n-    return len(status.walreceivers) == 0\n-\n-\n # Assert by xxd that WAL on given safekeepers is identical. No compute must be\n # running for this to be reliable.\n def cmp_sk_wal(sks: List[Safekeeper], tenant_id: TenantId, timeline_id: TimelineId):\n@@ -1347,6 +1342,36 @@ def test_peer_recovery(neon_env_builder: NeonEnvBuilder):\n     endpoint.safe_psql(\"insert into t select generate_series(1,100), 'payload'\")\n \n \n+# Test that when compute is terminated in fast (or smart) mode, walproposer is\n+# allowed to run and self terminate after shutdown checkpoint is written, so it\n+# commits it to safekeepers before exiting. This not required for correctness,\n+# but needed for tests using check_restored_datadir_content.\n+def test_wp_graceful_shutdown(neon_env_builder: NeonEnvBuilder, pg_bin: PgBin):\n+    neon_env_builder.num_safekeepers = 1\n+    env = neon_env_builder.init_start()\n+\n+    tenant_id = env.initial_tenant\n+    timeline_id = env.neon_cli.create_branch(\"test_wp_graceful_shutdown\")\n+    ep = env.endpoints.create_start(\"test_wp_graceful_shutdown\")\n+    ep.safe_psql(\"create table t(key int, value text)\")\n+    ep.stop()\n+\n+    # figure out checkpoint lsn\n+    ckpt_lsn = pg_bin.get_pg_controldata_checkpoint_lsn(ep.pg_data_dir_path())\n+\n+    sk_http_cli = env.safekeepers[0].http_client()\n+    commit_lsn = sk_http_cli.timeline_status(tenant_id, timeline_id).commit_lsn\n+    # Note: this is in memory value. Graceful shutdown of walproposer currently\n+    # doesn't guarantee persisted value, which is ok as we need it only for\n+    # tests. Persisting it without risking too many cf flushes needs a wp -> sk\n+    # protocol change. (though in reality shutdown sync-safekeepers does flush\n+    # of cf, so most of the time persisted value wouldn't lag)\n+    log.info(f\"sk commit_lsn {commit_lsn}\")\n+    # note that ckpt_lsn is the *beginning* of checkpoint record, so commit_lsn\n+    # must be actually higher\n+    assert commit_lsn > ckpt_lsn, \"safekeeper must have checkpoint record\"\n+\n+\n class SafekeeperEnv:\n     def __init__(\n         self,"
    },
    {
      "filename": "vendor/postgres-v14",
      "status": "modified",
      "patch": "@@ -1 +1 @@\n-Subproject commit f49a962b9b3715d6f47017d1dcf905c36f93ae5e\n+Subproject commit b980d6f090c676e55fb2c830fb2434f532f635c0"
    },
    {
      "filename": "vendor/postgres-v15",
      "status": "modified",
      "patch": "@@ -1 +1 @@\n-Subproject commit e8b9a28006a550d7ca7cbb9bd0238eb9cd57bbd8\n+Subproject commit 56f32c0e7330d17aaeee8bf211a73995180bd133"
    },
    {
      "filename": "vendor/postgres-v16",
      "status": "modified",
      "patch": "@@ -1 +1 @@\n-Subproject commit 072697b2250da3251af75887b577104554b9cd44\n+Subproject commit 90078947229aa7f9ac5f7ed4527b2c7386d5332b"
    },
    {
      "filename": "vendor/revisions.json",
      "status": "modified",
      "patch": "@@ -1,6 +1,5 @@\n {\n-    \"postgres-v16\": \"072697b2250da3251af75887b577104554b9cd44\",\n-    \"postgres-v15\": \"e8b9a28006a550d7ca7cbb9bd0238eb9cd57bbd8\",\n-    \"postgres-v14\": \"f49a962b9b3715d6f47017d1dcf905c36f93ae5e\"\n+  \"postgres-v16\": \"90078947229aa7f9ac5f7ed4527b2c7386d5332b\",\n+  \"postgres-v15\": \"56f32c0e7330d17aaeee8bf211a73995180bd133\",\n+  \"postgres-v14\": \"b980d6f090c676e55fb2c830fb2434f532f635c0\"\n }\n-"
    }
  ],
  "fix_category": "Wait/Join",
  "root_cause_category": "Async wait",
  "root_cause_subcategory": "Wait/Join"
}