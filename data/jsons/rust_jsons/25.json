{
  "id": 25,
  "repo": "iroha",
  "issue_url": "https://github.com/hyperledger-iroha/iroha/issues/4516",
  "pr_url": "https://github.com/hyperledger-iroha/iroha/pull/4697",
  "issue_description": "I encounter this time and time again: you open a PR, wait for checks for 10/20/30 minutes, and then you see that same workflows fail on the same tests. During that time I get distracted on something else and forget to re-start checks again just in time. Sometimes it takes 5-6 restarts to get them work. This way, the moment of all green lights might delay for days. And it delays development in my case.\r\n\r\nFrom my observation, the following workflows are flaky:\r\n\r\n- `I2::Dev::Tests` > `with_coverage`, `integration`, `unstable`\r\n\r\nAnd these are particular flaky tests:\r\n\r\n- `integration::extra_functional::offline_peers::genesis_block_is_committed_with_some_offline_peers`\r\n- `integration::extra_functional::unstable_network::soft_fork`\r\n- And _maybe_ some others, haven't collected much data\r\n\r\nAre these tests worth it?\r\n\r\nMy another concern is that I don't see the rationale behind having so many workflows:\r\n\r\n1. `I2::Dev::Static`\r\n    1. smart contracts\r\n    2. workspace\r\n2. `I2::Tests::UI`\r\n    1. test with all features\r\n    2. test with no default features\r\n3. `I2::Dev::Tests`\r\n    1. consistency\r\n    2. with_coverage\r\n    3. integration\r\n    4. unstable\r\n    5. client-cli-tests\r\n\r\n(there are some others too)\r\n\r\nThese workflows all run Cargo and compile more or less the same stuff. Yes, there are variations in features presets, but Cargo handles it for us. It can granularly reuse compilation artifacts depending on the context (apart from cases with different RUSTC flags, I suppose).\r\n\r\nSo, I guess that it is worth trying to combine all these workflows into a single one, and build it in a way so that it can report as many useful information as possible in a single run. I wonder how much more/less it would be efficient.\r\n\r\nAnother useful implication of this would be a shorter feedback on some early errors. For example, a certain change in PR introduces something and Iroha cannot even compile. Currently, all 8+ workflows will run and fail on the same error. In the case of a unified CI, there will be less work repetition.\r\n\r\n### Proposals\r\n\r\n- Prioritise zero-tolerance to flaky tests from development side\r\n- If flaky tests couldn't be easily fixed, possibly move them away from PR checks to after-merge checks.\r\n- Create a single unified workflow, and research the performance impact of it.\r\n- Explore ways to use a sane scripting language for CI, not Shell. That's for a separate issue, maybe.",
  "files_changed": [
    {
      "filename": "config/src/parameters/actual.rs",
      "status": "modified",
      "patch": "@@ -207,6 +207,11 @@ impl ChainWide {\n     pub fn pipeline_time(&self) -> Duration {\n         self.block_time + self.commit_time\n     }\n+\n+    /// Estimates as `block_time + commit_time / 2`\n+    pub fn consensus_estimation(&self) -> Duration {\n+        self.block_time + (self.commit_time / 2)\n+    }\n }\n \n impl Default for ChainWide {"
    },
    {
      "filename": "core/src/block.rs",
      "status": "modified",
      "patch": "@@ -6,7 +6,6 @@\n //! [`Block`]s are organised into a linear sequence over time (also known as the block chain).\n use std::error::Error as _;\n \n-use iroha_config::parameters::defaults::chain_wide::CONSENSUS_ESTIMATION as DEFAULT_CONSENSUS_ESTIMATION;\n use iroha_crypto::{HashOf, KeyPair, MerkleTree, SignatureOf, SignaturesOf};\n use iroha_data_model::{\n     block::*,\n@@ -101,7 +100,7 @@ pub enum SignatureVerificationError {\n pub struct BlockBuilder<B>(B);\n \n mod pending {\n-    use std::time::SystemTime;\n+    use std::time::{Duration, SystemTime};\n \n     use iroha_data_model::transaction::CommittedTransaction;\n \n@@ -149,6 +148,7 @@ mod pending {\n             prev_block_hash: Option<HashOf<SignedBlock>>,\n             view_change_index: usize,\n             transactions: &[CommittedTransaction],\n+            consensus_estimation: Duration,\n         ) -> BlockHeader {\n             BlockHeader {\n                 height: prev_height\n@@ -171,7 +171,7 @@ mod pending {\n                 view_change_index: view_change_index\n                     .try_into()\n                     .expect(\"INTERNAL BUG: Number of view changes exceeds u32::MAX\"),\n-                consensus_estimation_ms: DEFAULT_CONSENSUS_ESTIMATION\n+                consensus_estimation_ms: consensus_estimation\n                     .as_millis()\n                     .try_into()\n                     .expect(\"INTERNAL BUG: Time should fit into u64\"),\n@@ -222,6 +222,7 @@ mod pending {\n                     state.latest_block_hash(),\n                     view_change_index,\n                     &transactions,\n+                    state.config.consensus_estimation(),\n                 ),\n                 transactions,\n                 commit_topology: self.0.commit_topology.ordered_peers,\n@@ -500,10 +501,7 @@ mod valid {\n                     transactions_hash: None,\n                     timestamp_ms: 0,\n                     view_change_index: 0,\n-                    consensus_estimation_ms: DEFAULT_CONSENSUS_ESTIMATION\n-                        .as_millis()\n-                        .try_into()\n-                        .expect(\"Time should fit into u64\"),\n+                    consensus_estimation_ms: 4_000,\n                 },\n                 transactions: Vec::new(),\n                 commit_topology: UniqueVec::new(),"
    },
    {
      "filename": "core/src/smartcontracts/isi/world.rs",
      "status": "modified",
      "patch": "@@ -317,14 +317,14 @@ pub mod isi {\n             let parameter = self.parameter;\n             let parameter_id = parameter.id.clone();\n \n-            let world = &mut state_transaction.world;\n-            if !world.parameters.swap_remove(&parameter) {\n+            if !state_transaction.world.parameters.swap_remove(&parameter) {\n                 return Err(FindError::Parameter(parameter_id).into());\n             }\n-\n-            world.parameters.insert(parameter);\n-\n-            world.emit_events(Some(ConfigurationEvent::Changed(parameter_id)));\n+            state_transaction.world.parameters.insert(parameter.clone());\n+            state_transaction\n+                .world\n+                .emit_events(Some(ConfigurationEvent::Changed(parameter_id)));\n+            state_transaction.try_apply_core_parameter(parameter);\n \n             Ok(())\n         }\n@@ -340,16 +340,17 @@ pub mod isi {\n             let parameter = self.parameter;\n             let parameter_id = parameter.id.clone();\n \n-            let world = &mut state_transaction.world;\n-            if !world.parameters.insert(parameter) {\n+            if !state_transaction.world.parameters.insert(parameter.clone()) {\n                 return Err(RepetitionError {\n                     instruction_type: InstructionType::NewParameter,\n                     id: IdBox::ParameterId(parameter_id),\n                 }\n                 .into());\n             }\n-\n-            world.emit_events(Some(ConfigurationEvent::Created(parameter_id)));\n+            state_transaction\n+                .world\n+                .emit_events(Some(ConfigurationEvent::Created(parameter_id)));\n+            state_transaction.try_apply_core_parameter(parameter);\n \n             Ok(())\n         }"
    },
    {
      "filename": "core/src/state.rs",
      "status": "modified",
      "patch": "@@ -1,6 +1,10 @@\n //! This module provides the [`State`] \u2014 an in-memory representation of the current blockchain state.\n use std::{\n-    borrow::Borrow, collections::BTreeSet, marker::PhantomData, num::NonZeroUsize, sync::Arc,\n+    borrow::Borrow,\n+    collections::BTreeSet,\n+    marker::PhantomData,\n+    num::{NonZeroU32, NonZeroUsize},\n+    sync::Arc,\n     time::Duration,\n };\n \n@@ -1252,7 +1256,6 @@ impl<'state> StateBlock<'state> {\n \n         self.block_hashes.push(block_hash);\n \n-        self.apply_parameters();\n         self.world.events_buffer.push(\n             BlockEvent {\n                 header: block.as_ref().header().clone(),\n@@ -1266,20 +1269,18 @@ impl<'state> StateBlock<'state> {\n \n     /// Create time event using previous and current blocks\n     fn create_time_event(&self, block: &CommittedBlock) -> TimeEvent {\n-        use iroha_config::parameters::defaults::chain_wide::CONSENSUS_ESTIMATION as DEFAULT_CONSENSUS_ESTIMATION;\n-\n         let prev_interval = self.latest_block_ref().map(|latest_block| {\n             let header = &latest_block.as_ref().header();\n \n             TimeInterval {\n                 since: header.timestamp(),\n-                length: DEFAULT_CONSENSUS_ESTIMATION,\n+                length: header.consensus_estimation(),\n             }\n         });\n \n         let interval = TimeInterval {\n             since: block.as_ref().header().timestamp(),\n-            length: DEFAULT_CONSENSUS_ESTIMATION,\n+            length: block.as_ref().header().consensus_estimation(),\n         };\n \n         TimeEvent {\n@@ -1336,32 +1337,6 @@ impl<'state> StateBlock<'state> {\n \n         errors.is_empty().then_some(()).ok_or(errors)\n     }\n-\n-    fn apply_parameters(&mut self) {\n-        use iroha_data_model::parameter::default::*;\n-\n-        macro_rules! update_params {\n-            ($($param:expr => $config:expr),+ $(,)?) => {\n-                $(if let Some(param) = self.world.query_param($param) {\n-                    $config = param;\n-                })+\n-            };\n-        }\n-\n-        update_params! {\n-            WSV_DOMAIN_METADATA_LIMITS => self.config.domain_metadata_limits,\n-            WSV_ASSET_DEFINITION_METADATA_LIMITS => self.config.asset_definition_metadata_limits,\n-            WSV_ACCOUNT_METADATA_LIMITS => self.config.account_metadata_limits,\n-            WSV_ASSET_METADATA_LIMITS => self.config.asset_metadata_limits,\n-            WSV_TRIGGER_METADATA_LIMITS => self.config.trigger_metadata_limits,\n-            WSV_IDENT_LENGTH_LIMITS => self.config.ident_length_limits,\n-            EXECUTOR_FUEL_LIMIT => self.config.executor_runtime.fuel_limit,\n-            EXECUTOR_MAX_MEMORY => self.config.executor_runtime.max_memory_bytes,\n-            WASM_FUEL_LIMIT => self.config.wasm_runtime.fuel_limit,\n-            WASM_MAX_MEMORY => self.config.wasm_runtime.max_memory_bytes,\n-            TRANSACTION_LIMITS => self.config.transaction_limits,\n-        }\n-    }\n }\n \n impl StateTransaction<'_, '_> {\n@@ -1373,6 +1348,96 @@ impl StateTransaction<'_, '_> {\n         self.world.apply();\n     }\n \n+    /// If given [`Parameter`] represents some of the core chain-wide\n+    /// parameters ([`Config`]), apply it\n+    pub fn try_apply_core_parameter(&mut self, parameter: Parameter) {\n+        use iroha_data_model::parameter::default::*;\n+\n+        struct Reader(Option<Parameter>);\n+\n+        impl Reader {\n+            fn try_and_then<T: TryFrom<ParameterValueBox>>(\n+                self,\n+                id: &str,\n+                fun: impl FnOnce(T),\n+            ) -> Self {\n+                if let Some(param) = self.0 {\n+                    if param.id().name().as_ref() == id {\n+                        if let Some(value) = param.val.try_into().ok() {\n+                            fun(value);\n+                        }\n+                        Self(None)\n+                    } else {\n+                        Self(Some(param))\n+                    }\n+                } else {\n+                    Self(None)\n+                }\n+            }\n+\n+            fn try_and_write<T: TryFrom<ParameterValueBox>>(\n+                self,\n+                id: &str,\n+                destination: &mut T,\n+            ) -> Self {\n+                self.try_and_then(id, |value| {\n+                    *destination = value;\n+                })\n+            }\n+        }\n+\n+        Reader(Some(parameter))\n+            .try_and_then(MAX_TRANSACTIONS_IN_BLOCK, |value| {\n+                if let Some(checked) = NonZeroU32::new(value) {\n+                    self.config.max_transactions_in_block = checked;\n+                }\n+            })\n+            .try_and_then(BLOCK_TIME, |value| {\n+                self.config.block_time = Duration::from_millis(value);\n+            })\n+            .try_and_then(COMMIT_TIME_LIMIT, |value| {\n+                self.config.commit_time = Duration::from_millis(value);\n+            })\n+            .try_and_write(\n+                WSV_DOMAIN_METADATA_LIMITS,\n+                &mut self.config.domain_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_ASSET_DEFINITION_METADATA_LIMITS,\n+                &mut self.config.asset_definition_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_ACCOUNT_METADATA_LIMITS,\n+                &mut self.config.account_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_ASSET_METADATA_LIMITS,\n+                &mut self.config.asset_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_TRIGGER_METADATA_LIMITS,\n+                &mut self.config.trigger_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_IDENT_LENGTH_LIMITS,\n+                &mut self.config.ident_length_limits,\n+            )\n+            .try_and_write(\n+                EXECUTOR_FUEL_LIMIT,\n+                &mut self.config.executor_runtime.fuel_limit,\n+            )\n+            .try_and_write(\n+                EXECUTOR_MAX_MEMORY,\n+                &mut self.config.executor_runtime.max_memory_bytes,\n+            )\n+            .try_and_write(WASM_FUEL_LIMIT, &mut self.config.wasm_runtime.fuel_limit)\n+            .try_and_write(\n+                WASM_MAX_MEMORY,\n+                &mut self.config.wasm_runtime.max_memory_bytes,\n+            )\n+            .try_and_write(TRANSACTION_LIMITS, &mut self.config.transaction_limits);\n+    }\n+\n     fn process_executable(&mut self, executable: &Executable, authority: AccountId) -> Result<()> {\n         match executable {\n             Executable::Instructions(instructions) => {"
    },
    {
      "filename": "core/src/sumeragi/main_loop.rs",
      "status": "modified",
      "patch": "@@ -359,20 +359,9 @@ impl Sumeragi {\n     }\n \n     fn update_params(&mut self, state_block: &StateBlock<'_>) {\n-        use iroha_data_model::parameter::default::*;\n-\n-        if let Some(block_time) = state_block.world.query_param(BLOCK_TIME) {\n-            self.block_time = Duration::from_millis(block_time);\n-        }\n-        if let Some(commit_time) = state_block.world.query_param(COMMIT_TIME_LIMIT) {\n-            self.commit_time = Duration::from_millis(commit_time);\n-        }\n-        if let Some(max_txs_in_block) = state_block\n-            .world\n-            .query_param::<u32, _>(MAX_TRANSACTIONS_IN_BLOCK)\n-        {\n-            self.max_txs_in_block = max_txs_in_block as usize;\n-        }\n+        self.block_time = state_block.config.block_time;\n+        self.commit_time = state_block.config.commit_time;\n+        self.max_txs_in_block = state_block.config.max_transactions_in_block.get() as usize;\n     }\n \n     fn cache_transaction(&mut self, state_block: &StateBlock<'_>) {"
    },
    {
      "filename": "data_model/src/block.rs",
      "status": "modified",
      "patch": "@@ -127,9 +127,14 @@ impl BlockHeader {\n     }\n \n     /// Creation timestamp\n-    pub fn timestamp(&self) -> Duration {\n+    pub const fn timestamp(&self) -> Duration {\n         Duration::from_millis(self.timestamp_ms)\n     }\n+\n+    /// Consensus estimation\n+    pub const fn consensus_estimation(&self) -> Duration {\n+        Duration::from_millis(self.consensus_estimation_ms)\n+    }\n }\n \n impl SignedBlockV1 {"
    }
  ],
  "fix_category": "Tweak durations",
  "root_cause_category": "Async wait",
  "root_cause_subcategory": "Configuration"
}