{
  "id": 78,
  "repo": "a10",
  "issue_url": "https://github.com/Thomasdezeeuw/a10/issues/128",
  "pr_url": "https://github.com/Thomasdezeeuw/a10/pull/130",
  "issue_description": "After a1e25956b6e04e9c678293318967db3f2e4b905a some of the cancellation tests have become a bit flaky. It seems io_uring doesn't behave consistent when `IOSQE_ASYNC` is and isn't set, but I'm a 100% sure yet.",
  "files_changed": [
    {
      "filename": "src/cancel.rs",
      "status": "modified",
      "patch": "@@ -77,7 +77,7 @@ pub trait Cancel {\n     /// If the operation was found and canceled this returns `Ok(())`.\n     ///\n     /// If this is called on an [`AsyncIterator`] it will cause them to return\n-    /// `None` (eventuaully, it may still return pending items).\n+    /// `None` (eventually, it may still return pending items).\n     ///\n     /// [`AsyncIterator`]: std::async_iter::AsyncIterator\n     fn cancel(&mut self) -> CancelOp;"
    },
    {
      "filename": "src/io/read_buf.rs",
      "status": "modified",
      "patch": "@@ -467,6 +467,7 @@ impl ReadBuf {\n     }\n \n     /// Returns the remaining spare capacity of the buffer.\n+    #[allow(clippy::needless_pass_by_ref_mut)] // See https://github.com/rust-lang/rust-clippy/issues/12905.\n     pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {\n         if let Some(ptr) = self.owned {\n             let unused_len = self.shared.buf_size as usize - ptr.len();"
    },
    {
      "filename": "src/lib.rs",
      "status": "modified",
      "patch": "@@ -231,6 +231,7 @@ impl Ring {\n     ///\n     /// This only required when starting the ring in disabled mode, see\n     /// [`Config::disable`].\n+    #[allow(clippy::needless_pass_by_ref_mut)]\n     pub fn enable(&mut self) -> io::Result<()> {\n         self.sq\n             .register(libc::IORING_REGISTER_ENABLE_RINGS, ptr::null(), 0)\n@@ -348,6 +349,7 @@ impl Ring {\n     }\n \n     /// Wake [`SharedSubmissionQueue::blocked_futures`].\n+    #[allow(clippy::needless_pass_by_ref_mut)]\n     fn wake_blocked_futures(&mut self) {\n         // This not particullary efficient, but with a large enough number of\n         // entries, `IORING_SETUP_SQPOLL` and suffcient calls to [`Ring::poll`]"
    },
    {
      "filename": "tests/async_fd/direct.rs",
      "status": "modified",
      "patch": "@@ -27,7 +27,7 @@ fn to_file_descriptor() {\n     let waker = Waker::new();\n \n     let open_file = OpenOptions::new().open(sq, LOREM_IPSUM_5.path.into());\n-    let direct_fd = dbg!(waker.block_on(open_file)).unwrap();\n+    let direct_fd = waker.block_on(open_file).unwrap();\n     let regular_fd = waker.block_on(direct_fd.to_file_descriptor()).unwrap();\n \n     check_fs_fd(waker, regular_fd, direct_fd);"
    },
    {
      "filename": "tests/async_fd/io.rs",
      "status": "modified",
      "patch": "@@ -6,7 +6,6 @@ use std::io;\n use std::ops::Bound;\n use std::os::fd::{AsFd, AsRawFd, RawFd};\n use std::panic::{self, AssertUnwindSafe};\n-use std::pin::Pin;\n \n use a10::fd::{AsyncFd, File};\n use a10::fs::{Open, OpenOptions};\n@@ -17,7 +16,7 @@ use a10::io::{\n use a10::{Extract, Ring, SubmissionQueue};\n \n use crate::util::{\n-    bind_and_listen_ipv4, block_on, defer, expect_io_errno, init, is_send, is_sync, poll_nop,\n+    bind_and_listen_ipv4, block_on, cancel_all, defer, expect_io_errno, init, is_send, is_sync,\n     remove_test_file, require_kernel, start_op, tcp_ipv4_socket, test_queue, Waker, LOREM_IPSUM_5,\n     LOREM_IPSUM_50,\n };\n@@ -732,15 +731,9 @@ fn cancel_all_accept() {\n     let listener = waker.block_on(tcp_ipv4_socket(sq));\n     bind_and_listen_ipv4(&listener);\n \n-    let mut accept = listener.accept::<libc::sockaddr_in>();\n-    // Poll the future to schedule the operation, can't use `start_op` as the\n-    // address doesn't implement `fmt::Debug`.\n-    assert!(poll_nop(Pin::new(&mut accept)).is_pending());\n+    let mut accept = listener.accept::<a10::net::NoAddress>();\n \n-    let n = waker\n-        .block_on(listener.cancel_all())\n-        .expect(\"failed to cancel all calls\");\n-    assert!(n == 1);\n+    cancel_all(&waker, &listener, || start_op(&mut accept), 1);\n \n     expect_io_errno(waker.block_on(accept), libc::ECANCELED);\n }\n@@ -755,19 +748,13 @@ fn cancel_all_twice_accept() {\n     let listener = waker.block_on(tcp_ipv4_socket(sq));\n     bind_and_listen_ipv4(&listener);\n \n-    let mut accept = listener.accept::<libc::sockaddr_in>();\n-    // Poll the future to schedule the operation, can't use `start_op` as the\n-    // address doesn't implement `fmt::Debug`.\n-    assert!(poll_nop(Pin::new(&mut accept)).is_pending());\n+    let mut accept = listener.accept::<a10::net::NoAddress>();\n \n+    cancel_all(&waker, &listener, || start_op(&mut accept), 1);\n     let n = waker\n         .block_on(listener.cancel_all())\n-        .expect(\"failed to cancel all calls\");\n-    assert!(n == 1);\n-    let n2 = waker\n-        .block_on(listener.cancel_all())\n-        .expect(\"failed to cancel all calls\");\n-    assert!(n2 == 0);\n+        .expect(\"failed to cancel all operations\");\n+    assert_eq!(n, 0);\n \n     expect_io_errno(waker.block_on(accept), libc::ECANCELED);\n }\n@@ -783,7 +770,7 @@ fn cancel_all_no_operation_in_progress() {\n \n     let n = waker\n         .block_on(socket.cancel_all())\n-        .expect(\"failed to cancel\");\n+        .expect(\"failed to cancel all operations\");\n     assert_eq!(n, 0);\n }\n "
    },
    {
      "filename": "tests/async_fd/net.rs",
      "status": "modified",
      "patch": "@@ -20,9 +20,9 @@ use a10::{Extract, Ring};\n \n use crate::async_fd::io::{BadBuf, BadBufSlice, BadReadBuf, BadReadBufSlice};\n use crate::util::{\n-    bind_and_listen_ipv4, bind_ipv4, block_on, expect_io_errno, expect_io_error_kind, init,\n-    is_send, is_sync, new_socket, next, poll_nop, require_kernel, syscall, tcp_ipv4_socket,\n-    test_queue, udp_ipv4_socket, Waker,\n+    bind_and_listen_ipv4, bind_ipv4, block_on, cancel, expect_io_errno, expect_io_error_kind, init,\n+    is_send, is_sync, new_socket, next, require_kernel, start_mulitshot_op, start_op, syscall,\n+    tcp_ipv4_socket, test_queue, udp_ipv4_socket, Waker,\n };\n \n const DATA1: &[u8] = b\"Hello, World!\";\n@@ -121,14 +121,10 @@ fn cancel_accept() {\n     let listener = waker.block_on(tcp_ipv4_socket(sq));\n     bind_and_listen_ipv4(&listener);\n \n-    let accept = listener.accept::<(libc::sockaddr_storage, libc::socklen_t)>();\n-    let mut accept = std::pin::pin!(accept);\n-    // Poll the future to schedule the operation, can't use `start_op` as the\n-    // address doesn't implement `fmt::Debug`.\n-    assert!(poll_nop(accept.as_mut()).is_pending());\n+    let mut accept = listener.accept::<NoAddress>();\n \n     // Then cancel the accept multishot call.\n-    waker.block_on(accept.as_mut().cancel()).unwrap();\n+    cancel(&waker, &mut accept, start_op);\n \n     expect_io_errno(waker.block_on(accept), libc::ECANCELED);\n }\n@@ -254,9 +250,7 @@ fn cancel_multishot_accept() {\n     let c_addr1 = peer_addr(client1.as_fd()).expect(\"failed to get address\");\n \n     // Then cancel the accept multishot call.\n-    waker\n-        .block_on(accept_stream.cancel())\n-        .expect(\"failed to cancel\");\n+    cancel(&waker, &mut accept_stream, start_mulitshot_op);\n \n     // We should still be able to accept the second connection.\n     let client2 = waker\n@@ -387,13 +381,8 @@ fn connect() {\n     client.write_all(DATA2).expect(\"failed to write\");\n     buf.clear();\n     buf.reserve(DATA2.len() + 1);\n-    let mut buf = waker.block_on(stream.read(buf)).expect(\"failed to read\");\n+    let buf = waker.block_on(stream.read(buf)).expect(\"failed to read\");\n     assert_eq!(buf, DATA2);\n-\n-    // Dropping the stream should closing it.\n-    drop(stream);\n-    let n = client.read(&mut buf).expect(\"failed to read\");\n-    assert_eq!(n, 0);\n }\n \n #[test]"
    },
    {
      "filename": "tests/ring.rs",
      "status": "modified",
      "patch": "@@ -15,7 +15,6 @@ use std::task::{self, Poll, Wake};\n use std::thread;\n use std::time::{Duration, Instant};\n \n-use a10::cancel::Cancel;\n use a10::fs::{Open, OpenOptions};\n use a10::io::ReadBufPool;\n use a10::msg::{msg_listener, send_msg, try_send_msg, MsgListener, MsgToken, SendMsg};\n@@ -24,7 +23,7 @@ use a10::{fd, mem, process, Config, Ring, SubmissionQueue};\n \n mod util;\n use util::{\n-    defer, expect_io_errno, init, is_send, is_sync, next, poll_nop, require_kernel,\n+    cancel, defer, expect_io_errno, init, is_send, is_sync, next, poll_nop, require_kernel,\n     start_mulitshot_op, start_op, test_queue, Waker,\n };\n \n@@ -279,7 +278,7 @@ fn message_sending() {\n \n     let (msg_listener, msg_token) = msg_listener(sq.clone()).unwrap();\n     let mut msg_listener = pin!(msg_listener);\n-    start_mulitshot_op(msg_listener.as_mut());\n+    start_mulitshot_op(&mut msg_listener);\n \n     // Send some messages.\n     try_send_msg(&sq, msg_token, DATA1).unwrap();\n@@ -340,10 +339,9 @@ fn cancel_oneshot_poll() {\n \n     let (receiver, sender) = pipe2().unwrap();\n \n-    let mut receiver_read = pin!(oneshot_poll(&sq, receiver.as_fd(), libc::POLLIN as _));\n-    start_op(&mut receiver_read);\n+    let mut receiver_read = oneshot_poll(&sq, receiver.as_fd(), libc::POLLIN as _);\n \n-    waker.block_on(receiver_read.cancel()).unwrap();\n+    cancel(&waker, &mut receiver_read, start_op);\n     expect_io_errno(waker.block_on(receiver_read), libc::ECANCELED);\n     drop(sender);\n }\n@@ -359,7 +357,7 @@ fn test_multishot_poll() {\n     let (mut receiver, mut sender) = pipe2().unwrap();\n \n     let mut receiver_read = pin!(multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _));\n-    start_mulitshot_op(Pin::new(&mut receiver_read));\n+    start_mulitshot_op(&mut receiver_read);\n \n     let mut buf = vec![0; DATA.len() + 1];\n     for _ in 0..3 {\n@@ -384,10 +382,9 @@ fn cancel_multishot_poll() {\n \n     let (receiver, sender) = pipe2().unwrap();\n \n-    let mut receiver_read = pin!(multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _));\n-    start_mulitshot_op(receiver_read.as_mut());\n+    let mut receiver_read = multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _);\n \n-    waker.block_on(receiver_read.cancel()).unwrap();\n+    cancel(&waker, &mut receiver_read, start_mulitshot_op);\n     assert!(waker.block_on(next(receiver_read)).is_none());\n     drop(sender);\n }\n@@ -400,7 +397,7 @@ fn drop_multishot_poll() {\n \n     let mut receiver_read = multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _);\n \n-    start_mulitshot_op(Pin::new(&mut receiver_read));\n+    start_mulitshot_op(&mut receiver_read);\n \n     drop(receiver_read);\n     drop(receiver);\n@@ -478,12 +475,15 @@ fn process_wait_on_cancel() {\n     let mut process = Command::new(\"sleep\").arg(\"1000\").spawn().unwrap();\n \n     let mut future = process::wait_on(sq, &process, libc::WEXITED);\n-    let result = poll_nop(Pin::new(&mut future));\n-    if !result.is_pending() {\n-        panic!(\"unexpected result, expected it to return Poll::Pending\");\n-    }\n \n-    waker.block_on(future.cancel()).unwrap();\n+    cancel(&waker, &mut future, |future| {\n+        // NOTE: can't use `start_op` as `siginfo_t` doesn't implemented\n+        // `fmt::Debug`.\n+        let result = poll_nop(Pin::new(future));\n+        if !result.is_pending() {\n+            panic!(\"unexpected result, expected it to return Poll::Pending\");\n+        }\n+    });\n \n     process.kill().unwrap();\n     process.wait().unwrap();"
    },
    {
      "filename": "tests/util/mod.rs",
      "status": "modified",
      "patch": "@@ -18,6 +18,8 @@ use std::task::{self, Poll};\n use std::thread::{self, Thread};\n use std::{fmt, mem, panic, process, ptr, str};\n \n+use a10::cancel::Cancel;\n+use a10::fd::Descriptor;\n use a10::net::socket;\n use a10::{AsyncFd, Ring, SubmissionQueue};\n \n@@ -203,6 +205,69 @@ impl Waker {\n     }\n }\n \n+/// Cancel `operation`.\n+///\n+/// `Future`s are inert and we can't determine if an operation has actually been\n+/// started or the submission queue was full. This means that we can't ensure\n+/// that the operation has been queued with the Kernel. This means that in some\n+/// cases we won't actually cancel the operation simply because it hasn't\n+/// started. This introduces flakyness in the tests.\n+///\n+/// To work around this we have this cancel function. If we fail to cancel the\n+/// operation we try to start the operation using `start_op`, before canceling t\n+/// again. Looping until the operation is canceled.\n+#[track_caller]\n+pub(crate) fn cancel<O, F>(waker: &Arc<Waker>, operation: &mut O, start_op: F)\n+where\n+    O: Cancel,\n+    F: Fn(&mut O),\n+{\n+    for _ in 0..100 {\n+        match waker.block_on(operation.cancel()) {\n+            Ok(()) => return,\n+            Err(ref err) if err.kind() == io::ErrorKind::NotFound => {}\n+            Err(err) => panic!(\"unexpected error canceling operation: {err}\"),\n+        }\n+\n+        start_op(operation);\n+    }\n+    panic!(\"couldn't cancel operation\");\n+}\n+\n+/// Cancel all operations of `fd`.\n+///\n+/// `Future`s are inert and we can't determine if an operation has actually been\n+/// started or the submission queue was full. This means that we can't ensure\n+/// that the operation has been queued with the Kernel. This means that in some\n+/// cases we won't actually cancel any operations simply because they haven't\n+/// started. This introduces flakyness in the tests.\n+///\n+/// To work around this we have this cancel all function. If we fail to get the\n+/// `expected` number of canceled operations we try to start the operations\n+/// using `start_op`, before canceling them again. Looping until we get the\n+/// expected number of canceled operations.\n+#[track_caller]\n+pub(crate) fn cancel_all<D: Descriptor, F: FnMut()>(\n+    waker: &Arc<Waker>,\n+    fd: &AsyncFd<D>,\n+    mut start_op: F,\n+    expected: usize,\n+) {\n+    let mut canceled = 0;\n+    for _ in 0..100 {\n+        let n = waker\n+            .block_on(fd.cancel_all())\n+            .expect(\"failed to cancel all operations\");\n+        canceled += n;\n+        if canceled >= expected {\n+            return;\n+        }\n+\n+        start_op();\n+    }\n+    panic!(\"couldn't cancel all expected operations\");\n+}\n+\n /// Start an A10 operation, assumes `future` is a A10 `Future`.\n #[track_caller]\n pub(crate) fn start_op<Fut>(future: &mut Fut)\n@@ -218,7 +283,7 @@ where\n \n /// Start an A10 multishot operation, assumes `iter` is a A10 `AsyncIterator`.\n #[track_caller]\n-pub(crate) fn start_mulitshot_op<I>(iter: I)\n+pub(crate) fn start_mulitshot_op<I>(iter: &mut I)\n where\n     I: AsyncIterator + Unpin,\n     I::Item: fmt::Debug,"
    }
  ],
  "fix_category": "Wait/Poll",
  "root_cause_category": "Async wait",
  "root_cause_subcategory": "Unknown state"
}