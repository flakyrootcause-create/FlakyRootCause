{
  "id": 290,
  "repo": "iceberg",
  "issue_url": "https://github.com/apache/iceberg/pull/11470",
  "pr_url": "https://github.com/apache/iceberg/pull/11470",
  "issue_description": "Follow-up of #10811 to ignore other types of `FileSystemException` like `DirectoryNotEmptyException` as well\r\n\r\n```\r\nTestDataFrameWrites > testFaultToleranceOnWrite() > format = parquet FAILED\r\n    java.nio.file.DirectoryNotEmptyException: /tmp/junit-7768099913831474039/parquet/test\r\n        at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:289)\r\n        at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:104)\r\n        at java.base/java.nio.file.Files.delete(Files.java:1152)\r\n        at org.apache.commons.io.FileUtils.delete(FileUtils.java:1222)\r\n        at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1242)\r\n        at org.apache.iceberg.spark.source.TestDataFrameWrites.testFaultToleranceOnWrite(TestDataFrameWrites.java:427)\r\n```\r\n\r\ncc @Fokko @nastra ",
  "files_changed": [
    {
      "filename": "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java",
      "status": "modified",
      "patch": "@@ -28,15 +28,13 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n-import java.nio.file.NoSuchFileException;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n import org.apache.avro.generic.GenericData.Record;\n-import org.apache.commons.io.FileUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Parameter;\n@@ -76,6 +74,7 @@\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n @ExtendWith(ParameterizedTestExtension.class)\n public class TestDataFrameWrites extends ParameterizedAvroDataTest {\n@@ -88,6 +87,8 @@ public static Collection<String> parameters() {\n \n   @Parameter private String format;\n \n+  @TempDir private File location;\n+\n   private static SparkSession spark = null;\n   private static JavaSparkContext sc = null;\n \n@@ -140,47 +141,37 @@ public static void stopSpark() {\n \n   @Override\n   protected void writeAndValidate(Schema schema) throws IOException {\n-    File location = createTableFolder();\n-    Table table = createTable(schema, location);\n-    writeAndValidateWithLocations(table, location, new File(location, \"data\"));\n+    Table table = createTable(schema);\n+    writeAndValidateWithLocations(table, new File(location, \"data\"));\n   }\n \n   @TestTemplate\n   public void testWriteWithCustomDataLocation() throws IOException {\n-    File location = createTableFolder();\n     File tablePropertyDataLocation = temp.resolve(\"test-table-property-data-dir\").toFile();\n-    Table table = createTable(new Schema(SUPPORTED_PRIMITIVES.fields()), location);\n+    Table table = createTable(new Schema(SUPPORTED_PRIMITIVES.fields()));\n     table\n         .updateProperties()\n         .set(TableProperties.WRITE_DATA_LOCATION, tablePropertyDataLocation.getAbsolutePath())\n         .commit();\n-    writeAndValidateWithLocations(table, location, tablePropertyDataLocation);\n-  }\n-\n-  private File createTableFolder() throws IOException {\n-    File parent = temp.resolve(\"parquet\").toFile();\n-    File location = new File(parent, \"test\");\n-    assertThat(location.mkdirs()).as(\"Mkdir should succeed\").isTrue();\n-    return location;\n+    writeAndValidateWithLocations(table, tablePropertyDataLocation);\n   }\n \n-  private Table createTable(Schema schema, File location) {\n+  private Table createTable(Schema schema) {\n     HadoopTables tables = new HadoopTables(CONF);\n     return tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n   }\n \n-  private void writeAndValidateWithLocations(Table table, File location, File expectedDataDir)\n-      throws IOException {\n+  private void writeAndValidateWithLocations(Table table, File expectedDataDir) throws IOException {\n     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned\n \n     table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n \n     Iterable<Record> expected = RandomData.generate(tableSchema, 100, 0L);\n-    writeData(expected, tableSchema, location.toString());\n+    writeData(expected, tableSchema);\n \n     table.refresh();\n \n-    List<Row> actual = readTable(location.toString());\n+    List<Row> actual = readTable();\n \n     Iterator<Record> expectedIter = expected.iterator();\n     Iterator<Row> actualIter = actual.iterator();\n@@ -204,21 +195,20 @@ private void writeAndValidateWithLocations(Table table, File location, File expe\n                     .startsWith(expectedDataDir.getAbsolutePath()));\n   }\n \n-  private List<Row> readTable(String location) {\n-    Dataset<Row> result = spark.read().format(\"iceberg\").load(location);\n+  private List<Row> readTable() {\n+    Dataset<Row> result = spark.read().format(\"iceberg\").load(location.toString());\n \n     return result.collectAsList();\n   }\n \n-  private void writeData(Iterable<Record> records, Schema schema, String location)\n-      throws IOException {\n+  private void writeData(Iterable<Record> records, Schema schema) throws IOException {\n     Dataset<Row> df = createDataset(records, schema);\n     DataFrameWriter<?> writer = df.write().format(\"iceberg\").mode(\"append\");\n-    writer.save(location);\n+    writer.save(location.toString());\n   }\n \n-  private void writeDataWithFailOnPartition(\n-      Iterable<Record> records, Schema schema, String location) throws IOException, SparkException {\n+  private void writeDataWithFailOnPartition(Iterable<Record> records, Schema schema)\n+      throws IOException, SparkException {\n     final int numPartitions = 10;\n     final int partitionToFail = new Random().nextInt(numPartitions);\n     MapPartitionsFunction<Row, Row> failOnFirstPartitionFunc =\n@@ -241,7 +231,7 @@ private void writeDataWithFailOnPartition(\n     // Setting \"check-nullability\" option to \"false\" doesn't help as it fails at Spark analyzer.\n     Dataset<Row> convertedDf = df.sqlContext().createDataFrame(df.rdd(), convert(schema));\n     DataFrameWriter<?> writer = convertedDf.write().format(\"iceberg\").mode(\"append\");\n-    writer.save(location);\n+    writer.save(location.toString());\n   }\n \n   private Dataset<Row> createDataset(Iterable<Record> records, Schema schema) throws IOException {\n@@ -287,7 +277,6 @@ public void testNullableWithWriteOption() throws IOException {\n         .as(\"Spark 3 rejects writing nulls to a required column\")\n         .startsWith(\"2\");\n \n-    File location = temp.resolve(\"parquet\").resolve(\"test\").toFile();\n     String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location);\n     String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location);\n \n@@ -341,7 +330,6 @@ public void testNullableWithSparkSqlOption() throws IOException {\n         .as(\"Spark 3 rejects writing nulls to a required column\")\n         .startsWith(\"2\");\n \n-    File location = temp.resolve(\"parquet\").resolve(\"test\").toFile();\n     String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location);\n     String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location);\n \n@@ -397,37 +385,28 @@ public void testNullableWithSparkSqlOption() throws IOException {\n \n   @TestTemplate\n   public void testFaultToleranceOnWrite() throws IOException {\n-    File location = createTableFolder();\n     Schema schema = new Schema(SUPPORTED_PRIMITIVES.fields());\n-    Table table = createTable(schema, location);\n+    Table table = createTable(schema);\n \n     Iterable<Record> records = RandomData.generate(schema, 100, 0L);\n-    writeData(records, schema, location.toString());\n+    writeData(records, schema);\n \n     table.refresh();\n \n     Snapshot snapshotBeforeFailingWrite = table.currentSnapshot();\n-    List<Row> resultBeforeFailingWrite = readTable(location.toString());\n+    List<Row> resultBeforeFailingWrite = readTable();\n \n     Iterable<Record> records2 = RandomData.generate(schema, 100, 0L);\n \n-    assertThatThrownBy(() -> writeDataWithFailOnPartition(records2, schema, location.toString()))\n+    assertThatThrownBy(() -> writeDataWithFailOnPartition(records2, schema))\n         .isInstanceOf(SparkException.class);\n \n     table.refresh();\n \n     Snapshot snapshotAfterFailingWrite = table.currentSnapshot();\n-    List<Row> resultAfterFailingWrite = readTable(location.toString());\n+    List<Row> resultAfterFailingWrite = readTable();\n \n     assertThat(snapshotBeforeFailingWrite).isEqualTo(snapshotAfterFailingWrite);\n     assertThat(resultBeforeFailingWrite).isEqualTo(resultAfterFailingWrite);\n-\n-    while (location.exists()) {\n-      try {\n-        FileUtils.deleteDirectory(location);\n-      } catch (NoSuchFileException e) {\n-        // ignore NoSuchFileException when a file is already deleted\n-      }\n-    }\n   }\n }"
    }
  ],
  "fix_category": "Setup/clean up state",
  "root_cause_category": "Test order dependency",
  "root_cause_subcategory": NaN
}