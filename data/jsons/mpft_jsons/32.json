{
  "id": 32,
  "repo": "pytorch",
  "issue_url": "https://github.com/pytorch/pytorch/pull/162472",
  "pr_url": "https://github.com/pytorch/pytorch/pull/162472",
  "issue_description": "[PR Linked Issue]\nPlatforms: linux, rocm, slow\n\n\n\n  This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_aoti_fx_add&suite=AOTFxirTestCase&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/49795173420).\n\n  Over the past 6 hours, it has been determined flaky in 10 workflow(s) with 20 failures and 10 successes.\n\n  **Debugging instructions (after clicking on the recent samples link):**\n  DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\n  To find relevant log snippets:\n  1. Click on the workflow logs linked above\n  2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n  3. Grep for `test_aoti_fx_add`\n  4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n  \n  \n  Test file path: `inductor/test_fxir_backend.py`\n\n  For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.\n\ncc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",
  "files_changed": [
    {
      "filename": "test/inductor/test_fxir_backend.py",
      "status": "modified",
      "patch": "@@ -693,7 +693,7 @@ def check(self, model, inp, dynamic_shapes=None, strict=False):\n                 model, inp, dynamic_shapes=dynamic_shapes, strict=strict\n             )\n             gm = torch._inductor.aot_compile(\n-                ep.module(), inp, options={\"fx_wrapper\": True}\n+                ep.module(), inp, options={\"fx_wrapper\": True, \"compile_threads\": 1}\n             )\n             self.assertTrue(torch.allclose(model(*inp), gm(*inp)))\n "
    }
  ],
  "fix_category": "Make deterministic",
  "root_cause_category": "Concurrency",
  "root_cause_subcategory": NaN
}