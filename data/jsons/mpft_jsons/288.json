{
  "id": 288,
  "repo": "pulsar-client-go",
  "issue_url": "https://github.com/apache/pulsar-client-go/pull/1003",
  "pr_url": "https://github.com/apache/pulsar-client-go/pull/1003",
  "issue_description": "Fixes #983 \r\n\r\n### Motivation\r\n\r\nOld TestMaxPendingChunkMessages() uses the concurrent message publish to make the consumer discard unavailable chunk. And it's flaky.\r\n\r\nSo the `sendSingeChunk()` is introduced to manual create scenarios where old chunks should be discarded.\r\n\r\n### Modifications\r\n\r\n- Fix `TestMaxPendingChunkMessages()`\r\n\r\n### Verifying this change\r\n\r\n- [x] Make sure that the change passes the CI checks.\r\n",
  "files_changed": [
    {
      "filename": "pulsar/message_chunking_test.go",
      "status": "modified",
      "patch": "@@ -24,10 +24,13 @@ import (\n \t\"math/rand\"\n \t\"net/http\"\n \t\"strings\"\n-\t\"sync\"\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/apache/pulsar-client-go/pulsar/internal\"\n+\n+\t\"google.golang.org/protobuf/proto\"\n+\n \t\"github.com/stretchr/testify/assert\"\n )\n \n@@ -148,92 +151,59 @@ func TestLargeMessage(t *testing.T) {\n }\n \n func TestMaxPendingChunkMessages(t *testing.T) {\n-\trand.Seed(time.Now().Unix())\n-\n \tclient, err := NewClient(ClientOptions{\n \t\tURL: lookupURL,\n \t})\n \tassert.Nil(t, err)\n \tdefer client.Close()\n \n \ttopic := newTopicName()\n+\tproducer, err := client.CreateProducer(ProducerOptions{\n+\t\tTopic:               topic,\n+\t\tDisableBatching:     true,\n+\t\tEnableChunking:      true,\n+\t\tChunkMaxMessageSize: 10,\n+\t})\n+\tassert.NoError(t, err)\n+\tassert.NotNil(t, producer)\n \n-\ttotalProducers := 5\n-\tproducers := make([]Producer, 0, 20)\n-\tdefer func() {\n-\t\tfor _, p := range producers {\n-\t\t\tp.Close()\n-\t\t}\n-\t}()\n-\n-\tclients := make([]Client, 0, 20)\n-\tdefer func() {\n-\t\tfor _, c := range clients {\n-\t\t\tc.Close()\n-\t\t}\n-\t}()\n-\n-\tfor j := 0; j < totalProducers; j++ {\n-\t\tpc, err := NewClient(ClientOptions{\n-\t\t\tURL: lookupURL,\n-\t\t})\n-\t\tassert.Nil(t, err)\n-\t\tclients = append(clients, pc)\n-\t\tproducer, err := pc.CreateProducer(ProducerOptions{\n-\t\t\tTopic:               topic,\n-\t\t\tDisableBatching:     true,\n-\t\t\tEnableChunking:      true,\n-\t\t\tChunkMaxMessageSize: 10,\n-\t\t})\n-\t\tassert.NoError(t, err)\n-\t\tassert.NotNil(t, producer)\n-\t\tproducers = append(producers, producer)\n-\t}\n-\n-\tconsumer, err := client.Subscribe(ConsumerOptions{\n+\tc, err := client.Subscribe(ConsumerOptions{\n \t\tTopic:                    topic,\n \t\tType:                     Exclusive,\n \t\tSubscriptionName:         \"chunk-subscriber\",\n \t\tMaxPendingChunkedMessage: 1,\n \t})\n \tassert.NoError(t, err)\n-\tassert.NotNil(t, consumer)\n-\tdefer consumer.Close()\n+\tassert.NotNil(t, c)\n+\tdefer c.Close()\n+\tpc := c.(*consumer).consumers[0]\n \n-\ttotalMsgs := 40\n-\twg := sync.WaitGroup{}\n-\twg.Add(totalMsgs * totalProducers)\n-\tfor i := 0; i < totalMsgs; i++ {\n-\t\tfor j := 0; j < totalProducers; j++ {\n-\t\t\tp := producers[j]\n-\t\t\tgo func() {\n-\t\t\t\tID, err := p.Send(context.Background(), &ProducerMessage{\n-\t\t\t\t\tPayload: createTestMessagePayload(50),\n-\t\t\t\t})\n-\t\t\t\tassert.NoError(t, err)\n-\t\t\t\tassert.NotNil(t, ID)\n-\t\t\t\twg.Done()\n-\t\t\t}()\n-\t\t}\n-\t}\n-\twg.Wait()\n+\tsendSingleChunk(producer, \"0\", 0, 2)\n+\t// MaxPendingChunkedMessage is 1, the chunked message with uuid 0 will be discarded\n+\tsendSingleChunk(producer, \"1\", 0, 2)\n \n-\treceived := 0\n-\tfor i := 0; i < totalMsgs*totalProducers; i++ {\n-\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*10)\n-\t\tmsg, err := consumer.Receive(ctx)\n-\t\tcancel()\n-\t\tif msg == nil || (err != nil && errors.Is(err, context.DeadlineExceeded)) {\n-\t\t\tbreak\n-\t\t}\n+\t// chunkedMsgCtx with uuid 0 should be discarded\n+\tretryAssert(t, 3, 200, func() {}, func(t assert.TestingT) bool {\n+\t\tpc.chunkedMsgCtxMap.mu.Lock()\n+\t\tdefer pc.chunkedMsgCtxMap.mu.Unlock()\n+\t\treturn assert.Equal(t, 1, len(pc.chunkedMsgCtxMap.chunkedMsgCtxs))\n+\t})\n \n-\t\treceived++\n+\tsendSingleChunk(producer, \"1\", 1, 2)\n \n-\t\terr = consumer.Ack(msg)\n-\t\tassert.NoError(t, err)\n-\t}\n+\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n+\tmsg, err := c.Receive(ctx)\n+\tcancel()\n \n-\tassert.NotEqual(t, totalMsgs*totalProducers, received)\n+\tassert.NoError(t, err)\n+\tassert.Equal(t, \"chunk-1-0|chunk-1-1|\", string(msg.Payload()))\n+\n+\t// Ensure that the chunked message of uuid 0 is discarded.\n+\tsendSingleChunk(producer, \"0\", 1, 2)\n+\tctx, cancel = context.WithTimeout(context.Background(), 5*time.Second)\n+\tmsg, err = c.Receive(ctx)\n+\tcancel()\n+\tassert.True(t, errors.Is(err, context.DeadlineExceeded))\n }\n \n func TestExpireIncompleteChunks(t *testing.T) {\n@@ -576,3 +546,32 @@ func createTestMessagePayload(size int) []byte {\n \t}\n \treturn payload\n }\n+\n+//nolint:all\n+func sendSingleChunk(p Producer, uuid string, chunkID int, totalChunks int) {\n+\tmsg := &ProducerMessage{\n+\t\tPayload: []byte(fmt.Sprintf(\"chunk-%s-%d|\", uuid, chunkID)),\n+\t}\n+\tproducerImpl := p.(*producer).producers[0].(*partitionProducer)\n+\tmm := producerImpl.genMetadata(msg, len(msg.Payload), time.Now())\n+\tmm.Uuid = proto.String(uuid)\n+\tmm.NumChunksFromMsg = proto.Int32(int32(totalChunks))\n+\tmm.TotalChunkMsgSize = proto.Int32(int32(len(msg.Payload)))\n+\tmm.ChunkId = proto.Int32(int32(chunkID))\n+\tproducerImpl.updateMetadataSeqID(mm, msg)\n+\n+\tdoneCh := make(chan struct{})\n+\tproducerImpl.internalSingleSend(\n+\t\tmm,\n+\t\tmsg.Payload,\n+\t\t&sendRequest{\n+\t\t\tcallback: func(id MessageID, producerMessage *ProducerMessage, err error) {\n+\t\t\t\tclose(doneCh)\n+\t\t\t},\n+\t\t\tmsg: msg,\n+\t\t},\n+\t\tuint32(internal.MaxMessageSize),\n+\t)\n+\n+\t<-doneCh\n+}"
    }
  ],
  "fix_category": "Make deterministic",
  "root_cause_category": "Concurrency",
  "root_cause_subcategory": NaN
}