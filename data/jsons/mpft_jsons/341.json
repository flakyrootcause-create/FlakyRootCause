{
  "id": 341,
  "repo": "airflow",
  "issue_url": "https://github.com/apache/airflow/pull/52936",
  "pr_url": "https://github.com/apache/airflow/pull/52936",
  "issue_description": "[PR Linked Issue]\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe test class has a 20% failure rate.\n\nhttps://github.com/xBis7/airflow/actions/runs/16086980198\n\nhttps://github.com/xBis7/airflow/actions/runs/16087424463\n\nhttps://github.com/xBis7/airflow/actions/runs/16087596566\n\nI'm attaching a file with the logs\n\n[fail_logs.txt](https://github.com/user-attachments/files/21076055/fail_logs.txt)\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nI can't reproduce it locally, it's occurring only on the remote CI. It can be reproduced by running the test on the CI on repeat. Increasing execution concurrency makes it more likely to occur. For example, for 10 iterations\n\n```\nmax-parallel: 10\n```\n\n\n\n### Operating System\n\nThe CI uses ubuntu-22.04.\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n",
  "files_changed": [
    {
      "filename": "airflow-core/tests/integration/otel/test_otel.py",
      "status": "modified",
      "patch": "@@ -873,6 +873,11 @@ def test_scheduler_change_after_the_first_task_finishes(\n         will handle the rest of the dag processing. The paused thread will be resumed afterwards.\n         \"\"\"\n \n+        # For this test, scheduler1 must be idle but still considered healthy by scheduler2.\n+        # If scheduler2 marks the job as unhealthy, then it will recreate scheduler1's spans\n+        # because it will consider them lost.\n+        os.environ[\"AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_THRESHOLD\"] = \"90\"\n+\n         celery_worker_process = None\n         scheduler_process_1 = None\n         apiserver_process = None\n@@ -937,13 +942,12 @@ def test_scheduler_change_after_the_first_task_finishes(\n             with open(self.control_file, \"w\") as file:\n                 file.write(\"continue\")\n \n-            # Wait for scheduler2 to be up and running.\n-            time.sleep(10)\n-\n             wait_for_dag_run_and_check_span_status(\n                 dag_id=dag_id, run_id=run_id, max_wait_time=120, span_status=SpanStatus.SHOULD_END\n             )\n \n+            # Stop scheduler2 in case it still has a db lock on the dag_run.\n+            scheduler_process_2.terminate()\n             scheduler_process_1.send_signal(signal.SIGCONT)\n \n             # Wait for the scheduler to start again and continue running.\n@@ -959,6 +963,9 @@ def test_scheduler_change_after_the_first_task_finishes(\n                 with create_session() as session:\n                     dump_airflow_metadata_db(session)\n \n+            # Reset for the rest of the tests.\n+            os.environ[\"AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_THRESHOLD\"] = \"15\"\n+\n             # Terminate the processes.\n             celery_worker_process.terminate()\n             celery_worker_process.wait()\n@@ -969,7 +976,6 @@ def test_scheduler_change_after_the_first_task_finishes(\n             apiserver_process.terminate()\n             apiserver_process.wait()\n \n-            scheduler_process_2.terminate()\n             scheduler_process_2.wait()\n \n         out, err = capfd.readouterr()\n@@ -991,7 +997,6 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n         \"\"\"\n \n         celery_worker_process = None\n-        scheduler_process_1 = None\n         apiserver_process = None\n         scheduler_process_2 = None\n         try:\n@@ -1032,7 +1037,7 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n             with capfd.disabled():\n                 scheduler_process_1.terminate()\n \n-            assert scheduler_process_1.wait(timeout=30) == 0\n+            assert scheduler_process_1.wait() == 0\n \n             check_dag_run_state_and_span_status(\n                 dag_id=dag_id, run_id=run_id, state=State.RUNNING, span_status=SpanStatus.NEEDS_CONTINUANCE\n@@ -1049,9 +1054,6 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n             with open(self.control_file, \"w\") as file:\n                 file.write(\"continue\")\n \n-            # Wait for scheduler2 to be up and running.\n-            time.sleep(10)\n-\n             wait_for_dag_run_and_check_span_status(\n                 dag_id=dag_id, run_id=run_id, max_wait_time=120, span_status=SpanStatus.ENDED\n             )\n@@ -1066,8 +1068,6 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n             celery_worker_process.terminate()\n             celery_worker_process.wait()\n \n-            scheduler_process_1.wait()\n-\n             apiserver_process.terminate()\n             apiserver_process.wait()\n \n@@ -1253,9 +1253,6 @@ def test_scheduler_exits_forcefully_after_the_first_task_finishes(\n                 stderr=None,\n             )\n \n-            # Wait for scheduler2 to be up and running.\n-            time.sleep(10)\n-\n             wait_for_dag_run_and_check_span_status(\n                 dag_id=dag_id, run_id=run_id, max_wait_time=120, span_status=SpanStatus.ENDED\n             )"
    }
  ],
  "fix_category": "Remove dependency",
  "root_cause_category": "Concurrency",
  "root_cause_subcategory": NaN
}