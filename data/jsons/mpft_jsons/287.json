{
  "id": 287,
  "repo": "talos",
  "issue_url": "https://github.com/siderolabs/talos/pull/2651",
  "pr_url": "https://github.com/siderolabs/talos/pull/2651",
  "issue_description": "This enables golangci-lint via build tags for integration tests (this\r\nshould have been done long ago!), and fixes the linting errors.\r\n\r\nTwo tests were updated to reduce flakiness:\r\n\r\n* apply config: wait for nodes to issue \"boot done\" sequence event\r\nbefore proceeding\r\n* recover: kill pods even if they appear after the initial set gets\r\nkilled (potential race condition with previous test).\r\n\r\nSigned-off-by: Andrey Smirnov <smirnov.andrey@gmail.com>\r\n\r\n\r\n",
  "files_changed": [
    {
      "filename": ".golangci.yml",
      "status": "modified",
      "patch": "@@ -22,6 +22,14 @@ run:\n   skip-files:\n     - .*\\\\.pb\\\\.go$\n \n+  # list of build tags, all linters use it. Default is empty list.\n+  build-tags:\n+    - integration\n+    - integration_api\n+    - integration_cli\n+    - integration_k8s\n+    - integration_provision\n+\n # output configuration options\n output:\n   # colored-line-number|line-number|json|tab|checkstyle, default is \"colored-line-number\""
    },
    {
      "filename": "internal/integration/api/apply-config.go",
      "status": "modified",
      "patch": "@@ -16,6 +16,8 @@ import (\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/talos-systems/go-retry/retry\"\n+\n \t\"github.com/talos-systems/talos/internal/integration/base\"\n \tmachineapi \"github.com/talos-systems/talos/pkg/machinery/api/machine\"\n \t\"github.com/talos-systems/talos/pkg/machinery/client\"\n@@ -40,6 +42,7 @@ const applyConfigTestSysctlVal = \"1\"\n \n const assertRebootedRebootTimeout = 10 * time.Minute\n \n+// ApplyConfigSuite ...\n type ApplyConfigSuite struct {\n \tbase.K8sSuite\n \n@@ -69,15 +72,17 @@ func (suite *ApplyConfigSuite) TearDownTest() {\n \t}\n }\n \n-// TestRecoverControlPlane removes the control plane components and attempts to recover them with the recover API.\n-func (suite *ApplyConfigSuite) TestRecoverControlPlane() {\n+// TestApply verifies the apply config API.\n+func (suite *ApplyConfigSuite) TestApply() {\n \tif !suite.Capabilities().SupportsReboot {\n \t\tsuite.T().Skip(\"cluster doesn't support reboot\")\n \t}\n \n \tnodes := suite.DiscoverNodes().NodesByType(machine.TypeJoin)\n \tsuite.Require().NotEmpty(nodes)\n \n+\tsuite.WaitForBootDone(suite.ctx)\n+\n \tsort.Strings(nodes)\n \n \tnode := nodes[0]\n@@ -93,7 +98,7 @@ func (suite *ApplyConfigSuite) TestRecoverControlPlane() {\n \tsuite.Assert().Nilf(err, \"failed to marshal updated machine config data (node %q): %w\", node, err)\n \n \tsuite.AssertRebooted(suite.ctx, node, func(nodeCtx context.Context) error {\n-\t\t_, err := suite.Client.ApplyConfiguration(nodeCtx, &machineapi.ApplyConfigurationRequest{\n+\t\t_, err = suite.Client.ApplyConfiguration(nodeCtx, &machineapi.ApplyConfigurationRequest{\n \t\t\tData: cfgDataOut,\n \t\t})\n \t\tif err != nil {\n@@ -105,8 +110,16 @@ func (suite *ApplyConfigSuite) TestRecoverControlPlane() {\n \t}, assertRebootedRebootTimeout)\n \n \t// Verify configuration change\n-\tnewProvider, err := suite.readConfigFromNode(nodeCtx)\n-\tsuite.Assert().Nilf(err, \"failed to read updated configuration from node %q: %w\", node, err)\n+\tvar newProvider config.Provider\n+\n+\tsuite.Require().Nilf(retry.Constant(time.Minute, retry.WithUnits(time.Second)).Retry(func() error {\n+\t\tnewProvider, err = suite.readConfigFromNode(nodeCtx)\n+\t\tif err != nil {\n+\t\t\treturn retry.ExpectedError(err)\n+\t\t}\n+\n+\t\treturn nil\n+\t}), \"failed to read updated configuration from node %q: %w\", node, err)\n \n \tsuite.Assert().Equal(\n \t\tnewProvider.Machine().Sysctls()[applyConfigTestSysctl],\n@@ -122,9 +135,9 @@ func (suite *ApplyConfigSuite) readConfigFromNode(nodeCtx context.Context) (conf\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"error creating reader: %w\", err)\n \t}\n-\tdefer reader.Close()\n+\tdefer reader.Close() //nolint: errcheck\n \n-\tif err := copyFromReaderWithErrChan(cfgData, reader, errCh); err != nil {\n+\tif err = copyFromReaderWithErrChan(cfgData, reader, errCh); err != nil {\n \t\treturn nil, fmt.Errorf(\"error reading: %w\", err)\n \t}\n \n@@ -142,6 +155,7 @@ func copyFromReaderWithErrChan(out io.Writer, in io.Reader, errCh <-chan error)\n \tvar chanErr error\n \n \twg.Add(1)\n+\n \tgo func() {\n \t\tdefer wg.Done()\n "
    },
    {
      "filename": "internal/integration/api/diskusage.go",
      "status": "modified",
      "patch": "@@ -63,27 +63,27 @@ func (suite *DiskUsageSuite) TestDiskUsageRequests() {\n \t}\n \n \tcases := []*testParams{\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 0,\n \t\t\tall:            false,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 1,\n \t\t\tall:            false,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 0,\n \t\t\tall:            true,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 1,\n \t\t\tall:            true,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 0,\n \t\t\tall:            true,\n \t\t\tpaths:          append([]string{\"/this/is/going/to/fail\"}, defaultPaths...),\n@@ -93,7 +93,6 @@ func (suite *DiskUsageSuite) TestDiskUsageRequests() {\n \tsizes := map[string]int64{}\n \n \tfor _, params := range cases {\n-\n \t\tlookupPaths := map[string]bool{}\n \t\tfor _, path := range params.paths {\n \t\t\tlookupPaths[path] = true\n@@ -110,16 +109,19 @@ func (suite *DiskUsageSuite) TestDiskUsageRequests() {\n \t\tsuite.Require().NoError(err)\n \n \t\tresponseCount := 0\n+\n \t\tfor {\n \t\t\tinfo, err := stream.Recv()\n \t\t\tresponseCount++\n+\n \t\t\tif err != nil {\n \t\t\t\tif err == io.EOF || status.Code(err) == codes.Canceled {\n \t\t\t\t\tbreak\n \t\t\t\t}\n \n \t\t\t\tsuite.Require().NoError(err)\n \t\t\t}\n+\n \t\t\tif size, ok := sizes[info.Name]; ok {\n \t\t\t\tsuite.Require().EqualValues(size, info.Size)\n \t\t\t}"
    },
    {
      "filename": "internal/integration/api/dmesg.go",
      "status": "modified",
      "patch": "@@ -139,6 +139,7 @@ func (suite *DmesgSuite) TestClusterHasDmesg() {\n \t\t\tif err == io.EOF {\n \t\t\t\tbreak\n \t\t\t}\n+\n \t\t\tsuite.Require().NoError(err)\n \t\t}\n "
    },
    {
      "filename": "internal/integration/api/etcd.go",
      "status": "modified",
      "patch": "@@ -23,6 +23,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/constants\"\n )\n \n+// EtcdSuite ...\n type EtcdSuite struct {\n \tbase.APISuite\n \n@@ -115,7 +116,9 @@ func (suite *EtcdSuite) TestEtcdLeaveCluster() {\n \tsuite.Require().NoError(err)\n \n \tfor {\n-\t\tinfo, err := stream.Recv()\n+\t\tvar info *machineapi.FileInfo\n+\n+\t\tinfo, err = stream.Recv()\n \t\tif err != nil {\n \t\t\tif err == io.EOF || status.Code(err) == codes.Canceled {\n \t\t\t\tbreak"
    },
    {
      "filename": "internal/integration/api/logs.go",
      "status": "modified",
      "patch": "@@ -160,6 +160,7 @@ func (suite *LogsSuite) TestTailStreaming0() {\n \tsuite.testStreaming(0)\n }\n \n+//nolint: gocyclo\n func (suite *LogsSuite) testStreaming(tailLines int32) {\n \tif tailLines >= 0 {\n \t\t// invoke machined enough times to generate\n@@ -189,7 +190,9 @@ func (suite *LogsSuite) testStreaming(tailLines int32) {\n \t\tdefer close(respCh)\n \n \t\tfor {\n-\t\t\tmsg, err := logsStream.Recv()\n+\t\t\tvar msg *common.Data\n+\n+\t\t\tmsg, err = logsStream.Recv()\n \t\t\tif err != nil {\n \t\t\t\terrCh <- err\n \t\t\t\treturn"
    },
    {
      "filename": "internal/integration/api/reboot.go",
      "status": "modified",
      "patch": "@@ -19,6 +19,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/client\"\n )\n \n+// RebootSuite ...\n type RebootSuite struct {\n \tbase.APISuite\n \n@@ -67,6 +68,8 @@ func (suite *RebootSuite) TestRebootNodeByNode() {\n }\n \n // TestRebootAllNodes reboots all cluster nodes at the same time.\n+//\n+//nolint: gocyclo\n func (suite *RebootSuite) TestRebootAllNodes() {\n \tif !suite.Capabilities().SupportsReboot {\n \t\tsuite.T().Skip(\"cluster doesn't support reboots\")\n@@ -128,7 +131,7 @@ func (suite *RebootSuite) TestRebootAllNodes() {\n \n \t\t\t\t\tif bootIDAfter == bootIDBefore {\n \t\t\t\t\t\t// bootID should be different after reboot\n-\t\t\t\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s + %s, after %s\", node, bootIDBefore, bootIDAfter))\n+\t\t\t\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s, after %s\", node, bootIDBefore, bootIDAfter))\n \t\t\t\t\t}\n \n \t\t\t\t\treturn nil"
    },
    {
      "filename": "internal/integration/api/recover.go",
      "status": "modified",
      "patch": "@@ -21,6 +21,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/config/types/v1alpha1/machine\"\n )\n \n+// RecoverSuite ...\n type RecoverSuite struct {\n \tbase.K8sSuite\n \n@@ -73,6 +74,8 @@ func (suite *RecoverSuite) TestRecoverControlPlane() {\n \n \t\t\tsuite.Assert().NoError(err)\n \n+\t\t\tdeletedPods := make(map[string]struct{})\n+\n \t\t\tvar eg errgroup.Group\n \n \t\t\tfor _, pod := range pods.Items {\n@@ -81,12 +84,9 @@ func (suite *RecoverSuite) TestRecoverControlPlane() {\n \t\t\t\teg.Go(func() error {\n \t\t\t\t\tsuite.T().Logf(\"Deleting %s\", pod.GetName())\n \n-\t\t\t\t\terr := suite.Clientset.CoreV1().Pods(pod.GetNamespace()).Delete(suite.ctx, pod.GetName(), metav1.DeleteOptions{})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n+\t\t\t\t\tdeletedPods[pod.GetName()] = struct{}{}\n \n-\t\t\t\t\treturn err\n+\t\t\t\t\treturn suite.Clientset.CoreV1().Pods(pod.GetNamespace()).Delete(suite.ctx, pod.GetName(), metav1.DeleteOptions{})\n \t\t\t\t})\n \t\t\t}\n \n@@ -100,6 +100,16 @@ func (suite *RecoverSuite) TestRecoverControlPlane() {\n \t\t\t\t})\n \n \t\t\t\tsuite.Assert().NoError(err)\n+\n+\t\t\t\tfor _, pod := range pods.Items {\n+\t\t\t\t\tif _, ok := deletedPods[pod.GetName()]; !ok {\n+\t\t\t\t\t\tsuite.T().Logf(\"Deleting %s\", pod.GetName())\n+\n+\t\t\t\t\t\tdeletedPods[pod.GetName()] = struct{}{}\n+\n+\t\t\t\t\t\tsuite.Require().NoError(suite.Clientset.CoreV1().Pods(pod.GetNamespace()).Delete(suite.ctx, pod.GetName(), metav1.DeleteOptions{}))\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t}\n \n \t\t\tnodes := suite.DiscoverNodes().NodesByType(machine.TypeControlPlane)"
    },
    {
      "filename": "internal/integration/api/reset.go",
      "status": "modified",
      "patch": "@@ -16,6 +16,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/config/types/v1alpha1/machine\"\n )\n \n+// ResetSuite ...\n type ResetSuite struct {\n \tbase.APISuite\n \n@@ -56,6 +57,7 @@ func (suite *ResetSuite) TestResetNodeByNode() {\n \t}\n \n \tinitNodeAddress := \"\"\n+\n \tfor _, node := range suite.Cluster.Info().Nodes {\n \t\tif node.Type == machine.TypeInit {\n \t\t\tinitNodeAddress = node.PrivateIP.String()\n@@ -78,13 +80,13 @@ func (suite *ResetSuite) TestResetNodeByNode() {\n \n \t\tsuite.T().Log(\"Resetting node\", node)\n \n+\t\t// TODO: there is no good way to assert that node was reset and disk contents were really wiped\n+\n \t\t// uptime should go down after Reset, as it reboots the node\n \t\tsuite.AssertRebooted(suite.ctx, node, func(nodeCtx context.Context) error {\n \t\t\t// force reboot after reset, as this is the only mode we can test\n \t\t\treturn suite.Client.Reset(nodeCtx, true, true)\n \t\t}, 10*time.Minute)\n-\n-\t\t// TODO: there is no good way to assert that node was reset and disk contents were really wiped\n \t}\n }\n "
    },
    {
      "filename": "internal/integration/api/version.go",
      "status": "modified",
      "patch": "@@ -74,6 +74,7 @@ func (suite *VersionSuite) TestSameVersionCluster() {\n \tsuite.Require().Len(v.Messages, len(nodes))\n \n \texpectedVersion := v.Messages[0].Version.Tag\n+\n \tfor _, version := range v.Messages {\n \t\tsuite.Assert().Equal(expectedVersion, version.Version.Tag)\n \t}"
    },
    {
      "filename": "internal/integration/base/api.go",
      "status": "modified",
      "patch": "@@ -20,6 +20,7 @@ import (\n \t\"github.com/talos-systems/talos/internal/app/machined/pkg/runtime\"\n \t\"github.com/talos-systems/talos/pkg/cluster\"\n \t\"github.com/talos-systems/talos/pkg/cluster/check\"\n+\tmachineapi \"github.com/talos-systems/talos/pkg/machinery/api/machine\"\n \t\"github.com/talos-systems/talos/pkg/machinery/client\"\n \t\"github.com/talos-systems/talos/pkg/machinery/client/config\"\n \t\"github.com/talos-systems/talos/pkg/machinery/config/types/v1alpha1/machine\"\n@@ -50,6 +51,20 @@ func (apiSuite *APISuite) SetupSuite() {\n \n \tapiSuite.Client, err = client.New(context.TODO(), opts...)\n \tapiSuite.Require().NoError(err)\n+\n+\t// clear any connection refused errors left after the previous tests\n+\tnodes := apiSuite.DiscoverNodes().Nodes()\n+\n+\tif len(nodes) > 0 {\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n+\t\tdefer cancel()\n+\n+\t\tfor i := 0; i < len(nodes); i++ {\n+\t\t\t_, err = apiSuite.Client.Version(client.WithNodes(ctx, nodes...))\n+\t\t}\n+\n+\t\tapiSuite.Require().NoError(err)\n+\t}\n }\n \n // DiscoverNodes provides list of Talos nodes in the cluster.\n@@ -76,7 +91,7 @@ func (apiSuite *APISuite) DiscoverNodes() cluster.Info {\n \treturn apiSuite.discoveredNodes\n }\n \n-// RandomNode returns a random node of the specified type (or any type if no types are specified).\n+// RandomDiscoveredNode returns a random node of the specified type (or any type if no types are specified).\n func (apiSuite *APISuite) RandomDiscoveredNode(types ...machine.Type) string {\n \tnodeInfo := apiSuite.DiscoverNodes()\n \n@@ -204,7 +219,7 @@ func (apiSuite *APISuite) AssertRebooted(ctx context.Context, node string, reboo\n \n \t\tif bootIDAfter == bootIDBefore {\n \t\t\t// bootID should be different after reboot\n-\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s + %s, after %s\", node, bootIDBefore, bootIDAfter))\n+\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s, after %s\", node, bootIDBefore, bootIDAfter))\n \t\t}\n \n \t\treturn nil\n@@ -217,6 +232,38 @@ func (apiSuite *APISuite) AssertRebooted(ctx context.Context, node string, reboo\n \t}\n }\n \n+// WaitForBootDone waits for boot phase done event.\n+func (apiSuite *APISuite) WaitForBootDone(ctx context.Context) {\n+\tnodes := apiSuite.DiscoverNodes().Nodes()\n+\n+\tnodesNotDoneBooting := make(map[string]struct{})\n+\n+\tfor _, node := range nodes {\n+\t\tnodesNotDoneBooting[node] = struct{}{}\n+\t}\n+\n+\tctx, cancel := context.WithTimeout(client.WithNodes(ctx, nodes...), 3*time.Minute)\n+\tdefer cancel()\n+\n+\tapiSuite.Require().NoError(apiSuite.Client.EventsWatch(ctx, func(ch <-chan client.Event) {\n+\t\tdefer cancel()\n+\n+\t\tfor event := range ch {\n+\t\t\tif msg, ok := event.Payload.(*machineapi.SequenceEvent); ok {\n+\t\t\t\tif msg.GetAction() == machineapi.SequenceEvent_STOP && msg.GetSequence() == runtime.SequenceBoot.String() {\n+\t\t\t\t\tdelete(nodesNotDoneBooting, event.Node)\n+\n+\t\t\t\t\tif len(nodesNotDoneBooting) == 0 {\n+\t\t\t\t\t\treturn\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}, client.WithTailEvents(-1)))\n+\n+\tapiSuite.Require().Empty(nodesNotDoneBooting)\n+}\n+\n // TearDownSuite closes Talos API client.\n func (apiSuite *APISuite) TearDownSuite() {\n \tif apiSuite.Client != nil {"
    },
    {
      "filename": "internal/integration/base/base.go",
      "status": "modified",
      "patch": "@@ -52,8 +52,8 @@ type ConfiguredSuite interface {\n }\n \n // SetConfig implements ConfiguredSuite.\n-func (suite *TalosSuite) SetConfig(config TalosSuite) {\n-\t*suite = config\n+func (talosSuite *TalosSuite) SetConfig(config TalosSuite) {\n+\t*talosSuite = config\n }\n \n // NamedSuite interface provides names for test suites."
    },
    {
      "filename": "internal/integration/base/cli.go",
      "status": "modified",
      "patch": "@@ -52,7 +52,7 @@ func (cliSuite *CLISuite) DiscoverNodes() cluster.Info {\n \treturn nil\n }\n \n-// RandomNode returns a random node of the specified type (or any type if no types are specified).\n+// RandomDiscoveredNode returns a random node of the specified type (or any type if no types are specified).\n func (cliSuite *CLISuite) RandomDiscoveredNode(types ...machine.Type) string {\n \tnodeInfo := cliSuite.DiscoverNodes()\n \n@@ -97,7 +97,6 @@ func (cliSuite *CLISuite) discoverKubectl() cluster.Info {\n \n func (cliSuite *CLISuite) buildCLICmd(args []string) *exec.Cmd {\n \t// TODO: add support for calling `talosctl config endpoint` before running talosctl\n-\n \targs = append([]string{\"--talosconfig\", cliSuite.TalosConfig}, args...)\n \n \treturn exec.Command(cliSuite.TalosctlPath, args...)\n@@ -108,6 +107,7 @@ func (cliSuite *CLISuite) RunCLI(args []string, options ...RunOption) {\n \tRun(&cliSuite.Suite, cliSuite.buildCLICmd(args), options...)\n }\n \n+// RunAndWaitForMatch retries command until output matches.\n func (cliSuite *CLISuite) RunAndWaitForMatch(args []string, regex *regexp.Regexp, duration time.Duration, options ...retry.Option) {\n \tcliSuite.Assert().NoError(retry.Constant(duration, options...).Retry(func() error {\n \t\tstdout, _, err := RunAndWait(&cliSuite.Suite, cliSuite.buildCLICmd(args))"
    },
    {
      "filename": "internal/integration/base/cluster.go",
      "status": "modified",
      "patch": "@@ -25,6 +25,8 @@ func (wrapper *infoWrapper) NodesByType(t machine.Type) []string {\n \t\treturn append([]string(nil), wrapper.masterNodes...)\n \tcase machine.TypeJoin:\n \t\treturn append([]string(nil), wrapper.workerNodes...)\n+\tcase machine.TypeUnknown:\n+\t\tfallthrough\n \tdefault:\n \t\tpanic(\"unreachable\")\n \t}"
    },
    {
      "filename": "internal/integration/base/discovery_k8s.go",
      "status": "modified",
      "patch": "@@ -21,6 +21,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/constants\"\n )\n \n+//nolint: gocyclo\n func discoverNodesK8s(client *client.Client, suite *TalosSuite) (cluster.Info, error) {\n \tctx, ctxCancel := context.WithTimeout(context.Background(), time.Minute)\n \tdefer ctxCancel()"
    },
    {
      "filename": "internal/integration/base/run.go",
      "status": "modified",
      "patch": "@@ -100,7 +100,7 @@ func StdoutMatchFunc(f MatchFunc) RunOption {\n \t}\n }\n \n-// StderrtMatchFunc appends to the list of MatchFuncs to run against stderr.\n+// StderrMatchFunc appends to the list of MatchFuncs to run against stderr.\n func StderrMatchFunc(f MatchFunc) RunOption {\n \treturn func(opts *runOptions) {\n \t\topts.stderrMatchers = append(opts.stderrMatchers, f)\n@@ -124,6 +124,7 @@ func RunAndWait(suite *suite.Suite, cmd *exec.Cmd) (stdoutBuf, stderrBuf *bytes.\n \t\tif index < 0 {\n \t\t\tcontinue\n \t\t}\n+\n \t\tswitch strings.ToUpper(keyvalue[:index]) {\n \t\tcase \"PATH\":\n \t\t\tfallthrough\n@@ -142,6 +143,8 @@ func RunAndWait(suite *suite.Suite, cmd *exec.Cmd) (stdoutBuf, stderrBuf *bytes.\n }\n \n // Run executes command and asserts on its exit status/output.\n+//\n+//nolint: gocyclo\n func Run(suite *suite.Suite, cmd *exec.Cmd, options ...RunOption) {\n \tvar opts runOptions\n "
    },
    {
      "filename": "internal/integration/cli/crashdump.go",
      "status": "modified",
      "patch": "@@ -30,6 +30,7 @@ func (suite *CrashdumpSuite) TestRun() {\n \t}\n \n \targs := []string{}\n+\n \tfor _, node := range suite.Cluster.Info().Nodes {\n \t\tswitch node.Type {\n \t\tcase machine.TypeInit:\n@@ -38,6 +39,8 @@ func (suite *CrashdumpSuite) TestRun() {\n \t\t\targs = append(args, \"--control-plane-nodes\", node.PrivateIP.String())\n \t\tcase machine.TypeJoin:\n \t\t\targs = append(args, \"--worker-nodes\", node.PrivateIP.String())\n+\t\tcase machine.TypeUnknown:\n+\t\t\tpanic(\"unexpected\")\n \t\t}\n \t}\n "
    },
    {
      "filename": "internal/integration/cli/diskusage.go",
      "status": "modified",
      "patch": "@@ -39,6 +39,7 @@ func splitLine(line string) []string {\n \t\t\tcolumns = append(columns, strings.TrimSpace(part))\n \t\t}\n \t}\n+\n \treturn columns\n }\n \n@@ -54,7 +55,7 @@ func parseLine(line string) (*duInfo, error) {\n \n \tif len(columns) == 3 {\n \t\tres.node = columns[0]\n-\t\toffset += 1\n+\t\toffset++\n \t}\n \n \tsize, err := strconv.ParseInt(columns[offset], 10, 64)\n@@ -74,6 +75,7 @@ func (suite *DiskUsageSuite) TestSuccess() {\n \tnode := suite.RandomDiscoveredNode()\n \n \tvar folderSize int64 = 4096\n+\n \tsuite.RunCLI([]string{\"list\", \"--nodes\", node, folder, \"-l\"},\n \t\tbase.StdoutMatchFunc(func(stdout string) error {\n \t\t\tlines := strings.Split(strings.TrimSpace(stdout), \"\\n\")"
    },
    {
      "filename": "internal/integration/cli/gen.go",
      "status": "modified",
      "patch": "@@ -26,6 +26,7 @@ func (suite *GenSuite) SuiteName() string {\n \treturn \"cli.GenSuite\"\n }\n \n+// SetupTest ...\n func (suite *GenSuite) SetupTest() {\n \tvar err error\n \tsuite.tmpDir, err = ioutil.TempDir(\"\", \"talos\")\n@@ -37,6 +38,7 @@ func (suite *GenSuite) SetupTest() {\n \tsuite.Require().NoError(os.Chdir(suite.tmpDir))\n }\n \n+// TearDownTest ...\n func (suite *GenSuite) TearDownTest() {\n \tif suite.savedCwd != \"\" {\n \t\tsuite.Require().NoError(os.Chdir(suite.savedCwd))"
    },
    {
      "filename": "internal/integration/cli/health.go",
      "status": "modified",
      "patch": "@@ -25,6 +25,8 @@ func (suite *HealthSuite) SuiteName() string {\n }\n \n // TestClientSide does successful health check run from client-side.\n+//\n+//nolint: gocyclo\n func (suite *HealthSuite) TestClientSide() {\n \tif suite.Cluster == nil {\n \t\tsuite.T().Skip(\"Cluster is not available, skipping test\")\n@@ -47,6 +49,8 @@ func (suite *HealthSuite) TestClientSide() {\n \t\t\t\targs = append(args, \"--control-plane-nodes\", node.PrivateIP.String())\n \t\t\tcase machine.TypeJoin:\n \t\t\t\targs = append(args, \"--worker-nodes\", node.PrivateIP.String())\n+\t\t\tcase machine.TypeInit, machine.TypeUnknown:\n+\t\t\t\tpanic(\"unexpected\")\n \t\t\t}\n \t\t}\n \t} else {\n@@ -58,6 +62,8 @@ func (suite *HealthSuite) TestClientSide() {\n \t\t\t\targs = append(args, \"--control-plane-nodes\", node.PrivateIP.String())\n \t\t\tcase machine.TypeJoin:\n \t\t\t\targs = append(args, \"--worker-nodes\", node.PrivateIP.String())\n+\t\t\tcase machine.TypeUnknown:\n+\t\t\t\tpanic(\"unexpected\")\n \t\t\t}\n \t\t}\n \t}"
    },
    {
      "filename": "internal/integration/cli/restart.go",
      "status": "modified",
      "patch": "@@ -42,7 +42,7 @@ func (suite *RestartSuite) TestSystem() {\n \tsuite.RunAndWaitForMatch([]string{\"service\", \"-n\", node, \"trustd\"}, regexp.MustCompile(`EVENTS\\s+\\[Running\\]: Health check successful`), 30*time.Second)\n }\n \n-// TestKubernetes restarts K8s container.\n+// TestK8s restarts K8s container.\n func (suite *RestartSuite) TestK8s() {\n \tif testing.Short() {\n \t\tsuite.T().Skip(\"skipping in short mode\")"
    },
    {
      "filename": "internal/integration/cli/validate.go",
      "status": "modified",
      "patch": "@@ -27,6 +27,7 @@ func (suite *ValidateSuite) SuiteName() string {\n \treturn \"cli.ValidateSuite\"\n }\n \n+// SetupTest ...\n func (suite *ValidateSuite) SetupTest() {\n \tvar err error\n \tsuite.tmpDir, err = ioutil.TempDir(\"\", \"talos\")\n@@ -38,6 +39,7 @@ func (suite *ValidateSuite) SetupTest() {\n \tsuite.Require().NoError(os.Chdir(suite.tmpDir))\n }\n \n+// TearDownTest ...\n func (suite *ValidateSuite) TearDownTest() {\n \tif suite.savedCwd != \"\" {\n \t\tsuite.Require().NoError(os.Chdir(suite.savedCwd))\n@@ -53,7 +55,11 @@ func (suite *ValidateSuite) TestValidate() {\n \tsuite.RunCLI([]string{\"gen\", \"config\", \"foobar\", \"https://10.0.0.1\"})\n \n \tfor _, configFile := range []string{\"init.yaml\", \"controlplane.yaml\", \"join.yaml\"} {\n+\t\tconfigFile := configFile\n+\n \t\tfor _, mode := range []string{\"cloud\", \"container\"} {\n+\t\t\tmode := mode\n+\n \t\t\tsuite.Run(fmt.Sprintf(\"%s-%s\", configFile, mode), func() {\n \t\t\t\tsuite.RunCLI([]string{\"validate\", \"-m\", mode, \"-c\", configFile})\n \t\t\t})"
    },
    {
      "filename": "internal/integration/cli/version.go",
      "status": "modified",
      "patch": "@@ -37,7 +37,7 @@ func (suite *VersionSuite) TestShortVersion() {\n \t)\n }\n \n-// TestClientVersion verifies only client version output.\n+// TestClient verifies only client version output.\n func (suite *VersionSuite) TestClient() {\n \tsuite.RunCLI([]string{\"version\", \"--client\"},\n \t\tbase.StdoutShouldMatch(regexp.MustCompile(`Client:\\n\\s*Tag:\\s*`+regexp.QuoteMeta(suite.Version))),"
    },
    {
      "filename": "internal/integration/integration_test.go",
      "status": "modified",
      "patch": "@@ -45,6 +45,9 @@ var (\n \tstateDir         string\n )\n \n+// TestIntegration ...\n+//\n+//nolint: gocyclo\n func TestIntegration(t *testing.T) {\n \tif talosConfig == \"\" {\n \t\tt.Error(\"--talos.config is not provided\")\n@@ -139,7 +142,8 @@ func init() {\n \tflag.Int64Var(&provision_test.DefaultSettings.DiskGB, \"talos.provision.disk\", provision_test.DefaultSettings.DiskGB, \"disk size (in GiB) for each VM (provision tests only)\")\n \tflag.IntVar(&provision_test.DefaultSettings.MasterNodes, \"talos.provision.masters\", provision_test.DefaultSettings.MasterNodes, \"master node count (provision tests only)\")\n \tflag.IntVar(&provision_test.DefaultSettings.WorkerNodes, \"talos.provision.workers\", provision_test.DefaultSettings.WorkerNodes, \"worker node count (provision tests only)\")\n-\tflag.StringVar(&provision_test.DefaultSettings.TargetInstallImageRegistry, \"talos.provision.target-installer-registry\", provision_test.DefaultSettings.TargetInstallImageRegistry, \"image registry for target installer image (provision tests only)\")\n+\tflag.StringVar(&provision_test.DefaultSettings.TargetInstallImageRegistry, \"talos.provision.target-installer-registry\",\n+\t\tprovision_test.DefaultSettings.TargetInstallImageRegistry, \"image registry for target installer image (provision tests only)\")\n \tflag.StringVar(&provision_test.DefaultSettings.CustomCNIURL, \"talos.provision.custom-cni-url\", provision_test.DefaultSettings.CustomCNIURL, \"custom CNI URL for the cluster (provision tests only)\")\n \n \tallSuites = append(allSuites, api.GetAllSuites()...)"
    },
    {
      "filename": "internal/integration/k8s/version.go",
      "status": "modified",
      "patch": "@@ -32,8 +32,8 @@ func (suite *VersionSuite) TestExpectedVersion() {\n \tapiServerVersion, err := suite.DiscoveryClient.ServerVersion()\n \tsuite.Require().NoError(err)\n \n-\texpectedApiServerVersion := fmt.Sprintf(\"v%s\", constants.DefaultKubernetesVersion)\n-\tsuite.Assert().Equal(expectedApiServerVersion, apiServerVersion.GitVersion)\n+\texpectedAPIServerVersion := fmt.Sprintf(\"v%s\", constants.DefaultKubernetesVersion)\n+\tsuite.Assert().Equal(expectedAPIServerVersion, apiServerVersion.GitVersion)\n \n \tcheckKernelVersion := suite.Capabilities().RunsTalosKernel\n \n@@ -51,6 +51,7 @@ func (suite *VersionSuite) TestExpectedVersion() {\n \t\tsuite.Assert().Equal(\"linux\", node.Status.NodeInfo.OperatingSystem)\n \t\tsuite.Assert().Equal(expectedContainerRuntimeVersion, node.Status.NodeInfo.ContainerRuntimeVersion)\n \t\tsuite.Assert().Equal(expectedKubeletVersion, node.Status.NodeInfo.KubeletVersion)\n+\n \t\tif checkKernelVersion {\n \t\t\tsuite.Assert().Equal(expectedKernelVersion, node.Status.NodeInfo.KernelVersion)\n \t\t}"
    },
    {
      "filename": "internal/integration/provision/upgrade.go",
      "status": "modified",
      "patch": "@@ -131,6 +131,8 @@ func upgradeLastReleaseToCurrent() upgradeSpec {\n }\n \n // upgradeSingeNodePreserve upgrade last release of Talos to the current version of Talos for single-node cluster with preserve.\n+//\n+//nolint: deadcode,unused\n func upgradeSingeNodePreserve() upgradeSpec {\n \treturn upgradeSpec{\n \t\tShortName: fmt.Sprintf(\"preserve-%s-%s\", nextVersion, DefaultSettings.CurrentVersion),\n@@ -151,6 +153,7 @@ func upgradeSingeNodePreserve() upgradeSpec {\n \t}\n }\n \n+// UpgradeSuite ...\n type UpgradeSuite struct {\n \tsuite.Suite\n \tbase.TalosSuite\n@@ -242,7 +245,7 @@ func (suite *UpgradeSuite) setupCluster() {\n \tsuite.stateDir, err = ioutil.TempDir(\"\", \"talos-integration\")\n \tsuite.Require().NoError(err)\n \n-\tsuite.T().Logf(\"initalizing provisioner with cluster name %q, state directory %q\", clusterName, suite.stateDir)\n+\tsuite.T().Logf(\"initializing provisioner with cluster name %q, state directory %q\", clusterName, suite.stateDir)\n \n \trequest := provision.ClusterRequest{\n \t\tName: clusterName,\n@@ -399,7 +402,7 @@ func (suite *UpgradeSuite) assertSameVersionCluster(client *talosclient.Client,\n \t}\n }\n \n-func (suite *UpgradeSuite) readVersion(client *talosclient.Client, nodeCtx context.Context) (version string, err error) {\n+func (suite *UpgradeSuite) readVersion(nodeCtx context.Context, client *talosclient.Client) (version string, err error) {\n \tvar v *machineapi.VersionResponse\n \n \tv, err = client.Version(nodeCtx)\n@@ -442,7 +445,7 @@ func (suite *UpgradeSuite) upgradeNode(client *talosclient.Client, node provisio\n \tsuite.Require().NoError(retry.Constant(10 * time.Minute).Retry(func() error {\n \t\tvar version string\n \n-\t\tversion, err = suite.readVersion(client, nodeCtx)\n+\t\tversion, err = suite.readVersion(nodeCtx, client)\n \t\tif err != nil {\n \t\t\t// API might be unresponsive during upgrade\n \t\t\treturn retry.ExpectedError(err)"
    }
  ],
  "fix_category": "WaitFor",
  "root_cause_category": "Async wait",
  "root_cause_subcategory": NaN
}