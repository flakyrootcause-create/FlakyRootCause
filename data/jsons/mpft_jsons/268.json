{
  "id": 268,
  "repo": "vllm",
  "issue_url": "https://github.com/vllm-project/vllm/commit/4ab3ac285e824542831c4326d01ce84bd8b65aad",
  "pr_url": "https://github.com/vllm-project/vllm/commit/4ab3ac285e824542831c4326d01ce84bd8b65aad",
  "issue_description": "[PR Linked Issue]\nReally love this repo, I've been using it to finetune CodeGen models with >2k context windows.\r\n\r\nIt's way faster than hugging face (3x) and slightly faster than Megatron for the 350M and 2.7b parameter CodeGen models but doesn't work for the 6.1B and 16B parameter models as they have a head dimension of 256.\r\n\r\n<img width=\"1006\" alt=\"Screen Shot 2022-11-01 at 5 32 47 PM\" src=\"https://user-images.githubusercontent.com/17725268/199345886-f8b7531e-9918-4fba-ab37-4ae980ec2796.png\">\r\n\r\nI would imagine CodeGen finetuning will be a solid use-case for flash attention since coding models can really benefit from long context windows. And CodeGen is basically SOTA for coding (competitive with Codex).\r\n\r\nIs this something that is even possible with flash attention?",
  "files_changed": [
    {
      "filename": "README.md",
      "status": "modified",
      "patch": "@@ -4,6 +4,7 @@\n \n ```bash\n pip install cmake torch transformers\n+pip install flash-attn # This may take up to 10 mins.\n pip install -e .\n ```\n "
    },
    {
      "filename": "cacheflow/models/attention.py",
      "status": "modified",
      "patch": "@@ -1,5 +1,6 @@\n from typing import List, Optional\n \n+from flash_attn.flash_attention import FlashAttention\n import torch\n import torch.nn as nn\n \n@@ -14,20 +15,7 @@ def __init__(self, scale: float) -> None:\n         super().__init__()\n         self.scale = float(scale)\n \n-    def _masked_attention(\n-        self,\n-        query: torch.Tensor,                        # [num_queries, num_heads, head_size]\n-        key: torch.Tensor,                          # [num_keys, num_heads, head_size]\n-        value: torch.Tensor,                        # [num_keys, num_heads, head_size]\n-        attn_mask: Optional[torch.Tensor] = None,   # [num_queries, num_keys]\n-    ) -> torch.Tensor:                              # [num_queries, num_heads, head_size]\n-        query = query * self.scale\n-        attn = torch.einsum('qhd,khd->hqk', query, key)\n-        if attn_mask is not None:\n-            attn = attn + attn_mask\n-        attn = torch.softmax(attn, dim=-1)\n-        out = torch.einsum('hqk,khd->qhd', attn, value)\n-        return out\n+        self.flash_attn = FlashAttention(softmax_scale=self.scale)\n \n     def multi_query_kv_attention(\n         self,\n@@ -37,21 +25,31 @@ def multi_query_kv_attention(\n         value: torch.Tensor,        # [num_prompt_tokens, num_heads, head_size]\n         prompt_lens: List[int],\n     ) -> None:\n-        # FIXME(woosuk): Replace the following with a custom op.\n-        start_idx = 0\n+        if query.dtype == torch.float:\n+            raise ValueError('The float data type is not supported by '\n+                             'FlashAttention. Use the half data type instead.')\n+        head_size = query.shape[2]\n+        if head_size > 128:\n+            raise ValueError('FlashAttention does not support head_size > 128.')\n+\n+        device = query.device\n+        prefix_sum = [0]\n         for prompt_len in prompt_lens:\n-            out = output[start_idx:start_idx + prompt_len]\n-            q = query[start_idx:start_idx + prompt_len]\n-            k = key[start_idx:start_idx + prompt_len]\n-            v = value[start_idx:start_idx + prompt_len]\n-\n-            attention_mask = torch.triu(\n-                torch.ones(q.shape[0], k.shape[0]), diagonal=1) * -1e5\n-            attention_mask = attention_mask.to(dtype=q.dtype, device=q.device)\n-            attention_out = self._masked_attention(q, k, v, attention_mask)\n-            out.copy_(attention_out, non_blocking=True)\n-\n-            start_idx += prompt_len\n+            prefix_sum.append(prefix_sum[-1] + prompt_len)\n+        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n+        max_prompt_len = max(prompt_lens)\n+\n+        # FIXME(woosuk): Unnecessary copy. Optimize this.\n+        qkv = torch.stack([query, key, value], dim=1)\n+        out = self.flash_attn(\n+            qkv,\n+            cu_seqlens=prefix_sum,\n+            max_s=max_prompt_len,\n+            causal=True,\n+        )[0]\n+        num_tokens = prefix_sum[-1]\n+        # FIXME(woosuk): Unnecessary copy. Optimize this.\n+        output[:num_tokens].copy_(out, non_blocking=True)\n \n     def single_query_cached_kv_attention(\n         self,\n@@ -61,6 +59,14 @@ def single_query_cached_kv_attention(\n         value_cache: torch.Tensor,      # [num_blocks, num_heads, head_size, block_size]\n         input_metadata: InputMetadata,\n     ) -> None:\n+        head_size = value_cache.shape[2]\n+        supported_head_sizes = [32, 64, 80, 96, 128, 160, 192, 256]\n+        if head_size not in supported_head_sizes:\n+            raise ValueError(f'head_size ({head_size}) is not supported by '\n+                             'the single_query_cached_kv_attention kernel. '\n+                             'Use one of the following head sizes: '\n+                             f'{supported_head_sizes}.')\n+\n         block_size = value_cache.shape[3]\n         attention_ops.single_query_cached_kv_attention(\n             output,\n@@ -101,8 +107,9 @@ def forward(\n         output = output.view(-1, num_heads, head_size)\n \n         # Compute the attention op for prompts.\n-        self.multi_query_kv_attention(\n-            output, query, key, value, input_metadata.prompt_lens)\n+        if input_metadata.num_prompts > 0:\n+            self.multi_query_kv_attention(\n+                output, query, key, value, input_metadata.prompt_lens)\n \n         # Wait until the cache op is done.\n         if cache_event is not None:"
    },
    {
      "filename": "server.py",
      "status": "modified",
      "patch": "@@ -9,10 +9,12 @@\n parser.add_argument('--model', type=str, default='facebook/opt-125m', help='model name')\n parser.add_argument('--num-nodes', type=int, default=1, help='number of nodes')\n parser.add_argument('--num-workers', type=int, default=1, help='number of workers per node')\n-parser.add_argument('--block-size', type=int, default=8, help='token block size')\n+parser.add_argument('--block-size', type=int, default=8, choices=[8, 16], help='token block size')\n # TODO(woosuk): Add an analytical model to determine the maximum number of GPU/CPU blocks.\n parser.add_argument('--num-gpu-blocks', type=int, default=1024, help='number of GPU blocks (per GPU)')\n-parser.add_argument('--num-cpu-blocks', type=int, default=256, help='number of CPU blocks (per GPU)')\n+parser.add_argument('--num-cpu-blocks', type=int, default=32, help='number of CPU blocks (per GPU)')\n+# NOTE(woosuk): If FlashAttention is used, the float data type is not supported.\n+parser.add_argument('--dtype', type=str, default='half', choices=['half', 'float'], help='data type')\n args = parser.parse_args()\n \n \n@@ -27,6 +29,7 @@ def main():\n             block_size=args.block_size,\n             num_gpu_blocks=args.num_gpu_blocks,\n             num_cpu_blocks=args.num_cpu_blocks,\n+            dtype=args.dtype,\n         )\n         controllers.append(controller)\n "
    },
    {
      "filename": "tests/kernels/attention.py",
      "status": "modified",
      "patch": "@@ -1,10 +1,13 @@\n import random\n from typing import Optional\n \n+from flash_attn.flash_attention import FlashAttention\n import torch\n \n from cacheflow import attention_ops\n \n+MAX_SEQ_LEN = 4096\n+\n \n def ref_masked_attention(\n     query: torch.Tensor,\n@@ -79,7 +82,7 @@ def test_single_query_cached_kv_attention(\n     value_cache = torch.randn(\n         size=(num_blocks, *value_block_shape), dtype=dtype, device='cuda')\n \n-    context_lens = [random.randint(1, 4096) for _ in range(num_tokens)] \n+    context_lens = [random.randint(1, MAX_SEQ_LEN) for _ in range(num_tokens)] \n     max_context_len = max(context_lens)\n     context_lens = torch.tensor(context_lens, dtype=torch.int, device='cuda')\n \n@@ -123,11 +126,60 @@ def test_single_query_cached_kv_attention(\n     assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n \n \n+def test_multi_query_kv_attention(\n+    num_seqs: int,\n+    num_heads: int,\n+    head_size: int,\n+    dtype: torch.dtype,\n+) -> None:\n+    seq_lens = random.sample(range(1, MAX_SEQ_LEN), num_seqs)\n+    max_seq_len = max(seq_lens)\n+    num_tokens = sum(seq_lens)\n+\n+    cu_seq_lens = [0]\n+    for seq_len in seq_lens:\n+        cu_seq_lens.append(cu_seq_lens[-1] + seq_len)\n+    cu_seq_lens = torch.tensor(cu_seq_lens, dtype=torch.int, device='cuda')\n+\n+    scale = float(1.0 / (head_size ** 0.5))\n+    query = torch.randn(\n+        num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n+    key = torch.rand_like(query)\n+    value = torch.rand_like(query)\n+\n+    qkv = torch.stack([query, key, value], dim=1)\n+    flash_attn = FlashAttention(softmax_scale=scale)\n+    output = flash_attn(\n+        qkv,\n+        cu_seqlens=cu_seq_lens,\n+        max_s=max_seq_len,\n+        causal=True,\n+    )[0]\n+\n+    ref_outputs = []\n+    for i, seq_len in enumerate(seq_lens):\n+        attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * -1e5\n+        attn_mask = attn_mask.to(dtype=dtype, device='cuda')\n+        start_idx = cu_seq_lens[i]\n+        end_idx = cu_seq_lens[i + 1]\n+        ref_output = ref_masked_attention(\n+            query[start_idx:end_idx],\n+            key[start_idx:end_idx],\n+            value[start_idx:end_idx],\n+            scale,\n+            attn_mask=attn_mask,\n+        )\n+        ref_outputs.append(ref_output)\n+    ref_output = torch.cat(ref_outputs, dim=0)\n+\n+    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n+\n+\n @torch.inference_mode()\n def test_attention() -> None:\n     for dtype in [torch.half, torch.float]:\n         for block_size in [8, 16]:\n-            for head_size in [64, 80, 96, 128, 256]:\n+            for head_size in [32, 64, 80, 96, 128, 160, 192, 256]:\n                 test_single_query_cached_kv_attention(\n                     num_tokens=37,\n                     num_heads=3,\n@@ -137,6 +189,17 @@ def test_attention() -> None:\n                     dtype=dtype,\n                 )\n \n+    # NOTE(woosuk): FlashAttention does not support FP32.\n+    for dtype in [torch.half]:\n+        # NOTE(woosuk): FlashAttention does not support head_size > 128.\n+        for head_size in [64, 80, 96, 128]:\n+            test_multi_query_kv_attention(\n+                num_seqs=11,\n+                num_heads=3,\n+                head_size=head_size,\n+                dtype=dtype,\n+            )\n+\n \n if __name__ == '__main__':\n     test_attention()"
    }
  ],
  "fix_category": "Other",
  "root_cause_category": "Randomness",
  "root_cause_subcategory": NaN
}