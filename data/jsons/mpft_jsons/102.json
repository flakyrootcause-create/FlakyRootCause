{
  "id": 102,
  "repo": "nifi",
  "issue_url": "https://github.com/apache/nifi/pull/6748",
  "pr_url": "https://github.com/apache/nifi/pull/6748",
  "issue_description": "<!-- Licensed to the Apache Software Foundation (ASF) under one or more -->\r\n<!-- contributor license agreements.  See the NOTICE file distributed with -->\r\n<!-- this work for additional information regarding copyright ownership. -->\r\n<!-- The ASF licenses this file to You under the Apache License, Version 2.0 -->\r\n<!-- (the \"License\"); you may not use this file except in compliance with -->\r\n<!-- the License.  You may obtain a copy of the License at -->\r\n<!--     http://www.apache.org/licenses/LICENSE-2.0 -->\r\n<!-- Unless required by applicable law or agreed to in writing, software -->\r\n<!-- distributed under the License is distributed on an \"AS IS\" BASIS, -->\r\n<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->\r\n<!-- See the License for the specific language governing permissions and -->\r\n<!-- limitations under the License. -->\r\n\r\n# Summary\r\n\r\nTests in the testSchemaWithCoreAttribuesToAttribute(), testSchemaToContent(), testSchemaWithCoreAttribuesToContent() and testSchemaToAttribute() under TestAttributesToCSV express non-deterministic behavior and change the order of the attributes. The fix is changing HashMap to LinkedHashMap in the test and Set to LinkedHashSet to ensure deterministic behavior.\r\n\r\n[NIFI-10928](https://issues.apache.org/jira/browse/NIFI-10928)\r\n\r\n# Tracking\r\n\r\nPlease complete the following tracking steps prior to pull request creation.\r\n\r\n### Issue Tracking\r\n\r\n- [x] [Apache NiFi Jira](https://issues.apache.org/jira/browse/NIFI) issue created\r\n\r\n### Pull Request Tracking\r\n\r\n- [x] Pull Request title starts with Apache NiFi Jira issue number, such as `NIFI-00000`\r\n- [x] Pull Request commit message starts with Apache NiFi Jira issue number, as such `NIFI-00000`\r\n\r\n### Pull Request Formatting\r\n\r\n- [x] Pull Request based on current revision of the `main` branch\r\n- [x] Pull Request refers to a feature branch with one commit containing changes\r\n\r\n# Verification\r\n\r\nPlease indicate the verification steps performed prior to pull request creation.\r\n\r\n### Build\r\n\r\n- [ ] Build completed using `mvn clean install -P contrib-check`\r\n  - [ ] JDK 8\r\n  - [x] JDK 11\r\n  - [ ] JDK 17\r\n\r\n### Licensing\r\n\r\n- [ ] New dependencies are compatible with the [Apache License 2.0](https://apache.org/licenses/LICENSE-2.0) according to the [License Policy](https://www.apache.org/legal/resolved.html)\r\n- [ ] New dependencies are documented in applicable `LICENSE` and `NOTICE` files\r\n\r\n### Documentation\r\n\r\n- [ ] Documentation formatting appears as expected in rendered files\r\n",
  "files_changed": [
    {
      "filename": "nifi-mock/src/main/java/org/apache/nifi/util/MockFlowFile.java",
      "status": "modified",
      "patch": "@@ -28,11 +28,12 @@\n import java.nio.file.StandardOpenOption;\n import java.util.Arrays;\n import java.util.Collections;\n-import java.util.HashMap;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Set;\n import java.util.UUID;\n+\n import org.apache.commons.lang3.builder.HashCodeBuilder;\n import org.apache.nifi.controller.repository.FlowFileRecord;\n import org.apache.nifi.controller.repository.claim.ContentClaim;\n@@ -42,7 +43,7 @@\n \n public class MockFlowFile implements FlowFileRecord {\n \n-    private final Map<String, String> attributes = new HashMap<>();\n+    private final Map<String, String> attributes = new LinkedHashMap<>();\n \n     private final long id;\n     private final long entryDate;"
    },
    {
      "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-site-to-site/src/test/java/org/apache/nifi/remote/protocol/http/TestHttpFlowFileServerProtocol.java",
      "status": "modified",
      "patch": "@@ -282,7 +282,7 @@ public void testTransferOneFile() throws Exception {\n         });\n \n         // Commit transaction\n-        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"3229577812\");\n+        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"1853411835\");\n         assertEquals(1, flowFileSent);\n \n         // Assert provenance\n@@ -378,7 +378,7 @@ public void testTransferTwoFiles() throws Exception {\n         );\n \n         // Commit transaction\n-        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"3058746557\");\n+        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"2295235398\");\n         assertEquals(2, flowFileSent);\n \n         // Assert provenance (SEND and DROP)"
    },
    {
      "filename": "nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/AttributesToCSV.java",
      "status": "modified",
      "patch": "@@ -271,7 +271,7 @@ private LinkedHashSet<String> attributeListStringToSet(String attributeList) {\n     @OnScheduled\n     public void onScheduled(ProcessContext context) {\n         includeCoreAttributes = context.getProperty(INCLUDE_CORE_ATTRIBUTES).asBoolean();\n-        coreAttributes = Arrays.stream(CoreAttributes.values()).map(CoreAttributes::key).collect(Collectors.toSet());\n+        coreAttributes = Arrays.stream(CoreAttributes.values()).map(CoreAttributes::key).collect(Collectors.toCollection(LinkedHashSet::new));\n         destinationContent = OUTPUT_OVERWRITE_CONTENT.getValue().equals(context.getProperty(DESTINATION).getValue());\n         nullValForEmptyString = context.getProperty(NULL_VALUE_FOR_EMPTY_STRING).asBoolean();\n         includeSchema = context.getProperty(INCLUDE_SCHEMA).asBoolean();"
    },
    {
      "filename": "nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/java/org/apache/nifi/processors/standard/TestAttributesToCSV.java",
      "status": "modified",
      "patch": "@@ -25,6 +25,7 @@\n \n import java.io.IOException;\n import java.util.Arrays;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.HashMap;\n import java.util.Set;\n@@ -689,7 +690,7 @@ public void testSchemaToAttribute() {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n@@ -721,7 +722,7 @@ public void testSchemaToContent() throws Exception {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n@@ -754,7 +755,7 @@ public void testSchemaWithCoreAttribuesToAttribute() {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n@@ -788,7 +789,7 @@ public void testSchemaWithCoreAttribuesToContent() throws Exception {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");"
    }
  ],
  "fix_category": "Other",
  "root_cause_category": "Unordered collections",
  "root_cause_subcategory": NaN
}