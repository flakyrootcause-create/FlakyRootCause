{
  "id": 283,
  "repo": "kafka",
  "issue_url": "https://github.com/apache/kafka/pull/16094",
  "pr_url": "https://github.com/apache/kafka/pull/16094",
  "issue_description": "Resolves : https://issues.apache.org/jira/browse/KAFKA-16518\r\n\r\n- Adds a new argument \"standalone\" to kafka-storage.sh\r\n- If standalone mode, creates a checkpoint file in metadata dir ${kafkaConfig.metadataLogDir}/__cluster_metadata-0/\r\n\r\n\r\n### Committer Checklist (excluded from commit message)\r\n- [ ] Verify design and implementation \r\n- [ ] Verify test coverage and CI build status\r\n- [ ] Verify documentation (including upgrade notes)\r\n",
  "files_changed": [
    {
      "filename": "build.gradle",
      "status": "modified",
      "patch": "@@ -202,27 +202,13 @@ def determineCommitId() {\n }\n \n def excludedSpotlessModules = [':clients',\n-                               ':connect:api',\n-                               ':connect:basic-auth-extension',\n-                               ':connect:file',\n-                               ':connect:json',\n-                               ':connect:mirror',\n-                               ':connect:mirror-client',\n                                ':connect:runtime',\n-                               ':connect:test-plugins',\n-                               ':connect:transforms',\n                                ':core',\n-                               ':examples',\n-                               ':generator',\n                                ':group-coordinator:group-coordinator-api', // https://github.com/apache/kafka/pull/16198\n                                ':group-coordinator',\n-                               ':jmh-benchmarks',\n-                               ':log4j-appender',\n                                ':metadata',\n                                ':raft',\n                                ':server',\n-                               ':server-common',\n-                               ':shell',\n                                ':storage',\n                                ':storage:storage-api', //  rename in settings.gradle\n                                ':streams',\n@@ -251,11 +237,7 @@ def excludedSpotlessModules = [':clients',\n                                ':streams:upgrade-system-tests-34',\n                                ':streams:upgrade-system-tests-35',\n                                ':streams:upgrade-system-tests-36',\n-                               ':streams:upgrade-system-tests-37',\n-                               ':tools',\n-                               ':tools:tools-api',\n-                               ':transaction-coordinator',\n-                               ':trogdor']\n+                               ':streams:upgrade-system-tests-37']\n \n \n apply from: file('wrapper.gradle')\n@@ -445,9 +427,6 @@ subprojects {\n     }\n   }\n \n-  // Remove the relevant project name once it's converted to JUnit 5\n-  def shouldUseJUnit5 = !([\"runtime\"].contains(it.project.name))\n-\n   def testLoggingEvents = [\"passed\", \"skipped\", \"failed\"]\n   def testShowStandardStreams = false\n   def testExceptionFormat = 'full'\n@@ -522,13 +501,6 @@ subprojects {\n   // The suites are for running sets of tests in IDEs.\n   // Gradle will run each test class, so we exclude the suites to avoid redundantly running the tests twice.\n   def testsToExclude = ['**/*Suite.class']\n-  // Exclude PowerMock tests when running with Java 16 or newer until a version of PowerMock that supports the relevant versions is released\n-  // The relevant issues are https://github.com/powermock/powermock/issues/1094 and https://github.com/powermock/powermock/issues/1099\n-  if (JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_16)) {\n-    testsToExclude.addAll([\n-      // connect tests\n-    ])\n-  }\n \n   test {\n     maxParallelForks = maxTestForks\n@@ -547,8 +519,7 @@ subprojects {\n \n     exclude testsToExclude\n \n-    if (shouldUseJUnit5)\n-      useJUnitPlatform()\n+    useJUnitPlatform()\n \n     retry {\n       maxRetries = userMaxTestRetries\n@@ -575,23 +546,18 @@ subprojects {\n \n     exclude testsToExclude\n \n-    if (shouldUseJUnit5) {\n-      if (project.name == 'streams') {\n-        useJUnitPlatform {\n-          includeTags \"integration\"\n-          includeTags \"org.apache.kafka.test.IntegrationTest\"\n-\t  // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n-          // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n-          includeEngines \"junit-vintage\", \"junit-jupiter\"\n-        }\n-      } else {\n-        useJUnitPlatform {\n-          includeTags \"integration\"\n-        }\n+    if (project.name == 'streams') {\n+      useJUnitPlatform {\n+        includeTags \"integration\"\n+        includeTags \"org.apache.kafka.test.IntegrationTest\"\n+        // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n+        // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n+        includeEngines \"junit-vintage\", \"junit-jupiter\"\n       }\n     } else {\n-      useJUnit {\n-        includeCategories 'org.apache.kafka.test.IntegrationTest'\n+      useJUnitPlatform {\n+        includeTags \"integration\"\n+        includeTags 'org.apache.kafka.test.IntegrationTest'\n       }\n     }\n \n@@ -618,23 +584,18 @@ subprojects {\n \n     exclude testsToExclude\n \n-    if (shouldUseJUnit5) {\n-      if (project.name == 'streams') {\n-        useJUnitPlatform {\n-          excludeTags \"integration\"\n-          excludeTags \"org.apache.kafka.test.IntegrationTest\"\n-\t  // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n-          // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n-          includeEngines \"junit-vintage\", \"junit-jupiter\"\n-        }\n-      } else {\n-        useJUnitPlatform {\n-          excludeTags \"integration\"\n-        }\n+    if (project.name == 'streams') {\n+      useJUnitPlatform {\n+        excludeTags \"integration\"\n+        excludeTags \"org.apache.kafka.test.IntegrationTest\"\n+        // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n+        // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n+        includeEngines \"junit-vintage\", \"junit-jupiter\"\n       }\n     } else {\n-      useJUnit {\n-        excludeCategories 'org.apache.kafka.test.IntegrationTest'\n+      useJUnitPlatform {\n+        excludeTags \"integration\"\n+        excludeTags 'org.apache.kafka.test.IntegrationTest'\n       }\n     }\n \n@@ -825,10 +786,7 @@ subprojects {\n       jacoco {\n         toolVersion = versions.jacoco\n       }\n-\n-      // NOTE: Jacoco Gradle plugin does not support \"offline instrumentation\" this means that classes mocked by PowerMock\n-      // may report 0 coverage, since the source was modified after initial instrumentation.\n-      // See https://github.com/jacoco/jacoco/issues/51\n+      \n       jacocoTestReport {\n         dependsOn tasks.test\n         sourceSets sourceSets.main\n@@ -852,8 +810,10 @@ subprojects {\n     skipProjects = [ \":jmh-benchmarks\", \":trogdor\" ]\n     skipConfigurations = [ \"zinc\" ]\n   }\n-  \n-  if(JavaVersion.current().isJava11Compatible() && project.path !in excludedSpotlessModules) {\n+  //  the task `removeUnusedImports` is implemented by google-java-format, \n+  //  and unfortunately the google-java-format version used by spotless 6.14.0 can't work with JDK 21. \n+  //  Hence, we apply spotless tasks only if the env is either JDK11 or JDK17\n+  if ((JavaVersion.current().isJava11() || (JavaVersion.current() == JavaVersion.VERSION_17)) && project.path !in excludedSpotlessModules) {\n     apply plugin: 'com.diffplug.spotless'\n     spotless {\n       java {\n@@ -2177,7 +2137,6 @@ project(':tools') {\n   }\n \n   dependencies {\n-    compileOnly libs.slf4jlog4j\n     implementation project(':clients')\n     implementation project(':storage')\n     implementation project(':server-common')\n@@ -2188,6 +2147,7 @@ project(':tools') {\n     implementation libs.jacksonDataformatCsv\n     implementation libs.jacksonJDK8Datatypes\n     implementation libs.slf4jApi\n+    implementation libs.slf4jlog4j\n     implementation libs.joptSimple\n \n     implementation libs.jose4j                    // for SASL/OAUTHBEARER JWT validation\n@@ -2214,7 +2174,6 @@ project(':tools') {\n       exclude group: 'junit', module: 'junit'\n     }\n     testImplementation libs.log4j\n-    testRuntimeOnly libs.slf4jlog4j\n   }\n \n   javadoc {\n@@ -2376,7 +2335,6 @@ project(':streams') {\n     testImplementation libs.log4j\n     testImplementation libs.junitJupiter\n     testImplementation libs.junitVintageEngine\n-    testImplementation libs.easymock\n     testImplementation libs.bcpkix\n     testImplementation libs.hamcrest\n     testImplementation libs.mockitoCore\n@@ -3258,12 +3216,12 @@ project(':connect:runtime') {\n     testImplementation project(':connect:test-plugins')\n     testImplementation project(':group-coordinator')\n \n-    testImplementation libs.easymock\n-    testImplementation libs.junitJupiterApi\n+    testImplementation libs.junitJupiter\n     testImplementation libs.junitVintageEngine\n-    testImplementation libs.powermockJunit4\n-    testImplementation libs.powermockEasymock\n+    testImplementation libs.mockitoJunitJupiter\n     testImplementation libs.mockitoCore\n+    testImplementation libs.hamcrest\n+    testImplementation libs.mockitoJunitJupiter\n     testImplementation libs.httpclient\n \n     testRuntimeOnly libs.slf4jlog4j"
    },
    {
      "filename": "checkstyle/checkstyle.xml",
      "status": "modified",
      "patch": "@@ -150,6 +150,9 @@\n \n     <!-- Allows the use of the @SuppressWarnings annotation in the code -->\n     <module name=\"SuppressWarningsHolder\"/>\n+\n+    <module name=\"ModifierOrder\"/>\n+    \n   </module>\n \n   <module name=\"SuppressionFilter\">"
    },
    {
      "filename": "checkstyle/import-control-core.xml",
      "status": "modified",
      "patch": "@@ -38,6 +38,7 @@\n   <allow pkg=\"org.apache.kafka.common\" />\n   <allow pkg=\"org.mockito\" class=\"AssignmentsManagerTest\"/>\n   <allow pkg=\"org.apache.kafka.server\"/>\n+  <allow pkg=\"org.opentest4j\" class=\"RemoteLogManagerTest\"/>\n   <!-- see KIP-544 for why KafkaYammerMetrics should be used instead of the global default yammer metrics registry\n        https://cwiki.apache.org/confluence/display/KAFKA/KIP-544%3A+Make+metrics+exposed+via+JMX+configurable -->\n   <disallow class=\"com.yammer.metrics.Metrics\" />"
    },
    {
      "filename": "checkstyle/import-control.xml",
      "status": "modified",
      "patch": "@@ -30,8 +30,6 @@\n   <allow pkg=\"org.opentest4j\" />\n   <allow pkg=\"org.hamcrest\" />\n   <allow pkg=\"org.mockito\" />\n-  <allow pkg=\"org.easymock\" />\n-  <allow pkg=\"org.powermock\" />\n   <allow pkg=\"java.security\" />\n   <allow pkg=\"javax.net.ssl\" />\n   <allow pkg=\"javax.security\" />\n@@ -622,9 +620,6 @@\n     <subpackage name=\"file\">\n       <allow pkg=\"org.apache.kafka.connect\" />\n       <allow pkg=\"org.apache.kafka.clients.consumer\" />\n-      <!-- for tests -->\n-      <allow pkg=\"org.easymock\" />\n-      <allow pkg=\"org.powermock\" />\n     </subpackage>\n \n     <subpackage name=\"tools\">"
    },
    {
      "filename": "checkstyle/suppressions.xml",
      "status": "modified",
      "patch": "@@ -200,7 +200,7 @@\n               files=\"StreamThread.java\"/>\n \n     <suppress checks=\"ClassDataAbstractionCoupling\"\n-              files=\"(InternalTopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl).java\"/>\n+              files=\"(InternalTopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamsPartitionAssignor).java\"/>\n \n     <suppress checks=\"CyclomaticComplexity\"\n               files=\"(KafkaStreams|StreamsPartitionAssignor|StreamThread|TaskManager|PartitionGroup|SubscriptionWrapperSerde|AssignorConfiguration).java\"/>\n@@ -209,7 +209,7 @@\n               files=\"StreamsMetricsImpl.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n-              files=\"(KafkaStreams|StreamsPartitionAssignor|StreamThread|TaskManager|GlobalStateManagerImpl|KStreamImplJoin|TopologyConfig|KTableKTableOuterJoin).java\"/>\n+              files=\"(KafkaStreams|StreamsPartitionAssignor|StreamThread|TaskManager|TaskAssignmentUtils|GlobalStateManagerImpl|KStreamImplJoin|TopologyConfig|KTableKTableOuterJoin).java\"/>\n \n     <suppress checks=\"(FinalLocalVariable|UnnecessaryParentheses|BooleanExpressionComplexity|CyclomaticComplexity|WhitespaceAfter|LocalVariableName)\"\n               files=\"Murmur3.java\"/>"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/ClientUtils.java",
      "status": "modified",
      "patch": "@@ -245,7 +245,9 @@ public static NetworkClient createNetworkClient(AbstractConfig config,\n                     throttleTimeSensor,\n                     logContext,\n                     hostResolver,\n-                    clientTelemetrySender);\n+                    clientTelemetrySender,\n+                    MetadataRecoveryStrategy.forName(config.getString(CommonClientConfigs.METADATA_RECOVERY_STRATEGY_CONFIG))\n+            );\n         } catch (Throwable t) {\n             closeQuietly(selector, \"Selector\");\n             closeQuietly(channelBuilder, \"ChannelBuilder\");"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/ClusterConnectionStates.java",
      "status": "modified",
      "patch": "@@ -37,10 +37,10 @@\n  *\n  */\n final class ClusterConnectionStates {\n-    final static int RECONNECT_BACKOFF_EXP_BASE = 2;\n-    final static double RECONNECT_BACKOFF_JITTER = 0.2;\n-    final static int CONNECTION_SETUP_TIMEOUT_EXP_BASE = 2;\n-    final static double CONNECTION_SETUP_TIMEOUT_JITTER = 0.2;\n+    static final int RECONNECT_BACKOFF_EXP_BASE = 2;\n+    static final double RECONNECT_BACKOFF_JITTER = 0.2;\n+    static final int CONNECTION_SETUP_TIMEOUT_EXP_BASE = 2;\n+    static final double CONNECTION_SETUP_TIMEOUT_JITTER = 0.2;\n     private final Map<String, NodeConnectionState> nodeState;\n     private final Logger log;\n     private final HostResolver hostResolver;"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/CommonClientConfigs.java",
      "status": "modified",
      "patch": "@@ -219,6 +219,19 @@ public class CommonClientConfigs {\n     public static final String DEFAULT_API_TIMEOUT_MS_DOC = \"Specifies the timeout (in milliseconds) for client APIs. \" +\n             \"This configuration is used as the default timeout for all client operations that do not specify a <code>timeout</code> parameter.\";\n \n+    public static final String METADATA_RECOVERY_STRATEGY_CONFIG = \"metadata.recovery.strategy\";\n+    public static final String METADATA_RECOVERY_STRATEGY_DOC = \"Controls how the client recovers when none of the brokers known to it is available. \" +\n+            \"If set to <code>none</code>, the client fails. If set to <code>rebootstrap</code>, \" +\n+            \"the client repeats the bootstrap process using <code>bootstrap.servers</code>. \" +\n+            \"Rebootstrapping is useful when a client communicates with brokers so infrequently \" +\n+            \"that the set of brokers may change entirely before the client refreshes metadata. \" +\n+            \"Metadata recovery is triggered when all last-known brokers appear unavailable simultaneously. \" +\n+            \"Brokers appear unavailable when disconnected and no current retry attempt is in-progress. \" +\n+            \"Consider increasing <code>reconnect.backoff.ms</code> and <code>reconnect.backoff.max.ms</code> and \" +\n+            \"decreasing <code>socket.connection.setup.timeout.ms</code> and <code>socket.connection.setup.timeout.max.ms</code> \" +\n+            \"for the client.\";\n+    public static final String DEFAULT_METADATA_RECOVERY_STRATEGY = MetadataRecoveryStrategy.NONE.name;\n+\n     /**\n      * Postprocess the configuration so that exponential backoff is disabled when reconnect backoff\n      * is explicitly configured but the maximum reconnect backoff is not explicitly configured."
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/KafkaClient.java",
      "status": "modified",
      "patch": "@@ -130,7 +130,7 @@ public interface KafkaClient extends Closeable {\n      * @param now The current time in ms\n      * @return The node with the fewest in-flight requests.\n      */\n-    Node leastLoadedNode(long now);\n+    LeastLoadedNode leastLoadedNode(long now);\n \n     /**\n      * The number of currently in-flight requests for which we have not yet returned a response"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/LeastLoadedNode.java",
      "status": "added",
      "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients;\n+\n+import org.apache.kafka.common.Node;\n+\n+public class LeastLoadedNode {\n+    private final Node node;\n+    private final boolean atLeastOneConnectionReady;\n+\n+    public LeastLoadedNode(Node node, boolean atLeastOneConnectionReady) {\n+        this.node = node;\n+        this.atLeastOneConnectionReady = atLeastOneConnectionReady;\n+    }\n+\n+    public Node node() {\n+        return node;\n+    }\n+\n+    /**\n+     * Indicates if the least loaded node is available or at least a ready connection exists.\n+     *\n+     * <p>There may be no node available while ready connections to live nodes exist. This may happen when\n+     * the connections are overloaded with in-flight requests. This function takes this into account.\n+     */\n+    public boolean hasNodeAvailableOrConnectionReady() {\n+        return node != null || atLeastOneConnectionReady;\n+    }\n+}"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/Metadata.java",
      "status": "modified",
      "patch": "@@ -82,6 +82,8 @@ public class Metadata implements Closeable {\n     private final ClusterResourceListeners clusterResourceListeners;\n     private boolean isClosed;\n     private final Map<TopicPartition, Integer> lastSeenLeaderEpochs;\n+    /** Addresses with which the metadata was originally bootstrapped. */\n+    private List<InetSocketAddress> bootstrapAddresses;\n \n     /**\n      * Create a new Metadata instance\n@@ -304,6 +306,12 @@ public synchronized void bootstrap(List<InetSocketAddress> addresses) {\n         this.needFullUpdate = true;\n         this.updateVersion += 1;\n         this.metadataSnapshot = MetadataSnapshot.bootstrap(addresses);\n+        this.bootstrapAddresses = addresses;\n+    }\n+\n+    public synchronized void rebootstrap() {\n+        log.info(\"Rebootstrapping with {}\", this.bootstrapAddresses);\n+        this.bootstrap(this.bootstrapAddresses);\n     }\n \n     /**"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/MetadataRecoveryStrategy.java",
      "status": "added",
      "patch": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Defines the strategies which clients can follow to deal with the situation when none of the known nodes is available.\n+ */\n+public enum MetadataRecoveryStrategy {\n+    NONE(\"none\"),\n+    REBOOTSTRAP(\"rebootstrap\");\n+\n+    public final String name;\n+\n+    MetadataRecoveryStrategy(String name) {\n+        this.name = name;\n+    }\n+\n+    public static MetadataRecoveryStrategy forName(String name) {\n+        if (name == null) {\n+            throw new IllegalArgumentException(\"Illegal MetadataRecoveryStrategy: null\");\n+        }\n+        try {\n+            return MetadataRecoveryStrategy.valueOf(name.toUpperCase(Locale.ROOT));\n+        } catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(\"Illegal MetadataRecoveryStrategy: \" + name);\n+        }\n+    }\n+}"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
      "status": "modified",
      "patch": "@@ -114,6 +114,8 @@ private enum State {\n     /* time in ms to wait before retrying to create connection to a server */\n     private final long reconnectBackoffMs;\n \n+    private final MetadataRecoveryStrategy metadataRecoveryStrategy;\n+\n     private final Time time;\n \n     /**\n@@ -147,7 +149,8 @@ public NetworkClient(Selectable selector,\n                          Time time,\n                          boolean discoverBrokerVersions,\n                          ApiVersions apiVersions,\n-                         LogContext logContext) {\n+                         LogContext logContext,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         this(selector,\n              metadata,\n              clientId,\n@@ -163,7 +166,8 @@ public NetworkClient(Selectable selector,\n              discoverBrokerVersions,\n              apiVersions,\n              null,\n-             logContext);\n+             logContext,\n+             metadataRecoveryStrategy);\n     }\n \n     public NetworkClient(Selectable selector,\n@@ -181,7 +185,8 @@ public NetworkClient(Selectable selector,\n                          boolean discoverBrokerVersions,\n                          ApiVersions apiVersions,\n                          Sensor throttleTimeSensor,\n-                         LogContext logContext) {\n+                         LogContext logContext,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         this(null,\n              metadata,\n              selector,\n@@ -200,7 +205,8 @@ public NetworkClient(Selectable selector,\n              throttleTimeSensor,\n              logContext,\n              new DefaultHostResolver(),\n-             null);\n+             null,\n+             metadataRecoveryStrategy);\n     }\n \n     public NetworkClient(Selectable selector,\n@@ -217,7 +223,8 @@ public NetworkClient(Selectable selector,\n                          Time time,\n                          boolean discoverBrokerVersions,\n                          ApiVersions apiVersions,\n-                         LogContext logContext) {\n+                         LogContext logContext,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         this(metadataUpdater,\n              null,\n              selector,\n@@ -236,7 +243,8 @@ public NetworkClient(Selectable selector,\n              null,\n              logContext,\n              new DefaultHostResolver(),\n-             null);\n+             null,\n+             metadataRecoveryStrategy);\n     }\n \n     public NetworkClient(MetadataUpdater metadataUpdater,\n@@ -257,7 +265,8 @@ public NetworkClient(MetadataUpdater metadataUpdater,\n                          Sensor throttleTimeSensor,\n                          LogContext logContext,\n                          HostResolver hostResolver,\n-                         ClientTelemetrySender clientTelemetrySender) {\n+                         ClientTelemetrySender clientTelemetrySender,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         /* It would be better if we could pass `DefaultMetadataUpdater` from the public constructor, but it's not\n          * possible because `DefaultMetadataUpdater` is an inner class and it can only be instantiated after the\n          * super constructor is invoked.\n@@ -288,6 +297,7 @@ public NetworkClient(MetadataUpdater metadataUpdater,\n         this.log = logContext.logger(NetworkClient.class);\n         this.state = new AtomicReference<>(State.ACTIVE);\n         this.telemetrySender = (clientTelemetrySender != null) ? new TelemetrySender(clientTelemetrySender) : null;\n+        this.metadataRecoveryStrategy = metadataRecoveryStrategy;\n     }\n \n     /**\n@@ -695,7 +705,7 @@ public void close() {\n      * @return The node with the fewest in-flight requests.\n      */\n     @Override\n-    public Node leastLoadedNode(long now) {\n+    public LeastLoadedNode leastLoadedNode(long now) {\n         List<Node> nodes = this.metadataUpdater.fetchNodes();\n         if (nodes.isEmpty())\n             throw new IllegalStateException(\"There are no nodes in the Kafka cluster\");\n@@ -705,16 +715,25 @@ public Node leastLoadedNode(long now) {\n         Node foundCanConnect = null;\n         Node foundReady = null;\n \n+        boolean atLeastOneConnectionReady = false;\n+\n         int offset = this.randOffset.nextInt(nodes.size());\n         for (int i = 0; i < nodes.size(); i++) {\n             int idx = (offset + i) % nodes.size();\n             Node node = nodes.get(idx);\n+\n+            if (!atLeastOneConnectionReady\n+                    && connectionStates.isReady(node.idString(), now)\n+                    && selector.isChannelReady(node.idString())) {\n+                atLeastOneConnectionReady = true;\n+            }\n+\n             if (canSendRequest(node.idString(), now)) {\n                 int currInflight = this.inFlightRequests.count(node.idString());\n                 if (currInflight == 0) {\n                     // if we find an established connection with no in-flight requests we can stop right away\n                     log.trace(\"Found least loaded node {} connected with no in-flight requests\", node);\n-                    return node;\n+                    return new LeastLoadedNode(node, true);\n                 } else if (currInflight < inflight) {\n                     // otherwise if this is the best we have found so far, record that\n                     inflight = currInflight;\n@@ -738,16 +757,16 @@ public Node leastLoadedNode(long now) {\n         // which are being established before connecting to new nodes.\n         if (foundReady != null) {\n             log.trace(\"Found least loaded node {} with {} inflight requests\", foundReady, inflight);\n-            return foundReady;\n+            return new LeastLoadedNode(foundReady, atLeastOneConnectionReady);\n         } else if (foundConnecting != null) {\n             log.trace(\"Found least loaded connecting node {}\", foundConnecting);\n-            return foundConnecting;\n+            return new LeastLoadedNode(foundConnecting, atLeastOneConnectionReady);\n         } else if (foundCanConnect != null) {\n             log.trace(\"Found least loaded node {} with no active connection\", foundCanConnect);\n-            return foundCanConnect;\n+            return new LeastLoadedNode(foundCanConnect, atLeastOneConnectionReady);\n         } else {\n             log.trace(\"Least loaded node selection failed to find an available node\");\n-            return null;\n+            return new LeastLoadedNode(null, atLeastOneConnectionReady);\n         }\n     }\n \n@@ -821,9 +840,8 @@ private void processDisconnection(List<ClientResponse> responses,\n                 break;\n             case AUTHENTICATE:\n                 log.warn(\"Connection to node {} ({}) terminated during authentication. This may happen \" +\n-                    \"due to any of the following reasons: (1) Authentication failed due to invalid \" +\n-                    \"credentials with brokers older than 1.0.0, (2) Firewall blocking Kafka TLS \" +\n-                    \"traffic (eg it may only allow HTTPS traffic), (3) Transient network issue.\",\n+                    \"due to any of the following reasons: (1) Firewall blocking Kafka TLS \" +\n+                    \"traffic (eg it may only allow HTTPS traffic), (2) Transient network issue.\",\n                     nodeId, disconnectState.remoteAddress());\n                 break;\n             case NOT_CONNECTED:\n@@ -1122,13 +1140,22 @@ public long maybeUpdate(long now) {\n \n             // Beware that the behavior of this method and the computation of timeouts for poll() are\n             // highly dependent on the behavior of leastLoadedNode.\n-            Node node = leastLoadedNode(now);\n-            if (node == null) {\n+            LeastLoadedNode leastLoadedNode = leastLoadedNode(now);\n+\n+            // Rebootstrap if needed and configured.\n+            if (metadataRecoveryStrategy == MetadataRecoveryStrategy.REBOOTSTRAP\n+                    && !leastLoadedNode.hasNodeAvailableOrConnectionReady()) {\n+                metadata.rebootstrap();\n+\n+                leastLoadedNode = leastLoadedNode(now);\n+            }\n+\n+            if (leastLoadedNode.node() == null) {\n                 log.debug(\"Give up sending metadata request since no node is available\");\n                 return reconnectBackoffMs;\n             }\n \n-            return maybeUpdate(now, node);\n+            return maybeUpdate(now, leastLoadedNode.node());\n         }\n \n         @Override\n@@ -1266,7 +1293,7 @@ public long maybeUpdate(long now) {\n \n             // Per KIP-714, let's continue to re-use the same broker for as long as possible.\n             if (stickyNode == null) {\n-                stickyNode = leastLoadedNode(now);\n+                stickyNode = leastLoadedNode(now).node();\n                 if (stickyNode == null) {\n                     log.debug(\"Give up sending telemetry request since no node is available\");\n                     return reconnectBackoffMs;"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java",
      "status": "modified",
      "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.kafka.clients.ClientDnsLookup;\n import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.MetadataRecoveryStrategy;\n import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n import org.apache.kafka.common.config.ConfigDef.Importance;\n@@ -139,6 +140,10 @@ public class AdminClientConfig extends AbstractConfig {\n     public static final String RETRIES_CONFIG = CommonClientConfigs.RETRIES_CONFIG;\n     public static final String DEFAULT_API_TIMEOUT_MS_CONFIG = CommonClientConfigs.DEFAULT_API_TIMEOUT_MS_CONFIG;\n \n+    public static final String METADATA_RECOVERY_STRATEGY_CONFIG = CommonClientConfigs.METADATA_RECOVERY_STRATEGY_CONFIG;\n+    public static final String METADATA_RECOVERY_STRATEGY_DOC = CommonClientConfigs.METADATA_RECOVERY_STRATEGY_DOC;\n+    public static final String DEFAULT_METADATA_RECOVERY_STRATEGY = CommonClientConfigs.DEFAULT_METADATA_RECOVERY_STRATEGY;\n+\n     /**\n      * <code>security.providers</code>\n      */\n@@ -262,7 +267,14 @@ public class AdminClientConfig extends AbstractConfig {\n                                         Importance.MEDIUM,\n                                         SECURITY_PROTOCOL_DOC)\n                                 .withClientSslSupport()\n-                                .withClientSaslSupport();\n+                                .withClientSaslSupport()\n+                                .define(METADATA_RECOVERY_STRATEGY_CONFIG,\n+                                        Type.STRING,\n+                                        DEFAULT_METADATA_RECOVERY_STRATEGY,\n+                                        ConfigDef.CaseInsensitiveValidString\n+                                                .in(Utils.enumOptions(MetadataRecoveryStrategy.class)),\n+                                        Importance.LOW,\n+                                        METADATA_RECOVERY_STRATEGY_DOC);\n     }\n \n     @Override"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/CreateTopicsResult.java",
      "status": "modified",
      "patch": "@@ -32,7 +32,7 @@\n  */\n @InterfaceStability.Evolving\n public class CreateTopicsResult {\n-    final static int UNKNOWN = -1;\n+    static final int UNKNOWN = -1;\n \n     private final Map<String, KafkaFuture<TopicMetadataAndConfig>> futures;\n "
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeReplicaLogDirsResult.java",
      "status": "modified",
      "patch": "@@ -66,7 +66,7 @@ public KafkaFuture<Map<TopicPartitionReplica, ReplicaLogDirInfo>> all() {\n             });\n     }\n \n-    static public class ReplicaLogDirInfo {\n+    public static class ReplicaLogDirInfo {\n         // The current log directory of the replica of this partition on the given broker.\n         // Null if no replica is not found for this partition on the given broker.\n         private final String currentReplicaLogDir;"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/ElectLeadersOptions.java",
      "status": "modified",
      "patch": "@@ -28,5 +28,5 @@\n  * The API of this class is evolving, see {@link Admin} for details.\n  */\n @InterfaceStability.Evolving\n-final public class ElectLeadersOptions extends AbstractOptions<ElectLeadersOptions> {\n+public final class ElectLeadersOptions extends AbstractOptions<ElectLeadersOptions> {\n }"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/ElectLeadersResult.java",
      "status": "modified",
      "patch": "@@ -34,7 +34,7 @@\n  * The API of this class is evolving, see {@link Admin} for details.\n  */\n @InterfaceStability.Evolving\n-final public class ElectLeadersResult {\n+public final class ElectLeadersResult {\n     private final KafkaFuture<Map<TopicPartition, Optional<Throwable>>> electionFuture;\n \n     ElectLeadersResult(KafkaFuture<Map<TopicPartition, Optional<Throwable>>> electionFuture) {"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java",
      "status": "modified",
      "patch": "@@ -25,6 +25,8 @@\n import org.apache.kafka.clients.DefaultHostResolver;\n import org.apache.kafka.clients.HostResolver;\n import org.apache.kafka.clients.KafkaClient;\n+import org.apache.kafka.clients.LeastLoadedNode;\n+import org.apache.kafka.clients.MetadataRecoveryStrategy;\n import org.apache.kafka.clients.NetworkClient;\n import org.apache.kafka.clients.StaleMetadataException;\n import org.apache.kafka.clients.admin.CreateTopicsResult.TopicMetadataAndConfig;\n@@ -398,6 +400,7 @@ public class KafkaAdminClient extends AdminClient {\n     private final long retryBackoffMaxMs;\n     private final ExponentialBackoff retryBackoff;\n     private final boolean clientTelemetryEnabled;\n+    private final MetadataRecoveryStrategy metadataRecoveryStrategy;\n \n     /**\n      * The telemetry requests client instance id.\n@@ -611,6 +614,7 @@ private KafkaAdminClient(AdminClientConfig config,\n             retryBackoffMaxMs,\n             CommonClientConfigs.RETRY_BACKOFF_JITTER);\n         this.clientTelemetryEnabled = config.getBoolean(AdminClientConfig.ENABLE_METRICS_PUSH_CONFIG);\n+        this.metadataRecoveryStrategy = MetadataRecoveryStrategy.forName(config.getString(AdminClientConfig.METADATA_RECOVERY_STRATEGY_CONFIG));\n         config.logUnused();\n         AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());\n         log.debug(\"Kafka admin client initialized\");\n@@ -697,7 +701,13 @@ private interface NodeProvider {\n     private class MetadataUpdateNodeIdProvider implements NodeProvider {\n         @Override\n         public Node provide() {\n-            return client.leastLoadedNode(time.milliseconds());\n+            LeastLoadedNode leastLoadedNode = client.leastLoadedNode(time.milliseconds());\n+            if (metadataRecoveryStrategy == MetadataRecoveryStrategy.REBOOTSTRAP\n+                    && !leastLoadedNode.hasNodeAvailableOrConnectionReady()) {\n+                metadataManager.rebootstrap(time.milliseconds());\n+            }\n+\n+            return leastLoadedNode.node();\n         }\n \n         @Override\n@@ -779,7 +789,7 @@ public Node provide() {\n             if (metadataManager.isReady()) {\n                 // This may return null if all nodes are busy.\n                 // In that case, we will postpone node assignment.\n-                return client.leastLoadedNode(time.milliseconds());\n+                return client.leastLoadedNode(time.milliseconds()).node();\n             }\n             metadataManager.requestUpdate();\n             return null;\n@@ -834,7 +844,7 @@ public Node provide() {\n                 } else {\n                     // This may return null if all nodes are busy.\n                     // In that case, we will postpone node assignment.\n-                    return client.leastLoadedNode(time.milliseconds());\n+                    return client.leastLoadedNode(time.milliseconds()).node();\n                 }\n             }\n             metadataManager.requestUpdate();\n@@ -2245,7 +2255,7 @@ void handleResponse(AbstractResponse abstractResponse) {\n                         continue;\n                     }\n \n-                    TopicDescription currentTopicDescription = getTopicDescriptionFromDescribeTopicsResponseTopic(topic, nodes);\n+                    TopicDescription currentTopicDescription = getTopicDescriptionFromDescribeTopicsResponseTopic(topic, nodes, options.includeAuthorizedOperations());\n \n                     if (partiallyFinishedTopicDescription != null && partiallyFinishedTopicDescription.name().equals(topicName)) {\n                         // Add the partitions for the cursor topic of the previous batch.\n@@ -2326,7 +2336,7 @@ private Map<String, KafkaFuture<TopicDescription>> handleDescribeTopicsByNamesWi\n         clusterResult.nodes().whenComplete(\n             (nodes, exception) -> {\n                 if (exception != null) {\n-                    completeAllExceptionally(topicFutures.values(), exception.getCause());\n+                    completeAllExceptionally(topicFutures.values(), exception);\n                     return;\n                 }\n \n@@ -2408,14 +2418,16 @@ void handleFailure(Throwable throwable) {\n \n     private TopicDescription getTopicDescriptionFromDescribeTopicsResponseTopic(\n         DescribeTopicPartitionsResponseTopic topic,\n-        Map<Integer, Node> nodes\n+        Map<Integer, Node> nodes,\n+        boolean includeAuthorizedOperations\n     ) {\n         List<DescribeTopicPartitionsResponsePartition> partitionInfos = topic.partitions();\n         List<TopicPartitionInfo> partitions = new ArrayList<>(partitionInfos.size());\n         for (DescribeTopicPartitionsResponsePartition partitionInfo : partitionInfos) {\n             partitions.add(DescribeTopicPartitionsResponse.partitionToTopicPartitionInfo(partitionInfo, nodes));\n         }\n-        return new TopicDescription(topic.name(), topic.isInternal(), partitions, validAclOperations(topic.topicAuthorizedOperations()), topic.topicId());\n+        Set<AclOperation> authorisedOperations = includeAuthorizedOperations ? validAclOperations(topic.topicAuthorizedOperations()) : null;\n+        return new TopicDescription(topic.name(), topic.isInternal(), partitions, authorisedOperations, topic.topicId());\n     }\n \n     private TopicDescription getTopicDescriptionFromCluster(Cluster cluster, String topicName, Uuid topicId,\n@@ -3471,7 +3483,7 @@ private Set<AclOperation> validAclOperations(final int authorizedOperations) {\n             .collect(Collectors.toSet());\n     }\n \n-    private final static class ListConsumerGroupsResults {\n+    private static final class ListConsumerGroupsResults {\n         private final List<Throwable> errors;\n         private final HashMap<String, ConsumerGroupListing> listings;\n         private final HashSet<Node> remaining;\n@@ -4414,12 +4426,13 @@ public DescribeMetadataQuorumResult describeMetadataQuorum(DescribeMetadataQuoru\n             private QuorumInfo.ReplicaState translateReplicaState(DescribeQuorumResponseData.ReplicaState replica) {\n                 return new QuorumInfo.ReplicaState(\n                         replica.replicaId(),\n+                        replica.replicaDirectoryId() == null ? Uuid.ZERO_UUID : replica.replicaDirectoryId(),\n                         replica.logEndOffset(),\n                         replica.lastFetchTimestamp() == -1 ? OptionalLong.empty() : OptionalLong.of(replica.lastFetchTimestamp()),\n                         replica.lastCaughtUpTimestamp() == -1 ? OptionalLong.empty() : OptionalLong.of(replica.lastCaughtUpTimestamp()));\n             }\n \n-            private QuorumInfo createQuorumResult(final DescribeQuorumResponseData.PartitionData partition) {\n+            private QuorumInfo createQuorumResult(final DescribeQuorumResponseData.PartitionData partition, DescribeQuorumResponseData.NodeCollection nodeCollection) {\n                 List<QuorumInfo.ReplicaState> voters = partition.currentVoters().stream()\n                     .map(this::translateReplicaState)\n                     .collect(Collectors.toList());\n@@ -4428,12 +4441,21 @@ private QuorumInfo createQuorumResult(final DescribeQuorumResponseData.Partition\n                     .map(this::translateReplicaState)\n                     .collect(Collectors.toList());\n \n+                Map<Integer, QuorumInfo.Node> nodes = nodeCollection.stream().map(n -> {\n+                    List<RaftVoterEndpoint> endpoints = n.listeners().stream()\n+                        .map(l -> new RaftVoterEndpoint(l.name(), l.host(), l.port()))\n+                        .collect(Collectors.toList());\n+\n+                    return new QuorumInfo.Node(n.nodeId(), endpoints);\n+                }).collect(Collectors.toMap(QuorumInfo.Node::nodeId, Function.identity()));\n+\n                 return new QuorumInfo(\n                     partition.leaderId(),\n                     partition.leaderEpoch(),\n                     partition.highWatermark(),\n                     voters,\n-                    observers\n+                    observers,\n+                    nodes\n                 );\n             }\n \n@@ -4447,7 +4469,7 @@ DescribeQuorumRequest.Builder createRequest(int timeoutMs) {\n             void handleResponse(AbstractResponse response) {\n                 final DescribeQuorumResponse quorumResponse = (DescribeQuorumResponse) response;\n                 if (quorumResponse.data().errorCode() != Errors.NONE.code()) {\n-                    throw Errors.forCode(quorumResponse.data().errorCode()).exception();\n+                    throw Errors.forCode(quorumResponse.data().errorCode()).exception(quorumResponse.data().errorMessage());\n                 }\n                 if (quorumResponse.data().topics().size() != 1) {\n                     String msg = String.format(\"DescribeMetadataQuorum received %d topics when 1 was expected\",\n@@ -4476,9 +4498,9 @@ void handleResponse(AbstractResponse response) {\n                     throw new UnknownServerException(msg);\n                 }\n                 if (partition.errorCode() != Errors.NONE.code()) {\n-                    throw Errors.forCode(partition.errorCode()).exception();\n+                    throw Errors.forCode(partition.errorCode()).exception(partition.errorMessage());\n                 }\n-                future.complete(createQuorumResult(partition));\n+                future.complete(createQuorumResult(partition, quorumResponse.data().nodes()));\n             }\n \n             @Override"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/QuorumInfo.java",
      "status": "modified",
      "patch": "@@ -16,7 +16,10 @@\n  */\n package org.apache.kafka.clients.admin;\n \n+import org.apache.kafka.common.Uuid;\n+\n import java.util.List;\n+import java.util.Map;\n import java.util.Objects;\n import java.util.OptionalLong;\n \n@@ -29,19 +32,22 @@ public class QuorumInfo {\n     private final long highWatermark;\n     private final List<ReplicaState> voters;\n     private final List<ReplicaState> observers;\n+    private final Map<Integer, Node> nodes;\n \n     QuorumInfo(\n         int leaderId,\n         long leaderEpoch,\n         long highWatermark,\n         List<ReplicaState> voters,\n-        List<ReplicaState> observers\n+        List<ReplicaState> observers,\n+        Map<Integer, Node> nodes\n     ) {\n         this.leaderId = leaderId;\n         this.leaderEpoch = leaderEpoch;\n         this.highWatermark = highWatermark;\n         this.voters = voters;\n         this.observers = observers;\n+        this.nodes = nodes;\n     }\n \n     public int leaderId() {\n@@ -64,6 +70,13 @@ public List<ReplicaState> observers() {\n         return observers;\n     }\n \n+    /**\n+     * @return The voter nodes in the Raft cluster, or an empty map if KIP-853 is not enabled.\n+     */\n+    public Map<Integer, Node> nodes() {\n+        return nodes;\n+    }\n+\n     @Override\n     public boolean equals(Object o) {\n         if (this == o) return true;\n@@ -73,12 +86,13 @@ public boolean equals(Object o) {\n             && leaderEpoch == that.leaderEpoch\n             && highWatermark == that.highWatermark\n             && Objects.equals(voters, that.voters)\n-            && Objects.equals(observers, that.observers);\n+            && Objects.equals(observers, that.observers)\n+            && Objects.equals(nodes, that.nodes);\n     }\n \n     @Override\n     public int hashCode() {\n-        return Objects.hash(leaderId, leaderEpoch, highWatermark, voters, observers);\n+        return Objects.hash(leaderId, leaderEpoch, highWatermark, voters, observers, nodes);\n     }\n \n     @Override\n@@ -89,26 +103,30 @@ public String toString() {\n             \", highWatermark=\" + highWatermark +\n             \", voters=\" + voters +\n             \", observers=\" + observers +\n+            \", nodes=\" + nodes +\n             ')';\n     }\n \n     public static class ReplicaState {\n         private final int replicaId;\n+        private final Uuid replicaDirectoryId;\n         private final long logEndOffset;\n         private final OptionalLong lastFetchTimestamp;\n         private final OptionalLong lastCaughtUpTimestamp;\n \n         ReplicaState() {\n-            this(0, 0, OptionalLong.empty(), OptionalLong.empty());\n+            this(0, Uuid.ZERO_UUID, 0, OptionalLong.empty(), OptionalLong.empty());\n         }\n \n         ReplicaState(\n             int replicaId,\n+            Uuid replicaDirectoryId,\n             long logEndOffset,\n             OptionalLong lastFetchTimestamp,\n             OptionalLong lastCaughtUpTimestamp\n         ) {\n             this.replicaId = replicaId;\n+            this.replicaDirectoryId = replicaDirectoryId;\n             this.logEndOffset = logEndOffset;\n             this.lastFetchTimestamp = lastFetchTimestamp;\n             this.lastCaughtUpTimestamp = lastCaughtUpTimestamp;\n@@ -122,6 +140,13 @@ public int replicaId() {\n             return replicaId;\n         }\n \n+        /**\n+         * Return the directory id of the replica if configured, or Uuid.ZERO_UUID if not.\n+         */\n+        public Uuid replicaDirectoryId() {\n+            return replicaDirectoryId;\n+        }\n+\n         /**\n          * Return the logEndOffset known by the leader for this replica.\n          * @return The logEndOffset for this replica\n@@ -154,24 +179,65 @@ public boolean equals(Object o) {\n             if (o == null || getClass() != o.getClass()) return false;\n             ReplicaState that = (ReplicaState) o;\n             return replicaId == that.replicaId\n+                && Objects.equals(replicaDirectoryId, that.replicaDirectoryId)\n                 && logEndOffset == that.logEndOffset\n                 && lastFetchTimestamp.equals(that.lastFetchTimestamp)\n                 && lastCaughtUpTimestamp.equals(that.lastCaughtUpTimestamp);\n         }\n \n         @Override\n         public int hashCode() {\n-            return Objects.hash(replicaId, logEndOffset, lastFetchTimestamp, lastCaughtUpTimestamp);\n+            return Objects.hash(replicaId, replicaDirectoryId, logEndOffset, lastFetchTimestamp, lastCaughtUpTimestamp);\n         }\n \n         @Override\n         public String toString() {\n             return \"ReplicaState(\" +\n                 \"replicaId=\" + replicaId +\n+                \", replicaDirectoryId=\" + replicaDirectoryId +\n                 \", logEndOffset=\" + logEndOffset +\n                 \", lastFetchTimestamp=\" + lastFetchTimestamp +\n                 \", lastCaughtUpTimestamp=\" + lastCaughtUpTimestamp +\n                 ')';\n         }\n     }\n+\n+    public static class Node {\n+        private final int nodeId;\n+        private final List<RaftVoterEndpoint> endpoints;\n+\n+        Node(int nodeId, List<RaftVoterEndpoint> endpoints) {\n+            this.nodeId = nodeId;\n+            this.endpoints = endpoints;\n+        }\n+\n+        public int nodeId() {\n+            return nodeId;\n+        }\n+\n+        public List<RaftVoterEndpoint> endpoints() {\n+            return endpoints;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) return true;\n+            if (o == null || getClass() != o.getClass()) return false;\n+            Node node = (Node) o;\n+            return nodeId == node.nodeId && Objects.equals(endpoints, node.endpoints);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(nodeId, endpoints);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Node{\" +\n+                \"nodeId=\" + nodeId +\n+                \", endpoints=\" + endpoints +\n+                '}';\n+        }\n+    }\n }"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/TransactionState.java",
      "status": "modified",
      "patch": "@@ -34,7 +34,7 @@ public enum TransactionState {\n     PREPARE_EPOCH_FENCE(\"PrepareEpochFence\"),\n     UNKNOWN(\"Unknown\");\n \n-    private final static Map<String, TransactionState> NAME_TO_ENUM = Arrays.stream(values())\n+    private static final Map<String, TransactionState> NAME_TO_ENUM = Arrays.stream(values())\n         .collect(Collectors.toMap(state -> state.name, Function.identity()));\n \n     private final String name;"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminBootstrapAddresses.java",
      "status": "modified",
      "patch": "@@ -27,7 +27,7 @@\n import java.util.List;\n import java.util.Objects;\n \n-final public class AdminBootstrapAddresses {\n+public final class AdminBootstrapAddresses {\n     private final boolean usingBootstrapControllers;\n     private final List<InetSocketAddress> addresses;\n "
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java",
      "status": "modified",
      "patch": "@@ -92,6 +92,11 @@ public class AdminMetadataManager {\n      */\n     private ApiException fatalException = null;\n \n+    /**\n+     * The cluster with which the metadata was bootstrapped.\n+     */\n+    private Cluster bootstrapCluster;\n+\n     public class AdminMetadataUpdater implements MetadataUpdater {\n         @Override\n         public List<Node> fetchNodes() {\n@@ -275,6 +280,7 @@ public void updateFailed(Throwable exception) {\n     public void update(Cluster cluster, long now) {\n         if (cluster.isBootstrapConfigured()) {\n             log.debug(\"Setting bootstrap cluster metadata {}.\", cluster);\n+            bootstrapCluster = cluster;\n         } else {\n             log.debug(\"Updating cluster metadata to {}\", cluster);\n             this.lastMetadataUpdateMs = now;\n@@ -287,4 +293,12 @@ public void update(Cluster cluster, long now) {\n             this.cluster = cluster;\n         }\n     }\n+\n+    /**\n+     * Rebootstrap metadata with the cluster previously used for bootstrapping.\n+     */\n+    public void rebootstrap(long now) {\n+        log.info(\"Rebootstrapping with {}\", this.bootstrapCluster);\n+        update(bootstrapCluster, now);\n+    }\n }"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/internals/FenceProducersHandler.java",
      "status": "modified",
      "patch": "@@ -134,6 +134,10 @@ private ApiResult<CoordinatorKey, ProducerIdAndEpoch> handleError(\n                                 \"coordinator is still in the process of loading state. Will retry\",\n                         transactionalIdKey.idValue);\n                 return ApiResult.empty();\n+            case CONCURRENT_TRANSACTIONS:\n+                log.debug(\"InitProducerId request for transactionalId `{}` failed because of \" +\n+                                \"a concurrent transaction. Will retry\", transactionalIdKey.idValue);\n+                return ApiResult.empty();\n \n             case NOT_COORDINATOR:\n             case COORDINATOR_NOT_AVAILABLE:"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java",
      "status": "modified",
      "patch": "@@ -18,6 +18,7 @@\n \n import org.apache.kafka.clients.ClientDnsLookup;\n import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.MetadataRecoveryStrategy;\n import org.apache.kafka.common.IsolationLevel;\n import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n@@ -656,7 +657,14 @@ public class ConsumerConfig extends AbstractConfig {\n                                         Importance.MEDIUM,\n                                         CommonClientConfigs.SECURITY_PROTOCOL_DOC)\n                                 .withClientSslSupport()\n-                                .withClientSaslSupport();\n+                                .withClientSaslSupport()\n+                                .define(CommonClientConfigs.METADATA_RECOVERY_STRATEGY_CONFIG,\n+                                        Type.STRING,\n+                                        CommonClientConfigs.DEFAULT_METADATA_RECOVERY_STRATEGY,\n+                                        ConfigDef.CaseInsensitiveValidString\n+                                                .in(Utils.enumOptions(MetadataRecoveryStrategy.class)),\n+                                        Importance.LOW,\n+                                        CommonClientConfigs.METADATA_RECOVERY_STRATEGY_DOC);\n     }\n \n     @Override"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadata.java",
      "status": "modified",
      "patch": "@@ -26,10 +26,10 @@\n  * Note: Any change to this class is considered public and requires a KIP.\n  */\n public class ConsumerGroupMetadata {\n-    final private String groupId;\n-    final private int generationId;\n-    final private String memberId;\n-    final private Optional<String> groupInstanceId;\n+    private final String groupId;\n+    private final int generationId;\n+    private final String memberId;\n+    private final Optional<String> groupInstanceId;\n \n     public ConsumerGroupMetadata(String groupId,\n                                  int generationId,"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
      "status": "modified",
      "patch": "@@ -523,7 +523,7 @@\n  */\n public class KafkaConsumer<K, V> implements Consumer<K, V> {\n \n-    private final static ConsumerDelegateCreator CREATOR = new ConsumerDelegateCreator();\n+    private static final ConsumerDelegateCreator CREATOR = new ConsumerDelegateCreator();\n \n     private final ConsumerDelegate<K, V> delegate;\n \n@@ -1803,4 +1803,4 @@ KafkaConsumerMetrics kafkaConsumerMetrics() {\n     boolean updateAssignmentMetadataIfNeeded(final Timer timer) {\n         return delegate.updateAssignmentMetadataIfNeeded(timer);\n     }\n-}\n\\ No newline at end of file\n+}"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaShareConsumer.java",
      "status": "modified",
      "patch": "@@ -332,7 +332,7 @@\n @InterfaceStability.Evolving\n public class KafkaShareConsumer<K, V> implements ShareConsumer<K, V> {\n \n-    private final static ShareConsumerDelegateCreator CREATOR = new ShareConsumerDelegateCreator();\n+    private static final ShareConsumerDelegateCreator CREATOR = new ShareConsumerDelegateCreator();\n \n     private final ShareConsumerDelegate<K, V> delegate;\n "
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java",
      "status": "modified",
      "patch": "@@ -85,7 +85,7 @@ public MemberData(List<TopicPartition> partitions, Optional<Integer> generation)\n         }\n     }\n \n-    abstract protected MemberData memberData(Subscription subscription);\n+    protected abstract MemberData memberData(Subscription subscription);\n \n     @Override\n     public Map<String, List<TopicPartition>> assignPartitions(Map<String, List<PartitionInfo>> partitionsPerTopic,"
    },
    {
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java",
      "status": "modified",
      "patch": "@@ -331,7 +331,8 @@ private void process(final ConsumerRebalanceListenerCallbackNeededEvent event) {\n                     apiVersions,\n                     metrics,\n                     fetchMetricsManager,\n-                    clientTelemetryReporter.map(ClientTelemetryReporter::telemetrySender).orElse(null));\n+                    clientTelemetryReporter.map(ClientTelemetryReporter::telemetrySender).orElse(null),\n+                    backgroundEventHandler);\n             this.offsetCommitCallbackInvoker = new OffsetCommitCallbackInvoker(interceptors);\n             this.asyncCommitFenced = new AtomicBoolean(false);\n             this.groupMetadata.set(initializeGroupMetadata(config, groupRebalanceConfig));\n@@ -504,7 +505,9 @@ private void process(final ConsumerRebalanceListenerCallbackNeededEvent event) {\n             time,\n             config,\n             logContext,\n-            client\n+            client,\n+            metadata,\n+            backgroundEventHandler\n         );\n         this.offsetCommitCallbackInvoker = new OffsetCommitCallbackInvoker(interceptors);\n         this.asyncCommitFenced = new AtomicBoolean(false);\n@@ -1231,7 +1234,7 @@ private void close(Duration timeout, boolean swallowException) {\n         clientTelemetryReporter.ifPresent(reporter -> reporter.initiateClose(timeout.toMillis()));\n         closeTimer.update();\n         // Prepare shutting down the network thread\n-        prepareShutdown(closeTimer, firstException);\n+        releaseAssignmentAndLeaveGroup(closeTimer, firstException);\n         closeTimer.update();\n         swallow(log, Level.ERROR, \"Failed invoking asynchronous commit callback.\",\n             () -> awaitPendingAsyncCommitsAndExecuteCommitCallbacks(closeTimer, false), firstException);\n@@ -1267,12 +1270,12 @@ private void close(Duration timeout, boolean swallowException) {\n      * 2. revoke all partitions\n      * 3. if partition revocation completes successfully, send leave group\n      */\n-    void prepareShutdown(final Timer timer, final AtomicReference<Throwable> firstException) {\n+    void releaseAssignmentAndLeaveGroup(final Timer timer, final AtomicReference<Throwable> firstException) {\n         if (!groupMetadata.get().isPresent())\n             return;\n \n         if (autoCommitEnabled)\n-            autoCommitSync(timer);\n+            commitSyncAllConsumed(timer);\n \n         applicationEventHandler.add(new CommitOnCloseEvent());\n         completeQuietly(\n@@ -1284,7 +1287,7 @@ void prepareShutdown(final Timer timer, final AtomicReference<Throwable> firstEx\n     }\n \n     // Visible for testing\n-    void autoCommitSync(final Timer timer) {\n+    void commitSyncAllConsumed(final Timer timer) {\n         Map<TopicPartition, OffsetAndMetadata> allConsumed = subscriptions.allConsumed();\n         log.debug(\"Sending synchronous auto-commit of offsets {} on closing\", allConsumed);\n         try {"
    }
  ],
  "fix_category": "WaitFor",
  "root_cause_category": "Async wait",
  "root_cause_subcategory": NaN
}