{
  "id": 39,
  "repo": "xgboost",
  "issue_url": "https://github.com/dmlc/xgboost/pull/7749",
  "pr_url": "https://github.com/dmlc/xgboost/pull/7749",
  "issue_description": "[PR Linked Issue]\n```\r\n[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-801)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-802)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-803)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-804)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-805)[2022-03-24T11:45:03.974Z] tests/python/test_data_iterator.py:111: AssertionError\r\n\r\n[2022-03-24T11:45:03.974Z] ---------------------------------- Hypothesis ----------------------------------\r\n\r\n[2022-03-24T11:45:03.974Z] Falsifying example: test_data_iterator(\r\n\r\n[2022-03-24T11:45:03.974Z]     n_samples_per_batch=4, n_features=2, n_batches=1, subsample=True,\r\n\r\n[2022-03-24T11:45:03.974Z] )\r\n```\r\n\r\nhttps://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline",
  "files_changed": [
    {
      "filename": "tests/python-gpu/test_gpu_data_iterator.py",
      "status": "modified",
      "patch": "@@ -22,7 +22,7 @@ def test_gpu_single_batch() -> None:\n     strategies.integers(0, 13),\n     strategies.booleans(),\n )\n-@settings(deadline=None)\n+@settings(deadline=None, print_blob=True)\n def test_gpu_data_iterator(\n     n_samples_per_batch: int, n_features: int, n_batches: int, subsample: bool\n ) -> None:"
    },
    {
      "filename": "tests/python-gpu/test_gpu_linear.py",
      "status": "modified",
      "patch": "@@ -30,7 +30,7 @@ def train_result(param, dmat, num_rounds):\n class TestGPULinear:\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_coordinate(self, param, num_rounds, dataset):\n         assume(len(dataset.y) > 0)\n         param['updater'] = 'gpu_coord_descent'\n@@ -45,7 +45,7 @@ def test_gpu_coordinate(self, param, num_rounds, dataset):\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, strategies.floats(1e-5, 1.0),\n            strategies.floats(1e-5, 1.0))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_coordinate_regularised(self, param, num_rounds, dataset, alpha, lambd):\n         assume(len(dataset.y) > 0)\n         param['updater'] = 'gpu_coord_descent'"
    },
    {
      "filename": "tests/python-gpu/test_gpu_prediction.py",
      "status": "modified",
      "patch": "@@ -247,7 +247,7 @@ def predict_df(x):\n \n     @given(strategies.integers(1, 10),\n            tm.dataset_strategy, shap_parameter_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_shap(self, num_rounds, dataset, param):\n         param.update({\"predictor\": \"gpu_predictor\", \"gpu_id\": 0})\n         param = dataset.set_params(param)\n@@ -261,7 +261,7 @@ def test_shap(self, num_rounds, dataset, param):\n \n     @given(strategies.integers(1, 10),\n            tm.dataset_strategy, shap_parameter_strategy)\n-    @settings(deadline=None, max_examples=20)\n+    @settings(deadline=None, max_examples=20, print_blob=True)\n     def test_shap_interactions(self, num_rounds, dataset, param):\n         param.update({\"predictor\": \"gpu_predictor\", \"gpu_id\": 0})\n         param = dataset.set_params(param)\n@@ -312,14 +312,14 @@ def run_predict_leaf_booster(self, param, num_rounds, dataset):\n         np.testing.assert_equal(cpu_leaf, gpu_leaf)\n \n     @given(predict_parameter_strategy, tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_predict_leaf_gbtree(self, param, dataset):\n         param['booster'] = 'gbtree'\n         param['tree_method'] = 'gpu_hist'\n         self.run_predict_leaf_booster(param, 10, dataset)\n \n     @given(predict_parameter_strategy, tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_predict_leaf_dart(self, param, dataset):\n         param['booster'] = 'dart'\n         param['tree_method'] = 'gpu_hist'\n@@ -330,7 +330,7 @@ def test_predict_leaf_dart(self, param, dataset):\n     @given(df=data_frames([column('x0', elements=strategies.integers(min_value=0, max_value=3)),\n                            column('x1', elements=strategies.integers(min_value=0, max_value=5))],\n                           index=range_indexes(min_size=20, max_size=50)))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_predict_categorical_split(self, df):\n         from sklearn.metrics import mean_squared_error\n "
    },
    {
      "filename": "tests/python-gpu/test_gpu_updaters.py",
      "status": "modified",
      "patch": "@@ -46,7 +46,7 @@ class TestGPUUpdaters:\n     cputest = test_up.TestTreeMethod()\n \n     @given(parameter_strategy, strategies.integers(1, 20), tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_hist(self, param, num_rounds, dataset):\n         param[\"tree_method\"] = \"gpu_hist\"\n         param = dataset.set_params(param)\n@@ -56,7 +56,7 @@ def test_gpu_hist(self, param, num_rounds, dataset):\n \n     @given(strategies.integers(10, 400), strategies.integers(3, 8),\n            strategies.integers(1, 2), strategies.integers(4, 7))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     @pytest.mark.skipif(**tm.no_pandas())\n     def test_categorical(self, rows, cols, rounds, cats):\n         self.cputest.run_categorical_basic(rows, cols, rounds, cats, \"gpu_hist\")\n@@ -76,7 +76,7 @@ def test_invalid_category(self):\n     @pytest.mark.skipif(**tm.no_cupy())\n     @given(parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_hist_device_dmatrix(self, param, num_rounds, dataset):\n         # We cannot handle empty dataset yet\n         assume(len(dataset.y) > 0)\n@@ -88,7 +88,7 @@ def test_gpu_hist_device_dmatrix(self, param, num_rounds, dataset):\n \n     @given(parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_external_memory(self, param, num_rounds, dataset):\n         # We cannot handle empty dataset yet\n         assume(len(dataset.y) > 0)\n@@ -127,7 +127,7 @@ def test_empty_dmatrix_prediction(self):\n \n     @pytest.mark.mgpu\n     @given(tm.dataset_strategy, strategies.integers(0, 10))\n-    @settings(deadline=None, max_examples=10)\n+    @settings(deadline=None, max_examples=10, print_blob=True)\n     def test_specified_gpu_id_gpu_update(self, dataset, gpu_id):\n         param = {'tree_method': 'gpu_hist', 'gpu_id': gpu_id}\n         param = dataset.set_params(param)"
    },
    {
      "filename": "tests/python-gpu/test_gpu_with_dask.py",
      "status": "modified",
      "patch": "@@ -27,7 +27,7 @@\n from test_with_dask import run_empty_dmatrix_auc      # noqa\n from test_with_dask import run_auc                    # noqa\n from test_with_dask import run_boost_from_prediction  # noqa\n-from test_with_dask import run_boost_from_prediction_multi_clasas  # noqa\n+from test_with_dask import run_boost_from_prediction_multi_class  # noqa\n from test_with_dask import run_dask_classifier        # noqa\n from test_with_dask import run_empty_dmatrix_cls      # noqa\n from test_with_dask import _get_client_workers        # noqa\n@@ -216,7 +216,7 @@ def test_boost_from_prediction(local_cuda_cluster: LocalCUDACluster) -> None:\n         X_, y_ = load_digits(return_X_y=True)\n         X = dd.from_array(X_, chunksize=100).map_partitions(cudf.from_pandas)\n         y = dd.from_array(y_, chunksize=100).map_partitions(cudf.from_pandas)\n-        run_boost_from_prediction_multi_clasas(X, y, \"gpu_hist\", client)\n+        run_boost_from_prediction_multi_class(X, y, \"gpu_hist\", client)\n \n \n class TestDistributedGPU:\n@@ -231,7 +231,7 @@ def test_dask_dataframe(self, local_cuda_cluster: LocalCUDACluster) -> None:\n         num_rounds=strategies.integers(1, 20),\n         dataset=tm.dataset_strategy,\n     )\n-    @settings(deadline=duration(seconds=120), suppress_health_check=suppress)\n+    @settings(deadline=duration(seconds=120), suppress_health_check=suppress, print_blob=True)\n     @pytest.mark.skipif(**tm.no_cupy())\n     @pytest.mark.parametrize(\n         \"local_cuda_cluster\", [{\"n_workers\": 2}], indirect=[\"local_cuda_cluster\"]"
    },
    {
      "filename": "tests/python/test_data_iterator.py",
      "status": "modified",
      "patch": "@@ -108,7 +108,8 @@ def run_data_iterator(\n         evals_result=results_from_it,\n         verbose_eval=False,\n     )\n-    assert non_increasing(results_from_it[\"Train\"][\"rmse\"])\n+    if not subsample:\n+        assert non_increasing(results_from_it[\"Train\"][\"rmse\"])\n \n     X, y = it.as_arrays()\n     Xy = xgb.DMatrix(X, y)\n@@ -125,7 +126,8 @@ def run_data_iterator(\n         verbose_eval=False,\n     )\n     arr_predt = from_arrays.predict(Xy)\n-    assert non_increasing(results_from_arrays[\"Train\"][\"rmse\"])\n+    if not subsample:\n+        assert non_increasing(results_from_arrays[\"Train\"][\"rmse\"])\n \n     rtol = 1e-2\n     # CPU sketching is more memory efficient but less consistent due to small chunks\n@@ -146,7 +148,7 @@ def run_data_iterator(\n     strategies.integers(0, 13),\n     strategies.booleans(),\n )\n-@settings(deadline=None)\n+@settings(deadline=None, print_blob=True)\n def test_data_iterator(\n     n_samples_per_batch: int,\n     n_features: int,"
    },
    {
      "filename": "tests/python/test_linear.py",
      "status": "modified",
      "patch": "@@ -26,7 +26,7 @@ def train_result(param, dmat, num_rounds):\n class TestLinear:\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, coord_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_coordinate(self, param, num_rounds, dataset, coord_param):\n         param['updater'] = 'coord_descent'\n         param.update(coord_param)\n@@ -41,7 +41,7 @@ def test_coordinate(self, param, num_rounds, dataset, coord_param):\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, coord_strategy, strategies.floats(1e-5, 1.0),\n            strategies.floats(1e-5, 1.0))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_coordinate_regularised(self, param, num_rounds, dataset, coord_param, alpha, lambd):\n         param['updater'] = 'coord_descent'\n         param['alpha'] = alpha\n@@ -54,7 +54,7 @@ def test_coordinate_regularised(self, param, num_rounds, dataset, coord_param, a\n \n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_shotgun(self, param, num_rounds, dataset):\n         param['updater'] = 'shotgun'\n         param = dataset.set_params(param)\n@@ -71,7 +71,7 @@ def test_shotgun(self, param, num_rounds, dataset):\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, strategies.floats(1e-5, 1.0),\n            strategies.floats(1e-5, 1.0))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_shotgun_regularised(self, param, num_rounds, dataset, alpha, lambd):\n         param['updater'] = 'shotgun'\n         param['alpha'] = alpha"
    },
    {
      "filename": "tests/python/test_updaters.py",
      "status": "modified",
      "patch": "@@ -38,7 +38,7 @@ def train_result(param, dmat, num_rounds):\n class TestTreeMethod:\n     @given(exact_parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_exact(self, param, num_rounds, dataset):\n         param['tree_method'] = 'exact'\n         param = dataset.set_params(param)\n@@ -51,7 +51,7 @@ def test_exact(self, param, num_rounds, dataset):\n         strategies.integers(1, 20),\n         tm.dataset_strategy,\n     )\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_approx(self, param, hist_param, num_rounds, dataset):\n         param[\"tree_method\"] = \"approx\"\n         param = dataset.set_params(param)\n@@ -86,7 +86,7 @@ def test_pruner(self):\n \n     @given(exact_parameter_strategy, hist_parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_hist(self, param, hist_param, num_rounds, dataset):\n         param['tree_method'] = 'hist'\n         param = dataset.set_params(param)\n@@ -241,7 +241,7 @@ def run_categorical_basic(self, rows, cols, rounds, cats, tree_method):\n \n     @given(strategies.integers(10, 400), strategies.integers(3, 8),\n            strategies.integers(1, 2), strategies.integers(4, 7))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     @pytest.mark.skipif(**tm.no_pandas())\n     def test_categorical(self, rows, cols, rounds, cats):\n         self.run_categorical_basic(rows, cols, rounds, cats, \"approx\")"
    },
    {
      "filename": "tests/python/test_with_dask.py",
      "status": "modified",
      "patch": "@@ -337,33 +337,33 @@ def test_dask_predict_shape_infer(client: \"Client\") -> None:\n     assert prediction.shape[1] == 3\n \n \n-def run_boost_from_prediction_multi_clasas(\n+def run_boost_from_prediction_multi_class(\n     X: xgb.dask._DaskCollection,\n     y: xgb.dask._DaskCollection,\n     tree_method: str,\n-    client: \"Client\"\n+    client: \"Client\",\n ) -> None:\n     model_0 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4, tree_method=tree_method\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=768\n     )\n     model_0.fit(X=X, y=y)\n     margin = xgb.dask.inplace_predict(\n         client, model_0.get_booster(), X, predict_type=\"margin\"\n     )\n \n     model_1 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4, tree_method=tree_method\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=768\n     )\n     model_1.fit(X=X, y=y, base_margin=margin)\n     predictions_1 = xgb.dask.predict(\n         client,\n         model_1.get_booster(),\n         xgb.dask.DaskDMatrix(client, X, base_margin=margin),\n-        output_margin=True\n+        output_margin=True,\n     )\n \n     model_2 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=8, tree_method=tree_method\n+        learning_rate=0.3, n_estimators=8, tree_method=tree_method, max_bin=768\n     )\n     model_2.fit(X=X, y=y)\n     predictions_2 = xgb.dask.inplace_predict(\n@@ -382,26 +382,29 @@ def run_boost_from_prediction_multi_clasas(\n \n \n def run_boost_from_prediction(\n-    X: xgb.dask._DaskCollection, y: xgb.dask._DaskCollection, tree_method: str, client: \"Client\"\n+    X: xgb.dask._DaskCollection,\n+    y: xgb.dask._DaskCollection,\n+    tree_method: str,\n+    client: \"Client\",\n ) -> None:\n     X = client.persist(X)\n     y = client.persist(y)\n \n     model_0 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4,\n-        tree_method=tree_method)\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=512\n+    )\n     model_0.fit(X=X, y=y)\n     margin = model_0.predict(X, output_margin=True)\n \n     model_1 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4,\n-        tree_method=tree_method)\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=512\n+    )\n     model_1.fit(X=X, y=y, base_margin=margin)\n     predictions_1 = model_1.predict(X, base_margin=margin)\n \n     cls_2 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=8,\n-        tree_method=tree_method)\n+        learning_rate=0.3, n_estimators=8, tree_method=tree_method, max_bin=512\n+    )\n     cls_2.fit(X=X, y=y)\n     predictions_2 = cls_2.predict(X)\n \n@@ -415,8 +418,8 @@ def run_boost_from_prediction(\n     unmargined = xgb.dask.DaskXGBClassifier(n_estimators=4)\n     unmargined.fit(X=X, y=y, eval_set=[(X, y)], base_margin=margin)\n \n-    margined_res = margined.evals_result()['validation_0']['logloss']\n-    unmargined_res = unmargined.evals_result()['validation_0']['logloss']\n+    margined_res = margined.evals_result()[\"validation_0\"][\"logloss\"]\n+    unmargined_res = unmargined.evals_result()[\"validation_0\"][\"logloss\"]\n \n     assert len(margined_res) == len(unmargined_res)\n     for i in range(len(margined_res)):\n@@ -429,12 +432,11 @@ def test_boost_from_prediction(tree_method: str, client: \"Client\") -> None:\n     from sklearn.datasets import load_breast_cancer, load_digits\n     X_, y_ = load_breast_cancer(return_X_y=True)\n     X, y = dd.from_array(X_, chunksize=200), dd.from_array(y_, chunksize=200)\n-\n     run_boost_from_prediction(X, y, tree_method, client)\n \n     X_, y_ = load_digits(return_X_y=True)\n     X, y = dd.from_array(X_, chunksize=100), dd.from_array(y_, chunksize=100)\n-    run_boost_from_prediction_multi_clasas(X, y, tree_method, client)\n+    run_boost_from_prediction_multi_class(X, y, tree_method, client)\n \n \n def test_inplace_predict(client: \"Client\") -> None:\n@@ -1292,7 +1294,7 @@ def minimum_bin():\n \n     @given(params=hist_parameter_strategy,\n            dataset=tm.dataset_strategy)\n-    @settings(deadline=None, suppress_health_check=suppress)\n+    @settings(deadline=None, suppress_health_check=suppress, print_blob=True)\n     def test_hist(\n             self, params: Dict, dataset: tm.TestDataset, client: \"Client\"\n     ) -> None:\n@@ -1301,7 +1303,7 @@ def test_hist(\n \n     @given(params=exact_parameter_strategy,\n            dataset=tm.dataset_strategy)\n-    @settings(deadline=None, suppress_health_check=suppress)\n+    @settings(deadline=None, suppress_health_check=suppress, print_blob=True)\n     def test_approx(\n             self, client: \"Client\", params: Dict, dataset: tm.TestDataset\n     ) -> None:"
    }
  ],
  "fix_category": "Change condition",
  "root_cause_category": "Randomness",
  "root_cause_subcategory": NaN
}