{
  "id": 143,
  "repo": "airflow",
  "issue_url": "https://github.com/apache/airflow/pull/25231",
  "pr_url": "https://github.com/apache/airflow/pull/25231",
  "issue_description": "In Postgres especially (but in generaly in all databases, if\r\nthere is no order specified in select query, the rows might\r\ncome in random order. It depends on many factors.\r\n\r\nThe test query here retrieved the dags without any order but\r\nexpected the list to be in specific order.\r\n\r\nThis PR adds ordering - it also removes side-effects of the\r\ntest by using fixture that clears the datasets before and after\r\nthe tests that rely/modify datasets - because othrwise failure of\r\none of the tests can create side effects that fail the other\r\ntests (this is what happened in this case)\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n",
  "files_changed": [
    {
      "filename": "tests/models/test_taskinstance.py",
      "status": "modified",
      "patch": "@@ -119,6 +119,15 @@ def success_handler(self, context):\n         self.task_state_in_callback = context['ti'].state\n \n \n+@pytest.fixture()\n+def clear_datasets():\n+    # Clean up before ourselves\n+    db.clear_db_datasets()\n+    yield\n+    # and after\n+    db.clear_db_datasets()\n+\n+\n class TestTaskInstance:\n     @staticmethod\n     def clean_db():\n@@ -1513,7 +1522,7 @@ def test_success_callback_no_race_condition(self, create_task_instance):\n         ti.refresh_from_db()\n         assert ti.state == State.SUCCESS\n \n-    def test_outlet_datasets(self, create_task_instance):\n+    def test_outlet_datasets(self, create_task_instance, clear_datasets):\n         \"\"\"\n         Verify that when we have an outlet dataset on a task, and the task\n         completes successfully, a DatasetDagRunQueue is logged.\n@@ -1540,7 +1549,7 @@ def test_outlet_datasets(self, create_task_instance):\n         # check that one queue record created for each dag that depends on dataset 1\n         assert session.query(DatasetDagRunQueue.target_dag_id).filter(\n             DatasetTaskRef.dag_id == dag1.dag_id, DatasetTaskRef.task_id == 'upstream_task_1'\n-        ).all() == [\n+        ).order_by(DatasetDagRunQueue.target_dag_id).all() == [\n             ('example_dataset_dag3_req_dag1',),\n             ('example_dataset_dag4_req_dag1_dag2',),\n             ('example_dataset_dag5_req_dag1_D',),\n@@ -1559,10 +1568,7 @@ def test_outlet_datasets(self, create_task_instance):\n             .count()\n         ) == 1\n \n-        # Clean up after ourselves\n-        db.clear_db_datasets()\n-\n-    def test_outlet_datasets_failed(self, create_task_instance):\n+    def test_outlet_datasets_failed(self, create_task_instance, clear_datasets):\n         \"\"\"\n         Verify that when we have an outlet dataset on a task, and the task\n         failed, a DatasetDagRunQueue is not logged, and a DatasetEvent is\n@@ -1593,7 +1599,7 @@ def test_outlet_datasets_failed(self, create_task_instance):\n         # check that no dataset events were generated\n         assert session.query(DatasetEvent).count() == 0\n \n-    def test_outlet_datasets_skipped(self, create_task_instance):\n+    def test_outlet_datasets_skipped(self, create_task_instance, clear_datasets):\n         \"\"\"\n         Verify that when we have an outlet dataset on a task, and the task\n         is skipped, a DatasetDagRunQueue is not logged, and a DatasetEvent is"
    }
  ],
  "fix_category": "Make deterministic",
  "root_cause_category": "Unordered collections",
  "root_cause_subcategory": NaN
}