{
  "id": 180,
  "repo": "airflow",
  "issue_url": "https://github.com/apache/airflow/pull/33309",
  "pr_url": "https://github.com/apache/airflow/pull/33309",
  "issue_description": "The `render_template_fields` method of mapped operator needs to use database session object to render mapped fields, but it cannot get the session passed by @provide_session decorator, because it is used in derived classes and we cannot change the signature without impacting those classes.\r\n\r\nSo far it was done by creating new session in mapped_operator, but it has the drawback of creating an extra session while one is already created (remnder_template_fields is always run in the context of task run and it always has a session created already in _run_raw_task). It also causes problems in our tests where two opened database session accessed database at the same time and it cases sqlite exception on concurrent access and mysql error on running operations out of sync - likely when the same object was modified in both sessions.\r\n\r\nThis PR changes the approach - rather than creating a new session in the mapped_operator, we are retrieving the session from one stored by the _run_raw_task. It is done by context manager and adequate protection has been added to make sure that:\r\n\r\na) the call is made within the context manager\r\nb) context manageer is never initialized twice in the same\r\n   call stack\r\n\r\nAfter this change, resources used by running task will be smaller, and mapped tasks will not always open 2 DB sesions.\r\n\r\nFixes: #33178\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n",
  "files_changed": [
    {
      "filename": "airflow/cli/commands/task_command.py",
      "status": "modified",
      "patch": "@@ -67,6 +67,7 @@\n from airflow.utils.providers_configuration_loader import providers_configuration_loaded\n from airflow.utils.session import NEW_SESSION, create_session, provide_session\n from airflow.utils.state import DagRunState\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n \n if TYPE_CHECKING:\n     from sqlalchemy.orm.session import Session\n@@ -649,7 +650,8 @@ def task_render(args, dag: DAG | None = None) -> None:\n     ti, _ = _get_ti(\n         task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, create_if_necessary=\"memory\"\n     )\n-    ti.render_templates()\n+    with create_session() as session, set_current_task_instance_session(session=session):\n+        ti.render_templates()\n     for attr in task.template_fields:\n         print(\n             textwrap.dedent("
    },
    {
      "filename": "airflow/models/mappedoperator.py",
      "status": "modified",
      "patch": "@@ -26,7 +26,6 @@\n \n import attr\n \n-from airflow import settings\n from airflow.compat.functools import cache\n from airflow.exceptions import AirflowException, UnmappableOperator\n from airflow.models.abstractoperator import (\n@@ -54,6 +53,7 @@\n from airflow.typing_compat import Literal\n from airflow.utils.context import context_update_for_unmapped\n from airflow.utils.helpers import is_container, prevent_duplicates\n+from airflow.utils.task_instance_session import get_current_task_instance_session\n from airflow.utils.types import NOTSET\n from airflow.utils.xcom import XCOM_RETURN_KEY\n \n@@ -720,12 +720,13 @@ def render_template_fields(\n         if not jinja_env:\n             jinja_env = self.get_template_env()\n \n-        # Ideally we'd like to pass in session as an argument to this function,\n-        # but we can't easily change this function signature since operators\n-        # could override this. We can't use @provide_session since it closes and\n-        # expunges everything, which we don't want to do when we are so \"deep\"\n-        # in the weeds here. We don't close this session for the same reason.\n-        session = settings.Session()\n+        # We retrieve the session here, stored by _run_raw_task in set_current_task_session\n+        # context manager - we cannot pass the session via @provide_session because the signature\n+        # of render_template_fields is defined by BaseOperator and there are already many subclasses\n+        # overriding it, so changing the signature is not an option. However render_template_fields is\n+        # always executed within \"_run_raw_task\" so we make sure that _run_raw_task uses the\n+        # set_current_task_session context manager to store the session in the current task.\n+        session = get_current_task_instance_session()\n \n         mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)\n         unmapped_task = self.unmap(mapped_kwargs)"
    },
    {
      "filename": "airflow/models/taskinstance.py",
      "status": "modified",
      "patch": "@@ -118,6 +118,7 @@\n )\n from airflow.utils.state import DagRunState, JobState, State, TaskInstanceState\n from airflow.utils.task_group import MappedTaskGroup\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.timeout import timeout\n from airflow.utils.xcom import XCOM_RETURN_KEY\n \n@@ -1511,98 +1512,98 @@ def _run_raw_task(\n                 count=0,\n                 tags={**self.stats_tags, \"state\": str(state)},\n             )\n+        with set_current_task_instance_session(session=session):\n+            self.task = self.task.prepare_for_execution()\n+            context = self.get_template_context(ignore_param_exceptions=False)\n \n-        self.task = self.task.prepare_for_execution()\n-        context = self.get_template_context(ignore_param_exceptions=False)\n-\n-        try:\n-            if not mark_success:\n-                self._execute_task_with_callbacks(context, test_mode, session=session)\n-            if not test_mode:\n-                self.refresh_from_db(lock_for_update=True, session=session)\n-            self.state = TaskInstanceState.SUCCESS\n-        except TaskDeferred as defer:\n-            # The task has signalled it wants to defer execution based on\n-            # a trigger.\n-            self._defer_task(defer=defer, session=session)\n-            self.log.info(\n-                \"Pausing task as DEFERRED. dag_id=%s, task_id=%s, execution_date=%s, start_date=%s\",\n-                self.dag_id,\n-                self.task_id,\n-                self._date_or_empty(\"execution_date\"),\n-                self._date_or_empty(\"start_date\"),\n-            )\n-            if not test_mode:\n-                session.add(Log(self.state, self))\n-                session.merge(self)\n-                session.commit()\n-            return TaskReturnCode.DEFERRED\n-        except AirflowSkipException as e:\n-            # Recording SKIP\n-            # log only if exception has any arguments to prevent log flooding\n-            if e.args:\n-                self.log.info(e)\n-            if not test_mode:\n-                self.refresh_from_db(lock_for_update=True, session=session)\n-            self.state = TaskInstanceState.SKIPPED\n-        except AirflowRescheduleException as reschedule_exception:\n-            self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, session=session)\n-            session.commit()\n-            return None\n-        except (AirflowFailException, AirflowSensorTimeout) as e:\n-            # If AirflowFailException is raised, task should not retry.\n-            # If a sensor in reschedule mode reaches timeout, task should not retry.\n-            self.handle_failure(e, test_mode, context, force_fail=True, session=session)\n-            session.commit()\n-            raise\n-        except AirflowException as e:\n-            if not test_mode:\n-                self.refresh_from_db(lock_for_update=True, session=session)\n-            # for case when task is marked as success/failed externally\n-            # or dagrun timed out and task is marked as skipped\n-            # current behavior doesn't hit the callbacks\n-            if self.state in State.finished:\n-                self.clear_next_method_args()\n-                session.merge(self)\n+            try:\n+                if not mark_success:\n+                    self._execute_task_with_callbacks(context, test_mode, session=session)\n+                if not test_mode:\n+                    self.refresh_from_db(lock_for_update=True, session=session)\n+                self.state = TaskInstanceState.SUCCESS\n+            except TaskDeferred as defer:\n+                # The task has signalled it wants to defer execution based on\n+                # a trigger.\n+                self._defer_task(defer=defer, session=session)\n+                self.log.info(\n+                    \"Pausing task as DEFERRED. dag_id=%s, task_id=%s, execution_date=%s, start_date=%s\",\n+                    self.dag_id,\n+                    self.task_id,\n+                    self._date_or_empty(\"execution_date\"),\n+                    self._date_or_empty(\"start_date\"),\n+                )\n+                if not test_mode:\n+                    session.add(Log(self.state, self))\n+                    session.merge(self)\n+                    session.commit()\n+                return TaskReturnCode.DEFERRED\n+            except AirflowSkipException as e:\n+                # Recording SKIP\n+                # log only if exception has any arguments to prevent log flooding\n+                if e.args:\n+                    self.log.info(e)\n+                if not test_mode:\n+                    self.refresh_from_db(lock_for_update=True, session=session)\n+                self.state = TaskInstanceState.SKIPPED\n+            except AirflowRescheduleException as reschedule_exception:\n+                self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, session=session)\n                 session.commit()\n                 return None\n-            else:\n+            except (AirflowFailException, AirflowSensorTimeout) as e:\n+                # If AirflowFailException is raised, task should not retry.\n+                # If a sensor in reschedule mode reaches timeout, task should not retry.\n+                self.handle_failure(e, test_mode, context, force_fail=True, session=session)\n+                session.commit()\n+                raise\n+            except AirflowException as e:\n+                if not test_mode:\n+                    self.refresh_from_db(lock_for_update=True, session=session)\n+                # for case when task is marked as success/failed externally\n+                # or dagrun timed out and task is marked as skipped\n+                # current behavior doesn't hit the callbacks\n+                if self.state in State.finished:\n+                    self.clear_next_method_args()\n+                    session.merge(self)\n+                    session.commit()\n+                    return None\n+                else:\n+                    self.handle_failure(e, test_mode, context, session=session)\n+                    session.commit()\n+                    raise\n+            except (Exception, KeyboardInterrupt) as e:\n                 self.handle_failure(e, test_mode, context, session=session)\n                 session.commit()\n                 raise\n-        except (Exception, KeyboardInterrupt) as e:\n-            self.handle_failure(e, test_mode, context, session=session)\n-            session.commit()\n-            raise\n-        finally:\n-            Stats.incr(f\"ti.finish.{self.dag_id}.{self.task_id}.{self.state}\", tags=self.stats_tags)\n-            # Same metric with tagging\n-            Stats.incr(\"ti.finish\", tags={**self.stats_tags, \"state\": str(self.state)})\n-\n-        # Recording SKIPPED or SUCCESS\n-        self.clear_next_method_args()\n-        self.end_date = timezone.utcnow()\n-        self._log_state()\n-        self.set_duration()\n+            finally:\n+                Stats.incr(f\"ti.finish.{self.dag_id}.{self.task_id}.{self.state}\", tags=self.stats_tags)\n+                # Same metric with tagging\n+                Stats.incr(\"ti.finish\", tags={**self.stats_tags, \"state\": str(self.state)})\n+\n+            # Recording SKIPPED or SUCCESS\n+            self.clear_next_method_args()\n+            self.end_date = timezone.utcnow()\n+            self._log_state()\n+            self.set_duration()\n+\n+            # run on_success_callback before db committing\n+            # otherwise, the LocalTaskJob sees the state is changed to `success`,\n+            # but the task_runner is still running, LocalTaskJob then treats the state is set externally!\n+            self._run_finished_callback(self.task.on_success_callback, context, \"on_success\")\n \n-        # run on_success_callback before db committing\n-        # otherwise, the LocalTaskJob sees the state is changed to `success`,\n-        # but the task_runner is still running, LocalTaskJob then treats the state is set externally!\n-        self._run_finished_callback(self.task.on_success_callback, context, \"on_success\")\n-\n-        if not test_mode:\n-            session.add(Log(self.state, self))\n-            session.merge(self).task = self.task\n-            if self.state == TaskInstanceState.SUCCESS:\n-                self._register_dataset_changes(session=session)\n+            if not test_mode:\n+                session.add(Log(self.state, self))\n+                session.merge(self).task = self.task\n+                if self.state == TaskInstanceState.SUCCESS:\n+                    self._register_dataset_changes(session=session)\n \n-            session.commit()\n-            if self.state == TaskInstanceState.SUCCESS:\n-                get_listener_manager().hook.on_task_instance_success(\n-                    previous_state=TaskInstanceState.RUNNING, task_instance=self, session=session\n-                )\n+                session.commit()\n+                if self.state == TaskInstanceState.SUCCESS:\n+                    get_listener_manager().hook.on_task_instance_success(\n+                        previous_state=TaskInstanceState.RUNNING, task_instance=self, session=session\n+                    )\n \n-        return None\n+            return None\n \n     def _register_dataset_changes(self, *, session: Session) -> None:\n         for obj in self.task.outlets or []:"
    },
    {
      "filename": "airflow/utils/task_instance_session.py",
      "status": "added",
      "patch": "@@ -0,0 +1,60 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import contextlib\n+import logging\n+import traceback\n+from typing import TYPE_CHECKING\n+\n+from airflow.utils.session import create_session\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+__current_task_instance_session: Session | None = None\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def get_current_task_instance_session() -> Session:\n+    global __current_task_instance_session\n+    if not __current_task_instance_session:\n+        log.warning(\"No task session set for this task. Continuing but this likely causes a resource leak.\")\n+        log.warning(\"Please report this and stacktrace below to https://github.com/apache/airflow/issues\")\n+        for filename, line_number, name, line in traceback.extract_stack():\n+            log.warning('File: \"%s\", %s , in %s', filename, line_number, name)\n+            if line:\n+                log.warning(\"  %s\", line.strip())\n+        __current_task_instance_session = create_session()\n+    return __current_task_instance_session\n+\n+\n+@contextlib.contextmanager\n+def set_current_task_instance_session(session: Session):\n+    global __current_task_instance_session\n+    if __current_task_instance_session:\n+        raise RuntimeError(\n+            \"Session already set for this task. \"\n+            \"You can only have one 'set_current_task_session' context manager active at a time.\"\n+        )\n+    __current_task_instance_session = session\n+    try:\n+        yield\n+    finally:\n+        __current_task_instance_session = None"
    },
    {
      "filename": "tests/decorators/test_python.py",
      "status": "modified",
      "patch": "@@ -36,6 +36,7 @@\n from airflow.utils import timezone\n from airflow.utils.state import State\n from airflow.utils.task_group import TaskGroup\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.trigger_rule import TriggerRule\n from airflow.utils.types import DagRunType\n from airflow.utils.xcom import XCOM_RETURN_KEY\n@@ -747,36 +748,37 @@ def test_mapped_render_template_fields(dag_maker, session):\n     def fn(arg1, arg2):\n         ...\n \n-    with dag_maker(session=session):\n-        task1 = BaseOperator(task_id=\"op1\")\n-        mapped = fn.partial(arg2=\"{{ ti.task_id }}\").expand(arg1=task1.output)\n+    with set_current_task_instance_session(session=session):\n+        with dag_maker(session=session):\n+            task1 = BaseOperator(task_id=\"op1\")\n+            mapped = fn.partial(arg2=\"{{ ti.task_id }}\").expand(arg1=task1.output)\n \n-    dr = dag_maker.create_dagrun()\n-    ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n-\n-    ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n-\n-    session.add(\n-        TaskMap(\n-            dag_id=dr.dag_id,\n-            task_id=task1.task_id,\n-            run_id=dr.run_id,\n-            map_index=-1,\n-            length=1,\n-            keys=None,\n+        dr = dag_maker.create_dagrun()\n+        ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+\n+        ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n+\n+        session.add(\n+            TaskMap(\n+                dag_id=dr.dag_id,\n+                task_id=task1.task_id,\n+                run_id=dr.run_id,\n+                map_index=-1,\n+                length=1,\n+                keys=None,\n+            )\n         )\n-    )\n-    session.flush()\n+        session.flush()\n \n-    mapped_ti: TaskInstance = dr.get_task_instance(mapped.operator.task_id, session=session)\n-    mapped_ti.map_index = 0\n+        mapped_ti: TaskInstance = dr.get_task_instance(mapped.operator.task_id, session=session)\n+        mapped_ti.map_index = 0\n \n-    assert isinstance(mapped_ti.task, MappedOperator)\n-    mapped.operator.render_template_fields(context=mapped_ti.get_template_context(session=session))\n-    assert isinstance(mapped_ti.task, BaseOperator)\n+        assert isinstance(mapped_ti.task, MappedOperator)\n+        mapped.operator.render_template_fields(context=mapped_ti.get_template_context(session=session))\n+        assert isinstance(mapped_ti.task, BaseOperator)\n \n-    assert mapped_ti.task.op_kwargs[\"arg1\"] == \"{{ ds }}\"\n-    assert mapped_ti.task.op_kwargs[\"arg2\"] == \"fn\"\n+        assert mapped_ti.task.op_kwargs[\"arg1\"] == \"{{ ds }}\"\n+        assert mapped_ti.task.op_kwargs[\"arg2\"] == \"fn\"\n \n \n def test_task_decorator_has_wrapped_attr():"
    },
    {
      "filename": "tests/models/test_mappedoperator.py",
      "status": "modified",
      "patch": "@@ -39,6 +39,7 @@\n from airflow.operators.python import PythonOperator\n from airflow.utils.state import TaskInstanceState\n from airflow.utils.task_group import TaskGroup\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.trigger_rule import TriggerRule\n from airflow.utils.xcom import XCOM_RETURN_KEY\n from tests.models import DEFAULT_DATE\n@@ -403,109 +404,114 @@ def test_mapped_expand_against_params(dag_maker, dag_params, task_params, expect\n \n \n def test_mapped_render_template_fields_validating_operator(dag_maker, session):\n-    class MyOperator(BaseOperator):\n-        template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n-        template_ext = (\".ext\",)\n-\n-        def __init__(\n-            self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n-        ):\n-            for value in [partial_template, partial_static, map_template, map_static, file_template]:\n-                assert isinstance(value, str), \"value should have been resolved before unmapping\"\n-            super().__init__(**kwargs)\n-            self.partial_template = partial_template\n-            self.partial_static = partial_static\n-            self.map_template = map_template\n-            self.map_static = map_static\n-            self.file_template = file_template\n+    with set_current_task_instance_session(session=session):\n+\n+        class MyOperator(BaseOperator):\n+            template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n+            template_ext = (\".ext\",)\n+\n+            def __init__(\n+                self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n+            ):\n+                for value in [partial_template, partial_static, map_template, map_static, file_template]:\n+                    assert isinstance(value, str), \"value should have been resolved before unmapping\"\n+                    super().__init__(**kwargs)\n+                    self.partial_template = partial_template\n+                self.partial_static = partial_static\n+                self.map_template = map_template\n+                self.map_static = map_static\n+                self.file_template = file_template\n \n         def execute(self, context):\n             pass\n \n-    with dag_maker(session=session):\n-        task1 = BaseOperator(task_id=\"op1\")\n-        output1 = task1.output\n-        mapped = MyOperator.partial(\n-            task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n-        ).expand(map_template=output1, map_static=output1, file_template=[\"/path/to/file.ext\"])\n+        with dag_maker(session=session):\n+            task1 = BaseOperator(task_id=\"op1\")\n+            output1 = task1.output\n+            mapped = MyOperator.partial(\n+                task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n+            ).expand(map_template=output1, map_static=output1, file_template=[\"/path/to/file.ext\"])\n+\n+        dr = dag_maker.create_dagrun()\n+        ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+\n+        ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n+\n+        session.add(\n+            TaskMap(\n+                dag_id=dr.dag_id,\n+                task_id=task1.task_id,\n+                run_id=dr.run_id,\n+                map_index=-1,\n+                length=1,\n+                keys=None,\n+            )\n+        )\n+        session.flush()\n \n-    dr = dag_maker.create_dagrun()\n-    ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+        mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n+        mapped_ti.map_index = 0\n \n-    ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n+        assert isinstance(mapped_ti.task, MappedOperator)\n+        with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n+            \"os.path.isfile\", return_value=True\n+        ), patch(\"os.path.getmtime\", return_value=0):\n+            mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n+        assert isinstance(mapped_ti.task, MyOperator)\n \n-    session.add(\n-        TaskMap(\n-            dag_id=dr.dag_id,\n-            task_id=task1.task_id,\n-            run_id=dr.run_id,\n-            map_index=-1,\n-            length=1,\n-            keys=None,\n-        )\n-    )\n-    session.flush()\n+        assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n+        assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.map_template == \"{{ ds }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n \n-    mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n-    mapped_ti.map_index = 0\n \n-    assert isinstance(mapped_ti.task, MappedOperator)\n-    with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n-        \"os.path.isfile\", return_value=True\n-    ), patch(\"os.path.getmtime\", return_value=0):\n-        mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n-    assert isinstance(mapped_ti.task, MyOperator)\n+def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):\n \n-    assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n-    assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.map_template == \"{{ ds }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n+    with set_current_task_instance_session(session=session):\n \n+        class MyOperator(BaseOperator):\n+            template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n+            template_ext = (\".ext\",)\n \n-def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):\n-    class MyOperator(BaseOperator):\n-        template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n-        template_ext = (\".ext\",)\n-\n-        def __init__(\n-            self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n-        ):\n-            for value in [partial_template, partial_static, map_template, map_static, file_template]:\n-                assert isinstance(value, str), \"value should have been resolved before unmapping\"\n-            super().__init__(**kwargs)\n-            self.partial_template = partial_template\n-            self.partial_static = partial_static\n-            self.map_template = map_template\n-            self.map_static = map_static\n-            self.file_template = file_template\n+            def __init__(\n+                self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n+            ):\n+                for value in [partial_template, partial_static, map_template, map_static, file_template]:\n+                    assert isinstance(value, str), \"value should have been resolved before unmapping\"\n+                super().__init__(**kwargs)\n+                self.partial_template = partial_template\n+                self.partial_static = partial_static\n+                self.map_template = map_template\n+                self.map_static = map_static\n+                self.file_template = file_template\n \n-        def execute(self, context):\n-            pass\n+            def execute(self, context):\n+                pass\n \n-    with dag_maker(session=session):\n-        mapped = MyOperator.partial(\n-            task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n-        ).expand_kwargs(\n-            [{\"map_template\": \"{{ ds }}\", \"map_static\": \"{{ ds }}\", \"file_template\": \"/path/to/file.ext\"}]\n-        )\n+        with dag_maker(session=session):\n+            mapped = MyOperator.partial(\n+                task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n+            ).expand_kwargs(\n+                [{\"map_template\": \"{{ ds }}\", \"map_static\": \"{{ ds }}\", \"file_template\": \"/path/to/file.ext\"}]\n+            )\n \n-    dr = dag_maker.create_dagrun()\n+        dr = dag_maker.create_dagrun()\n \n-    mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)\n+        mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)\n \n-    assert isinstance(mapped_ti.task, MappedOperator)\n-    with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n-        \"os.path.isfile\", return_value=True\n-    ), patch(\"os.path.getmtime\", return_value=0):\n-        mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n-    assert isinstance(mapped_ti.task, MyOperator)\n+        assert isinstance(mapped_ti.task, MappedOperator)\n+        with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n+            \"os.path.isfile\", return_value=True\n+        ), patch(\"os.path.getmtime\", return_value=0):\n+            mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n+        assert isinstance(mapped_ti.task, MyOperator)\n \n-    assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n-    assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.map_template == \"2016-01-01\", \"Should be templated!\"\n-    assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n+        assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n+        assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.map_template == \"2016-01-01\", \"Should be templated!\"\n+        assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n \n \n def test_mapped_render_nested_template_fields(dag_maker, session):\n@@ -607,35 +613,36 @@ def test_expand_kwargs_mapped_task_instance(dag_maker, session, num_existing_tis\n     ],\n )\n def test_expand_kwargs_render_template_fields_validating_operator(dag_maker, session, map_index, expected):\n-    with dag_maker(session=session):\n-        task1 = BaseOperator(task_id=\"op1\")\n-        mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)\n-\n-    dr = dag_maker.create_dagrun()\n-    ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n-\n-    ti.xcom_push(key=XCOM_RETURN_KEY, value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}], session=session)\n-\n-    session.add(\n-        TaskMap(\n-            dag_id=dr.dag_id,\n-            task_id=task1.task_id,\n-            run_id=dr.run_id,\n-            map_index=-1,\n-            length=2,\n-            keys=None,\n+    with set_current_task_instance_session(session=session):\n+        with dag_maker(session=session):\n+            task1 = BaseOperator(task_id=\"op1\")\n+            mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)\n+\n+        dr = dag_maker.create_dagrun()\n+        ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+\n+        ti.xcom_push(key=XCOM_RETURN_KEY, value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}], session=session)\n+\n+        session.add(\n+            TaskMap(\n+                dag_id=dr.dag_id,\n+                task_id=task1.task_id,\n+                run_id=dr.run_id,\n+                map_index=-1,\n+                length=2,\n+                keys=None,\n+            )\n         )\n-    )\n-    session.flush()\n-\n-    ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n-    ti.refresh_from_task(mapped)\n-    ti.map_index = map_index\n-    assert isinstance(ti.task, MappedOperator)\n-    mapped.render_template_fields(context=ti.get_template_context(session=session))\n-    assert isinstance(ti.task, MockOperator)\n-    assert ti.task.arg1 == expected\n-    assert ti.task.arg2 == \"a\"\n+        session.flush()\n+\n+        ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n+        ti.refresh_from_task(mapped)\n+        ti.map_index = map_index\n+        assert isinstance(ti.task, MappedOperator)\n+        mapped.render_template_fields(context=ti.get_template_context(session=session))\n+        assert isinstance(ti.task, MockOperator)\n+        assert ti.task.arg1 == expected\n+        assert ti.task.arg2 == \"a\"\n \n \n def test_xcomarg_property_of_mapped_operator(dag_maker):"
    },
    {
      "filename": "tests/models/test_renderedtifields.py",
      "status": "modified",
      "patch": "@@ -29,6 +29,7 @@\n from airflow.models import Variable\n from airflow.models.renderedtifields import RenderedTaskInstanceFields as RTIF\n from airflow.operators.bash import BashOperator\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.timezone import datetime\n from tests.test_utils.asserts import assert_queries_count\n from tests.test_utils.db import clear_db_dags, clear_db_runs, clear_rendered_ti_fields\n@@ -153,44 +154,46 @@ def test_get_templated_fields(self, templated_field, expected_rendered_field, da\n         ],\n     )\n     def test_delete_old_records(\n-        self, rtif_num, num_to_keep, remaining_rtifs, expected_query_count, dag_maker\n+        self, rtif_num, num_to_keep, remaining_rtifs, expected_query_count, dag_maker, session\n     ):\n         \"\"\"\n         Test that old records are deleted from rendered_task_instance_fields table\n         for a given task_id and dag_id.\n         \"\"\"\n-        session = settings.Session()\n-        with dag_maker(\"test_delete_old_records\") as dag:\n-            task = BashOperator(task_id=\"test\", bash_command=\"echo {{ ds }}\")\n-        rtif_list = []\n-        for num in range(rtif_num):\n-            dr = dag_maker.create_dagrun(run_id=str(num), execution_date=dag.start_date + timedelta(days=num))\n-            ti = dr.task_instances[0]\n-            ti.task = task\n-            rtif_list.append(RTIF(ti))\n-\n-        session.add_all(rtif_list)\n-        session.flush()\n-\n-        result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n-\n-        for rtif in rtif_list:\n-            assert rtif in result\n-\n-        assert rtif_num == len(result)\n-\n-        # Verify old records are deleted and only 'num_to_keep' records are kept\n-        # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n-        expected_query_count_based_on_db = (\n-            expected_query_count + 1\n-            if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n-            else expected_query_count\n-        )\n+        with set_current_task_instance_session(session=session):\n+            with dag_maker(\"test_delete_old_records\") as dag:\n+                task = BashOperator(task_id=\"test\", bash_command=\"echo {{ ds }}\")\n+            rtif_list = []\n+            for num in range(rtif_num):\n+                dr = dag_maker.create_dagrun(\n+                    run_id=str(num), execution_date=dag.start_date + timedelta(days=num)\n+                )\n+                ti = dr.task_instances[0]\n+                ti.task = task\n+                rtif_list.append(RTIF(ti))\n+\n+            session.add_all(rtif_list)\n+            session.flush()\n+\n+            result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n+\n+            for rtif in rtif_list:\n+                assert rtif in result\n+\n+            assert rtif_num == len(result)\n+\n+            # Verify old records are deleted and only 'num_to_keep' records are kept\n+            # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n+            expected_query_count_based_on_db = (\n+                expected_query_count + 1\n+                if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n+                else expected_query_count\n+            )\n \n-        with assert_queries_count(expected_query_count_based_on_db):\n-            RTIF.delete_old_records(task_id=task.task_id, dag_id=task.dag_id, num_to_keep=num_to_keep)\n-        result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n-        assert remaining_rtifs == len(result)\n+            with assert_queries_count(expected_query_count_based_on_db):\n+                RTIF.delete_old_records(task_id=task.task_id, dag_id=task.dag_id, num_to_keep=num_to_keep)\n+            result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n+            assert remaining_rtifs == len(result)\n \n     @pytest.mark.parametrize(\n         \"num_runs, num_to_keep, remaining_rtifs, expected_query_count\",\n@@ -207,40 +210,41 @@ def test_delete_old_records_mapped(\n         Test that old records are deleted from rendered_task_instance_fields table\n         for a given task_id and dag_id with mapped tasks.\n         \"\"\"\n-        with dag_maker(\"test_delete_old_records\", session=session) as dag:\n-            mapped = BashOperator.partial(task_id=\"mapped\").expand(bash_command=[\"a\", \"b\"])\n-        for num in range(num_runs):\n-            dr = dag_maker.create_dagrun(\n-                run_id=f\"run_{num}\", execution_date=dag.start_date + timedelta(days=num)\n+        with set_current_task_instance_session(session=session):\n+            with dag_maker(\"test_delete_old_records\", session=session) as dag:\n+                mapped = BashOperator.partial(task_id=\"mapped\").expand(bash_command=[\"a\", \"b\"])\n+            for num in range(num_runs):\n+                dr = dag_maker.create_dagrun(\n+                    run_id=f\"run_{num}\", execution_date=dag.start_date + timedelta(days=num)\n+                )\n+\n+                mapped.expand_mapped_task(dr.run_id, session=dag_maker.session)\n+                session.refresh(dr)\n+                for ti in dr.task_instances:\n+                    ti.task = dag.get_task(ti.task_id)\n+                    session.add(RTIF(ti))\n+            session.flush()\n+\n+            result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id).all()\n+            assert len(result) == num_runs * 2\n+\n+            # Verify old records are deleted and only 'num_to_keep' records are kept\n+            # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n+            expected_query_count_based_on_db = (\n+                expected_query_count + 1\n+                if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n+                else expected_query_count\n             )\n \n-            mapped.expand_mapped_task(dr.run_id, session=dag_maker.session)\n-            session.refresh(dr)\n-            for ti in dr.task_instances:\n-                ti.task = dag.get_task(ti.task_id)\n-                session.add(RTIF(ti))\n-        session.flush()\n-\n-        result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id).all()\n-        assert len(result) == num_runs * 2\n-\n-        # Verify old records are deleted and only 'num_to_keep' records are kept\n-        # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n-        expected_query_count_based_on_db = (\n-            expected_query_count + 1\n-            if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n-            else expected_query_count\n-        )\n-\n-        with assert_queries_count(expected_query_count_based_on_db):\n-            RTIF.delete_old_records(\n-                task_id=mapped.task_id, dag_id=dr.dag_id, num_to_keep=num_to_keep, session=session\n-            )\n-        result = session.query(RTIF).filter_by(dag_id=dag.dag_id, task_id=mapped.task_id).all()\n-        rtif_num_runs = Counter(rtif.run_id for rtif in result)\n-        assert len(rtif_num_runs) == remaining_rtifs\n-        # Check that we have _all_ the data for each row\n-        assert len(result) == remaining_rtifs * 2\n+            with assert_queries_count(expected_query_count_based_on_db):\n+                RTIF.delete_old_records(\n+                    task_id=mapped.task_id, dag_id=dr.dag_id, num_to_keep=num_to_keep, session=session\n+                )\n+            result = session.query(RTIF).filter_by(dag_id=dag.dag_id, task_id=mapped.task_id).all()\n+            rtif_num_runs = Counter(rtif.run_id for rtif in result)\n+            assert len(rtif_num_runs) == remaining_rtifs\n+            # Check that we have _all_ the data for each row\n+            assert len(result) == remaining_rtifs * 2\n \n     def test_write(self, dag_maker):\n         \"\"\""
    },
    {
      "filename": "tests/models/test_xcom_arg_map.py",
      "status": "modified",
      "patch": "@@ -41,7 +41,7 @@ def pull(value):\n     # The function passed to \"map\" is *NOT* a task.\n     assert set(dag.task_dict) == {\"push\", \"pull\"}\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -79,7 +79,7 @@ def c_to_none(v):\n \n         pull.expand(value=push().map(c_to_none))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -113,7 +113,7 @@ def c_to_none(v):\n \n         pull.expand_kwargs(push().map(c_to_none))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -158,7 +158,7 @@ def does_not_work_with_c(v):\n \n         pull.expand_kwargs(push().map(does_not_work_with_c))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # The \"push\" task should not fail.\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -211,7 +211,7 @@ def skip_c(v):\n \n         collect(value=forward.expand_kwargs(push().map(skip_c)))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -246,7 +246,7 @@ def pull(value):\n         converted = push().map(lambda v: v * 2).map(lambda v: {\"value\": v})\n         pull.expand_kwargs(converted)\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -289,7 +289,7 @@ def convert_zipped(zipped):\n \n         pull.expand(value=combined.map(convert_zipped))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push_letters\" and \"push_numbers\".\n     decision = dr.task_instance_scheduling_decisions(session=session)"
    }
  ],
  "fix_category": "Lock atomic region",
  "root_cause_category": "Concurrency",
  "root_cause_subcategory": NaN
}