{
  "id": 11,
  "repo": "spark",
  "issue_url": "https://github.com/apache/spark/commit/97ba9d29cd1ff83755b0d02251d249a625caace5",
  "pr_url": "https://github.com/apache/spark/commit/97ba9d29cd1ff83755b0d02251d249a625caace5",
  "issue_description": "Since python does not have a library for max heap and usual tricks like inverting values etc.. does not work for all cases. \n\nWe have our own implementation of max heap. \n",
  "files_changed": [
    {
      "filename": "python/pyspark/rdd.py",
      "status": "modified",
      "patch": "@@ -29,7 +29,7 @@\n from tempfile import NamedTemporaryFile\n from threading import Thread\n import warnings\n-from heapq import heappush, heappop, heappushpop\n+import heapq\n \n from pyspark.serializers import NoOpSerializer, CartesianDeserializer, \\\n     BatchedSerializer, CloudPickleSerializer, PairDeserializer, pack_long\n@@ -41,9 +41,9 @@\n \n from py4j.java_collections import ListConverter, MapConverter\n \n-\n __all__ = [\"RDD\"]\n \n+\n def _extract_concise_traceback():\n     \"\"\"\n     This function returns the traceback info for a callsite, returns a dict\n@@ -91,6 +91,73 @@ def __exit__(self, type, value, tb):\n         if _spark_stack_depth == 0:\n             self._context._jsc.setCallSite(None)\n \n+class MaxHeapQ(object):\n+    \"\"\"\n+    An implementation of MaxHeap.\n+    >>> import pyspark.rdd\n+    >>> heap = pyspark.rdd.MaxHeapQ(5)\n+    >>> [heap.insert(i) for i in range(10)]\n+    [None, None, None, None, None, None, None, None, None, None]\n+    >>> sorted(heap.getElements())\n+    [0, 1, 2, 3, 4]\n+    >>> heap = pyspark.rdd.MaxHeapQ(5)\n+    >>> [heap.insert(i) for i in range(9, -1, -1)]\n+    [None, None, None, None, None, None, None, None, None, None]\n+    >>> sorted(heap.getElements())\n+    [0, 1, 2, 3, 4]\n+    >>> heap = pyspark.rdd.MaxHeapQ(1)\n+    >>> [heap.insert(i) for i in range(9, -1, -1)]\n+    [None, None, None, None, None, None, None, None, None, None]\n+    >>> heap.getElements()\n+    [0]\n+    \"\"\"\n+\n+    def __init__(self, maxsize):\n+        # we start from q[1], this makes calculating children as trivial as 2 * k\n+        self.q = [0]\n+        self.maxsize = maxsize\n+\n+    def _swim(self, k):\n+        while (k > 1) and (self.q[k/2] < self.q[k]):\n+            self._swap(k, k/2)\n+            k = k/2\n+\n+    def _swap(self, i, j):\n+        t = self.q[i]\n+        self.q[i] = self.q[j]\n+        self.q[j] = t\n+\n+    def _sink(self, k):\n+        N = self.size()\n+        while 2 * k <= N:\n+            j = 2 * k\n+            # Here we test if both children are greater than parent\n+            # if not swap with larger one.\n+            if j < N and self.q[j] < self.q[j + 1]:\n+                j = j + 1\n+            if(self.q[k] > self.q[j]):\n+                break\n+            self._swap(k, j)\n+            k = j\n+\n+    def size(self):\n+        return len(self.q) - 1\n+\n+    def insert(self, value):\n+        if (self.size()) < self.maxsize:\n+            self.q.append(value)\n+            self._swim(self.size())\n+        else:\n+            self._replaceRoot(value)\n+\n+    def getElements(self):\n+        return self.q[1:]\n+\n+    def _replaceRoot(self, value):\n+        if(self.q[1] > value):\n+            self.q[1] = value\n+            self._sink(1)\n+\n class RDD(object):\n     \"\"\"\n     A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n@@ -696,23 +763,53 @@ def top(self, num):\n         Note: It returns the list sorted in descending order.\n         >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n         [12]\n-        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().top(2)\n+        >>> sc.parallelize([2, 3, 4, 5, 6], 2).cache().top(2)\n         [6, 5]\n         \"\"\"\n         def topIterator(iterator):\n             q = []\n             for k in iterator:\n                 if len(q) < num:\n-                    heappush(q, k)\n+                    heapq.heappush(q, k)\n                 else:\n-                    heappushpop(q, k)\n+                    heapq.heappushpop(q, k)\n             yield q\n \n         def merge(a, b):\n             return next(topIterator(a + b))\n \n         return sorted(self.mapPartitions(topIterator).reduce(merge), reverse=True)\n \n+    def takeOrdered(self, num, key=None):\n+        \"\"\"\n+        Get the N elements from a RDD ordered in ascending order or as specified\n+        by the optional key function. \n+\n+        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n+        [1, 2, 3, 4, 5, 6]\n+        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n+        [10, 9, 7, 6, 5, 4]\n+        \"\"\"\n+\n+        def topNKeyedElems(iterator, key_=None):\n+            q = MaxHeapQ(num)\n+            for k in iterator:\n+                if key_ != None:\n+                    k = (key_(k), k)\n+                q.insert(k)\n+            yield q.getElements()\n+\n+        def unKey(x, key_=None):\n+            if key_ != None:\n+                x = [i[1] for i in x]\n+            return x\n+        \n+        def merge(a, b):\n+            return next(topNKeyedElems(a + b))\n+        result = self.mapPartitions(lambda i: topNKeyedElems(i, key)).reduce(merge)\n+        return sorted(unKey(result, key), key=key)\n+\n+\n     def take(self, num):\n         \"\"\"\n         Take the first num elements of the RDD."
    }
  ],
  "fix_category": "Sleep",
  "root_cause_category": "Time",
  "root_cause_subcategory": NaN
}