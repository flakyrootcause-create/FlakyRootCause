{
  "id": 232,
  "repo": "pytorch",
  "issue_url": "https://github.com/pytorch/pytorch/pull/159215",
  "pr_url": "https://github.com/pytorch/pytorch/pull/159215",
  "issue_description": "When playing around with it, I noticed some flakiness in this test across sessions.\r\n \r\nAfter debugging, turns out the heavy sync primitives that I was calling (like `nvshmem_quiet()` or `nvshmem_fence()`) from inside Triton kernels was causing deadlocks. The original test tried to guarantee ordering: `put(data) -> fence/quiet -> put(flag)`. But the GPU thread got stuck in `quiet()` waiting for network confirmation while holding the SM, creating a deadlock.\r\n\r\nThe fix was realizing `wait_until` already provides all the sync you need. Just do:\r\n- PE A: `nvshmem_wait_until(&ivar, ...)`  \r\n- PE B: `nvshmem_put(&ivar_on_PE_A, ...)`\r\n\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #159788\n* #159756\n* #159755\n* #159734\n* #159701\n* __->__ #159215\n* #159136\n* #158718\n* #158515\n\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",
  "files_changed": [
    {
      "filename": "test/distributed/test_nvshmem_triton.py",
      "status": "modified",
      "patch": "@@ -172,6 +172,11 @@ def barrier_test_kernel(\n         tl.store(p_dst, received + 1)\n \n \n+@triton.jit\n+def barrier_all_kernel():\n+    nvshmem.barrier_all()\n+\n+\n @triton.jit\n def sync_test_kernel(\n     dst_ptr,\n@@ -530,66 +535,49 @@ def test_triton_wait_until(self) -> None:\n \n         rank = self.rank\n         peer = (self.world_size - 1) - rank\n-        NVSHMEM_CMP_EQ = 0  # from nvshmem.h\n-\n-        # Allocate symmetric buffers\n-        msg_size_bytes = 8\n-        dtype = torch.int8\n-        numel = msg_size_bytes // dtype.itemsize\n-        val = 13\n-        flag_val = 21\n+        NVSHMEM_CMP_EQ = 0  # equal comparison\n+        FLAG_INITIAL_VALUE = 0\n+        FLAG_FINAL_VALUE = 42\n \n-        inp = symm_mem.empty(numel, dtype=dtype, device=self.device).fill_(val)\n-        out = symm_mem.empty(numel, dtype=dtype, device=self.device).fill_(-1)\n+        # Use a single int64 symmetric tensor as our synchronization flag.\n+        flag = symm_mem.empty(1, dtype=torch.int64, device=self.device).fill_(\n+            FLAG_INITIAL_VALUE\n+        )\n+        flag_hdl = symm_mem.rendezvous(flag, group=group_name)\n \n-        inp_hdl = symm_mem.rendezvous(inp, group=group_name)\n-        out_hdl = symm_mem.rendezvous(out, group=group_name)\n+        barrier_all_kernel[(1,)](extern_libs=nvshmem_lib)\n \n         if rank == 0:\n-            # Rank 0 waits for the flag to be set by Rank 1, then checks the data\n-            ivar_ptr = out_hdl.signal_pad_ptrs[rank]\n-\n-            wait_until_kernel[(1, 1, 1)](\n+            # Rank 0 (the waiter)\n+            ivar_ptr = flag_hdl.buffer_ptrs[rank]\n+            wait_until_kernel[(1,)](\n                 ivar_ptr,\n                 cmp_op=NVSHMEM_CMP_EQ,\n-                cmp_val=flag_val,\n+                cmp_val=FLAG_FINAL_VALUE,\n                 extern_libs=nvshmem_lib,\n             )\n \n+            # Verification\n             torch.testing.assert_close(\n-                out,\n-                val * torch.ones(numel, dtype=dtype, device=self.device),\n+                flag,\n+                torch.tensor([FLAG_FINAL_VALUE], dtype=torch.int64, device=self.device),\n             )\n \n         if rank == 1:\n-            # Rank 1 puts data into Rank 0's output buffer\n-            dst_ptr = out_hdl.buffer_ptrs[peer]\n-            src_ptr = inp_hdl.buffer_ptrs[rank]\n-\n-            putmem_block_kernel[(1, 1, 1)](\n-                dst_ptr,\n-                src_ptr,\n-                size_bytes=msg_size_bytes,\n-                peer=peer,\n-                extern_libs=nvshmem_lib,\n+            # Rank 1 (the signaler)\n+            val_to_put = torch.tensor(\n+                [FLAG_FINAL_VALUE], dtype=torch.int64, device=self.device\n             )\n \n-            # Fence to order data put before flag put\n-            @triton.jit\n-            def fence_kernel():\n-                nvshmem.fence()\n-\n-            fence_kernel[(1, 1, 1)](extern_libs=nvshmem_lib)\n+            # The destination is Rank 0's flag buffer.\n+            dst_ptr = flag_hdl.buffer_ptrs[rank]\n \n-            # Put the flag value (do not use signal_op here)\n-            flag_src = torch.tensor([flag_val], dtype=torch.int64, device=self.device)\n-            flag_dst_ptr = out_hdl.signal_pad_ptrs[peer]\n-\n-            putmem_block_kernel[(1, 1, 1)](\n-                flag_dst_ptr,\n-                flag_src.data_ptr(),\n-                size_bytes=8,  # 8 bytes for int64\n-                peer=peer,\n+            # Launch a kernel to put the value to Rank 0.\n+            putmem_block_kernel[(1,)](\n+                dst_ptr,  # Destination pointer on the remote PE\n+                val_to_put.data_ptr(),  # Source data pointer (local)\n+                size_bytes=8,  # Size of one int64\n+                peer=peer,  # The target PE (Rank 0)\n                 extern_libs=nvshmem_lib,\n             )\n "
    }
  ],
  "fix_category": "Other",
  "root_cause_category": "Concurrency",
  "root_cause_subcategory": NaN
}