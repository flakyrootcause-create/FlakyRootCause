{
  "id": 234,
  "repo": "pytorch",
  "issue_url": "https://github.com/pytorch/pytorch/pull/158788",
  "pr_url": "https://github.com/pytorch/pytorch/pull/158788",
  "issue_description": "TunableOp will sometimes find a less precise solution due to the small input vectors used in this UT. Bumping op tolerance to eliminate flakiness.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
  "files_changed": [
    {
      "filename": "test/test_linalg.py",
      "status": "modified",
      "patch": "@@ -4762,6 +4762,7 @@ def test_matmul_small_brute_force_3d_Nd(self, device, dtype):\n     @onlyCUDA\n     @skipCUDAIfNotRocm  # Skipping due to SM89 OOM in CI, UT doesn't do much on NV anyways\n     @dtypes(*floating_types_and(torch.half))\n+    @precisionOverride({torch.float16: 1e-1})  # TunableOp may occasionally find less precise solution\n     def test_matmul_small_brute_force_tunableop(self, device, dtype):\n         # disable tunableop buffer rotation for all tests everywhere, it can be slow\n         # We set the TunableOp numerical check environment variable here because it is"
    }
  ],
  "fix_category": "Other",
  "root_cause_category": "Floating point operations",
  "root_cause_subcategory": NaN
}