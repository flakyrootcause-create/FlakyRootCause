{"id": 421, "ground_truth": "Unordered collections", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### What changes are proposed in this pull request?\r\n\r\nFix the flaky test in the two-choice random cache eviction policy\r\n\r\n### Why are the changes needed?\r\n\r\nAs the test is flaky, it will influence other PRs.\r\n\r\n### Does this PR introduce any user facing changes?\r\n\r\nNo.\r\n\n\n### Code Patch:\n@@ -40,7 +40,8 @@ public void evictGetOrder() {\n     mEvictor.updateOnGet(mFirst);\n     Assert.assertEquals(mFirst, mEvictor.evict());\n     mEvictor.updateOnGet(mSecond);\n-    Assert.assertEquals(mSecond, mEvictor.evict());\n+    PageId evictedPage = mEvictor.evict();\n+    Assert.assertTrue(evictedPage.equals(mFirst) || evictedPage.equals(mSecond));\n   }\n \n   @Test\n"}
{"id": 240, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test is flaky likely due to synchronization of the file ingestion thread and the live write thread with test sync points are not working as expected sometimes. Very occasionally, the live write thread can enter the write queue after file ingestion job already dequeued. Or it entered and waited for a very short period of time and quickly returned in the fast path: https://github.com/facebook/rocksdb/blob/833a2266a394fe5f140d2a22f406c82bb605c726/db/write_thread.cc#L83-L86\r\n\r\nTo fix the flakiness, I moved the test sync points to make sure the write thread is already linked into the write queue before the file ingestion writer get dequeued, so it definitely would need to wait some time in order to do its write.\r\n\r\nTest plan:\r\nI'm able to reproduce the flakiness with this command before the fix  with every two or three runs:\r\n./gtest-parallel external_sst_file_basic_test --gtest_filter=ExternalSSTFileBasicTest.Basic --repeat=10000 --workers=100\r\n\r\nAfter the fix, I have tried the command for 10 runs, and there is no failure detected.\n\n### Code Patch:\n@@ -5952,6 +5952,7 @@ Status DBImpl::IngestExternalFiles(\n \n     num_running_ingest_file_ += static_cast<int>(num_cfs);\n     TEST_SYNC_POINT(\"DBImpl::IngestExternalFile:AfterIncIngestFileCounter\");\n+    TEST_SYNC_POINT(\"DBImpl::IngestExternalFile:AfterIncIngestFileCounter:2\");\n \n     bool at_least_one_cf_need_flush = false;\n     std::vector<bool> need_flush(num_cfs, false);\n\n@@ -265,8 +265,8 @@ TEST_F(ExternalSSTFileBasicTest, Basic) {\n   SyncPoint::GetInstance()->LoadDependency({\n       {\"DBImpl::IngestExternalFile:AfterIncIngestFileCounter\",\n        \"ExternalSSTFileBasicTest.LiveWriteStart\"},\n-      {\"ExternalSSTFileBasicTest.LiveWriteStart\",\n-       \"DBImpl::IngestExternalFiles:InstallSVForFirstCF:0\"},\n+      {\"WriteThread::JoinBatchGroup:Wait\",\n+       \"DBImpl::IngestExternalFile:AfterIncIngestFileCounter:2\"},\n   });\n   SyncPoint::GetInstance()->EnableProcessing();\n   PerfContext* write_thread_perf_context;\n"}
{"id": 310, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe changes in #4577 introduced a bit of flakiness on pre-3.10 due to a weird interaction of `capsys`, `stderr` logging and our object lifecycles.\r\n\r\nIn this PR, I'm removing all the explicit `__del__` `kill`s since we [call them all explicitly in `client.close`](https://github.com/getsentry/sentry-python/blob/09c2e32cc7a618e49f5d8ae59e22d8b12f253687/sentry_sdk/client.py#L1001-L1021) anyway and that's already cleaner. Having logic in `__del__` causes non-deterministic GC behavior, especially with threaded code.\r\n\r\nStacktrace is linked in a comment below where you can see the `transport.__del__` method is causing the weird `reentrant` logging bug to `stderr`.\r\n  \n\n### Code Patch:\n@@ -118,7 +118,3 @@ def downsample_factor(self):\n     def kill(self):\n         # type: () -> None\n         self._running = False\n-\n-    def __del__(self):\n-        # type: () -> None\n-        self.kill()\n\n@@ -271,7 +271,3 @@ def add_session(\n     def kill(self):\n         # type: (...) -> None\n         self.__shutdown_requested.set()\n-\n-    def __del__(self):\n-        # type: (...) -> None\n-        self.kill()\n\n@@ -158,13 +158,6 @@ def is_healthy(self):\n         # type: (Self) -> bool\n         return True\n \n-    def __del__(self):\n-        # type: (Self) -> None\n-        try:\n-            self.kill()\n-        except Exception:\n-            pass\n-\n \n def _parse_rate_limits(header, now=None):\n     # type: (str, Optional[datetime]) -> Iterable[Tuple[Optional[EventDataCategory], datetime]]\n"}
{"id": 430, "ground_truth": "Randomness", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe bucket counts can vary run to run. Also don't hardcode the port\n\n### Code Patch:\n@@ -20,7 +20,7 @@\n         {\n           \"key\": \"url.full\",\n           \"value\": {\n-            \"stringValue\": \"http://localhost:8080/\"\n+            \"stringValue\": \"http://localhost:[WILDLINE]/\"\n           }\n         },\n         {\n@@ -78,7 +78,7 @@\n         {\n           \"key\": \"url.full\",\n           \"value\": {\n-            \"stringValue\": \"http://localhost:8080/\"\n+            \"stringValue\": \"http://localhost:[WILDLINE]/\"\n           }\n         },\n         {\n@@ -136,7 +136,7 @@\n         {\n           \"key\": \"url.full\",\n           \"value\": {\n-            \"stringValue\": \"http://localhost:8080/\"\n+            \"stringValue\": \"http://localhost:[WILDLINE]/\"\n           }\n         },\n         {\n@@ -194,7 +194,7 @@\n         {\n           \"key\": \"url.full\",\n           \"value\": {\n-            \"stringValue\": \"http://localhost:8080/\"\n+            \"stringValue\": \"http://localhost:[WILDLINE]/\"\n           }\n         },\n         {\n@@ -252,7 +252,7 @@\n         {\n           \"key\": \"url.full\",\n           \"value\": {\n-            \"stringValue\": \"http://localhost:8080/\"\n+            \"stringValue\": \"http://localhost:[WILDLINE]/\"\n           }\n         },\n         {\n@@ -310,7 +310,7 @@\n         {\n           \"key\": \"url.full\",\n           \"value\": {\n-            \"stringValue\": \"http://localhost:8080/\"\n+            \"stringValue\": \"http://localhost:[WILDLINE]/\"\n           }\n         },\n         {\n@@ -493,21 +493,21 @@\n             \"count\": 3,\n             \"sum\": [WILDCARD],\n             \"bucketCounts\": [\n-              3,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE]\n             ],\n             \"explicitBounds\": [\n               0.005,\n@@ -573,16 +573,16 @@\n             \"count\": 3,\n             \"sum\": 0,\n             \"bucketCounts\": [\n-              3,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0,\n-              0\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE],\n+              [WILDLINE]\n             ],\n             \"explicitBounds\": [\n               0,\n\n@@ -1,10 +1,16 @@\n+let port;\n const server = Deno.serve(\n-  { port: 8080, onListen: () => {} },\n+  {\n+    port: 0,\n+    onListen: (addr) => {\n+      port = addr.port;\n+    },\n+  },\n   () => new Response(\"foo\"),\n );\n \n for (let i = 0; i < 3; i++) {\n-  await fetch(`http://localhost:8080`);\n+  await fetch(`http://localhost:${port}`);\n }\n \n await server.shutdown();\n"}
{"id": 160, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe test JedisPoolTest.testCloseConnectionOnMakeObject occasionally fails with the following error:\r\n\r\n    `JedisPoolTest.testCloseConnectionOnMakeObject:403 expected:<4> but was:<5>`\r\n\r\n**Root Cause:**\r\nThe assertion is based on the output of getClientCount(jedis.clientList()), which is called immediately after closing a connection. However, Redis may take a short time to remove the closed connection from the client list, leading to a transient count mismatch and a flaky test result.\r\n\r\n**Example stack trace:**\r\n```\r\nError:  redis.clients.jedis.JedisPoolTest.testCloseConnectionOnMakeObject -- Time elapsed: 0.006 s <<< FAILURE!\r\njava.lang.AssertionError: expected:<4> but was:<5>\r\n\tat org.junit.Assert.fail(Assert.java:89)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:835)\r\n\tat org.junit.Assert.assertEquals(Assert.java:647)\r\n\tat org.junit.Assert.assertEquals(Assert.java:633)\r\n\tat redis.clients.jedis.JedisPoolTest.testCloseConnectionOnMakeObject(JedisPoolTest.java:403)\r\n```\r\n\t\r\ncloses #4135\n\n### Code Patch:\n@@ -1,12 +1,15 @@\n package redis.clients.jedis;\n \n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static org.awaitility.Awaitility.await;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n import java.net.URI;\n import java.net.URISyntaxException;\n+import java.time.Duration;\n \n import io.redis.test.annotations.SinceRedisVersion;\n import org.apache.commons.pool2.impl.GenericObjectPoolConfig;\n@@ -268,6 +271,8 @@ public void testCloseConnectionOnMakeObject() {\n         pool.getResource();\n         fail(\"Should throw exception as password is incorrect.\");\n       } catch (Exception e) {\n+        await().pollDelay(Duration.ofMillis(10)).atMost(50, MILLISECONDS)\n+            .until(() -> getClientCount(jedis.clientList()) == currentClientCount);\n         assertEquals(currentClientCount, getClientCount(jedis.clientList()));\n       }\n     }\n\n@@ -1,5 +1,7 @@\n package redis.clients.jedis;\n \n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static org.awaitility.Awaitility.await;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertSame;\n@@ -8,6 +10,7 @@\n \n import java.net.URI;\n import java.net.URISyntaxException;\n+import java.time.Duration;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.commons.pool2.PooledObject;\n import org.apache.commons.pool2.PooledObjectFactory;\n@@ -400,6 +403,8 @@ public void testCloseConnectionOnMakeObject() {\n         pool.getResource();\n         fail(\"Should throw exception as password is incorrect.\");\n       } catch (Exception e) {\n+        await().pollDelay(Duration.ofMillis(10)).atMost(50, MILLISECONDS)\n+            .until(() -> getClientCount(jedis.clientList()) == currentClientCount);\n         assertEquals(currentClientCount, getClientCount(jedis.clientList()));\n       }\n     }\n"}
{"id": 30, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nWe currently use `package.json` for our configuration. I think we should move away from using `package.json` and use something like `next.config.js` since it gives us more power than JSON.\r\n\r\nAn example is the issue for custom Webpack support: https://github.com/zeit/next.js/issues/40.\r\nWith a config like this you could do this for example:\r\n\r\n```js\r\n// next.config.js\r\n\r\nexport default {\r\n  webpack: (webpackConfig) => {\r\n    const newConfig = { ...webpackConfig };\r\n    newConfig.module.preloaders.push({ test: /\\.js$/, loader: 'eslint-loader' });\r\n    return newConfig;\r\n  },\r\n  cdn: false\r\n}\r\n```\r\n\r\nWhich is in my opinion better than creating a new file for webpack since this is more centralised. This does however still give users the option to use different files, for the webpack example:\r\n\r\n```js\r\n// next.config.js\r\n\r\nexport default {\r\n  webpack: require('./webpack').default,\r\n  cdn: false\r\n}\r\n```\r\n\r\nThese are just my thoughts, I found out that I needed a sort of config file in `.js` when starting a PR for this project. Very curious what others think of this üòÑ.\n\n### Code Patch:\n@@ -285,6 +285,66 @@ Then run `now` and enjoy!\n \n Note: we recommend putting `.next` in `.npmignore` or `.gitignore`. Otherwise, use `files` or `now.files` to opt-into a whitelist of files you want to deploy (and obviously exclude `.next`)\n \n+## Configuration\n+\n+While Next.js aims to work without any configuration, sometimes there is a need to add custom behaviour.\n+You can define custom configuration in a file called `next.config.js` in the project root directory.\n+An example of a configuration looks like this:\n+\n+```javascript\n+// next.config.js\n+module.exports = {\n+  cdn: true\n+}\n+```\n+\n+### Customizing webpack config\n+\n+Sometimes the user needs to have custom configuration for webpack to add a specific behaviour in the build process.\n+An example of this is using `eslint-loader` to lint the files before compiling. This can be done by defining \n+`webpack` in the config.\n+\n+```javascript\n+module.exports = {\n+  webpack: (webpackConfig, { dev }) => {\n+    webpackConfig.module.preLoaders.push({ test: /\\.js$/, loader: 'eslint-loader' })\n+    return webpackConfig\n+  }\n+}\n+```\n+\n+As you can see you need to provide a function which has two parameters `webpackConfig`, which is the config used by Next.js, and `options`, which contains\n+`dev` (`true` if dev environment). The config you return is the config used by Next.js.\n+You can also return a `Promise` which will be resolved first.\n+\n+_NOTE: Use this option with care, because you can potentially break the existing webpack build configuration by using this option._\n+\n+These are some more examples:\n+\n+```javascript\n+const I18nPlugin = require('i18n-webpack-plugin');\n+\n+module.exports = {\n+  webpack: (webpackConfig, { dev }) => {\n+    // Read image files:\n+    webpackConfig.module.loaders.push({\n+      test: /\\.png$/,\n+      loader: 'file'\n+    })\n+\n+    // Adding a plugin\n+    webpackConfig.plugins.push(new I18nPlugin())\n+\n+    // Or adding an alias\n+    // Create webpackConfig.resolve.alias if it doesn't exist yet:\n+    webpackConfig.resolve.alias = webpackConfig.resolve.alias || {}\n+    webpackConfig.resolve.alias.src = './src'\n+\n+    return webpackConfig\n+  }\n+}\n+```\n+\n ## FAQ\n \n <details>\n@@ -423,7 +483,7 @@ For this reason we want to promote a situation where users can share the cache f\n \n We are committed to providing a great uptime and levels of security for our CDN. Even so, we also **automatically fall back** if the CDN script fails to load [with a simple trick](http://www.hanselman.com/blog/CDNsFailButYourScriptsDontHaveToFallbackFromCDNToLocalJQuery.aspx).\n \n-To turn the CDN off, just set `{ ‚Äúnext‚Äù: { ‚Äúcdn‚Äù: false } }` in `package.json`.\n+To turn the CDN off, just set `module.exports = { cdn: false }` in `next.config.js`.\n </details>\n \n <details>\n\n@@ -2,6 +2,7 @@\n \n import { join } from 'path'\n import { spawn } from 'cross-spawn'\n+import { watchFile } from 'fs'\n \n const defaultCommand = 'dev'\n const commands = new Set([\n@@ -23,9 +24,26 @@ if (commands.has(cmd)) {\n \n const bin = join(__dirname, 'next-' + cmd)\n \n-const proc = spawn(bin, args, { stdio: 'inherit', customFds: [0, 1, 2] })\n-proc.on('close', (code) => process.exit(code))\n-proc.on('error', (err) => {\n-  console.error(err)\n-  process.exit(1)\n-})\n+const startProcess = () => {\n+  const proc = spawn(bin, args, { stdio: 'inherit', customFds: [0, 1, 2] })\n+  proc.on('close', (code) => process.exit(code))\n+  proc.on('error', (err) => {\n+    console.error(err)\n+    process.exit(1)\n+  })\n+  return proc\n+}\n+\n+let proc = startProcess()\n+\n+if (cmd === 'dev') {\n+  watchFile(join(process.cwd(), 'next.config.js'), (cur, prev) => {\n+    if (cur.size > 0 || prev.size > 0) {\n+      console.log('\\n> Found a change in next.config.js, restarting the server...')\n+      // Don't listen to 'close' now since otherwise parent gets killed by listener\n+      proc.removeAllListeners('close')\n+      proc.kill()\n+      proc = startProcess()\n+    }\n+  })\n+}\n\n@@ -9,6 +9,7 @@ import WatchPagesPlugin from './plugins/watch-pages-plugin'\n import WatchRemoveEventPlugin from './plugins/watch-remove-event-plugin'\n import DynamicEntryPlugin from './plugins/dynamic-entry-plugin'\n import DetachPlugin from './plugins/detach-plugin'\n+import getConfig from '../config'\n \n export default async function createCompiler (dir, { dev = false } = {}) {\n   dir = resolve(dir)\n@@ -166,7 +167,7 @@ export default async function createCompiler (dir, { dev = false } = {}) {\n     [errorDebugPath, 'dist/pages/_error-debug.js']\n   ])\n \n-  return webpack({\n+  let webpackConfig = {\n     context: dir,\n     entry,\n     output: {\n@@ -206,5 +207,11 @@ export default async function createCompiler (dir, { dev = false } = {}) {\n     customInterpolateName: function (url, name, opts) {\n       return interpolateNames.get(this.resourcePath) || url\n     }\n-  })\n+  }\n+  const config = getConfig(dir)\n+  if (config.webpack) {\n+    console.log('> Using Webpack config function defined in next.config.js.')\n+    webpackConfig = await config.webpack(webpackConfig, { dev })\n+  }\n+  return webpack(webpackConfig)\n }\n\n@@ -1,9 +1,11 @@\n import { join } from 'path'\n-import { readFile } from 'mz/fs'\n+import { existsSync } from 'fs'\n \n const cache = new Map()\n \n-const defaultConfig = {}\n+const defaultConfig = {\n+  webpack: null\n+}\n \n export default function getConfig (dir) {\n   if (!cache.has(dir)) {\n@@ -12,22 +14,16 @@ export default function getConfig (dir) {\n   return cache.get(dir)\n }\n \n-async function loadConfig (dir) {\n-  const path = join(dir, 'package.json')\n+function loadConfig (dir) {\n+  const path = join(dir, 'next.config.js')\n \n-  let data\n-  try {\n-    data = await readFile(path, 'utf8')\n-  } catch (err) {\n-    if (err.code === 'ENOENT') {\n-      data = '{}'\n-    } else {\n-      throw err\n-    }\n-  }\n+  let userConfig = {}\n \n-  // no try-cache, it must be a valid json\n-  const config = JSON.parse(data).next || {}\n+  const userHasConfig = existsSync(path)\n+  if (userHasConfig) {\n+    const userConfigModule = require(path)\n+    userConfig = userConfigModule.default || userConfigModule\n+  }\n \n-  return Object.assign({}, defaultConfig, config)\n+  return Object.assign({}, defaultConfig, userConfig)\n }\n"}
{"id": 212, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #2495: **Kubelet doesn't kill old pods when BoundPods is empty**\n\nThe first time kubelet starts, while there are no updates, `kl.pods` is `nil` ([pkg/kubelet/kubelet.go#L1118](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1118)):\n\n``` GO\ncase <-time.After(kl.resyncInterval):\n    glog.V(4).Infof(\"Periodic sync\")\n    if kl.pods == nil {\n        continue\n    }\n```\n\nwhich guarantees that [`syncPods(...)`](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L992), which contains the container killing logic is never called.\n\nSo the first part of the fix is to remove this check and allow `syncPods(...)` to be called even when nil, so that the first periodic sync can clean up old containers.\n\nHowever, this isn't enough because inside `syncPods(...)` there is a check to make sure that all sources have been \"seen\" before it will kill old containers ([pkg/kubelet/kubelet.go#L1026](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1026)):\n\n``` GO\nif !kl.sourcesReady() {\n    // If the sources aren't ready, skip deletion, as we may accidentally delete pods\n    // for sources that haven't reported yet.\n    glog.V(4).Infof(\"Skipping deletes, sources aren't ready yet.\")\n    return nil\n}\n```\n\nThis check is performed in the [`SeenAllSources()`](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L87) function, and is `true` only if a `SET` update has been seen for all registered kubelet sources.\n\nHowever, no `SET` update is emitted by any of the sources until they successfully read a value, which means the original problem still exists.\n\nSo the second part of the fix is to have the kubelet boundpod sources (etcd, file, and http) all emit a `SET` update with an empty pod list when the source is first read but no value exists; the empty update will not make it back to the kubelet sync loop because it will be filtered out by the [update merge logic](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L140); but it will cause the source to be [marked as seen](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L238) so the periodic sync (enabled by the first fix) will clean up the old containers (since all sources will now be marked as seen after the first read attempt).\n\n\n### Code Patch:\n@@ -59,7 +59,16 @@ func NewSourceEtcd(key string, client tools.EtcdClient, updates chan<- interface\n }\n \n func (s *sourceEtcd) run() {\n-\twatching := s.helper.Watch(s.key, 0)\n+\tboundPods := api.BoundPods{}\n+\terr := s.helper.ExtractToList(s.key, &boundPods)\n+\tif err != nil {\n+\t\tglog.Errorf(\"etcd failed to retrieve the value for the key %q. Error: %v\", s.key, err)\n+\t\treturn\n+\t}\n+\t// Push update. Maybe an empty PodList to allow EtcdSource to be marked as seen\n+\ts.updates <- kubelet.PodUpdate{boundPods.Items, kubelet.SET, kubelet.EtcdSource}\n+\tindex, _ := s.helper.ResourceVersioner.ResourceVersion(&boundPods)\n+\twatching := s.helper.Watch(s.key, index)\n \tfor {\n \t\tselect {\n \t\tcase event, ok := <-watching.ResultChan():\n@@ -87,6 +96,9 @@ func (s *sourceEtcd) run() {\n // It returns a list of containers, or an error if one occurs.\n func eventToPods(ev watch.Event) ([]api.BoundPod, error) {\n \tpods := []api.BoundPod{}\n+\tif ev.Object == nil {\n+\t\treturn pods, nil\n+\t}\n \tboundPods, ok := ev.Object.(*api.BoundPods)\n \tif !ok {\n \t\treturn pods, errors.New(\"unable to parse response as BoundPods\")\n\n@@ -33,7 +33,7 @@ func TestEventToPods(t *testing.T) {\n \t\t{\n \t\t\tinput: watch.Event{Object: nil},\n \t\t\tpods:  []api.BoundPod{},\n-\t\t\tfail:  true,\n+\t\t\tfail:  false,\n \t\t},\n \t\t{\n \t\t\tinput: watch.Event{Object: &api.BoundPods{}},\n\n@@ -64,6 +64,8 @@ func (s *sourceFile) extractFromPath() error {\n \t\tif !os.IsNotExist(err) {\n \t\t\treturn err\n \t\t}\n+\t\t// Emit an update with an empty PodList to allow FileSource to be marked as seen\n+\t\ts.updates <- kubelet.PodUpdate{[]api.BoundPod{}, kubelet.SET, kubelet.FileSource}\n \t\treturn fmt.Errorf(\"path does not exist, ignoring\")\n \t}\n \n\n@@ -92,8 +92,14 @@ func TestUpdateOnNonExistentFile(t *testing.T) {\n \tNewSourceFile(\"random_non_existent_path\", time.Millisecond, ch)\n \tselect {\n \tcase got := <-ch:\n-\t\tt.Errorf(\"Expected no update, Got %#v\", got)\n+\t\tupdate := got.(kubelet.PodUpdate)\n+\t\texpected := CreatePodUpdate(kubelet.SET, kubelet.FileSource)\n+\t\tif !api.Semantic.DeepEqual(expected, update) {\n+\t\t\tt.Fatalf(\"Expected %#v, Got %#v\", expected, update)\n+\t\t}\n+\n \tcase <-time.After(2 * time.Millisecond):\n+\t\tt.Errorf(\"Expected update, timeout instead\")\n \t}\n }\n \n\n@@ -69,6 +69,8 @@ func (s *sourceURL) extractFromURL() error {\n \t\treturn fmt.Errorf(\"%v: %v\", s.url, resp.Status)\n \t}\n \tif len(data) == 0 {\n+\t\t// Emit an update with an empty PodList to allow HTTPSource to be marked as seen\n+\t\ts.updates <- kubelet.PodUpdate{[]api.BoundPod{}, kubelet.SET, kubelet.HTTPSource}\n \t\treturn fmt.Errorf(\"zero-length data received from %v\", s.url)\n \t}\n \t// Short circuit if the manifest has not changed since the last time it was read.\n\n@@ -1118,9 +1118,6 @@ func (kl *Kubelet) syncLoop(updates <-chan PodUpdate, handler SyncHandler) {\n \t\t\t}\n \t\tcase <-time.After(kl.resyncInterval):\n \t\t\tglog.V(4).Infof(\"Periodic sync\")\n-\t\t\tif kl.pods == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n \t\t}\n \n \t\terr := handler.SyncPods(kl.pods)\n"}
{"id": 415, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n# Description\r\nFixes 3 flaky tests.\r\n\r\n# Changes\r\n- `local_node_store_filtered_solutions`\r\n    - it failed due to timeouts => increased the http delay buffer to give a bit of wiggle room\r\n    - sometimes an auction ran while only 1 of the 2 solutions was included => configure hardcoded solver solutions after both orders have been included in the auction\r\n- `local_node_partially_fillable_pool`\r\n    - solver got deny listed because it failed to produce a working solution for some reason\r\n        - updated the flags to enable that feature to work like `--flag=true` instead of `--flag` (to enable)\r\n        - disabled the feature on all tests except tests specifically for those features\r\n- `local_node_replace_order`\r\n    - order got settled before it could be replaced\r\n        - ban solver until the original order is no longer inside the auction\r\n        - allow unverified quotes for that test due to the banned solver account\r\n\r\n## How to test\r\nused the flaky test runner\r\n- ran all local node tests for 2h -> ~30 successful executions\r\n- `local_node_replace_order` -> flaky test [job](https://github.com/cowprotocol/services/actions/runs/17316920910/job/49161443945) with 369 successful runs\n\n### Code Patch:\n@@ -290,7 +290,8 @@ pub struct NonSettlingSolversFinderConfig {\n         id = \"non_settling_solvers_blacklisting_enabled\",\n         long = \"non-settling-solvers-blacklisting-enabled\",\n         env = \"NON_SETTLING_SOLVERS_BLACKLISTING_ENABLED\",\n-        default_value = \"true\"\n+        default_value = \"true\",\n+        action = clap::ArgAction::Set,\n     )]\n     pub enabled: bool,\n \n@@ -311,7 +312,8 @@ pub struct LowSettlingSolversFinderConfig {\n         id = \"low_settling_solvers_blacklisting_enabled\",\n         long = \"low-settling-solvers-blacklisting-enabled\",\n         env = \"LOW_SETTLING_SOLVERS_BLACKLISTING_ENABLED\",\n-        default_value = \"true\"\n+        default_value = \"true\",\n+        action = clap::ArgAction::Set,\n     )]\n     pub enabled: bool,\n \n\n@@ -135,7 +135,7 @@ merge-solutions = {merge_solutions}\n quote-using-limit-orders = {quote_using_limit_orders}\n enable-simulation-bad-token-detection = true\n enable-metrics-bad-token-detection = true\n-http-time-buffer = \"10ms\"\n+http-time-buffer = \"100ms\"\n solving-share-of-deadline = 1.0\n \"#\n                 )\n\n@@ -308,6 +308,24 @@ impl OnchainComponents {\n         solvers\n     }\n \n+    pub async fn set_solver_allowed(&self, solver: H160, allowed: bool) {\n+        if allowed {\n+            self.contracts\n+                .gp_authenticator\n+                .add_solver(solver)\n+                .send()\n+                .await\n+                .expect(\"failed to add solver\");\n+        } else {\n+            self.contracts\n+                .gp_authenticator\n+                .remove_solver(solver)\n+                .send()\n+                .await\n+                .expect(\"failed to remove solver\");\n+        }\n+    }\n+\n     /// Generate next `N` accounts with the given initial balance and\n     /// authenticate them as solvers on a forked network.\n     pub async fn make_solvers_forked<const N: usize>(\n\n@@ -173,6 +173,8 @@ impl<'a> Services<'a> {\n \n         let args = [\n             \"autopilot\".to_string(),\n+            \"--non-settling-solvers-blacklisting-enabled=false\".to_string(),\n+            \"--low-settling-solvers-blacklisting-enabled=false\".to_string(),\n             \"--max-run-loop-delay=100ms\".to_string(),\n             \"--run-loop-native-price-timeout=500ms\".to_string(),\n             format!(\"--ethflow-contracts={ethflow_contracts}\"),\n\n@@ -320,7 +320,7 @@ async fn quote_timeout(web3: Web3) {\n \n     let assert_within_variance = |start_timestamp: Instant, target| {\n         const VARIANCE: u64 = 100; // small buffer to allow for variance in the test\n-        const HTTP_BUFFER: u64 = 10;\n+        const HTTP_BUFFER: u64 = 100;\n         let min = target - HTTP_BUFFER;\n         let max = min + VARIANCE;\n         let elapsed = start_timestamp.elapsed().as_millis() as u64;\n\n@@ -374,10 +374,25 @@ async fn single_replace_order_test(web3: Web3) {\n         token_a.approve(onchain.contracts().allowance, to_wei(15))\n     );\n \n-    // Place Orders\n+    // disble solver to prevent orders from being settled while we\n+    // want to replace them\n+    onchain.set_solver_allowed(solver.address(), false).await;\n+\n     let services = Services::new(&onchain).await;\n-    services.start_protocol(solver).await;\n+    services\n+        .start_protocol_with_args(\n+            ExtraServiceArgs {\n+                // To avoid race conditions we have to start the protocol\n+                // with the solver being banned. To allow us to still create\n+                // orders we override the quote verification to be disabled.\n+                api: vec![\"--quote-verification=prefer\".into()],\n+                ..Default::default()\n+            },\n+            solver.clone(),\n+        )\n+        .await;\n \n+    let balance_before = token_a.balance_of(trader.address()).call().await.unwrap();\n     let order = OrderCreation {\n         sell_token: token_a.address(),\n         sell_amount: to_wei(10),\n@@ -392,7 +407,6 @@ async fn single_replace_order_test(web3: Web3) {\n         &onchain.contracts().domain_separator,\n         SecretKeyRef::from(&SecretKey::from_slice(trader.private_key()).unwrap()),\n     );\n-    onchain.mint_block().await;\n     let order_id = services.create_order(&order).await.unwrap();\n \n     let app_data = format!(\n@@ -426,32 +440,41 @@ async fn single_replace_order_test(web3: Web3) {\n         &onchain.contracts().domain_separator,\n         SecretKeyRef::from(&SecretKey::from_slice(trader.private_key()).unwrap()),\n     );\n-    let balance_before = token_a.balance_of(trader.address()).call().await.unwrap();\n     let new_order_uid = services.create_order(&new_order).await.unwrap();\n \n+    {\n+        // assert that the new order has the expected appdata\n+        let new_order = services.get_order(&new_order_uid).await.unwrap();\n+        let new_order_appdata = new_order\n+            .metadata\n+            .full_app_data\n+            .expect(\"valid full appData\");\n+        assert_eq!(new_order_appdata, app_data);\n+    }\n+\n     // Check the previous order is cancelled\n     let old_order = services.get_order(&order_id).await.unwrap();\n     assert_eq!(old_order.metadata.status, OrderStatus::Cancelled);\n \n-    // Drive solution\n-    tracing::info!(\"Waiting for trade.\");\n+    tracing::info!(\"wait for old order to be removed\");\n     wait_for_condition(TIMEOUT, || async {\n-        let balance_after = token_a.balance_of(trader.address()).call().await.unwrap();\n         onchain.mint_block().await;\n-        balance_before.saturating_sub(balance_after) == to_wei(3)\n+        let auction = services.get_auction().await.auction;\n+        auction.orders.len() == 1 && auction.orders[0].uid == new_order_uid\n     })\n     .await\n     .unwrap();\n+    // now that the order has been cancelled and the original order\n+    // is no longer part of the auction we can reenable the solver\n+    onchain.set_solver_allowed(solver.address(), true).await;\n \n-    // Check the previous order is cancelled\n+    // Drive solution to verify that new order can be settled\n+    tracing::info!(\"Waiting for trade.\");\n     wait_for_condition(TIMEOUT, || async {\n-        let new_order = services.get_order(&new_order_uid).await.unwrap();\n-        let new_order_appdata = new_order\n-            .metadata\n-            .full_app_data\n-            .expect(\"valid full appData\");\n-        new_order_appdata == app_data\n+        let balance_after = token_a.balance_of(trader.address()).call().await.unwrap();\n+        onchain.mint_block().await;\n+        balance_before.saturating_sub(balance_after) == to_wei(3)\n     })\n     .await\n-    .unwrap()\n+    .unwrap();\n }\n\n@@ -557,7 +557,15 @@ async fn store_filtered_solutions(web3: Web3) {\n \n     let order_ab_id = services.create_order(&order_ab).await.unwrap();\n     let order_ac_id = services.create_order(&order_ac).await.unwrap();\n-    onchain.mint_block().await;\n+\n+    tracing::info!(\"Waiting for both orders to be in the auction\");\n+    wait_for_condition(TIMEOUT, || async {\n+        onchain.mint_block().await;\n+        let auction = services.get_auction().await.auction;\n+        auction.orders.len() == 2\n+    })\n+    .await\n+    .unwrap();\n \n     // good solver settles order_ab at a price 3:1\n     good_solver.configure_solution(Some(Solution {\n\n@@ -54,8 +54,12 @@ async fn non_settling_solver(web3: Web3) {\n \n     let services = Services::new(&onchain).await;\n     let args = ExtraServiceArgs {\n-        // The solver gets banned for 40s.\n-        autopilot: vec![\"--solver-blacklist-cache-ttl=40s\".to_string()],\n+        autopilot: vec![\n+            \"--non-settling-solvers-blacklisting-enabled=true\".to_string(),\n+            \"--low-settling-solvers-blacklisting-enabled=true\".to_string(),\n+            // The solver gets banned for 40s.\n+            \"--solver-blacklist-cache-ttl=40s\".to_string(),\n+        ],\n         ..Default::default()\n     };\n     services.start_protocol_with_args(args, solver).await;\n@@ -122,6 +126,8 @@ async fn low_settling_solver(web3: Web3) {\n     let services = Services::new(&onchain).await;\n     let args = ExtraServiceArgs {\n         autopilot: vec![\n+            \"--non-settling-solvers-blacklisting-enabled=true\".to_string(),\n+            \"--low-settling-solvers-blacklisting-enabled=true\".to_string(),\n             // The solver gets banned for 40s.\n             \"--solver-blacklist-cache-ttl=40s\".to_string(),\n             // The solver is banned if the failure settlement rate is above 55%.\n"}
{"id": 409, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nI moved this in a previous PR, but I think firstly it's only actually required by one of the tests, and more importantly it might be reloading the object before the page has finished submitting.\n\n### Code Patch:\n@@ -35,11 +35,12 @@\n   context do\n     before do\n       click_button \"Close case\"\n-      enforcement.reload\n     end\n \n     it \"can be closed\", :capybara do\n       expect(page).to have_content(\"Case successfully closed\")\n+\n+      enforcement.reload\n       expect(enforcement.status).to eq(\"closed\")\n       expect(enforcement.closed_reason).to eq(\"Because I want to\")\n     end\n"}
{"id": 93, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nReference: https://spec.openapis.org/arazzo/v1.0.0#literals\r\n\r\nThis enables basic parsing of string literals as values in conditions, which in Arazzo are contained in single quote (') characters.\n\n### Code Patch:\n@@ -44,7 +44,19 @@ func newCondition(rawCondition string) (*Condition, error) {\n \t\treturn nil, fmt.Errorf(\"condition must at least be in the format [expression] [operator] [value]\")\n \t}\n \n-\tif len(parts) > 3 || strings.ContainsAny(rawCondition, \"&|\") {\n+\tif strings.ContainsAny(rawCondition, \"&|\") {\n+\t\t// TODO this is a complex condition that we don't currently support\n+\t\treturn nil, nil\n+\t}\n+\n+\t// String literal value handling (single quotes) until parsing is tokenized.\n+\t// Reference: https://spec.openapis.org/arazzo/v1.0.0#literals\n+\tif len(parts) > 3 && strings.HasPrefix(parts[2], \"'\") && strings.HasSuffix(parts[len(parts)-1], \"'\") {\n+\t\tparts[2] = strings.Join(parts[2:], \" \")\n+\t\tparts = parts[:3]\n+\t}\n+\n+\tif len(parts) > 3 {\n \t\t// TODO this is a complex condition that we don't currently support\n \t\treturn nil, nil\n \t}\n\n@@ -0,0 +1,64 @@\n+package criterion\n+\n+import (\n+\t\"fmt\"\n+\t\"testing\"\n+\n+\t\"github.com/speakeasy-api/openapi/arazzo/expression\"\n+\t\"github.com/stretchr/testify/assert\"\n+)\n+\n+func TestNewCondition(t *testing.T) {\n+\tt.Parallel()\n+\n+\ttestCases := map[string]struct {\n+\t\traw           string\n+\t\texpected      *Condition\n+\t\texpectedError error\n+\t}{\n+\t\t\"empty string\": {\n+\t\t\traw:      \"\",\n+\t\t\texpected: nil,\n+\t\t},\n+\t\t\"expression only\": {\n+\t\t\traw:           \"$statusCode\",\n+\t\t\texpected:      nil,\n+\t\t\texpectedError: fmt.Errorf(\"condition must at least be in the format [expression] [operator] [value]\"),\n+\t\t},\n+\t\t\"expression and operator only\": {\n+\t\t\traw:           \"$statusCode ==\",\n+\t\t\texpected:      nil,\n+\t\t\texpectedError: fmt.Errorf(\"condition must at least be in the format [expression] [operator] [value]\"),\n+\t\t},\n+\t\t\"$statusCode == 200\": {\n+\t\t\traw: \"$statusCode == 200\",\n+\t\t\texpected: &Condition{\n+\t\t\t\tExpression: expression.Expression(\"$statusCode\"),\n+\t\t\t\tOperator:   OperatorEQ,\n+\t\t\t\tValue:      \"200\",\n+\t\t\t},\n+\t\t},\n+\t\t\"$response.body#/test == 'string literal with spaces'\": {\n+\t\t\traw: \"$response.body#/test == 'string literal with spaces'\",\n+\t\t\texpected: &Condition{\n+\t\t\t\tExpression: expression.Expression(\"$response.body#/test\"),\n+\t\t\t\tOperator:   OperatorEQ,\n+\t\t\t\tValue:      \"'string literal with spaces'\",\n+\t\t\t},\n+\t\t},\n+\t}\n+\n+\tfor testName, testCase := range testCases {\n+\t\tt.Run(testName, func(t *testing.T) {\n+\t\t\tt.Parallel()\n+\n+\t\t\tactual, actualError := newCondition(testCase.raw)\n+\n+\t\t\tif testCase.expectedError != nil {\n+\t\t\t\tassert.EqualError(t, actualError, testCase.expectedError.Error())\n+\t\t\t}\n+\n+\t\t\tassert.EqualExportedValues(t, testCase.expected, actual)\n+\t\t})\n+\t}\n+}\n"}
{"id": 341, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe test class has a 20% failure rate.\n\nhttps://github.com/xBis7/airflow/actions/runs/16086980198\n\nhttps://github.com/xBis7/airflow/actions/runs/16087424463\n\nhttps://github.com/xBis7/airflow/actions/runs/16087596566\n\nI'm attaching a file with the logs\n\n[fail_logs.txt](https://github.com/user-attachments/files/21076055/fail_logs.txt)\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nI can't reproduce it locally, it's occurring only on the remote CI. It can be reproduced by running the test on the CI on repeat. Increasing execution concurrency makes it more likely to occur. For example, for 10 iterations\n\n```\nmax-parallel: 10\n```\n\n\n\n### Operating System\n\nThe CI uses ubuntu-22.04.\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n\n### Code Patch:\n@@ -873,6 +873,11 @@ def test_scheduler_change_after_the_first_task_finishes(\n         will handle the rest of the dag processing. The paused thread will be resumed afterwards.\n         \"\"\"\n \n+        # For this test, scheduler1 must be idle but still considered healthy by scheduler2.\n+        # If scheduler2 marks the job as unhealthy, then it will recreate scheduler1's spans\n+        # because it will consider them lost.\n+        os.environ[\"AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_THRESHOLD\"] = \"90\"\n+\n         celery_worker_process = None\n         scheduler_process_1 = None\n         apiserver_process = None\n@@ -937,13 +942,12 @@ def test_scheduler_change_after_the_first_task_finishes(\n             with open(self.control_file, \"w\") as file:\n                 file.write(\"continue\")\n \n-            # Wait for scheduler2 to be up and running.\n-            time.sleep(10)\n-\n             wait_for_dag_run_and_check_span_status(\n                 dag_id=dag_id, run_id=run_id, max_wait_time=120, span_status=SpanStatus.SHOULD_END\n             )\n \n+            # Stop scheduler2 in case it still has a db lock on the dag_run.\n+            scheduler_process_2.terminate()\n             scheduler_process_1.send_signal(signal.SIGCONT)\n \n             # Wait for the scheduler to start again and continue running.\n@@ -959,6 +963,9 @@ def test_scheduler_change_after_the_first_task_finishes(\n                 with create_session() as session:\n                     dump_airflow_metadata_db(session)\n \n+            # Reset for the rest of the tests.\n+            os.environ[\"AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_THRESHOLD\"] = \"15\"\n+\n             # Terminate the processes.\n             celery_worker_process.terminate()\n             celery_worker_process.wait()\n@@ -969,7 +976,6 @@ def test_scheduler_change_after_the_first_task_finishes(\n             apiserver_process.terminate()\n             apiserver_process.wait()\n \n-            scheduler_process_2.terminate()\n             scheduler_process_2.wait()\n \n         out, err = capfd.readouterr()\n@@ -991,7 +997,6 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n         \"\"\"\n \n         celery_worker_process = None\n-        scheduler_process_1 = None\n         apiserver_process = None\n         scheduler_process_2 = None\n         try:\n@@ -1032,7 +1037,7 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n             with capfd.disabled():\n                 scheduler_process_1.terminate()\n \n-            assert scheduler_process_1.wait(timeout=30) == 0\n+            assert scheduler_process_1.wait() == 0\n \n             check_dag_run_state_and_span_status(\n                 dag_id=dag_id, run_id=run_id, state=State.RUNNING, span_status=SpanStatus.NEEDS_CONTINUANCE\n@@ -1049,9 +1054,6 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n             with open(self.control_file, \"w\") as file:\n                 file.write(\"continue\")\n \n-            # Wait for scheduler2 to be up and running.\n-            time.sleep(10)\n-\n             wait_for_dag_run_and_check_span_status(\n                 dag_id=dag_id, run_id=run_id, max_wait_time=120, span_status=SpanStatus.ENDED\n             )\n@@ -1066,8 +1068,6 @@ def test_scheduler_exits_gracefully_in_the_middle_of_the_first_task(\n             celery_worker_process.terminate()\n             celery_worker_process.wait()\n \n-            scheduler_process_1.wait()\n-\n             apiserver_process.terminate()\n             apiserver_process.wait()\n \n@@ -1253,9 +1253,6 @@ def test_scheduler_exits_forcefully_after_the_first_task_finishes(\n                 stderr=None,\n             )\n \n-            # Wait for scheduler2 to be up and running.\n-            time.sleep(10)\n-\n             wait_for_dag_run_and_check_span_status(\n                 dag_id=dag_id, run_id=run_id, max_wait_time=120, span_status=SpanStatus.ENDED\n             )\n"}
{"id": 419, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test checks if loading an app in the background will catch and re-raise an exception. It occasionally fails on pypy3. Possibly, the background loading thread does not finish before the assert line, and so there is no exception yet. Try to address this by waiting for the lock before asserting. This could still fail if the thread starts *very* slowly.\r\n\r\ncloses #4292 \n\n### Code Patch:\n@@ -334,7 +334,9 @@ def bad_load():\n     lazy = DispatchingApp(bad_load, use_eager_loading=False)\n \n     with pytest.raises(BadExc):\n-        lazy._flush_bg_loading_exception()\n+        # reduce flakiness by waiting for the internal loading lock\n+        with lazy._lock:\n+            lazy._flush_bg_loading_exception()\n \n \n def test_with_appcontext(runner):\n"}
{"id": 70, "ground_truth": "OS", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test has recently become flaky on windows CI, and before investigating further, see if it's just because the CI machines are overloaded and subprocesses are slower on windows.\n\n### Code Patch:\n@@ -853,7 +853,7 @@ def test_destructor_log(self):\n             stdout=subprocess.PIPE,\n             stderr=subprocess.STDOUT,\n             check=True,\n-            timeout=5,\n+            timeout=15,\n         )\n         if proc.stdout:\n             print(\"STDOUT:\")\n"}
{"id": 284, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nWhat I observed is that if I run both combinations useNewProtocol=true,\nuseNewProtocol=false it would often fail the second time, but if I only\nrun the second variation useNewProtocol=false it works, and only the\nfirst variation useNewProtocol=true also works. So this points to some\nstate that is not cleared between the tests  - and indeed, the test\ncreates a topic ‚ÄúinputTopic‚Äù, produces to it, but doesn‚Äôt delete it, so\nthe second variation will run with produce to it again and then run with\ntwice the data.\n\nI also reduced heartbeat interval and session timeout since some of the\ntests need to wait for the old consumer to leave which (sigh) Kafka\nStreams doesn't do, so we have to wait that it gets kicked out by\nsession timeout. So previously we waited for 45 seconds, now, we at\nleast wait only 1 second.\n\nReviewers: Bill Bejeck <bbejeck@apache.org>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n\n### Code Patch:\n@@ -175,6 +175,8 @@ private Properties props(final Properties extraProperties) {\n         streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.IntegerSerde.class);\n         streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.IntegerSerde.class);\n         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000L);\n+        streamsConfiguration.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 500);\n+        streamsConfiguration.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 1000);\n         streamsConfiguration.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\n         streamsConfiguration.putAll(extraProperties);\n \n@@ -191,6 +193,7 @@ public void shutdown() throws Exception {\n \n         IntegrationTestUtils.purgeLocalStreamsState(streamsConfigurations);\n         streamsConfigurations.clear();\n+        CLUSTER.deleteAllTopics();\n     }\n \n     @ParameterizedTest\n"}
{"id": 65, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n - [x] closes #14626 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n\r\n\n\n### Code Patch:\n@@ -913,7 +913,16 @@ def test_summary(self):\n     def test_format(self):\n         self._check_method_works(Index.format)\n \n-        index = Index([datetime.now()])\n+        # GH 14626\n+        def datetime_now_without_trailing_zeros():\n+            now = datetime.now()\n+\n+            while str(now).endswith(\"000\"):\n+                now = datetime.now()\n+\n+            return now\n+\n+        index = Index([datetime_now_without_trailing_zeros()])\n \n         # windows has different precision on datetime.datetime.now (it doesn't\n         # include us since the default for Timestamp shows these but Index\n"}
{"id": 204, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nWhen you have a rewrite in middleware like this:\r\n```ts\r\nreturn NextResponse.rewrite(u, {\r\n  status: 403,\r\n});\r\n```\r\nThe rewrite status code is not propogated to the cache interceptor's responses when you have an external middleware. \r\n\r\nWe can also safely remove the `x-next-cache-tags` header on the response.\r\n\r\nI did update some tests that were flakey aswell. We need to register the event listener before the navigation.\n\n### Code Patch:\n@@ -0,0 +1,5 @@\n+---\n+\"@opennextjs/aws\": patch\n+---\n+\n+fix: Ensure middleware rewrite status code is properly propagated to cache interceptor\n\n@@ -125,7 +125,10 @@ const defaultHandler = async (\n         }\n       }\n \n-      result.headers[INTERNAL_EVENT_REQUEST_ID] = requestId;\n+      if (process.env.OPEN_NEXT_REQUEST_ID_HEADER || globalThis.openNextDebug) {\n+        result.headers[INTERNAL_EVENT_REQUEST_ID] = requestId;\n+      }\n+\n       debug(\"Middleware response\", result);\n       return result;\n     },\n\n@@ -63,7 +63,7 @@ export async function openNextHandler(\n       }\n       debug(\"internalEvent\", internalEvent);\n \n-      // These 2 will get overwritten by the routing handler if not using an external middleware\n+      // These 3 will get overwritten by the routing handler if not using an external middleware\n       const internalHeaders = {\n         initialPath:\n           initialHeaders[INTERNAL_HEADER_INITIAL_URL] ?? internalEvent.rawPath,\n\n@@ -1,7 +1,11 @@\n import { createHash } from \"node:crypto\";\n \n import { NextConfig, PrerenderManifest } from \"config/index\";\n-import type { InternalEvent, InternalResult } from \"types/open-next\";\n+import type {\n+  InternalEvent,\n+  InternalResult,\n+  MiddlewareEvent,\n+} from \"types/open-next\";\n import type { CacheValue } from \"types/overrides\";\n import { emptyReadableStream, toReadableStream } from \"utils/stream\";\n \n@@ -100,7 +104,7 @@ async function computeCacheControl(\n }\n \n async function generateResult(\n-  event: InternalEvent,\n+  event: MiddlewareEvent,\n   localizedPath: string,\n   cachedValue: CacheValue<\"cache\">,\n   lastModified?: number,\n@@ -132,8 +136,12 @@ async function generateResult(\n   );\n   return {\n     type: \"core\",\n-    // sometimes other status codes can be cached, like 404. For these cases, we should return the correct status code\n-    statusCode: cachedValue.meta?.status ?? 200,\n+    // Sometimes other status codes can be cached, like 404. For these cases, we should return the correct status code\n+    // Also set the status code to the rewriteStatusCode if defined\n+    // This can happen in handleMiddleware in routingHandler.\n+    // `NextResponse.rewrite(url, { status: xxx})\n+    // The rewrite status code should take precedence over the cached one\n+    statusCode: event.rewriteStatusCode ?? cachedValue.meta?.status ?? 200,\n     body: toReadableStream(body, false),\n     isBase64Encoded: false,\n     headers: {\n@@ -179,7 +187,7 @@ function decodePathParams(pathname: string): string {\n }\n \n export async function cacheInterceptor(\n-  event: InternalEvent,\n+  event: MiddlewareEvent,\n ): Promise<InternalEvent | InternalResult> {\n   if (\n     Boolean(event.headers[\"next-action\"]) ||\n@@ -287,7 +295,8 @@ export async function cacheInterceptor(\n \n           return {\n             type: \"core\",\n-            statusCode: cachedData.value.meta?.status ?? 200,\n+            statusCode:\n+              event.rewriteStatusCode ?? cachedData.value.meta?.status ?? 200,\n             body: toReadableStream(cachedData.value.body, isBinary),\n             headers: {\n               ...cacheControl,\n\n@@ -6,7 +6,11 @@ import {\n   NextConfig,\n   PrerenderManifest,\n } from \"config/index.js\";\n-import type { InternalEvent, InternalResult } from \"types/open-next.js\";\n+import type {\n+  InternalEvent,\n+  InternalResult,\n+  MiddlewareEvent,\n+} from \"types/open-next.js\";\n import { emptyReadableStream } from \"utils/stream.js\";\n \n import { getQueryFromSearchParams } from \"../../overrides/converters/utils.js\";\n@@ -28,12 +32,6 @@ const middleMatch = getMiddlewareMatch(\n \n const REDIRECTS = new Set([301, 302, 303, 307, 308]);\n \n-type MiddlewareEvent = InternalEvent & {\n-  responseHeaders?: Record<string, string | string[]>;\n-  isExternalRewrite?: boolean;\n-  rewriteStatusCode?: number;\n-};\n-\n type Middleware = (request: Request) => Response | Promise<Response>;\n type MiddlewareLoader = () => Promise<{ default: Middleware }>;\n \n@@ -198,6 +196,7 @@ export async function handleMiddleware(\n     cookies: internalEvent.cookies,\n     remoteAddress: internalEvent.remoteAddress,\n     isExternalRewrite,\n-    rewriteStatusCode: statusCode,\n+    rewriteStatusCode:\n+      rewriteUrl && !isExternalRewrite ? statusCode : undefined,\n   } satisfies MiddlewareEvent;\n }\n\n@@ -32,6 +32,12 @@ export type InternalEvent = {\n   readonly remoteAddress: string;\n } & BaseEventOrResult<\"core\">;\n \n+export type MiddlewareEvent = InternalEvent & {\n+  responseHeaders?: Record<string, string | string[]>;\n+  isExternalRewrite?: boolean;\n+  rewriteStatusCode?: number;\n+};\n+\n export type InternalResult = {\n   statusCode: number;\n   headers: Record<string, string | string[]>;\n\n@@ -39,7 +39,10 @@ export function getTagsFromValue(value?: CacheValue<\"cache\">) {\n   }\n   // The try catch is necessary for older version of next.js that may fail on this\n   try {\n-    return value.meta?.headers?.[\"x-next-cache-tags\"]?.split(\",\") ?? [];\n+    const cacheTags =\n+      value.meta?.headers?.[\"x-next-cache-tags\"]?.split(\",\") ?? [];\n+    delete value.meta?.headers?.[\"x-next-cache-tags\"];\n+    return cacheTags;\n   } catch (e) {\n     return [];\n   }\n\n@@ -23,21 +23,25 @@ test(\"Middleware Rewrite\", async ({ page }) => {\n });\n \n test(\"Middleware Rewrite External Image\", async ({ page }) => {\n-  await page.goto(\"/rewrite-external\");\n   page.on(\"response\", async (response) => {\n     expect(response.status()).toBe(200);\n     expect(response.headers()[\"content-type\"]).toBe(\"image/png\");\n     expect(response.headers()[\"cache-control\"]).toBe(\"max-age=600\");\n     const bodyBuffer = await response.body();\n     expect(validateMd5(bodyBuffer, OPENNEXT_PNG_MD5)).toBe(true);\n   });\n+  await page.goto(\"/rewrite-external\");\n });\n \n test(\"Middleware Rewrite Status Code\", async ({ page }) => {\n+  page.on(\"response\", async (response) => {\n+    // Need to set up the event before navigating to the page to avoid missing it\n+    // We need to check the URL here also cause there will be multiple responses (i.e the fonts, css, js, etc)\n+    if (response.url() === \"/rewrite-status-code\") {\n+      expect(response.status()).toBe(403);\n+    }\n+  });\n   await page.goto(\"/rewrite-status-code\");\n   const el = page.getByText(\"Rewritten Destination\", { exact: true });\n   await expect(el).toBeVisible();\n-  page.on(\"response\", async (response) => {\n-    expect(response.status()).toBe(403);\n-  });\n });\n\n@@ -1,7 +1,7 @@\n /* eslint-disable sonarjs/no-duplicate-string */\n import { cacheInterceptor } from \"@opennextjs/aws/core/routing/cacheInterceptor.js\";\n import { convertFromQueryString } from \"@opennextjs/aws/core/routing/util.js\";\n-import type { InternalEvent } from \"@opennextjs/aws/types/open-next.js\";\n+import type { MiddlewareEvent } from \"@opennextjs/aws/types/open-next.js\";\n import type { Queue } from \"@opennextjs/aws/types/overrides.js\";\n import { fromReadableStream } from \"@opennextjs/aws/utils/stream.js\";\n import { vi } from \"vitest\";\n@@ -26,14 +26,14 @@ vi.mock(\"@opennextjs/aws/adapters/config/index.js\", () => ({\n }));\n \n vi.mock(\"@opennextjs/aws/core/routing/i18n/index.js\", () => ({\n-  localizePath: (event: InternalEvent) => event.rawPath,\n+  localizePath: (event: MiddlewareEvent) => event.rawPath,\n }));\n \n type PartialEvent = Partial<\n-  Omit<InternalEvent, \"body\" | \"rawPath\" | \"query\">\n+  Omit<MiddlewareEvent, \"body\" | \"rawPath\" | \"query\">\n > & { body?: string };\n \n-function createEvent(event: PartialEvent): InternalEvent {\n+function createEvent(event: PartialEvent): MiddlewareEvent {\n   const [rawPath, qs] = (event.url ?? \"/\").split(\"?\", 2);\n   return {\n     type: \"core\",\n@@ -45,6 +45,7 @@ function createEvent(event: PartialEvent): InternalEvent {\n     query: convertFromQueryString(qs ?? \"\"),\n     cookies: event.cookies ?? {},\n     remoteAddress: event.remoteAddress ?? \"::1\",\n+    rewriteStatusCode: event.rewriteStatusCode,\n   };\n }\n \n@@ -452,4 +453,72 @@ describe(\"cacheInterceptor\", () => {\n       }),\n     );\n   });\n+\n+  it(\"should return the rewrite status code when there is active cache\", async () => {\n+    const event = createEvent({\n+      url: \"/albums\",\n+      rewriteStatusCode: 403,\n+    });\n+    incrementalCache.get.mockResolvedValueOnce({\n+      value: {\n+        type: \"app\",\n+        html: \"Hello, world!\",\n+      },\n+    });\n+\n+    const result = await cacheInterceptor(event);\n+    expect(result.statusCode).toBe(403);\n+  });\n+\n+  it(\"should return the rewriteStatusCode if there is a cached status code\", async () => {\n+    const event = createEvent({\n+      url: \"/albums\",\n+      rewriteStatusCode: 203,\n+    });\n+    incrementalCache.get.mockResolvedValueOnce({\n+      value: {\n+        type: \"app\",\n+        html: \"Hello, world!\",\n+        meta: {\n+          status: 404,\n+        },\n+      },\n+    });\n+\n+    const result = await cacheInterceptor(event);\n+    expect(result.statusCode).toBe(203);\n+  });\n+\n+  it(\"should return the cached status code if there is one\", async () => {\n+    const event = createEvent({\n+      url: \"/albums\",\n+    });\n+    incrementalCache.get.mockResolvedValueOnce({\n+      value: {\n+        type: \"app\",\n+        html: \"Hello, world!\",\n+        meta: {\n+          status: 405,\n+        },\n+      },\n+    });\n+\n+    const result = await cacheInterceptor(event);\n+    expect(result.statusCode).toBe(405);\n+  });\n+\n+  it(\"should return 200 if there is no cached status code, nor a rewriteStatusCode\", async () => {\n+    const event = createEvent({\n+      url: \"/albums\",\n+    });\n+    incrementalCache.get.mockResolvedValueOnce({\n+      value: {\n+        type: \"app\",\n+        html: \"Hello, world!\",\n+      },\n+    });\n+\n+    const result = await cacheInterceptor(event);\n+    expect(result.statusCode).toBe(200);\n+  });\n });\n"}
{"id": 101, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nLink to issue: #8390\r\n\r\n## What is the purpose of the change\r\n\r\nThis PR is to fix a non-idempotent test `org.apache.dubbo.rpc.filter.ExecuteLimitFilterTest.testExecuteLimitInvokeWitException` \r\n- The test does not remove `RpcStatus` after running, which pollutes the state shared among tests.\r\n- It may be better to clean state pollutions so that some other tests won't fail in the future due to the shared state pollution.\r\n\r\n## Brief changelog\r\nRemoves the status of RPC when the test ends.\r\n\r\n## Verifying this change\r\n\r\nWith the proposed fix, the test does not pollute the shared state (and passes when run twice in the same JVM).\r\n\r\n<!-- Follow this checklist to help us incorporate your contribution quickly and easily: -->\r\n\r\n## Checklist\r\n- [x] Make sure there is a [GitHub_issue](https://github.com/apache/dubbo/issues) field for the change (usually before you start working on it). Trivial changes like typos do not require a GitHub issue. Your pull request should address just this issue, without pulling in other changes - one PR resolves one issue.\r\n- [x] Each commit in the pull request should have a meaningful subject line and body.\r\n- [x] Write a pull request description that is detailed enough to understand what the pull request does, how, and why.\r\n- [x] Check if is necessary to patch to Dubbo 3 if you are work on Dubbo 2.7\r\n- [x] Write necessary unit-test to verify your logic correction, more mock a little better when cross module dependency exist. If the new feature or significant change is committed, please remember to add sample in [dubbo samples](https://github.com/apache/dubbo-samples) project.\r\n- [x] Add some description to [dubbo-website](https://github.com/apache/dubbo-website) project if you are requesting to add a feature.\r\n- [x] GitHub Actions works fine on your own branch.\r\n- [x] If this contribution is large, please follow the [Software Donation Guide](https://github.com/apache/dubbo/wiki/Software-donation-guide).\n\n### Code Patch:\n@@ -85,6 +85,7 @@ public void testExecuteLimitInvokeWitException() throws Exception {\n             executeLimitFilter.onError(e, invoker, invocation);\n         }\n         Assertions.assertEquals(1, RpcStatus.getStatus(url, invocation.getMethodName()).getFailed());\n+        RpcStatus.removeStatus(url, invocation.getMethodName());\n     }\n \n     @Test\n"}
{"id": 408, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nTest was failing the 15th of each month\n\n### Code Patch:\n@@ -742,10 +742,11 @@ describe('evaluateFilterConditions', () => {\n \n         expect(evaluateFilterConditions({ filters: [filter] })).toBe(true);\n \n+        const otherDate = new Date('2023-01-16');\n         const differentFilter = createFilter(\n           ViewFilterOperand.Is,\n           sameDate1,\n-          now,\n+          otherDate,\n           'DATE',\n         );\n \n"}
{"id": 136, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'common/utils/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThe listener test in `ClientStreamingQuerySuite` is flaky.\r\n\r\nFor client side listeners, the terminated events might take a while before arriving to the client. This test is currently flaky, example: https://github.com/anishshri-db/spark/actions/runs/9785389228/job/27018350836\r\n\r\nThis PR tries to deflake it by waiting for a longer time.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nDeflake test\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nTest only change\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\n<!--\r\nIf generative AI tooling has been used in the process of authoring this patch, please include the\r\nphrase: 'Generated-by: ' followed by the name of the tool and its version.\r\nIf no, write 'No'.\r\nPlease refer to the [ASF Generative Tooling Guidance](https://www.apache.org/legal/generative-tooling.html) for details.\r\n-->\r\nNo\n\n### Code Patch:\n@@ -23,7 +23,7 @@ import java.util.concurrent.TimeUnit\n \n import scala.jdk.CollectionConverters._\n \n-import org.scalatest.concurrent.Eventually.eventually\n+import org.scalatest.concurrent.Eventually.{eventually, interval}\n import org.scalatest.concurrent.Futures.timeout\n import org.scalatest.time.SpanSugar._\n \n@@ -475,7 +475,7 @@ class ClientStreamingQuerySuite extends QueryTest with RemoteSparkSession with L\n     } finally {\n       q.stop()\n \n-      eventually(timeout(30.seconds)) {\n+      eventually(timeout(60.seconds), interval(1.seconds)) {\n         assert(!q.isActive)\n         assert(!spark.table(s\"listener_terminated_events$tablePostfix\").toDF().isEmpty)\n       }\n@@ -528,7 +528,7 @@ class ClientStreamingQuerySuite extends QueryTest with RemoteSparkSession with L\n       }\n     } finally {\n       q.stop()\n-      eventually(timeout(30.seconds)) {\n+      eventually(timeout(60.seconds), interval(1.seconds)) {\n         assert(!q.isActive)\n         assert(listener.terminate.nonEmpty)\n       }\n"}
{"id": 191, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nhttps://circleci.com/gh/square/okhttp/8330?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\r\n\r\ncloseReasonMaximumLength - okhttp3.internal.ws.WebSocketHttpTest\r\n```\r\norg.junit.ComparisonFailure: expected:<[0]> but was:<[1]>\r\n\tat okhttp3.OkHttpClientTestRule.ensureAllConnectionsReleased(OkHttpClientTestRule.kt:91)\r\n\tat okhttp3.OkHttpClientTestRule$apply$1.evaluate(OkHttpClientTestRule.kt:121)\r\n```\r\n\r\nserverCloseThenClientClose - okhttp3.internal.ws.RealWebSocketTest\r\n```\r\njava.lang.AssertionError: \r\nExpecting empty but was:<[Closed[1000 Hello!]]>\r\n\tat okhttp3.internal.ws.WebSocketRecorder.assertExhausted(WebSocketRecorder.java:163)\r\n\tat okhttp3.internal.ws.RealWebSocketTest.tearDown(RealWebSocketTest.java:58)\r\n```\n\n### Code Patch:\n@@ -130,9 +130,15 @@ public final class RealWebSocketTest {\n \n   @Test public void serverCloseThenClientClose() throws IOException {\n     server.webSocket.close(1000, \"Hello!\");\n+\n     client.processNextFrame();\n     client.listener.assertClosing(1000, \"Hello!\");\n     assertThat(client.webSocket.close(1000, \"Bye!\")).isTrue();\n+    client.listener.assertClosed(1000, \"Hello!\");\n+\n+    server.processNextFrame();\n+    server.listener.assertClosing(1000, \"Bye!\");\n+    server.listener.assertClosed(1000, \"Bye!\");\n   }\n \n   @Test public void emptyCloseInitiatesShutdown() throws IOException {\n"}
{"id": 202, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe `getContext` seem to randomly timing out on github action. \r\n<img width=\"494\" alt=\"image\" src=\"https://github.com/radarlabs/radar-sdk-ios/assets/139801512/9de1176f-0d6b-434e-b63a-f28b242da8e2\">\r\n<img width=\"417\" alt=\"image\" src=\"https://github.com/radarlabs/radar-sdk-ios/assets/139801512/ec3ec059-f3cc-47aa-8f3f-54ac4a7e72ad\">\r\nThis is likely an issue with github action because\r\n- This does not occur on circle CI\r\n- This does not occur on local testing\r\nAs a sanity check, the SDK endpoint for `getContext` was called on waypoint with no noticeable issue. \n\n### Code Patch:\n@@ -823,7 +823,7 @@ - (void)test_Radar_getContext_errorLocation {\n         [expectation fulfill];\n     }];\n \n-    [self waitForExpectationsWithTimeout:60\n+    [self waitForExpectationsWithTimeout:120\n                                  handler:^(NSError *_Nullable error) {\n                                      if (error) {\n                                          XCTFail();\n"}
{"id": 379, "ground_truth": "Resource leak", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nReduce flakiness/sensitivity of PyFunc cleanup test.\n\nOccasionally, unrelated-seeming changes would cause the GC-based cleanup test to fail, because `gc.collect()` did not collect all garbage. Calling `gc.collect()` twice in sequence will perform a more thorough collection, including cycles that were broken in the first call to `gc.collect()`. There is a plausible theory that the finalizers used in the `WeakValueDictionary` in `script_ops._py_funcs` could be contributing to the issue.\n\n\n### Code Patch:\n@@ -482,13 +482,15 @@ class PyFuncAndEagerPyFuncTest(PyFuncTestBase):\n   def verifyPyFuncsNoIncrease(self, make_graph):\n     ops.reset_default_graph()\n     gc.collect()\n+    gc.collect()\n     initial_size = script_ops._py_funcs.size()\n \n     for _ in range(1000):\n       make_graph()\n \n     ops.reset_default_graph()\n     gc.collect()\n+    gc.collect()\n     self.assertEqual(initial_size, script_ops._py_funcs.size())\n \n   def testCleanup(self):\n"}
{"id": 107, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Problem\nThe\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutTopic()`,\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutGroup()`,\n`ResetStreamsGroupOffsetTest.testResetOffsetsWithoutGroupOption()`,\n`DeleteStreamsGroupTest.testDeleteWithoutGroupOption()`,\n`DescribeStreamsGroupTest.testDescribeWithoutGroupOption()` tests were\nflaky due to a dependency on Set iteration order in error message\ngeneration.\n\n### Root Cause\nThe cleanup [commit](https://github.com/apache/kafka/pull/20091) that\nreplaced `new HashSet<>(Arrays.asList(...))` with `Set.of(...)` in\nShareGroupCommandOptions and StreamsGroupCommandOptions changed the\niteration characteristics of collections used for error message\ngeneration:\n\nThis produces different orders like `[topic], [group]` vs `[group],\n[topic]`, but the tests expected a specific order, causing intermittent\nfailures.\n\n### Solution\nFix the root cause by ensuring deterministic error message generation\nthrough alphabetical sorting of option names.\n\nReviewers: ShivsundarR <shr@confluent.io>, Ken Huang\n <s7133700@gmail.com>, TengYao Chi <frankvicky@apache.org>\n\n\n### Code Patch:\n@@ -159,11 +159,11 @@ public void checkArgs() {\n         if (options.has(describeOpt)) {\n             if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n             List<OptionSpec<?>> mutuallyExclusiveOpts = List.of(membersOpt, offsetsOpt, stateOpt);\n             if (mutuallyExclusiveOpts.stream().mapToInt(o -> options.has(o) ? 1 : 0).sum() > 1) {\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n             }\n             if (options.has(stateOpt) && options.valueOf(stateOpt) != null)\n                 CommandLineUtils.printUsageAndExit(parser,\n@@ -185,7 +185,7 @@ public void checkArgs() {\n         if (options.has(deleteOffsetsOpt)) {\n             if (!options.has(groupOpt) || !options.has(topicOpt))\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + deleteOffsetsOpt + \" takes the following options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + deleteOffsetsOpt + \" takes the following options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         }\n \n         if (options.has(resetOffsetsOpt)) {\n\n@@ -222,7 +222,7 @@ void checkArgs() {\n         if (options.has(deleteOpt)) {\n             if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + deleteOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + deleteOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n             if (options.has(inputTopicOpt) || options.has(allInputTopicsOpt))\n                 CommandLineUtils.printUsageAndExit(parser, \"Kafka Streams does not support topic-specific offset \" +\n                     \"deletion from a streams group.\");\n@@ -253,11 +253,11 @@ private void checkDescribeArgs() {\n \n         if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         List<OptionSpec<?>> mutuallyExclusiveOpts = List.of(membersOpt, offsetsOpt, stateOpt);\n         if (mutuallyExclusiveOpts.stream().mapToInt(o -> options.has(o) ? 1 : 0).sum() > 1) {\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         }\n         if (options.has(stateOpt) && options.valueOf(stateOpt) != null)\n             CommandLineUtils.printUsageAndExit(parser,\n@@ -267,7 +267,7 @@ private void checkDescribeArgs() {\n     private void checkDeleteOffsetsArgs() {\n         if ((!options.has(inputTopicOpt) && !options.has(allInputTopicsOpt)) || !options.has(groupOpt))\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + deleteOffsetsOpt + \" takes the \" + groupOpt + \" and one of these options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + deleteOffsetsOpt + \" takes the \" + groupOpt + \" and one of these options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         if (options.valuesOf(groupOpt).size() > 1)\n             CommandLineUtils.printUsageAndExit(parser,\n                 \"Option \" + deleteOffsetsOpt + \" supports only one \" + groupOpt + \" at a time, but found: \" + options.valuesOf(groupOpt));\n@@ -286,7 +286,7 @@ private void checkOffsetResetArgs() {\n \n         if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + resetOffsetsOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + resetOffsetsOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n \n         CommandLineUtils.checkInvalidArgs(parser, options, resetToOffsetOpt, minus(allResetOffsetScenarioOpts, resetToOffsetOpt));\n         CommandLineUtils.checkInvalidArgs(parser, options, resetToDatetimeOpt, minus(allResetOffsetScenarioOpts, resetToDatetimeOpt));\n@@ -301,7 +301,7 @@ private void checkOffsetResetArgs() {\n     private void checkDeleteAllInternalTopicsArgs() {\n         if (!options.has(resetOffsetsOpt) && !options.has(deleteOpt)) {\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + deleteAllInternalTopicsOpt + \" takes one of these options: \" + allDeleteInternalGroupsOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + deleteAllInternalTopicsOpt + \" takes one of these options: \" + allDeleteInternalGroupsOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         } else if (options.has(resetOffsetsOpt) && !options.has(executeOpt)) {\n             CommandLineUtils.printUsageAndExit(parser,\n                 \"Option \" + deleteAllInternalTopicsOpt + \" takes \" + executeOpt + \" when \" + resetOffsetsOpt + \" is used.\");\n\n@@ -626,7 +626,7 @@ public void testDeleteShareGroupOffsetsArgsWithoutTopic() {\n         AtomicBoolean exited = new AtomicBoolean(false);\n         Exit.setExitProcedure(((statusCode, message) -> {\n             assertNotEquals(0, statusCode);\n-            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [topic], [group]\"));\n+            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [group], [topic]\"));\n             exited.set(true);\n         }));\n         try {\n@@ -646,7 +646,7 @@ public void testDeleteShareGroupOffsetsArgsWithoutGroup() {\n         AtomicBoolean exited = new AtomicBoolean(false);\n         Exit.setExitProcedure(((statusCode, message) -> {\n             assertNotEquals(0, statusCode);\n-            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [topic], [group]\"));\n+            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [group], [topic]\"));\n             exited.set(true);\n         }));\n         try {\n"}
{"id": 235, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nFixing existing runs:\r\n- [x] Don't run Jedi on PyPy. (see https://github.com/python-trio/trio/pull/2887 and https://github.com/python-trio/trio/pull/3075) (need to update `test_static_tool_sees_class_members` too)\r\n- [x] Only run autodeps in `python-trio/trio` (see https://github.com/python-trio/trio/pull/2892)\r\n- [x] Add a `gc.collect()` to the top of `test_for_leaking_fds` (this seems to address test flakiness I've seen elsewhere) (see https://github.com/python-trio/trio/pull/2888)\r\n- [x] Start running `mypy` on PyPy. (see https://github.com/python-trio/trio/pull/3075)\r\n- [x] switch macos to run on m1 (see https://github.com/python-trio/trio/pull/2953)\r\n\r\nAdding more runs:\r\n- [ ] Add an anaconda run\r\n- [x] Alpine run (see https://github.com/python-trio/trio/pull/2933)\r\n- [ ] Try out qemu for a freebsd\r\n- [x] Cython run (see https://github.com/python-trio/trio/pull/2942)\r\n  - [ ] Run full test suite on Cython\r\n- [x] Update MacOS and Windows Python matrix to support 3.11 and 3.12. (see https://github.com/python-trio/trio/pull/3017)\r\n- [x] Add CPython 3.13 (see https://github.com/python-trio/trio/pull/3005)\r\n- [ ] ... Android run??? (https://github.com/marketplace/actions/android-emulator-runner ??)\r\n- [x] pypy nightly 3.11 (not possible)\r\n- [x] add pypy runs for macos (see https://github.com/python-trio/trio/pull/3074)\r\n- [x] add pypy runs for windows (see #2776 and #2678) (see https://github.com/python-trio/trio/pull/3074)\r\n\r\nRemoving runs:\r\n- [x] Don't run PyPy nightlies less than 3.11 (see https://github.com/python-trio/trio/pull/2952)\r\n\r\nThese feel like they will take more motivation than I currently have :(\n\n### Code Patch:\n@@ -1,5 +1,6 @@\n from __future__ import annotations\n \n+import gc\n import os\n import random\n import signal\n@@ -620,6 +621,8 @@ async def test_run_process_background_fail() -> None:\n     reason=\"requires a way to iterate through open files\",\n )\n async def test_for_leaking_fds() -> None:\n+    gc.collect()  # address possible flakiness on PyPy\n+\n     starting_fds = set(SyncPath(\"/dev/fd\").iterdir())\n     await run_process(EXIT_TRUE)\n     assert set(SyncPath(\"/dev/fd\").iterdir()) == starting_fds\n"}
{"id": 36, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #135370\n\nSummary: This test is flaky when run after `test_dynamic_shapes_persistent_reduction_mixed_x_dim_cuda_cuda_wrapper` because the TestCase sets config options globally in its setUp() that stick around for subsequent tests. For test isolation, we use a contextlib.ExitStack pattern in other tests to patch the config options and restore them in tearDown(). Update all TestCases in `test/inductor/test_combo_kernels.py` to use that pattern.\n\nTest Plan:\n```\npython test/inductor/test_combo_kernels.py\npython test/inductor/test_cuda_cpp_wrapper.py TestCudaWrapper.test_dynamic_shapes_persistent_reduction_mixed_x_dim_cuda_cuda_wrapper TestCudaWrapper.test_randint_cuda_cuda_wrapper\n```\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang\n\n### Code Patch:\n@@ -1,5 +1,6 @@\n # Owner(s): [\"module: inductor\"]\n \n+import contextlib\n import sys\n import unittest\n \n@@ -36,12 +37,20 @@ class ComboKernelTests(TestCase):\n     def setUp(self):\n         super().setUp()\n         torch._inductor.metrics.reset()\n-        torch._inductor.config.combo_kernels = True\n-        torch._inductor.config.benchmark_combo_kernel = False\n+        self._test_stack = contextlib.ExitStack()\n+        self._test_stack.enter_context(\n+            torch._inductor.config.patch(\n+                {\n+                    \"combo_kernels\": True,\n+                    \"benchmark_combo_kernel\": False,\n+                }\n+            )\n+        )\n \n     def tearDown(self):\n-        super().tearDown()\n+        self._test_stack.close()\n         torch._inductor.metrics.reset()\n+        super().tearDown()\n \n     @requires_cuda\n     def test_activation_functions(self):\n@@ -157,12 +166,20 @@ class ComboKernelBenchmarkTests(TestCase):\n     def setUp(self):\n         super().setUp()\n         torch._inductor.metrics.reset()\n-        torch._inductor.config.combo_kernels = True\n-        torch._inductor.config.benchmark_combo_kernel = True\n+        self._test_stack = contextlib.ExitStack()\n+        self._test_stack.enter_context(\n+            torch._inductor.config.patch(\n+                {\n+                    \"combo_kernels\": True,\n+                    \"benchmark_combo_kernel\": True,\n+                }\n+            )\n+        )\n \n     def tearDown(self):\n-        super().tearDown()\n+        self._test_stack.close()\n         torch._inductor.metrics.reset()\n+        super().tearDown()\n \n     @requires_cuda\n     def test_activation_benchmark(self):\n@@ -303,14 +320,28 @@ class ComboKernelDynamicShapesTests(TestCase):\n     def setUp(self):\n         super().setUp()\n         torch._inductor.metrics.reset()\n-        torch._inductor.config.combo_kernels = True\n-        torch._inductor.config.benchmark_combo_kernel = True\n-        torch._dynamo.config.automatic_dynamic_shapes = False\n-        torch._dynamo.config.assume_static_by_default = False\n+        self._test_stack = contextlib.ExitStack()\n+        self._test_stack.enter_context(\n+            torch._inductor.config.patch(\n+                {\n+                    \"combo_kernels\": True,\n+                    \"benchmark_combo_kernel\": True,\n+                }\n+            )\n+        )\n+        self._test_stack.enter_context(\n+            torch._dynamo.config.patch(\n+                {\n+                    \"automatic_dynamic_shapes\": False,\n+                    \"assume_static_by_default\": False,\n+                }\n+            )\n+        )\n \n     def tearDown(self):\n-        super().tearDown()\n+        self._test_stack.close()\n         torch._inductor.metrics.reset()\n+        super().tearDown()\n \n     @requires_cuda\n     def test_dynamic_shapes_activations(self):\n"}
{"id": 42, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\r\n\r\n`test/parallel/test-zlib-brotli-16GB.js`\r\n\r\n### Platform\r\n\r\nAIX\r\n\r\n### Console output\r\n\r\n```console\r\n17:07:37 not ok 3588 parallel/test-zlib-brotli-16GB\r\n17:07:37   ---\r\n17:07:37   duration_ms: 412.84800\r\n17:07:37   severity: fail\r\n17:07:37   exitcode: 1\r\n17:07:37   stack: |-\r\n17:07:37     node:assert:126\r\n17:07:37       throw new AssertionError(obj);\r\n17:07:37       ^\r\n17:07:37     \r\n17:07:37     AssertionError [ERR_ASSERTION]: Expected values to be strictly equal:\r\n17:07:37     \r\n17:07:37     0 !== 1\r\n17:07:37     \r\n17:07:37         at Timeout.<anonymous> (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/parallel/test-zlib-brotli-16GB.js:21:3)\r\n17:07:37         at Timeout._onTimeout (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/common/index.js:476:15)\r\n17:07:37         at listOnTimeout (node:internal/timers:573:17)\r\n17:07:37         at process.processTimers (node:internal/timers:514:7) {\r\n17:07:37       generatedMessage: true,\r\n17:07:37       code: 'ERR_ASSERTION',\r\n17:07:37       actual: 0,\r\n17:07:37       expected: 1,\r\n17:07:37       operator: 'strictEqual'\r\n17:07:37     }\r\n17:07:37     \r\n17:07:37     Node.js v21.6.3-pre\r\n17:07:37   ...\r\n```\r\n\r\n\r\n### Build links\r\n\r\n- https://ci.nodejs.org/job/node-test-commit-aix/50223/nodes=aix72-ppc64/console\r\n\r\n### Additional information\r\n\r\n_No response_\n\n### Code Patch:\n@@ -19,4 +19,4 @@ decoder.end(buf);\n setTimeout(common.mustCall(() => {\n   // There is only one chunk in the buffer\n   strictEqual(decoder._readableState.buffer.length, 1);\n-}), common.platformTimeout(100));\n+}), common.platformTimeout(500));\n"}
{"id": 76, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n##### SUMMARY\r\n\r\nThis PR fixes various issues with the ansible-playbook-callbacks integration test.\r\n\r\n##### ISSUE TYPE\r\n\r\n- Test Pull Request\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nThe timing of the async tasks was a little unpredictable, meaning that sometimes we would get an unexpected number of v2_runner_on_async_poll callbacks, and fail the test. This change fixes the issue by increasing the poll interval to 2 seconds and the sleep duration to 3 seconds, such that on a reasonably responsive system we will poll twice per task, with the sleep ending in the middle of the two polls.\r\n    \r\nThe include_me.yml file does not exist in this integration test. It has been added.\r\n    \r\nThe remote_tmp_dir.path expression is invalid - the setup_remote_tmp_dir role uses set_fact to set remote_tmp_dir to remote_tmp_dir.path.\r\n    \r\nThe integration tests run with ANSIBLE_HOST_PATTERN_MISMATCH=error, meaning that the final play was never reached. Set ANSIBLE_HOST_PATTERN_MISMATCH=warning to continue past the play and trigger the v2_playbook_on_no_hosts_matched callback.\n\n### Code Patch:\n@@ -96,22 +96,22 @@\n       ignore_errors: true\n \n     - name: async poll ok\n-      command: sleep 2\n-      async: 3\n-      poll: 1\n+      command: sleep 3\n+      async: 5\n+      poll: 2\n \n     - name: async poll failed\n-      shell: sleep 2; false\n-      async: 3\n-      poll: 1\n+      shell: sleep 3; false\n+      async: 5\n+      poll: 2\n       ignore_errors: true\n \n     - include_tasks: include_me.yml\n \n     - name: diff\n       copy:\n         content: diff\n-        dest: '{{ remote_tmp_dir.path }}/diff.txt'\n+        dest: '{{ remote_tmp_dir }}/diff.txt'\n       diff: true\n \n - hosts: i_dont_exist\n\n@@ -1,12 +1,14 @@\n  1 __init__\n-83 v2_on_any\n+92 v2_on_any\n+ 1 v2_on_file_diff\n  4 v2_playbook_on_handler_task_start\n  2 v2_playbook_on_include\n+ 1 v2_playbook_on_no_hosts_matched\n  3 v2_playbook_on_notify\n- 1 v2_playbook_on_play_start\n+ 3 v2_playbook_on_play_start\n  1 v2_playbook_on_start\n  1 v2_playbook_on_stats\n-17 v2_playbook_on_task_start\n+19 v2_playbook_on_task_start\n  1 v2_playbook_on_vars_prompt\n  1 v2_runner_item_on_failed\n  2 v2_runner_item_on_ok\n@@ -15,8 +17,8 @@\n  1 v2_runner_on_async_ok\n  2 v2_runner_on_async_poll\n  5 v2_runner_on_failed\n-15 v2_runner_on_ok\n+16 v2_runner_on_ok\n  1 v2_runner_on_skipped\n-21 v2_runner_on_start\n+23 v2_runner_on_start\n  1 v2_runner_on_unreachable\n  2 v2_runner_retry\n\n\n\n@@ -5,6 +5,7 @@ set -eux\n export ANSIBLE_CALLBACK_PLUGINS=../support-callback_plugins/callback_plugins\n export ANSIBLE_ROLES_PATH=../\n export ANSIBLE_STDOUT_CALLBACK=callback_debug\n+export ANSIBLE_HOST_PATTERN_MISMATCH=warning\n \n ansible-playbook all-callbacks.yml 2>/dev/null | sort | uniq -c | tee callbacks_list.out\n \n"}
{"id": 423, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### About\nThis PR attempts to removed the flakiness in\n`testAlterReadCommittedToReadUncommittedIsolationLevelWithReleaseAck`\nand `testAlterReadCommittedToReadUncommittedIsolationLevelWithRejectAck`\n([link](https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.test=testAlterReadCommittedToReadUncommittedIsolationLevelWithReleaseAck()%5B2%5D)).\nThis flakiness could potentially be caused because we were not ensuring\nthat the aborted transaction record produce happened. In this PR, I have\nadded a blocking call to make sure the produce future completes before\nwe abort the transaction.\n\n### Testing\nThe fix was tested locally by running the tests\n\nReviewers: Andrew Schofield <aschofield@confluent.io>\n\n\n### Code Patch:\n@@ -2623,8 +2623,9 @@ private void produceAbortedTransaction(Producer<byte[], byte[]> transactionalPro\n         try {\n             transactionalProducer.beginTransaction();\n             ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(tp.topic(), tp.partition(), null, message.getBytes(), message.getBytes());\n-            transactionalProducer.send(record);\n+            Future<RecordMetadata> future = transactionalProducer.send(record);\n             transactionalProducer.flush();\n+            future.get(); // Ensure producer send is complete before aborting\n             transactionalProducer.abortTransaction();\n         } catch (Exception e) {\n             transactionalProducer.abortTransaction();\n"}
{"id": 130, "ground_truth": "I/O", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Summary of the Pull Request\r\nThis might be the problem why from time to time `MarkdownPreviewHandlerControlAddsBrowserToFormWhenDoPreviewIsCalled ` test fails - timeouts after 10s\r\n\r\n**What is this about:**\r\n\r\n**What is included in the PR:** \r\n\r\n**How does someone test / validate:** \r\n\r\n## Quality Checklist\r\n\r\n- [ ] **Linked issue:** #xxx\r\n- [ ] **Communication:** I've discussed this with core contributors in the issue. \r\n- [ ] **Tests:** Added/updated and all pass\r\n- [ ] **Installer:** Added/updated and all pass\r\n- [ ] **Localization:** All end user facing strings can be localized\r\n- [ ] **Docs:** Added/ updated\r\n- [ ] **Binaries:** Any new files are added to WXS / YML\r\n   - [ ] No new binaries\r\n   - [ ] [YML for signing](https://github.com/microsoft/PowerToys/blob/main/.pipelines/pipeline.user.windows.yml#L68) for new binaries\r\n   - [ ] [WXS for installer](https://github.com/microsoft/PowerToys/blob/main/installer/PowerToysSetup/Product.wxs) for new binaries\r\n\r\n## Contributor License Agreement (CLA)\r\nA CLA must be signed. If not, go over [here](https://cla.opensource.microsoft.com/microsoft/PowerToys) and sign the CLA.\r\n\n\n### Code Patch:\n@@ -1,2 +1,2 @@\n-Ôªø![Minion](https://octodex.github.com/images/minion.png)\n+Ôªø![Some title](./a.png)\n <script>alert(\"hello\")</script>\n\\ No newline at end of file\n"}
{"id": 178, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #13547\n\n### Code Patch:\n@@ -116,9 +116,10 @@ public void testSubclassConcurrentMergeScheduler() throws IOException {\n     Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n     doc.add(idField);\n \n+    MyMergeScheduler mergeScheduler = new MyMergeScheduler();\n     IndexWriterConfig iwc =\n         newIndexWriterConfig(new MockAnalyzer(random()))\n-            .setMergeScheduler(new MyMergeScheduler())\n+            .setMergeScheduler(mergeScheduler)\n             .setMaxBufferedDocs(2)\n             .setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH)\n             .setMergePolicy(newLogMergePolicy());\n@@ -141,13 +142,7 @@ public void testSubclassConcurrentMergeScheduler() throws IOException {\n       // OK\n     }\n \n-    try {\n-      ((MyMergeScheduler) writer.getConfig().getMergeScheduler()).sync();\n-    } catch (\n-        @SuppressWarnings(\"unused\")\n-        IllegalStateException ise) {\n-      // OK\n-    }\n+    mergeScheduler.sync();\n     writer.rollback();\n \n     try {\n"}
{"id": 141, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nhttps://github.com/apache/airflow/actions/runs/16658216543/job/47149178223#step:10:696\r\n\r\nSometimes the test failing with cause of other test stored cache.\r\nWhen we run entire test file its working if we run only `task-sdk/tests/task_sdk/io/test_path.py::TestAttach::test_alias` , this was failing due to there is no previously cached object and its asserting for two fields.\r\n \r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n\n### Code Patch:\n@@ -74,6 +74,8 @@ def test_lazy_load():\n \n     assert o.fs is not None\n     assert o._fs_cached\n+    # Clear the cache to avoid side effects in other tests below\n+    _STORE_CACHE.clear()\n \n \n class _FakeRemoteFileSystem(MemoryFileSystem):\n@@ -123,7 +125,7 @@ def fake_files(self):\n     def test_alias(self):\n         store = attach(\"file\", alias=\"local\")\n         assert isinstance(store.fs, LocalFileSystem)\n-        assert {\"local\": store, \"file\": store} == _STORE_CACHE\n+        assert {\"local\": store} == _STORE_CACHE\n \n     def test_objectstoragepath_init_conn_id_in_uri(self):\n         attach(protocol=\"fake\", conn_id=\"fake\", fs=_FakeRemoteFileSystem(conn_id=\"fake\"))\n"}
{"id": 291, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nCloses #12889 \n\n### Code Patch:\n@@ -1427,7 +1427,11 @@ public void testParallelPartialProgressWithMaxCommitsLargerThanTotalGroupCount()\n \n     List<Object[]> postRewriteData = currentData();\n     assertEquals(\"We shouldn't have changed the data\", originalData, postRewriteData);\n-    shouldHaveSnapshots(table, 11);\n+    table.refresh();\n+    assertThat(table.snapshots())\n+        .as(\"Table did not have the expected number of snapshots\")\n+        // To tolerate 1 random commit failure\n+        .hasSizeGreaterThanOrEqualTo(10);\n     shouldHaveNoOrphans(table);\n     shouldHaveACleanCache(table);\n   }\n\n@@ -1427,7 +1427,11 @@ public void testParallelPartialProgressWithMaxCommitsLargerThanTotalGroupCount()\n \n     List<Object[]> postRewriteData = currentData();\n     assertEquals(\"We shouldn't have changed the data\", originalData, postRewriteData);\n-    shouldHaveSnapshots(table, 11);\n+    table.refresh();\n+    assertThat(table.snapshots())\n+        .as(\"Table did not have the expected number of snapshots\")\n+        // To tolerate 1 random commit failure\n+        .hasSizeGreaterThanOrEqualTo(10);\n     shouldHaveNoOrphans(table);\n     shouldHaveACleanCache(table);\n   }\n\n@@ -1427,7 +1427,11 @@ public void testParallelPartialProgressWithMaxCommitsLargerThanTotalGroupCount()\n \n     List<Object[]> postRewriteData = currentData();\n     assertEquals(\"We shouldn't have changed the data\", originalData, postRewriteData);\n-    shouldHaveSnapshots(table, 11);\n+    table.refresh();\n+    assertThat(table.snapshots())\n+        .as(\"Table did not have the expected number of snapshots\")\n+        // To tolerate 1 random commit failure\n+        .hasSizeGreaterThanOrEqualTo(10);\n     shouldHaveNoOrphans(table);\n     shouldHaveACleanCache(table);\n   }\n"}
{"id": 382, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Summary\n- Fixed flaky combobox tests that were failing due to CSS transitions\n\n## Problem\nThe combobox tests were failing with \"Unable to find an accessible element with the role 'listbox'\" because they were using `getByRole` which expects the element to be immediately visible. However, the Menu component applies CSS transitions (invisible scale-95 opacity-0) when closed, making the listbox technically present but not visible.\n\n## Solution\nChanged from `getByRole('listbox')` to `findByRole('listbox')` which waits for the element to become visible, accounting for the CSS transition timing.\n\n## Test Plan\n[ ] Run `pnpm stories:test` - combobox tests should pass\n[ ] Verify Async Select test passes\n[ ] Verify Multiselect test passes\n\n### Code Patch:\n@@ -96,7 +96,8 @@\n \n     await userEvent.type(combobox, 'Japanese');\n \n-    const menu = canvas.getByRole('listbox');\n+    // Wait for the listbox to appear since it may have a transition\n+    const menu = await canvas.findByRole('listbox');\n \n     expect(menu).toBeInTheDocument();\n   }}\n@@ -113,7 +114,8 @@\n \n     await userEvent.type(combobox, 'Jerseyan');\n \n-    const menu = canvas.getByRole('listbox');\n+    // Wait for the listbox to appear since it may have a transition\n+    const menu = await canvas.findByRole('listbox');\n     const noResults = canvas.getByText('No Results');\n \n     expect(menu).toBeInTheDocument();\n@@ -140,7 +142,8 @@\n     const canvas = within(canvasElement);\n     const combobox = canvas.getByTestId(id);\n     await userEvent.type(combobox, 'E');\n-    const menu = canvas.getByRole('listbox');\n+    // Wait for the listbox to appear since it may have a transition\n+    const menu = await canvas.findByRole('listbox');\n     expect(menu).toBeInTheDocument();\n   }}\n />\n@@ -153,7 +156,8 @@\n \n     await userEvent.type(combobox, 'one');\n \n-    const menu = canvas.getByRole('listbox');\n+    // Wait for the listbox to appear since it may have a transition\n+    const menu = await canvas.findByRole('listbox');\n \n     expect(menu).toBeInTheDocument();\n \n"}
{"id": 330, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n#### Description of Change\nI'm not sure what event to hook here, so poll instead.\n\n#### Checklist\n<!-- Remove items that do not apply. For completed items, change [ ] to [x]. -->\n\n- [x] PR description included and stakeholders cc'd\n- [x] `npm test` passes\n- [x] tests are [changed or added](https://github.com/electron/electron/blob/master/docs/development/testing.md)\n- [x] PR title follows semantic [commit guidelines](https://github.com/electron/electron/blob/master/docs/development/pull-requests.md#commit-message-guidelines)\n- [x] This is **NOT A BREAKING CHANGE**. Breaking changes may not be merged to master until 11-x-y is branched.\n\n#### Release Notes\n\nNotes: none\n\n\n### Code Patch:\n@@ -190,14 +190,13 @@ describe('ipcRenderer module', () => {\n         const childIpc = child.require('electron').ipcRenderer;\n         child.close();\n         return new Promise(resolve => {\n-          setTimeout(() => {\n+          setInterval(() => {\n             try {\n               childIpc.send('hello');\n             } catch (e) {\n               resolve(e);\n             }\n-            resolve(false);\n-          }, 100);\n+          }, 16);\n         });\n       }})()`);\n       expect(error).to.have.property('message', 'IPC method called after context was released');\n"}
{"id": 19, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n- **Google Hangouts Video**: http://www.youtube.com/watch?v=Z0KHYPlFI3E\n- **GitHub Issue**: https://github.com/iojs/io.js/issues/300\n- **Original Minutes Google Doc**: https://docs.google.com/document/d/1HDUayYxTjolYZhu_hGm9hc-6MGAwWrHlNGT2Zo708ck\n\n\n### Code Patch:\n@@ -0,0 +1,101 @@\n+# io.js TC Meeting 2015-01-13\n+\n+## Links\n+\n+* **Google Hangouts Video**: http://www.youtube.com/watch?v=Z0KHYPlFI3E\n+* **GitHub Issue**: https://github.com/iojs/io.js/issues/300\n+* **Original Minutes Google Doc**: https://docs.google.com/document/d/1HDUayYxTjolYZhu_hGm9hc-6MGAwWrHlNGT2Zo708ck\n+\n+## Agenda\n+\n+Extracted from https://github.com/iojs/io.js/labels/tc-agenda prior to meeting.\n+\n+* File copyright policy #216 https://github.com/iojs/io.js/issues/216\n+* governance: Add new Collaborators #234 https://github.com/iojs/io.js/issues/234\n+* The state of ES6 on io.js _(re V8 upgrade policy)_ #251 https://github.com/iojs/io.js/issues/251\n+  Quick recap:\n+  - Whether if it is ok to release 1.0.0 with V8 3.31.74 minus ES6 Classes and Object Literals Extensions, considering that the initial version of io.js will be \"beta\" quality and probably the stable version will coincide with the stable release of V8 3.31 (~6 weeks), which will have these features reinstated.\n+  - How to handle V8 updates with semver so that if the same issue arised with a different timing it could mean a bump to 2.0.0.\n+  - Community suggestions:\n+    * Different version naming (e.g. 1.x+v8beta, 1.x+v8dev, 1.x+v8canary)\n+    * Avoid BC by moving from tip-of-stable-Chrome-branch to the next tip-of-stable-Chrome-branch (~6 weeks release cycle)\n+  - How do nightly builds fit into the process.\n+* 1.0.0 release checklist https://github.com/iojs/io.js/issues/302\n+\n+## Minutes\n+\n+### Present\n+\n+* Trevor (TC)\n+* Chris (TC)\n+* Ben (TC)\n+* Bert (TC)\n+* Rod (facilitator)\n+\n+### Mini stand-up\n+\n+* Ben: 1.0.0 work\n+* Bert: 1.0.0 work (and other things)\n+* Chris: Docs mainly, made a new repo for it\n+* Trevor: Stuff\n+* Rod: tons of 1.0.0 release prep work, tidying up test slaves, preparing fresh release slaves and configuring release builds & installers\n+\n+### Review of last meeting\n+\n+* Invite Colin Ihrig to the TC #223 https://github.com/iojs/io.js/issues/223\n+* File copyright policy #216 https://github.com/iojs/io.js/issues/216\n+* Doc: clarified & split up contribution docs #233 https://github.com/iojs/io.js/pull/233\n+* Governance: Add new Collaborators #234 https://github.com/iojs/io.js/issues/234\n+* deps: upgrade v8 to 3.31.74.1 #243 https://github.com/iojs/io.js/pull/243\n+* Build & release (& Intl)\n+\n+\n+### File copyright policy #216 \n+\n+https://github.com/iojs/io.js/issues/216\n+\n+Rod: prepared a proposed new LICENSE file, needed for release, adds a new license block for io.js at the top, shifts the Joyent copyright block down but still acknowledges their claim on inherited work. https://github.com/iojs/io.js/pull/294\n+\n+Group agreed to merge for now until a better (or more legally appropriate) solution is proposed.\n+\n+### governance: Add new Collaborators #234 \n+\n+https://github.com/iojs/io.js/issues/234\n+\n+Rod asked if we should punt to next meeting since the group was small and we‚Äôre 1.0.0 obsessed.\n+\n+Group agreed.\n+\n+### The state of ES6 on io.js _(re V8 upgrade policy)_ #251 \n+\n+https://github.com/iojs/io.js/issues/251\n+\n+Rod: this is about V8 version matching and V8 version stability. We‚Äôre going live with 3.31.\n+\n+Discussion with the V8 team suggests that 3.31 is probably not appropriate because of stability concerns. We have already had to disable class support because the V8 team announced their intention to disable it in the next release anyway.\n+\n+Group agreed to stick with 3.31, too late to do otherwise. Come up with a more solid stability policy later.\n+\n+### v1.0.0 release checklist\n+\n+https://github.com/iojs/io.js/issues/302\n+\n+* Rod will land LICENSE change after meeting\n+* Bert spoke about the v0.10 and the things that remain unmerged\n+  - abort on uncaughtException for domains not landed, so may be missed; feature used by Hapi / Walmart Labs for postmoretem debugging\n+  - private buffer object patch not landed, repl will crash if you do this currently\n+  - sslv2 & sslv3 fixes -- we have agreed to disable these completely\n+  - re timers slowness - Bert and Ben don‚Äôt like the 0.10 fix, need to find our own - not landing in 1.0.0, probably our own fix in 1.0.1\n+* Bert spoke about node.exe, has code for it but it‚Äôs crashy, could release with a symlink for 1.0.0. Asked for input on the approach.\n+* Discussed node-gyp - Ben to PR node-gyp (and maybe land a modified version in io.js codebase) for iojs.org/dist/vx.y.z/... for source tarball.\n+  - discussed process.versions.iojs property, https://github.com/iojs/io.js/issues/269 - group agreed to do nothing on this. Rod to find discussion or create issue to have discussion on this.\n+  - discussed visual assets not being in the repo, currently copied by CI manually into src, Rod to land a PR with them\n+* Rod: Changelog, we don‚Äôt have one.\n+  - https://github.com/joyent/node/wiki/API-changes-between-v0.10-and-v0.12\n+  - Rod: is it a blocker? Ben: not per-se but very good to have.\n+  - Rod: proposed to come up with something on GitHub and ask for people to take work. Trevor will do Buffer, Chris streams, Bert child processes.\n+\n+\n+### Next meeting\n+\n+* Back to normal schedule next week\n"}
{"id": 194, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThese tests were causing flakes where the mock method was being called more than once. The tests were also difficult to understand.\n\nThis change removes the need for mocking (hopefully increasing test stability) and also should hopefully make it easier to understand what these tests are meant to be checking\n\n### Code Patch:\n@@ -9,7 +9,6 @@\n import pytest\n from sentry_sdk.client import Client\n from sentry_sdk.utils import datetime_from_isoformat\n-from tests.conftest import patch_start_tracing_child\n \n import sentry_sdk\n import sentry_sdk.scope\n@@ -935,46 +934,100 @@ def class_(cls, arg):\n         return cls, arg\n \n \n-def test_staticmethod_tracing(sentry_init):\n-    test_staticmethod_name = \"tests.test_basics.TracingTestClass.static\"\n+# We need to fork here because the test modifies tests.test_basics.TracingTestClass\n+@pytest.mark.forked\n+def test_staticmethod_class_tracing(sentry_init, capture_events):\n+    sentry_init(\n+        debug=True,\n+        traces_sample_rate=1.0,\n+        functions_to_trace=[\n+            {\"qualified_name\": \"tests.test_basics.TracingTestClass.static\"}\n+        ],\n+    )\n \n-    assert (\n-        \".\".join(\n-            [\n-                TracingTestClass.static.__module__,\n-                TracingTestClass.static.__qualname__,\n-            ]\n-        )\n-        == test_staticmethod_name\n-    ), \"The test static method was moved or renamed. Please update the name accordingly\"\n+    events = capture_events()\n \n-    sentry_init(functions_to_trace=[{\"qualified_name\": test_staticmethod_name}])\n+    with sentry_sdk.start_transaction(name=\"test\"):\n+        assert TracingTestClass.static(1) == 1\n \n-    for instance_or_class in (TracingTestClass, TracingTestClass()):\n-        with patch_start_tracing_child() as fake_start_child:\n-            assert instance_or_class.static(1) == 1\n-            assert fake_start_child.call_count == 1\n+    (event,) = events\n+    assert event[\"type\"] == \"transaction\"\n+    assert event[\"transaction\"] == \"test\"\n \n+    (span,) = event[\"spans\"]\n+    assert span[\"description\"] == \"tests.test_basics.TracingTestClass.static\"\n \n-def test_classmethod_tracing(sentry_init):\n-    test_classmethod_name = \"tests.test_basics.TracingTestClass.class_\"\n \n-    assert (\n-        \".\".join(\n-            [\n-                TracingTestClass.class_.__module__,\n-                TracingTestClass.class_.__qualname__,\n-            ]\n-        )\n-        == test_classmethod_name\n-    ), \"The test class method was moved or renamed. Please update the name accordingly\"\n+# We need to fork here because the test modifies tests.test_basics.TracingTestClass\n+@pytest.mark.forked\n+def test_staticmethod_instance_tracing(sentry_init, capture_events):\n+    sentry_init(\n+        debug=True,\n+        traces_sample_rate=1.0,\n+        functions_to_trace=[\n+            {\"qualified_name\": \"tests.test_basics.TracingTestClass.static\"}\n+        ],\n+    )\n+\n+    events = capture_events()\n+\n+    with sentry_sdk.start_transaction(name=\"test\"):\n+        assert TracingTestClass().static(1) == 1\n+\n+    (event,) = events\n+    assert event[\"type\"] == \"transaction\"\n+    assert event[\"transaction\"] == \"test\"\n \n-    sentry_init(functions_to_trace=[{\"qualified_name\": test_classmethod_name}])\n+    (span,) = event[\"spans\"]\n+    assert span[\"description\"] == \"tests.test_basics.TracingTestClass.static\"\n+\n+\n+# We need to fork here because the test modifies tests.test_basics.TracingTestClass\n+@pytest.mark.forked\n+def test_classmethod_class_tracing(sentry_init, capture_events):\n+    sentry_init(\n+        debug=True,\n+        traces_sample_rate=1.0,\n+        functions_to_trace=[\n+            {\"qualified_name\": \"tests.test_basics.TracingTestClass.class_\"}\n+        ],\n+    )\n+\n+    events = capture_events()\n+\n+    with sentry_sdk.start_transaction(name=\"test\"):\n+        assert TracingTestClass.class_(1) == (TracingTestClass, 1)\n+\n+    (event,) = events\n+    assert event[\"type\"] == \"transaction\"\n+    assert event[\"transaction\"] == \"test\"\n+\n+    (span,) = event[\"spans\"]\n+    assert span[\"description\"] == \"tests.test_basics.TracingTestClass.class_\"\n+\n+\n+# We need to fork here because the test modifies tests.test_basics.TracingTestClass\n+@pytest.mark.forked\n+def test_classmethod_instance_tracing(sentry_init, capture_events):\n+    sentry_init(\n+        debug=True,\n+        traces_sample_rate=1.0,\n+        functions_to_trace=[\n+            {\"qualified_name\": \"tests.test_basics.TracingTestClass.class_\"}\n+        ],\n+    )\n+\n+    events = capture_events()\n+\n+    with sentry_sdk.start_transaction(name=\"test\"):\n+        assert TracingTestClass().class_(1) == (TracingTestClass, 1)\n+\n+    (event,) = events\n+    assert event[\"type\"] == \"transaction\"\n+    assert event[\"transaction\"] == \"test\"\n \n-    for instance_or_class in (TracingTestClass, TracingTestClass()):\n-        with patch_start_tracing_child() as fake_start_child:\n-            assert instance_or_class.class_(1) == (TracingTestClass, 1)\n-            assert fake_start_child.call_count == 1\n+    (span,) = event[\"spans\"]\n+    assert span[\"description\"] == \"tests.test_basics.TracingTestClass.class_\"\n \n \n def test_last_event_id(sentry_init):\n"}
{"id": 137, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nWhat I observed is that if I run both combinations useNewProtocol=true,\nuseNewProtocol=false it would often fail the second time, but if I only\nrun the second variation useNewProtocol=false it works, and only the\nfirst variation useNewProtocol=true also works. So this points to some\nstate that is not cleared between the tests  - and indeed, the test\ncreates a topic ‚ÄúinputTopic‚Äù, produces to it, but doesn‚Äôt delete it, so\nthe second variation will run with produce to it again and then run with\ntwice the data.\n\nI also reduced heartbeat interval and session timeout since some of the\ntests need to wait for the old consumer to leave which (sigh) Kafka\nStreams doesn't do, so we have to wait that it gets kicked out by\nsession timeout. So previously we waited for 45 seconds, now, we at\nleast wait only 1 second.\n\nReviewers: Bill Bejeck <bbejeck@apache.org>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n\n### Code Patch:\n@@ -175,6 +175,8 @@ private Properties props(final Properties extraProperties) {\n         streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.IntegerSerde.class);\n         streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.IntegerSerde.class);\n         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000L);\n+        streamsConfiguration.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 500);\n+        streamsConfiguration.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 1000);\n         streamsConfiguration.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\n         streamsConfiguration.putAll(extraProperties);\n \n@@ -191,6 +193,7 @@ public void shutdown() throws Exception {\n \n         IntegrationTestUtils.purgeLocalStreamsState(streamsConfigurations);\n         streamsConfigurations.clear();\n+        CLUSTER.deleteAllTopics();\n     }\n \n     @ParameterizedTest\n"}
{"id": 395, "ground_truth": "OS", "predicted": "OS", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\ntest-child-process-cwd\n\n### Platform\n\nWindows\n\n### Console output\n\n```console\nnode:assert:126\r\n  throw new AssertionError(obj);\r\n  ^\r\n\r\nAssertionError [ERR_ASSERTION]: Expected values to be strictly equal:\r\n+ actual - expected\r\n\r\n+ 'C:\\\\Windows'\r\n- 'C:\\\\WINDOWS'\r\n        ^\r\n    at ChildProcess.<anonymous> (C:\\Users\\admin\\WebstormProjects\\node\\test\\parallel\\test-child-process-cwd.js:55:26)\r\n    at ChildProcess.<anonymous> (C:\\Users\\admin\\WebstormProjects\\node\\test\\common\\index.js:487:15)\r\n    at ChildProcess.emit (node:events:520:28)\r\n    at maybeClose (node:internal/child_process:1105:16)\r\n    at Socket.<anonymous> (node:internal/child_process:457:11)\r\n    at Socket.emit (node:events:520:28)\r\n    at Pipe.<anonymous> (node:net:337:12) {\r\n  generatedMessage: true,\r\n  code: 'ERR_ASSERTION',\r\n  actual: 'C:\\\\Windows',\r\n  expected: 'C:\\\\WINDOWS',\r\n  operator: 'strictEqual'\r\n}\r\n\r\nNode.js v23.0.0-pre\n```\n\n\n### Build links\n\n- \n\n### Additional information\n\n### Issue Description\r\n\r\nI built Node.js on my local computer using the `vcbuild.bat` script. After the build was completed, I verified it through the `Release\\node` directory. However, when running `.\\vcbuild test`, I consistently encountered the same error. The error seems to be caused by Windows not distinguishing between uppercase and lowercase folder names.\r\n\n\n### Code Patch:\n@@ -52,7 +52,14 @@ function testCwd(options, expectPidType, expectCode = 0, expectData) {\n   });\n \n   child.on('close', common.mustCall(function() {\n-    expectData && assert.strictEqual(data.trim(), expectData);\n+    if (expectData) {\n+      // In Windows, compare without considering case\n+      if (common.isWindows) {\n+        assert.strictEqual(data.trim().toLowerCase(), expectData.toLowerCase());\n+      } else {\n+        assert.strictEqual(data.trim(), expectData);\n+      }\n+    }\n   }));\n \n   return child;\n"}
{"id": 229, "ground_truth": "Time", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nAlso change cloudcfg.sh, to make it possible to call from the examples/guestbook directory.\n\n\n### Code Patch:\n@@ -50,7 +50,7 @@ You should see a single redis master task.  It will also display the machine tha\n sudo docker ps\n ```\n \n-And see the actual task.  (note that initial ```docker pull``` may take a few minutes, depending on network conditions.\n+And see the actual task.  (Note that initial ```docker pull``` may take a few minutes, depending on network conditions.)\n \n ### Step Two: Turn up the master service.\n A Kubernetes 'service' is named load balancer that proxies traffic to one or more containers.  The services in a Kubernetes cluster are discoverable inside other containers via environment variables.  Services find the containers to load balance based on task labels.  The task that you created in Step One has the label \"name=redis-master\", so the corresponding service is defined by that label.  Create a file named redis-master-service.json that contains:\n@@ -68,7 +68,7 @@ A Kubernetes 'service' is named load balancer that proxies traffic to one or mor\n Once you have that service description, you can create the service with the cloudcfg cli:\n \n ```shell\n-./src/scripts/cloudcfg.sh -c redis-master-service create /services\n+./src/scripts/cloudcfg.sh -c redis-master-service.json create /services\n ```\n \n Once created, the service proxy on each minion is configured to set up a proxy on the specified port (in this case port 10000).\n@@ -131,7 +131,7 @@ Just like the master, we want to have a service to proxy connections to the read\n }\n ```\n \n-This time the label query for the service is 'name=redis-slave'\n+This time the label query for the service is 'name=redis-slave'.\n \n Now that you have created the service specification, create it in your cluster with the cloudcfg cli:\n \n@@ -219,4 +219,4 @@ if (isset($_GET['cmd']) === true) {\n } ?>\n ```\n \n-To play with the service itself, find the name of a frontend, grab the external IP of that host from the Google Cloud Console, and visit http://&lt;host-ip&gt;:8080, note you may need to open the firewall for port 8080 using the console or the gcloud tool.\n+To play with the service itself, find the name of a frontend, grab the external IP of that host from the Google Cloud Console, and visit http://&lt;host-ip&gt;:8080. Note, you may need to open the firewall for port 8080 using the console or the gcloud tool.\n\n@@ -20,5 +20,5 @@\n \n detect-master\n \n-./target/cloudcfg -h https://${KUBE_MASTER_IP} $@\n+$(dirname $0)/../../target/cloudcfg -h https://${KUBE_MASTER_IP} $@\n \n"}
{"id": 17, "ground_truth": "Network", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nNo out-of-tree patches, yay!\n\nR=@indutny\n\n\n### Code Patch:\n@@ -382,9 +382,9 @@ void Accessors::RegExpSourceGetter(\n   i::Isolate* isolate = reinterpret_cast<i::Isolate*>(info.GetIsolate());\n   HandleScope scope(isolate);\n \n-  Handle<Object> receiver =\n-      Utils::OpenHandle(*v8::Local<v8::Value>(info.This()));\n-  Handle<JSRegExp> regexp = Handle<JSRegExp>::cast(receiver);\n+  Handle<Object> holder =\n+      Utils::OpenHandle(*v8::Local<v8::Value>(info.Holder()));\n+  Handle<JSRegExp> regexp = Handle<JSRegExp>::cast(holder);\n   Handle<String> result;\n   if (regexp->TypeTag() == JSRegExp::NOT_COMPILED) {\n     result = isolate->factory()->empty_string();\n\n@@ -59,7 +59,7 @@ inline Atomic32 NoBarrier_CompareAndSwap(volatile Atomic32* ptr,\n                        \"2:\\n\"\n                        \".set pop\\n\"\n                        : \"=&r\" (prev), \"=m\" (*ptr), \"=&r\" (tmp)\n-                       : \"Ir\" (old_value), \"r\" (new_value), \"m\" (*ptr)\n+                       : \"r\" (old_value), \"r\" (new_value), \"m\" (*ptr)\n                        : \"memory\");\n   return prev;\n }\n@@ -196,7 +196,7 @@ inline Atomic64 NoBarrier_CompareAndSwap(volatile Atomic64* ptr,\n                        \"2:\\n\"\n                        \".set pop\\n\"\n                        : \"=&r\" (prev), \"=m\" (*ptr), \"=&r\" (tmp)\n-                       : \"Ir\" (old_value), \"r\" (new_value), \"m\" (*ptr)\n+                       : \"r\" (old_value), \"r\" (new_value), \"m\" (*ptr)\n                        : \"memory\");\n   return prev;\n }\n\n@@ -36,7 +36,7 @@ inline Atomic32 NoBarrier_CompareAndSwap(volatile Atomic32* ptr,\n                        \"2:\\n\"\n                        \".set pop\\n\"\n                        : \"=&r\" (prev), \"=&r\" (tmp)\n-                       : \"Ir\" (old_value), \"r\" (new_value), \"r\" (ptr)\n+                       : \"r\" (old_value), \"r\" (new_value), \"r\" (ptr)\n                        : \"memory\");\n   return prev;\n }\n\n@@ -62,12 +62,13 @@ Handle<Code> PropertyICCompiler::CompilePolymorphic(TypeHandleList* types,\n       // Separate compare from branch, to provide path for above JumpIfSmi().\n       Handle<WeakCell> cell = Map::WeakCellForMap(map);\n       __ GetWeakValue(match, cell);\n+      __ Subu(match, match, Operand(map_reg));\n       if (type->Is(HeapType::Number())) {\n         DCHECK(!number_case.is_unused());\n         __ bind(&number_case);\n       }\n       __ Jump(handlers->at(current), RelocInfo::CODE_TARGET, eq, match,\n-              Operand(map_reg));\n+              Operand(zero_reg));\n     }\n   }\n   DCHECK(number_of_handled_maps != 0);\n\n@@ -62,12 +62,13 @@ Handle<Code> PropertyICCompiler::CompilePolymorphic(TypeHandleList* types,\n       // Separate compare from branch, to provide path for above JumpIfSmi().\n       Handle<WeakCell> cell = Map::WeakCellForMap(map);\n       __ GetWeakValue(match, cell);\n+      __ Dsubu(match, match, Operand(map_reg));\n       if (type->Is(HeapType::Number())) {\n         DCHECK(!number_case.is_unused());\n         __ bind(&number_case);\n       }\n       __ Jump(handlers->at(current), RelocInfo::CODE_TARGET, eq, match,\n-              Operand(map_reg));\n+              Operand(zero_reg));\n     }\n   }\n   DCHECK(number_of_handled_maps != 0);\n\n@@ -2905,7 +2905,7 @@ ParserBase<Traits>::ParseTemplateLiteral(ExpressionT tag, int start, bool* ok) {\n     Next();\n \n     if (next == Token::EOS) {\n-      ReportMessageAt(Scanner::Location(start, peek_position()),\n+      ReportMessageAt(Scanner::Location(start, position()),\n                       \"unterminated_template\");\n       *ok = false;\n       return Traits::EmptyExpression();\n\n@@ -35,7 +35,7 @@\n #define MAJOR_VERSION     4\n #define MINOR_VERSION     1\n #define BUILD_NUMBER      0\n-#define PATCH_LEVEL       7\n+#define PATCH_LEVEL       12\n // Use 1 for candidates and 0 otherwise.\n // (Boolean macro values are not supported by all preprocessors.)\n #define IS_CANDIDATE_VERSION 0\n"}
{"id": 163, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nNot too sure what the problem is exactly but my suspicion is that the profiler runs in a separate thread and needs time to flush the chunk, the test wasn't waiting long enough.\n\n### Code Patch:\n@@ -127,7 +127,7 @@ def test_continuous_profiler_setup_twice(mode, make_options, teardown_profiling)\n \n \n def assert_single_transaction_with_profile_chunks(\n-    envelopes, thread, max_chunks, transactions=1\n+    envelopes, thread, max_chunks=None, transactions=1\n ):\n     items = defaultdict(list)\n     for envelope in envelopes:\n@@ -136,7 +136,8 @@ def assert_single_transaction_with_profile_chunks(\n \n     assert len(items[\"transaction\"]) == transactions\n     assert len(items[\"profile_chunk\"]) > 0\n-    assert len(items[\"profile_chunk\"]) <= max_chunks\n+    if max_chunks is not None:\n+        assert len(items[\"profile_chunk\"]) <= max_chunks\n \n     transaction = items[\"transaction\"][0].payload.json\n \n@@ -235,7 +236,7 @@ def test_continuous_profiler_auto_start_and_manual_stop(\n         with sentry_sdk.start_span(op=\"op\"):\n             time.sleep(0.05)\n \n-    assert_single_transaction_with_profile_chunks(envelopes, thread, max_chunks=10)\n+    assert_single_transaction_with_profile_chunks(envelopes, thread)\n \n     for _ in range(3):\n         stop_profiler()\n@@ -256,7 +257,7 @@ def test_continuous_profiler_auto_start_and_manual_stop(\n             with sentry_sdk.start_span(op=\"op\"):\n                 time.sleep(0.05)\n \n-        assert_single_transaction_with_profile_chunks(envelopes, thread, max_chunks=10)\n+        assert_single_transaction_with_profile_chunks(envelopes, thread)\n \n \n @pytest.mark.parametrize(\n@@ -299,18 +300,27 @@ def test_continuous_profiler_manual_start_and_stop_sampled(\n         envelopes.clear()\n \n         with sentry_sdk.start_transaction(name=\"profiling\"):\n+            assert get_profiler_id() is not None, \"profiler should be running\"\n             with sentry_sdk.start_span(op=\"op\"):\n-                time.sleep(0.05)\n+                time.sleep(0.1)\n+            assert get_profiler_id() is not None, \"profiler should be running\"\n \n-        assert_single_transaction_with_profile_chunks(envelopes, thread, max_chunks=10)\n+        assert_single_transaction_with_profile_chunks(envelopes, thread)\n+\n+        assert get_profiler_id() is not None, \"profiler should be running\"\n \n         stop_profiler()\n \n+        # the profiler stops immediately in manual mode\n+        assert get_profiler_id() is None, \"profiler should not be running\"\n+\n         envelopes.clear()\n \n         with sentry_sdk.start_transaction(name=\"profiling\"):\n+            assert get_profiler_id() is None, \"profiler should not be running\"\n             with sentry_sdk.start_span(op=\"op\"):\n-                time.sleep(0.05)\n+                time.sleep(0.1)\n+            assert get_profiler_id() is None, \"profiler should not be running\"\n \n         assert_single_transaction_without_profile_chunks(envelopes)\n \n@@ -397,17 +407,17 @@ def test_continuous_profiler_auto_start_and_stop_sampled(\n         with sentry_sdk.start_transaction(name=\"profiling 1\"):\n             assert get_profiler_id() is not None, \"profiler should be running\"\n             with sentry_sdk.start_span(op=\"op\"):\n-                time.sleep(0.03)\n+                time.sleep(0.1)\n             assert get_profiler_id() is not None, \"profiler should be running\"\n \n-        # the profiler takes a while to stop so if we start a transaction\n-        # immediately, it'll be part of the same chunk\n+        # the profiler takes a while to stop in auto mode so if we start\n+        # a transaction immediately, it'll be part of the same chunk\n         assert get_profiler_id() is not None, \"profiler should be running\"\n \n         with sentry_sdk.start_transaction(name=\"profiling 2\"):\n             assert get_profiler_id() is not None, \"profiler should be running\"\n             with sentry_sdk.start_span(op=\"op\"):\n-                time.sleep(0.03)\n+                time.sleep(0.1)\n             assert get_profiler_id() is not None, \"profiler should be running\"\n \n         # wait at least 1 cycle for the profiler to stop\n"}
{"id": 349, "ground_truth": "Randomness", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThere is some flakiness for the checked error messages, as some include a \"verification_error:\" prefix. I've used regexes to avoid that, and removing the line / character information on the error being checked. \n\n### Code Patch:\n@@ -125,7 +125,7 @@ setup:\n \n   - match: { status: 400 }\n   - match: { error.type: verification_exception }\n-  - match: { error.reason: \"Found 1 problem\\nline 1:19: Unknown column [something]\" }\n+  - match: { error.reason: \"/Unknown.column.\\\\[something\\\\]/\" }\n \n ---\n \"match on eval column\":\n@@ -139,7 +139,7 @@ setup:\n \n   - match: { status: 400 }\n   - match: { error.type: verification_exception }\n-  - match: { error.reason: \"Found 1 problem\\nline 1:60: [:] operator cannot operate on [upper_content], which is not a field from an index mapping\" }\n+  - match: { error.reason: \"/operator.cannot.operate.on.\\\\[upper_content\\\\],.which.is.not.a.field.from.an.index.mapping/\" }\n \n ---\n \"match on overwritten column\":\n@@ -153,7 +153,7 @@ setup:\n \n   - match: { status: 400 }\n   - match: { error.type: verification_exception }\n-  - match: { error.reason: \"Found 1 problem\\nline 1:78: [:] operator cannot operate on [content], which is not a field from an index mapping\" }\n+  - match: { error.reason: \"/operator.cannot.operate.on.\\\\[content\\\\],.which.is.not.a.field.from.an.index.mapping/\" }\n \n ---\n \"match after stats\":\n@@ -167,7 +167,7 @@ setup:\n \n   - match: { status: 400 }\n   - match: { error.type: verification_exception }\n-  - match: { error.reason: \"Found 1 problem\\nline 1:36: Unknown column [content], did you mean [count(*)]?\" }\n+  - match: { error.reason: \"/Unknown.column.\\\\[content\\\\]/\" }\n \n ---\n \"match with disjunctions\":\n@@ -199,4 +199,4 @@ setup:\n \n   - match: { status: 400 }\n   - match: { error.type: verification_exception }\n-  - match: { error.reason: \"Found 1 problem\\nline 1:34: [:] operator is only supported in WHERE commands\" }\n+  - match: { error.reason: \"/operator.is.only.supported.in.WHERE.commands/\" }\n"}
{"id": 387, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n#### Trac ticket number\r\n\r\nN/A\r\n\r\n#### Branch description\r\n\r\nTwo separate calls to look up access time can result in sub-second differences which cause the test to fail.\r\n\r\nWhen running the full test suite locally, this is consistently failing for me.\r\n\r\n#### Checklist\r\n\r\n- [x] This PR targets the `main` branch. <!-- Backports will be evaluated and done by mergers, when necessary. -->\r\n- [x] The commit message is written in past tense, mentions the ticket number, and ends with a period.\r\n- [ ] I have checked the \"Has patch\" ticket flag in the Trac system.\r\n- [x] I have added or updated relevant tests.\r\n- [ ] I have added or updated relevant docs, including release notes if applicable.\r\n- [ ] I have attached screenshots in both light and dark modes for any UI changes.\n\n### Code Patch:\n@@ -186,17 +186,19 @@ def test_file_get_accessed_time(self):\n         f = ContentFile(\"custom contents\")\n         f_name = self.storage.save(\"test.file\", f)\n         self.addCleanup(self.storage.delete, f_name)\n+\n+        path = self.storage.path(f_name)\n         atime = self.storage.get_accessed_time(f_name)\n \n-        self.assertEqual(\n+        self.assertAlmostEqual(\n             atime,\n-            datetime.datetime.fromtimestamp(\n-                os.path.getatime(self.storage.path(f_name))\n-            ),\n+            datetime.datetime.fromtimestamp(os.path.getatime(path)),\n+            delta=datetime.timedelta(seconds=1),\n         )\n-        self.assertLess(\n-            timezone.now() - self.storage.get_accessed_time(f_name),\n-            datetime.timedelta(seconds=2),\n+        self.assertAlmostEqual(\n+            atime,\n+            timezone.now(),\n+            delta=datetime.timedelta(seconds=1),\n         )\n \n     @requires_tz_support\n@@ -212,17 +214,19 @@ def test_file_get_created_time(self):\n         f = ContentFile(\"custom contents\")\n         f_name = self.storage.save(\"test.file\", f)\n         self.addCleanup(self.storage.delete, f_name)\n+\n+        path = self.storage.path(f_name)\n         ctime = self.storage.get_created_time(f_name)\n \n-        self.assertEqual(\n+        self.assertAlmostEqual(\n             ctime,\n-            datetime.datetime.fromtimestamp(\n-                os.path.getctime(self.storage.path(f_name))\n-            ),\n+            datetime.datetime.fromtimestamp(os.path.getctime(path)),\n+            delta=datetime.timedelta(seconds=1),\n         )\n-        self.assertLess(\n-            timezone.now() - self.storage.get_created_time(f_name),\n-            datetime.timedelta(seconds=2),\n+        self.assertAlmostEqual(\n+            ctime,\n+            timezone.now(),\n+            delta=datetime.timedelta(seconds=1),\n         )\n \n     @requires_tz_support\n@@ -238,17 +242,19 @@ def test_file_get_modified_time(self):\n         f = ContentFile(\"custom contents\")\n         f_name = self.storage.save(\"test.file\", f)\n         self.addCleanup(self.storage.delete, f_name)\n+\n+        path = self.storage.path(f_name)\n         mtime = self.storage.get_modified_time(f_name)\n \n-        self.assertEqual(\n+        self.assertAlmostEqual(\n             mtime,\n-            datetime.datetime.fromtimestamp(\n-                os.path.getmtime(self.storage.path(f_name))\n-            ),\n+            datetime.datetime.fromtimestamp(os.path.getmtime(path)),\n+            delta=datetime.timedelta(seconds=1),\n         )\n-        self.assertLess(\n-            timezone.now() - self.storage.get_modified_time(f_name),\n-            datetime.timedelta(seconds=2),\n+        self.assertAlmostEqual(\n+            mtime,\n+            timezone.now(),\n+            delta=datetime.timedelta(seconds=1),\n         )\n \n     @requires_tz_support\n"}
{"id": 143, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nIn Postgres especially (but in generaly in all databases, if\r\nthere is no order specified in select query, the rows might\r\ncome in random order. It depends on many factors.\r\n\r\nThe test query here retrieved the dags without any order but\r\nexpected the list to be in specific order.\r\n\r\nThis PR adds ordering - it also removes side-effects of the\r\ntest by using fixture that clears the datasets before and after\r\nthe tests that rely/modify datasets - because othrwise failure of\r\none of the tests can create side effects that fail the other\r\ntests (this is what happened in this case)\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n\n### Code Patch:\n@@ -119,6 +119,15 @@ def success_handler(self, context):\n         self.task_state_in_callback = context['ti'].state\n \n \n+@pytest.fixture()\n+def clear_datasets():\n+    # Clean up before ourselves\n+    db.clear_db_datasets()\n+    yield\n+    # and after\n+    db.clear_db_datasets()\n+\n+\n class TestTaskInstance:\n     @staticmethod\n     def clean_db():\n@@ -1513,7 +1522,7 @@ def test_success_callback_no_race_condition(self, create_task_instance):\n         ti.refresh_from_db()\n         assert ti.state == State.SUCCESS\n \n-    def test_outlet_datasets(self, create_task_instance):\n+    def test_outlet_datasets(self, create_task_instance, clear_datasets):\n         \"\"\"\n         Verify that when we have an outlet dataset on a task, and the task\n         completes successfully, a DatasetDagRunQueue is logged.\n@@ -1540,7 +1549,7 @@ def test_outlet_datasets(self, create_task_instance):\n         # check that one queue record created for each dag that depends on dataset 1\n         assert session.query(DatasetDagRunQueue.target_dag_id).filter(\n             DatasetTaskRef.dag_id == dag1.dag_id, DatasetTaskRef.task_id == 'upstream_task_1'\n-        ).all() == [\n+        ).order_by(DatasetDagRunQueue.target_dag_id).all() == [\n             ('example_dataset_dag3_req_dag1',),\n             ('example_dataset_dag4_req_dag1_dag2',),\n             ('example_dataset_dag5_req_dag1_D',),\n@@ -1559,10 +1568,7 @@ def test_outlet_datasets(self, create_task_instance):\n             .count()\n         ) == 1\n \n-        # Clean up after ourselves\n-        db.clear_db_datasets()\n-\n-    def test_outlet_datasets_failed(self, create_task_instance):\n+    def test_outlet_datasets_failed(self, create_task_instance, clear_datasets):\n         \"\"\"\n         Verify that when we have an outlet dataset on a task, and the task\n         failed, a DatasetDagRunQueue is not logged, and a DatasetEvent is\n@@ -1593,7 +1599,7 @@ def test_outlet_datasets_failed(self, create_task_instance):\n         # check that no dataset events were generated\n         assert session.query(DatasetEvent).count() == 0\n \n-    def test_outlet_datasets_skipped(self, create_task_instance):\n+    def test_outlet_datasets_skipped(self, create_task_instance, clear_datasets):\n         \"\"\"\n         Verify that when we have an outlet dataset on a task, and the task\n         is skipped, a DatasetDagRunQueue is not logged, and a DatasetEvent is\n"}
{"id": 237, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->\r\n\r\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->\r\n\r\n## Why are these changes needed?\r\n\r\n<!-- Please give a short summary of the change and the problem this solves. -->\r\n\r\nFixing [flakey](https://buildkite.com/anyscale/rayturbo/builds/5914#0198edc9-d671-4075-b788-4c0c555b29db) test_operators by explicitly sorting before comparing the results.\r\n\r\n## Related issue number\r\n\r\n<!-- For example: \"Closes #1234\" -->\r\n\r\n## Checks\r\n\r\n- [ ] I've signed off every commit(by using the -s flag, i.e., `git commit -s`) in this PR.\r\n- [ ] I've run `scripts/format.sh` to lint the changes in this PR.\r\n- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.\r\n    - [ ] I've added any new APIs to the API Reference. For example, if I added a\r\n           method in Tune, I've added it in `doc/source/tune/api/` under the\r\n           corresponding `.rst` file.\r\n- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/\r\n- Testing Strategy\r\n   - [ ] Unit tests\r\n   - [ ] Release tests\r\n   - [ ] This PR is not tested :(\r\n\n\n### Code Patch:\n@@ -139,7 +139,9 @@ def dummy_all_transform(bundles: List[RefBundle], ctx):\n \n     # Check we return transformed bundles.\n     assert not op.completed()\n-    assert _take_outputs(op) == [[1, 2], [3, 4]]\n+    outputs = _take_outputs(op)\n+    expected = [[1, 2], [3, 4]]\n+    assert sorted(outputs) == expected, f\"Expected {expected}, got {outputs}\"\n     stats = op.get_stats()\n     assert \"FooStats\" in stats\n     assert op.completed()\n@@ -515,7 +517,9 @@ def test_map_operator_ray_args(shutdown_only, use_actors):\n     run_op_tasks_sync(op)\n \n     # Check we don't hang and complete with num_gpus=1.\n-    assert _take_outputs(op) == [[i * 2] for i in range(10)]\n+    outputs = _take_outputs(op)\n+    expected = [[i * 2] for i in range(10)]\n+    assert sorted(outputs) == expected, f\"Expected {expected}, got {outputs}\"\n     assert op.completed()\n \n \n"}
{"id": 90, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\nThank you for your contribution!\r\nUnless your change is trivial, please create an issue to discuss the change before creating a PR.\r\n-->\r\n\r\n### Describe your changes:\r\n\r\nFixes <issue-number>\r\n\r\n<!--\r\nShort blurb explaining:\r\n- What changes did you make?\r\n- Why did you make them?\r\n- How did you test your changes?\r\n-->\r\n\r\nI worked on ... because ...\r\n\r\n<!-- For frontend related change, please add screenshots and/or videos of your changes preview! -->\r\n\r\n#\r\n### Type of change:\r\n<!-- You should choose 1 option and delete options that aren't relevant -->\r\n- [ ] Bug fix\r\n- [ ] Improvement\r\n- [ ] New feature\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] Documentation\r\n\r\n#\r\n### Checklist:\r\n<!-- add an x in [] if done, don't mark items that you didn't do !-->\r\n- [x] I have read the [**CONTRIBUTING**](https://docs.open-metadata.org/developers/contribute) document.\r\n- [ ] My PR title is `Fixes <issue-number>: <short explanation>`\r\n- [ ] I have commented on my code, particularly in hard-to-understand areas. \r\n- [ ] For JSON Schema changes: I updated the migration scripts or explained why it is not needed.\r\n\r\n<!-- Based on the type(s) of your change, uncomment the required checklist üëá -->\r\n\r\n<!-- Bug fix\r\n- [ ] I have added a test that covers the exact scenario we are fixing. For complex issues, comment the issue number in the test for future reference.\r\n-->\r\n\r\n<!-- Improvement\r\n- [ ] I have added tests around the new logic.\r\n- [ ] For connector/ingestion changes: I updated the documentation.\r\n-->\r\n\r\n<!-- New feature\r\n- [ ] The issue properly describes why the new feature is needed, what's the goal, and how we are building it. Any discussion\r\n    or decision-making process is reflected in the issue.\r\n- [ ] I have updated the documentation.\r\n- [ ] I have added tests around the new logic.\r\n-->\r\n\r\n<!-- Breaking change\r\n- [ ] I have added the tag `Backward-Incompatible-Change`.\r\n-->\r\n\n\n### Code Patch:\n@@ -718,7 +718,9 @@ export const validateGlossaryTerm = async (\n   if (isGlossaryTermPage) {\n     await expect(page.getByTestId(term.name)).toBeVisible();\n   } else {\n+    await expect(page.locator(termSelector)).toBeVisible();\n     await expect(page.locator(termSelector)).toContainText(term.name);\n+    await expect(page.locator(statusSelector)).toBeVisible();\n     await expect(page.locator(statusSelector)).toContainText(status);\n   }\n };\n\n@@ -56,28 +56,28 @@ export const verifyDataFilters = async (page: Page, widgetKey: string) => {\n     page.getByTestId(widgetKey).getByTestId('widget-sort-by-dropdown')\n   ).toBeVisible();\n \n-  const aToZFilter = page.waitForResponse(\n-    '/api/v1/search/query?q=*&index=all*&sort_field=name.keyword&sort_order=asc'\n-  );\n   await page\n     .getByTestId(widgetKey)\n     .getByTestId('widget-sort-by-dropdown')\n     .click();\n+  const aToZFilter = page.waitForResponse(\n+    '/api/v1/search/query?q=*&index=all*&sort_field=name.keyword*&sort_order=asc*'\n+  );\n   await page.getByRole('menuitem', { name: 'A to Z' }).click();\n   await aToZFilter;\n \n-  const zToAFilter = page.waitForResponse(\n-    '/api/v1/search/query?q=*&index=all*&sort_field=name.keyword&sort_order=desc'\n-  );\n   await page\n     .getByTestId(widgetKey)\n     .getByTestId('widget-sort-by-dropdown')\n     .click();\n+  const zToAFilter = page.waitForResponse(\n+    '/api/v1/search/query?q=*&index=all*&sort_field=name.keyword*&sort_order=desc*'\n+  );\n   await page.getByRole('menuitem', { name: 'Z to A' }).click();\n   await zToAFilter;\n \n   const latestFilter = page.waitForResponse(\n-    '/api/v1/search/query?q=*&index=all*&sort_field=updatedAt&sort_order=desc'\n+    '/api/v1/search/query?q=*&index=all*&sort_field=updatedAt*&sort_order=desc*'\n   );\n   await page\n     .getByTestId(widgetKey)\n"}
{"id": 373, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nhttps://scans.gradle.com/s/bfgjiqwioneey/tests/task/:profiler:test/details/com.splunk.opentelemetry.profiler.snapshot.LongRunningBackgroundTaskTest/traceBackgroundThreadProfilingContinuesAfterEntrySpanEnds()?top-execution=1\n\n### Code Patch:\n@@ -24,6 +24,8 @@\n import com.splunk.opentelemetry.profiler.snapshot.simulation.Server;\n import io.opentelemetry.sdk.autoconfigure.OpenTelemetrySdkExtension;\n import java.time.Duration;\n+import java.util.concurrent.CountDownLatch;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.RegisterExtension;\n \n@@ -40,20 +42,28 @@ class LongRunningBackgroundTaskTest {\n           .with(new SnapshotVolumePropagator((c) -> true))\n           .build();\n \n+  private CountDownLatch slowTaskLatch = new CountDownLatch(1);\n+\n   @RegisterExtension\n   public final Server server =\n       Server.builder(sdk).named(\"server\").performing(Background.task(slowTask())).build();\n \n   private Runnable slowTask() {\n     return () -> {\n       try {\n-        Thread.sleep(250);\n+        slowTaskLatch.await();\n       } catch (InterruptedException e) {\n-        e.printStackTrace();\n+        Thread.currentThread().interrupt();\n       }\n     };\n   }\n \n+  @AfterEach\n+  void reset() {\n+    slowTaskLatch.countDown();\n+    slowTaskLatch = new CountDownLatch(1);\n+  }\n+\n   @Test\n   void traceBackgroundThreadProfilingContinuesAfterEntrySpanEnds() {\n     server.send(new Message());\n\n@@ -18,6 +18,7 @@\n \n import io.opentelemetry.context.Context;\n import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n import java.util.function.UnaryOperator;\n@@ -44,10 +45,22 @@ public static <T> UnaryOperator<Message> task(Callable<T> task) {\n \n   /** Perform background task within the context of the same trace. */\n   public static UnaryOperator<Message> task(Runnable task) {\n+    CountDownLatch latch = new CountDownLatch(1);\n     return message -> {\n       var executor = Context.current().wrap(Executors.newSingleThreadExecutor());\n       try {\n-        executor.submit(task);\n+        Runnable runnable =\n+            () -> {\n+              latch.countDown();\n+              task.run();\n+            };\n+        executor.submit(runnable);\n+        // wait until the task has started\n+        try {\n+          latch.await();\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+        }\n         return message;\n       } finally {\n         executor.shutdown();\n"}
{"id": 52, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Rationale for this change\nFixing flaky bloom filter memory check test\n\n### What changes are included in this PR?\nAdded an additional runtime.GC() call to ensure the releases are called.\n\n\n\n### Code Patch:\n@@ -52,6 +52,7 @@ func (suite *BloomFilterBuilderSuite) SetupTest() {\n \n func (suite *BloomFilterBuilderSuite) TearDownTest() {\n \truntime.GC() // we use setfinalizer to clean up the buffers, so run the GC\n+\truntime.GC()\n \tsuite.mem.AssertSize(suite.T(), 0)\n }\n \n"}
{"id": 299, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n##### SUMMARY\r\n\r\nThis PR fixes various issues with the ansible-playbook-callbacks integration test.\r\n\r\n##### ISSUE TYPE\r\n\r\n- Test Pull Request\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nThe timing of the async tasks was a little unpredictable, meaning that sometimes we would get an unexpected number of v2_runner_on_async_poll callbacks, and fail the test. This change fixes the issue by increasing the poll interval to 2 seconds and the sleep duration to 3 seconds, such that on a reasonably responsive system we will poll twice per task, with the sleep ending in the middle of the two polls.\r\n    \r\nThe include_me.yml file does not exist in this integration test. It has been added.\r\n    \r\nThe remote_tmp_dir.path expression is invalid - the setup_remote_tmp_dir role uses set_fact to set remote_tmp_dir to remote_tmp_dir.path.\r\n    \r\nThe integration tests run with ANSIBLE_HOST_PATTERN_MISMATCH=error, meaning that the final play was never reached. Set ANSIBLE_HOST_PATTERN_MISMATCH=warning to continue past the play and trigger the v2_playbook_on_no_hosts_matched callback.\n\n### Code Patch:\n@@ -96,22 +96,22 @@\n       ignore_errors: true\n \n     - name: async poll ok\n-      command: sleep 2\n-      async: 3\n-      poll: 1\n+      command: sleep 3\n+      async: 5\n+      poll: 2\n \n     - name: async poll failed\n-      shell: sleep 2; false\n-      async: 3\n-      poll: 1\n+      shell: sleep 3; false\n+      async: 5\n+      poll: 2\n       ignore_errors: true\n \n     - include_tasks: include_me.yml\n \n     - name: diff\n       copy:\n         content: diff\n-        dest: '{{ remote_tmp_dir.path }}/diff.txt'\n+        dest: '{{ remote_tmp_dir }}/diff.txt'\n       diff: true\n \n - hosts: i_dont_exist\n\n@@ -1,12 +1,14 @@\n  1 __init__\n-83 v2_on_any\n+92 v2_on_any\n+ 1 v2_on_file_diff\n  4 v2_playbook_on_handler_task_start\n  2 v2_playbook_on_include\n+ 1 v2_playbook_on_no_hosts_matched\n  3 v2_playbook_on_notify\n- 1 v2_playbook_on_play_start\n+ 3 v2_playbook_on_play_start\n  1 v2_playbook_on_start\n  1 v2_playbook_on_stats\n-17 v2_playbook_on_task_start\n+19 v2_playbook_on_task_start\n  1 v2_playbook_on_vars_prompt\n  1 v2_runner_item_on_failed\n  2 v2_runner_item_on_ok\n@@ -15,8 +17,8 @@\n  1 v2_runner_on_async_ok\n  2 v2_runner_on_async_poll\n  5 v2_runner_on_failed\n-15 v2_runner_on_ok\n+16 v2_runner_on_ok\n  1 v2_runner_on_skipped\n-21 v2_runner_on_start\n+23 v2_runner_on_start\n  1 v2_runner_on_unreachable\n  2 v2_runner_retry\n\n\n\n@@ -5,6 +5,7 @@ set -eux\n export ANSIBLE_CALLBACK_PLUGINS=../support-callback_plugins/callback_plugins\n export ANSIBLE_ROLES_PATH=../\n export ANSIBLE_STDOUT_CALLBACK=callback_debug\n+export ANSIBLE_HOST_PATTERN_MISMATCH=warning\n \n ansible-playbook all-callbacks.yml 2>/dev/null | sort | uniq -c | tee callbacks_list.out\n \n"}
{"id": 418, "ground_truth": "Randomness", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nExperiment with a new background task library that uses Redis Streams\n\n### Code Patch:\n@@ -279,3 +279,35 @@ python -m pytest\n 3. Commit your changes\n 4. Push to the branch\n 5. Create a Pull Request\n+\n+## Running the Background Task Worker\n+\n+The Redis Memory Server uses Docket for background task management. There are two ways to run the worker:\n+\n+### 1. Using the Docket CLI\n+\n+After installing the package, you can run the worker using the Docket CLI command:\n+\n+```bash\n+docket worker --tasks agent_memory_server.docket_tasks:task_collection\n+```\n+\n+You can customize the concurrency and redelivery timeout:\n+\n+```bash\n+docket worker --tasks agent_memory_server.docket_tasks:task_collection --concurrency 5 --redelivery-timeout 60\n+```\n+\n+### 2. Using Python Code\n+\n+Alternatively, you can run the worker directly in Python:\n+\n+```bash\n+python -m agent_memory_server.worker\n+```\n+\n+With customization options:\n+\n+```bash\n+python -m agent_memory_server.worker --concurrency 5 --redelivery-timeout 60\n+```\n\n@@ -1,9 +1,10 @@\n from typing import Literal\n \n-from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException\n+from fastapi import APIRouter, Depends, HTTPException\n \n from agent_memory_server import long_term_memory, messages\n from agent_memory_server.config import settings\n+from agent_memory_server.dependencies import get_background_tasks\n from agent_memory_server.llms import get_model_config\n from agent_memory_server.logging import get_logger\n from agent_memory_server.models import (\n@@ -130,14 +131,15 @@ async def get_session_memory(\n async def put_session_memory(\n     session_id: str,\n     memory: SessionMemory,\n-    background_tasks: BackgroundTasks,\n+    background_tasks=Depends(get_background_tasks),\n ):\n     \"\"\"\n     Set session memory. Replaces existing session memory.\n \n     Args:\n         session_id: The session ID\n         memory: Messages and context to save\n+        background_tasks: DocketBackgroundTasks instance (injected automatically)\n \n     Returns:\n         Acknowledgement response\n@@ -179,26 +181,25 @@ async def delete_session_memory(\n \n @router.post(\"/long-term-memory\", response_model=AckResponse)\n async def create_long_term_memory(\n-    payload: CreateLongTermMemoryPayload, background_tasks: BackgroundTasks\n+    payload: CreateLongTermMemoryPayload,\n+    background_tasks=Depends(get_background_tasks),\n ):\n     \"\"\"\n     Create a long-term memory\n \n     Args:\n         payload: Long-term memory payload\n+        background_tasks: DocketBackgroundTasks instance (injected automatically)\n \n     Returns:\n         Acknowledgement response\n     \"\"\"\n-    redis = get_redis_conn()\n-\n     if not settings.long_term_memory:\n         raise HTTPException(status_code=400, detail=\"Long-term memory is disabled\")\n \n-    await long_term_memory.index_long_term_memories(\n-        redis=redis,\n+    await background_tasks.add_task(\n+        long_term_memory.index_long_term_memories,\n         memories=payload.memories,\n-        background_tasks=background_tasks,\n     )\n     return AckResponse(status=\"ok\")\n \n\n@@ -30,5 +30,9 @@ class Settings(BaseSettings):\n     redisvl_index_name: str = \"memory\"\n     redisvl_index_prefix: str = \"memory\"\n \n+    # Docket settings\n+    docket_name: str = \"memory-server\"\n+    use_docket: bool = True\n+\n \n settings = Settings()\n\n@@ -0,0 +1,35 @@\n+from collections.abc import Callable\n+from typing import Any\n+\n+from fastapi import BackgroundTasks\n+\n+from agent_memory_server.config import settings\n+\n+\n+class DocketBackgroundTasks(BackgroundTasks):\n+    \"\"\"A BackgroundTasks implementation that uses Docket.\"\"\"\n+\n+    async def add_task(\n+        self, func: Callable[..., Any], *args: Any, **kwargs: Any\n+    ) -> None:\n+        \"\"\"Run tasks either directly or through Docket\"\"\"\n+        from docket import Docket\n+\n+        if settings.use_docket:\n+            async with Docket(\n+                name=settings.docket_name,\n+                url=settings.redis_url,\n+            ) as docket:\n+                # Schedule task through Docket\n+                await docket.add(func)(*args, **kwargs)\n+        else:\n+            await func(*args, **kwargs)\n+\n+\n+def get_background_tasks() -> DocketBackgroundTasks:\n+    \"\"\"\n+    Dependency function that returns a DocketBackgroundTasks instance.\n+\n+    This is used by API endpoints to inject a consistent background tasks object.\n+    \"\"\"\n+    return DocketBackgroundTasks()\n\n@@ -0,0 +1,43 @@\n+\"\"\"\n+Background task management using Docket.\n+\"\"\"\n+\n+import logging\n+\n+from docket import Docket\n+\n+from agent_memory_server.config import settings\n+from agent_memory_server.long_term_memory import (\n+    extract_memory_structure,\n+    index_long_term_memories,\n+)\n+from agent_memory_server.summarization import summarize_session\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+# Register functions in the task collection for the CLI worker\n+task_collection = [\n+    extract_memory_structure,\n+    summarize_session,\n+    index_long_term_memories,\n+]\n+\n+\n+async def register_tasks() -> None:\n+    \"\"\"Register all task functions with Docket.\"\"\"\n+    if not settings.use_docket:\n+        logger.info(\"Docket is disabled, skipping task registration\")\n+        return\n+\n+    # Initialize Docket client\n+    async with Docket(\n+        name=settings.docket_name,\n+        url=settings.redis_url,\n+    ) as docket:\n+        # Register all tasks\n+        for task in task_collection:\n+            docket.register(task)\n+\n+        logger.info(f\"Registered {len(task_collection)} background tasks with Docket\")\n\n@@ -3,11 +3,11 @@\n from functools import reduce\n \n import nanoid\n-from fastapi import BackgroundTasks\n from redis.asyncio import Redis\n from redisvl.query import VectorQuery, VectorRangeQuery\n from redisvl.utils.vectorize import OpenAITextVectorizer\n \n+from agent_memory_server.dependencies import get_background_tasks\n from agent_memory_server.extraction import handle_extraction\n from agent_memory_server.filters import (\n     CreatedAt,\n@@ -25,6 +25,7 @@\n )\n from agent_memory_server.utils import (\n     Keys,\n+    get_redis_conn,\n     get_search_index,\n     safe_get,\n )\n@@ -33,11 +34,10 @@\n logger = logging.getLogger(__name__)\n \n \n-async def extract_memory_structure(\n-    redis: Redis, _id: str, text: str, namespace: str | None\n-):\n+async def extract_memory_structure(_id: str, text: str, namespace: str | None):\n+    redis = get_redis_conn()\n+\n     # Process messages for topic/entity extraction\n-    # TODO: Move into background task.\n     topics, entities = await handle_extraction(text)\n \n     # Convert lists to comma-separated strings for TAG fields\n@@ -65,14 +65,13 @@ async def compact_long_term_memories(redis: Redis) -> None:\n \n \n async def index_long_term_memories(\n-    redis: Redis,\n     memories: list[LongTermMemory],\n-    background_tasks: BackgroundTasks,\n ) -> None:\n     \"\"\"\n     Index long-term memories in Redis for search\n     \"\"\"\n-\n+    redis = get_redis_conn()\n+    background_tasks = get_background_tasks()\n     vectorizer = OpenAITextVectorizer()\n     embeddings = await vectorizer.aembed_many(\n         [memory.text for memory in memories],\n@@ -100,8 +99,8 @@ async def index_long_term_memories(\n                 },\n             )\n \n-            background_tasks.add_task(\n-                extract_memory_structure, redis, id_, memory.text, memory.namespace\n+            await background_tasks.add_task(\n+                extract_memory_structure, id_, memory.text, memory.namespace\n             )\n \n         await pipe.execute()\n\n@@ -1,4 +1,5 @@\n import os\n+import sys\n from contextlib import asynccontextmanager\n \n import uvicorn\n@@ -7,6 +8,7 @@\n from agent_memory_server import utils\n from agent_memory_server.api import router as memory_router\n from agent_memory_server.config import settings\n+from agent_memory_server.docket_tasks import register_tasks\n from agent_memory_server.healthcheck import router as health_router\n from agent_memory_server.llms import MODEL_CONFIGS, ModelProvider\n from agent_memory_server.logging import configure_logging, get_logger\n@@ -87,6 +89,20 @@ async def lifespan(app: FastAPI):\n             logger.error(f\"Failed to ensure RediSearch index: {e}\")\n             raise\n \n+    # Initialize Docket for background tasks if enabled\n+    if settings.use_docket:\n+        try:\n+            await register_tasks()\n+            logger.info(\"Initialized Docket for background tasks\")\n+            logger.info(\"To run the worker, use one of these methods:\")\n+            logger.info(\n+                \"1. CLI: docket worker --tasks agent_memory_server.docket_tasks:task_collection\"\n+            )\n+            logger.info(\"2. Python: python -m agent_memory_server.worker\")\n+        except Exception as e:\n+            logger.error(f\"Failed to initialize Docket: {e}\")\n+            raise\n+\n     # Show available models\n     openai_models = [\n         model\n@@ -138,6 +154,31 @@ def on_start_logger(port: int):\n \n # Run the application\n if __name__ == \"__main__\":\n-    port = int(os.environ.get(\"PORT\", \"8000\"))\n+    # Parse command line arguments for port\n+    port = settings.port\n+\n+    # Check if --port argument is provided\n+    if \"--port\" in sys.argv:\n+        try:\n+            port_index = sys.argv.index(\"--port\") + 1\n+            if port_index < len(sys.argv):\n+                port = int(sys.argv[port_index])\n+                print(f\"Using port from command line: {port}\")\n+        except (ValueError, IndexError):\n+            # If conversion fails or index out of bounds, use default\n+            print(f\"Invalid port argument, using default: {port}\")\n+    else:\n+        print(f\"No port argument provided, using default: {port}\")\n+\n+    # Explicitly unset the PORT environment variable if it exists\n+    if \"PORT\" in os.environ:\n+        port_val = os.environ.pop(\"PORT\")\n+        print(f\"Removed environment variable PORT={port_val}\")\n+\n     on_start_logger(port)\n-    uvicorn.run(\"agent_memory_server.main:app\", host=\"0.0.0.0\", port=port, reload=False)\n+    uvicorn.run(\n+        app,  # Using the app instance directly\n+        host=\"0.0.0.0\",\n+        port=port,\n+        reload=False,\n+    )\n\n@@ -2,7 +2,7 @@\n import logging\n import sys\n \n-from fastapi import BackgroundTasks, HTTPException\n+from fastapi import HTTPException\n from mcp.server.fastmcp import FastMCP\n from mcp.server.fastmcp.prompts import base\n from mcp.types import TextContent\n@@ -13,6 +13,7 @@\n     search_long_term_memory as core_search_long_term_memory,\n )\n from agent_memory_server.config import settings\n+from agent_memory_server.dependencies import get_background_tasks\n from agent_memory_server.models import (\n     AckResponse,\n     CreateLongTermMemoryPayload,\n@@ -75,7 +76,7 @@ async def create_long_term_memories(\n         An acknowledgement response indicating success\n     \"\"\"\n     return await core_create_long_term_memory(\n-        payload, background_tasks=BackgroundTasks()\n+        payload, background_tasks=get_background_tasks()\n     )\n \n \n\n@@ -2,11 +2,11 @@\n import logging\n import time\n \n-from fastapi import BackgroundTasks\n from redis import WatchError\n from redis.asyncio import Redis\n \n from agent_memory_server.config import settings\n+from agent_memory_server.dependencies import DocketBackgroundTasks\n from agent_memory_server.long_term_memory import index_long_term_memories\n from agent_memory_server.models import (\n     LongTermMemory,\n@@ -87,18 +87,16 @@ async def set_session_memory(\n     redis: Redis,\n     session_id: str,\n     memory: SessionMemory,\n-    background_tasks: BackgroundTasks,\n+    background_tasks: DocketBackgroundTasks,\n ):\n     \"\"\"\n     Create or update a session's memory\n \n-    TODO: This shouldn't need BackgroundTasks.\n-\n     Args:\n         redis: The Redis client\n         session_id: The session ID\n         memory: The session memory to set\n-        background_tasks: The background tasks to add the summarization task to\n+        background_tasks: Background tasks instance\n     \"\"\"\n     sessions_key = Keys.sessions_key(namespace=memory.namespace)\n     messages_key = Keys.messages_key(session_id, namespace=memory.namespace)\n@@ -127,32 +125,28 @@ async def set_session_memory(\n     # Check if window size is exceeded\n     current_size = await redis.llen(messages_key)  # type: ignore\n     if current_size > settings.window_size:\n-        # Handle summarization in background\n-        background_tasks.add_task(\n+        # Add summarization task\n+        await background_tasks.add_task(\n             summarize_session,\n-            redis,\n             session_id,\n             settings.generation_model,\n             settings.window_size,\n         )\n \n     # If long-term memory is enabled, index messages\n-    # TODO: Use a distributed task queue\n-    # TODO: Allow strategies for long-term memory: indexing\n-    #       messages vs. extracting memories from messages, etc.\n     if settings.long_term_memory:\n-        background_tasks.add_task(\n+        memories = [\n+            LongTermMemory(\n+                session_id=session_id,\n+                text=f\"{msg.role}: {msg.content}\",\n+                namespace=memory.namespace,\n+            )\n+            for msg in memory.messages\n+        ]\n+\n+        await background_tasks.add_task(\n             index_long_term_memories,\n-            redis,\n-            [\n-                LongTermMemory(\n-                    session_id=session_id,\n-                    text=f\"{msg.role}: {msg.content}\",\n-                    namespace=memory.namespace,\n-                )\n-                for msg in memory.messages\n-            ],\n-            background_tasks,\n+            memories,\n         )\n \n \n\n@@ -3,7 +3,6 @@\n \n import tiktoken\n from redis import WatchError\n-from redis.asyncio import Redis\n \n from agent_memory_server.config import settings\n from agent_memory_server.llms import (\n@@ -12,7 +11,7 @@\n     get_model_config,\n )\n from agent_memory_server.models import MemoryMessage\n-from agent_memory_server.utils import Keys, get_model_client\n+from agent_memory_server.utils import Keys, get_model_client, get_redis_conn\n \n \n logger = logging.getLogger(__name__)\n@@ -110,7 +109,6 @@ async def _incremental_summary(\n \n \n async def summarize_session(\n-    redis: Redis,\n     session_id: str,\n     model: str,\n     window_size: int,\n@@ -132,6 +130,7 @@ async def summarize_session(\n         client: The client wrapper (OpenAI or Anthropic)\n         redis_conn: Redis connection\n     \"\"\"\n+    redis = get_redis_conn()\n     client = await get_model_client(settings.generation_model)\n \n     messages_key = Keys.messages_key(session_id)\n\n@@ -0,0 +1,111 @@\n+\"\"\"\n+Run the Docket worker directly from Python.\n+\n+This module provides a way to run the background task worker in-process\n+instead of using the CLI command.\n+\n+Usage:\n+    python -m agent_memory_server.worker\n+\"\"\"\n+\n+import asyncio\n+import signal\n+import sys\n+from datetime import timedelta\n+\n+from docket import Docket, Worker\n+\n+from agent_memory_server.config import settings\n+from agent_memory_server.docket_tasks import task_collection\n+from agent_memory_server.logging import configure_logging, get_logger\n+\n+\n+configure_logging()\n+logger = get_logger(__name__)\n+\n+\n+async def run_worker(concurrency: int = 10, redelivery_timeout: int = 30):\n+    \"\"\"\n+    Run the Docket worker in Python.\n+\n+    Args:\n+        concurrency: Number of tasks to process concurrently\n+        redelivery_timeout: Seconds to wait before redelivering a task to another worker\n+    \"\"\"\n+    if not settings.use_docket:\n+        logger.error(\"Docket is disabled in settings. Cannot run worker.\")\n+        return None\n+\n+    logger.info(f\"Starting Docket worker for {settings.docket_name}\")\n+    logger.info(\n+        f\"Concurrency: {concurrency}, Redelivery timeout: {redelivery_timeout}s\"\n+    )\n+\n+    # Create a signal handler to gracefully shut down\n+    shutdown_event = asyncio.Event()\n+\n+    def handle_signal(sig, frame):\n+        logger.info(f\"Received signal {sig}, shutting down...\")\n+        shutdown_event.set()\n+\n+    # Register signal handlers\n+    signal.signal(signal.SIGINT, handle_signal)\n+    signal.signal(signal.SIGTERM, handle_signal)\n+\n+    try:\n+        # Initialize Docket client\n+        async with Docket(\n+            name=settings.docket_name,\n+            url=settings.redis_url,\n+        ) as docket:\n+            # Register all tasks\n+            for task in task_collection:\n+                docket.register(task)\n+\n+            logger.info(f\"Registered {len(task_collection)} tasks\")\n+\n+            # Create and run the worker\n+            async with Worker(\n+                docket,\n+                concurrency=concurrency,\n+                redelivery_timeout=timedelta(seconds=redelivery_timeout),\n+            ) as worker:\n+                # Run until shutdown is requested\n+                await worker.run_forever()\n+\n+    except Exception as e:\n+        logger.error(f\"Error running worker: {e}\")\n+        return 1\n+\n+    logger.info(\"Worker shut down gracefully\")\n+    return 0\n+\n+\n+def main():\n+    \"\"\"Command line entry point\"\"\"\n+    # Parse command line arguments\n+    concurrency = 10\n+    redelivery_timeout = 30\n+\n+    args = sys.argv[1:]\n+    if \"--concurrency\" in args:\n+        try:\n+            idx = args.index(\"--concurrency\")\n+            concurrency = int(args[idx + 1])\n+        except (ValueError, IndexError):\n+            pass\n+\n+    if \"--redelivery-timeout\" in args:\n+        try:\n+            idx = args.index(\"--redelivery-timeout\")\n+            redelivery_timeout = int(args[idx + 1])\n+        except (ValueError, IndexError):\n+            pass\n+\n+    return asyncio.run(\n+        run_worker(concurrency=concurrency, redelivery_timeout=redelivery_timeout)\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main())\n\n@@ -5,31 +5,31 @@ build-backend = \"hatchling.build\"\n [project]\n name = \"agent-memory-server\"\n version = \"0.2.0\"\n-description = \"A Redis-powered memory server built for AI agents and applications.\"\n+description = \"A Memory Server for LLM Agents and Applications\"\n readme = \"README.md\"\n requires-python = \">=3.12,<3.13\"\n license = { text = \"MIT\" }\n authors = [{ name = \"Andrew Brookins\", email = \"andrew.brookins@redis.com\" }]\n dependencies = [\n+    \"accelerate>=1.6.0\",\n+    \"anthropic>=0.15.0\",\n+    \"bertopic<0.17.0,>=0.16.4\",\n     \"fastapi>=0.115.11\",\n-    \"uvicorn>=0.24.0\",\n-    \"redis>=5.0.1\",\n+    \"mcp>=1.6.0\",\n+    \"nanoid>=2.0.0\",\n+    \"numba>=0.60.0\",\n+    \"numpy>=2.1.0\",\n     \"openai>=1.3.7\",\n-    \"anthropic>=0.15.0\",\n     \"pydantic>=2.5.2\",\n-    \"python-dotenv>=1.0.0\",\n-    \"tiktoken>=0.5.1\",\n-    \"numpy>=2.1.0\",\n     \"pydantic-settings>=2.8.1\",\n-    \"bertopic>=0.16.4,<0.17.0\",\n-    \"structlog>=25.2.0\",\n-    \"transformers>=4.30.0,<=4.50.3\",\n-    \"numba>=0.60.0\",\n-    \"nanoid>=2.0.0\",\n-    \"mcp>=1.6.0\",\n-    \"sentence-transformers>=3.4.1\",\n-    \"accelerate>=1.6.0\",\n+    \"python-dotenv>=1.0.0\",\n+    \"pydocket>=0.6.3\",\n     \"redisvl>=0.5.1\",\n+    \"sentence-transformers>=3.4.1\",\n+    \"structlog>=25.2.0\",\n+    \"tiktoken>=0.5.1\",\n+    \"transformers<=4.50.3,>=4.30.0\",\n+    \"uvicorn>=0.24.0\",\n ]\n \n [tool.hatch.build.targets.wheel]\n\n@@ -0,0 +1 @@\n+worker_command=\"docket worker --tasks agent_memory_server.docket_tasks:task_collection\"\n\n@@ -6,14 +6,15 @@\n \n import pytest\n from dotenv import load_dotenv\n-from fastapi import BackgroundTasks, FastAPI\n+from fastapi import FastAPI\n from httpx import ASGITransport, AsyncClient\n from redis import Redis\n from redis.asyncio import ConnectionPool, Redis as AsyncRedis\n from testcontainers.compose import DockerCompose\n \n from agent_memory_server.api import router as memory_router\n from agent_memory_server.config import settings\n+from agent_memory_server.dependencies import DocketBackgroundTasks, get_background_tasks\n from agent_memory_server.healthcheck import router as health_router\n from agent_memory_server.llms import OpenAIClientWrapper\n from agent_memory_server.messages import (\n@@ -105,7 +106,6 @@ async def session(use_test_redis_connection, async_redis_client):\n     session_id = \"test-session\"\n \n     await index_long_term_memories(\n-        async_redis_client,\n         [\n             LongTermMemory(\n                 session_id=session_id,\n@@ -118,7 +118,6 @@ async def session(use_test_redis_connection, async_redis_client):\n                 namespace=\"test-namespace\",\n             ),\n         ],\n-        background_tasks=BackgroundTasks(),\n     )\n \n     # Add messages to session memory\n@@ -137,7 +136,7 @@ async def session(use_test_redis_connection, async_redis_client):\n             tokens=150,\n             namespace=\"test-namespace\",\n         ),\n-        background_tasks=BackgroundTasks(),\n+        background_tasks=DocketBackgroundTasks(),\n     )\n \n     return session_id\n@@ -243,7 +242,10 @@ def pytest_collection_modifyitems(\n             item.add_marker(skip_api)\n \n \n-MockBackgroundTasks = mock.Mock(name=\"BackgroundTasks\", spec=BackgroundTasks)\n+@pytest.fixture()\n+def mock_background_tasks():\n+    \"\"\"Create a mock DocketBackgroundTasks instance\"\"\"\n+    return mock.Mock(name=\"DocketBackgroundTasks\", spec=DocketBackgroundTasks)\n \n \n @pytest.fixture()\n@@ -259,16 +261,15 @@ def app(use_test_redis_connection):\n \n \n @pytest.fixture()\n-def app_with_mock_background_tasks(use_test_redis_connection):\n+def app_with_mock_background_tasks(use_test_redis_connection, mock_background_tasks):\n     \"\"\"Create a test FastAPI app with routers\"\"\"\n     app = FastAPI()\n \n     # Include routers\n     app.include_router(health_router)\n     app.include_router(memory_router)\n \n-    mock_background_tasks = MockBackgroundTasks()\n-    app.dependency_overrides[BackgroundTasks] = lambda: mock_background_tasks\n+    app.dependency_overrides[get_background_tasks] = lambda: mock_background_tasks\n \n     return app\n \n\n@@ -123,8 +123,11 @@ async def test_put_memory(self, client):\n \n     @pytest.mark.requires_api_keys\n     @pytest.mark.asyncio\n-    async def test_put_memory_stores_messages_in_long_term_memory(self, client):\n+    async def test_put_memory_stores_messages_in_long_term_memory(\n+        self, client_with_mock_background_tasks, mock_background_tasks\n+    ):\n         \"\"\"Test the put_memory endpoint\"\"\"\n+        client = client_with_mock_background_tasks\n         payload = {\n             \"messages\": [\n                 {\"role\": \"user\", \"content\": \"Hello\"},\n@@ -133,12 +136,8 @@ async def test_put_memory_stores_messages_in_long_term_memory(self, client):\n             \"context\": \"Previous context\",\n         }\n         mock_settings = Settings(long_term_memory=True)\n-        mock_add_task = MagicMock()\n \n-        with (\n-            patch(\"agent_memory_server.api.settings\", mock_settings),\n-            patch(\"agent_memory_server.api.BackgroundTasks.add_task\", mock_add_task),\n-        ):\n+        with patch(\"agent_memory_server.api.settings\", mock_settings):\n             response = await client.put(\"/sessions/test-session/memory\", json=payload)\n \n         assert response.status_code == 200\n@@ -148,15 +147,21 @@ async def test_put_memory_stores_messages_in_long_term_memory(self, client):\n         assert data[\"status\"] == \"ok\"\n \n         # Check that background tasks were called\n-        assert mock_add_task.call_count == 1\n+        assert mock_background_tasks.add_task.call_count == 1\n \n         # Check that the last call was for long-term memory indexing\n-        assert mock_add_task.call_args_list[-1][0][0] == index_long_term_memories\n+        assert (\n+            mock_background_tasks.add_task.call_args_list[-1][0][0]\n+            == index_long_term_memories\n+        )\n \n     @pytest.mark.requires_api_keys\n     @pytest.mark.asyncio\n-    async def test_post_memory_compacts_long_conversation(self, client):\n+    async def test_post_memory_compacts_long_conversation(\n+        self, client_with_mock_background_tasks, mock_background_tasks\n+    ):\n         \"\"\"Test the post_memory endpoint\"\"\"\n+        client = client_with_mock_background_tasks\n         payload = {\n             \"messages\": [\n                 {\"role\": \"user\", \"content\": \"Hello\"},\n@@ -165,12 +170,9 @@ async def test_post_memory_compacts_long_conversation(self, client):\n             \"context\": \"Previous context\",\n         }\n         mock_settings = Settings(window_size=1, long_term_memory=False)\n-        mock_add_task = MagicMock()\n+        MagicMock()\n \n-        with (\n-            patch(\"agent_memory_server.api.messages.settings\", mock_settings),\n-            patch(\"agent_memory_server.api.BackgroundTasks.add_task\", mock_add_task),\n-        ):\n+        with patch(\"agent_memory_server.api.messages.settings\", mock_settings):\n             response = await client.put(\"/sessions/test-session/memory\", json=payload)\n \n         assert response.status_code == 200\n@@ -180,10 +182,12 @@ async def test_post_memory_compacts_long_conversation(self, client):\n         assert data[\"status\"] == \"ok\"\n \n         # Check that background tasks were called\n-        assert mock_add_task.call_count == 1\n+        assert mock_background_tasks.add_task.call_count == 1\n \n         # Check that the last call was for compaction\n-        assert mock_add_task.call_args_list[-1][0][0] == summarize_session\n+        assert (\n+            mock_background_tasks.add_task.call_args_list[-1][0][0] == summarize_session\n+        )\n \n     @pytest.mark.asyncio\n     async def test_delete_memory(self, client, session):\n\n@@ -5,7 +5,6 @@\n import nanoid\n import numpy as np\n import pytest\n-from fastapi import BackgroundTasks\n from redis.commands.search.document import Document\n \n from agent_memory_server.filters import SessionId\n@@ -27,12 +26,14 @@ async def test_index_memories(\n             LongTermMemory(text=\"France is a country in Europe\", session_id=session),\n         ]\n \n-        mock_vector = np.array(\n-            [[0.1, 0.2, 0.3, 0.4], [0.1, 0.2, 0.3, 0.4]], dtype=np.float32\n-        )\n+        # Create two separate embedding vectors\n+        mock_vectors = [\n+            np.array([0.1, 0.2, 0.3, 0.4], dtype=np.float32).tobytes(),\n+            np.array([0.5, 0.6, 0.7, 0.8], dtype=np.float32).tobytes(),\n+        ]\n \n         mock_vectorizer = MagicMock()\n-        mock_vectorizer.aembed_many = AsyncMock(return_value=mock_vector)\n+        mock_vectorizer.aembed_many = AsyncMock(return_value=mock_vectors)\n \n         mock_async_redis_client.hset = AsyncMock()\n \n@@ -41,9 +42,7 @@ async def test_index_memories(\n             return_value=mock_vectorizer,\n         ):\n             await index_long_term_memories(\n-                mock_async_redis_client,\n                 long_term_memories,\n-                background_tasks=BackgroundTasks(),\n             )\n \n         # Check that create_embedding was called with the right arguments\n@@ -55,7 +54,7 @@ async def test_index_memories(\n         )\n \n         # Verify one of the calls to make sure the data is correct\n-        for call in mock_async_redis_client.hset.call_args_list:\n+        for i, call in enumerate(mock_async_redis_client.hset.call_args_list):\n             args, kwargs = call\n \n             # Check that the key starts with the memory key prefix\n@@ -64,13 +63,13 @@ async def test_index_memories(\n             # Check that the mapping contains the right keys\n             mapping = kwargs[\"mapping\"]\n             assert mapping == {\n-                \"text\": long_term_memories[0].text,\n-                \"id_\": long_term_memories[0].id_,\n-                \"session_id\": long_term_memories[0].session_id,\n-                \"user_id\": long_term_memories[0].user_id,\n-                \"last_accessed\": long_term_memories[0].last_accessed,\n-                \"created_at\": long_term_memories[0].created_at,\n-                \"vector\": mock_vector.tobytes(),\n+                \"text\": long_term_memories[i].text,\n+                \"id_\": long_term_memories[i].id_,\n+                \"session_id\": long_term_memories[i].session_id,\n+                \"user_id\": long_term_memories[i].user_id,\n+                \"last_accessed\": long_term_memories[i].last_accessed,\n+                \"created_at\": long_term_memories[i].created_at,\n+                \"vector\": mock_vectors[i],\n             }\n \n     @pytest.mark.asyncio\n@@ -168,11 +167,13 @@ async def test_search_messages(self, async_redis_client):\n             LongTermMemory(text=\"France is a country in Europe\", session_id=\"123\"),\n         ]\n \n-        await index_long_term_memories(\n-            async_redis_client,\n-            long_term_memories,\n-            background_tasks=BackgroundTasks(),\n-        )\n+        with mock.patch(\n+            \"agent_memory_server.long_term_memory.get_redis_conn\",\n+            return_value=async_redis_client,\n+        ):\n+            await index_long_term_memories(\n+                long_term_memories,\n+            )\n \n         results = await search_long_term_memories(\n             \"What is the capital of France?\",\n@@ -195,11 +196,13 @@ async def test_search_messages_with_distance_threshold(self, async_redis_client)\n             LongTermMemory(text=\"France is a country in Europe\", session_id=\"123\"),\n         ]\n \n-        await index_long_term_memories(\n-            async_redis_client,\n-            long_term_memories,\n-            background_tasks=BackgroundTasks(),\n-        )\n+        with mock.patch(\n+            \"agent_memory_server.long_term_memory.get_redis_conn\",\n+            return_value=async_redis_client,\n+        ):\n+            await index_long_term_memories(\n+                long_term_memories,\n+            )\n \n         results = await search_long_term_memories(\n             \"What is the capital of France?\",\n\n@@ -160,6 +160,7 @@ async def test_set_session_memory_window_size_exceeded(\n         mock_async_redis_client.pipeline = MagicMock(return_value=mock_pipeline)\n \n         mock_background_tasks = MagicMock()\n+        mock_background_tasks.add_task = AsyncMock()\n \n         memory = SessionMemory(\n             messages=[MemoryMessage(role=\"user\", content=\"Hello\")],\n@@ -181,7 +182,6 @@ async def test_set_session_memory_window_size_exceeded(\n         # Verify summarization task was added\n         mock_background_tasks.add_task.assert_called_with(\n             summarize_session,\n-            mock_async_redis_client,\n             \"test-session\",\n             \"gpt-4o-mini\",\n             20,\n@@ -205,6 +205,7 @@ async def test_set_session_memory_with_long_term_memory(\n         mock_async_redis_client.pipeline = MagicMock(return_value=mock_pipeline)\n \n         mock_background_tasks = MagicMock()\n+        mock_background_tasks.add_task = AsyncMock()\n \n         memory = SessionMemory(\n             messages=[MemoryMessage(role=\"user\", content=\"Hello\")],\n@@ -228,9 +229,7 @@ async def test_set_session_memory_with_long_term_memory(\n         assert mock_background_tasks.add_task.call_args_list == [\n             call(\n                 index_long_term_memories,\n-                mock_async_redis_client,\n                 [LongTermMemory(session_id=\"test-session\", text=\"user: Hello\")],\n-                mock_background_tasks,\n             ),\n         ]\n \n\n@@ -111,13 +111,18 @@ async def test_summarize_session(\n \n         mock_summarization.return_value = (\"New summary\", 300)\n \n-        with patch(\n-            \"agent_memory_server.summarization.get_model_client\"\n-        ) as mock_get_model_client:\n+        with (\n+            patch(\n+                \"agent_memory_server.summarization.get_model_client\"\n+            ) as mock_get_model_client,\n+            patch(\n+                \"agent_memory_server.summarization.get_redis_conn\",\n+                return_value=mock_async_redis_client,\n+            ),\n+        ):\n             mock_get_model_client.return_value = mock_openai_client\n \n             await summarize_session(\n-                mock_async_redis_client,\n                 session_id,\n                 model,\n                 window_size,\n@@ -178,12 +183,15 @@ async def test_handle_summarization_no_messages(\n         pipeline_mock.lpop = AsyncMock(return_value=True)\n         pipeline_mock.execute = AsyncMock(return_value=True)\n \n-        await summarize_session(\n-            mock_async_redis_client,\n-            session_id,\n-            model,\n-            window_size,\n-        )\n+        with patch(\n+            \"agent_memory_server.summarization.get_redis_conn\",\n+            return_value=mock_async_redis_client,\n+        ):\n+            await summarize_session(\n+                session_id,\n+                model,\n+                window_size,\n+            )\n \n         assert mock_summarization.call_count == 0\n         assert pipeline_mock.lrange.call_count == 0\n\n@@ -35,8 +35,8 @@ dependencies = [\n     { name = \"openai\" },\n     { name = \"pydantic\" },\n     { name = \"pydantic-settings\" },\n+    { name = \"pydocket\" },\n     { name = \"python-dotenv\" },\n-    { name = \"redis\" },\n     { name = \"redisvl\" },\n     { name = \"sentence-transformers\" },\n     { name = \"structlog\" },\n@@ -68,8 +68,8 @@ requires-dist = [\n     { name = \"openai\", specifier = \">=1.3.7\" },\n     { name = \"pydantic\", specifier = \">=2.5.2\" },\n     { name = \"pydantic-settings\", specifier = \">=2.8.1\" },\n+    { name = \"pydocket\", specifier = \">=0.6.3\" },\n     { name = \"python-dotenv\", specifier = \">=1.0.0\" },\n-    { name = \"redis\", specifier = \">=5.0.1\" },\n     { name = \"redisvl\", specifier = \">=0.5.1\" },\n     { name = \"sentence-transformers\", specifier = \">=3.4.1\" },\n     { name = \"structlog\", specifier = \">=25.2.0\" },\n@@ -213,6 +213,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl\", hash = \"sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2\", size = 98188 },\n ]\n \n+[[package]]\n+name = \"cloudpickle\"\n+version = \"3.1.1\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/52/39/069100b84d7418bc358d81669d5748efb14b9cceacd2f9c75f550424132f/cloudpickle-3.1.1.tar.gz\", hash = \"sha256:b216fa8ae4019d5482a8ac3c95d8f6346115d8835911fd4aefd1a445e4242c64\", size = 22113 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/7e/e8/64c37fadfc2816a7701fa8a6ed8d87327c7d54eacfbfb6edab14a2f2be75/cloudpickle-3.1.1-py3-none-any.whl\", hash = \"sha256:c8c5a44295039331ee9dad40ba100a9c7297b6f988e50e87ccdf3765a668350e\", size = 20992 },\n+]\n+\n [[package]]\n name = \"colorama\"\n version = \"0.4.6\"\n@@ -234,6 +243,18 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl\", hash = \"sha256:612ee75c546f53e92e70049c9dbfcc18c935a2b9a53b66085ce9ef6a6e5c0934\", size = 46018 },\n ]\n \n+[[package]]\n+name = \"deprecated\"\n+version = \"1.2.18\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"wrapt\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/98/97/06afe62762c9a8a86af0cfb7bfdab22a43ad17138b07af5b1a58442690a2/deprecated-1.2.18.tar.gz\", hash = \"sha256:422b6f6d859da6f2ef57857761bfb392480502a64c3028ca9bbe86085d72115d\", size = 2928744 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl\", hash = \"sha256:bd5011788200372a32418f888e326a09ff80d0214bd961147cfed01b5c018eec\", size = 9998 },\n+]\n+\n [[package]]\n name = \"distlib\"\n version = \"0.3.9\"\n@@ -417,6 +438,18 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl\", hash = \"sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3\", size = 70442 },\n ]\n \n+[[package]]\n+name = \"importlib-metadata\"\n+version = \"8.6.1\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"zipp\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/33/08/c1395a292bb23fd03bdf572a1357c5a733d3eecbab877641ceacab23db6e/importlib_metadata-8.6.1.tar.gz\", hash = \"sha256:310b41d755445d74569f993ccfc22838295d9fe005425094fad953d7f15c8580\", size = 55767 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/79/9d/0fb148dc4d6fa4a7dd1d8378168d9b4cd8d4560a6fbf6f0121c5fc34eb68/importlib_metadata-8.6.1-py3-none-any.whl\", hash = \"sha256:02a89390c1e15fdfdc0d7c6b25cb3e62650d0494005c97d6f148bf5b9787525e\", size = 26971 },\n+]\n+\n [[package]]\n name = \"iniconfig\"\n version = \"2.1.0\"\n@@ -512,6 +545,18 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/d0/81/e66fc86539293282fd9cb7c9417438e897f369e79ffb62e1ae5e5154d4dd/llvmlite-0.44.0-cp313-cp313-win_amd64.whl\", hash = \"sha256:2fb7c4f2fb86cbae6dca3db9ab203eeea0e22d73b99bc2341cdf9de93612e930\", size = 30331193 },\n ]\n \n+[[package]]\n+name = \"markdown-it-py\"\n+version = \"3.0.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"mdurl\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz\", hash = \"sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb\", size = 74596 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl\", hash = \"sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1\", size = 87528 },\n+]\n+\n [[package]]\n name = \"markupsafe\"\n version = \"3.0.2\"\n@@ -569,6 +614,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/10/30/20a7f33b0b884a9d14dd3aa94ff1ac9da1479fe2ad66dd9e2736075d2506/mcp-1.6.0-py3-none-any.whl\", hash = \"sha256:7bd24c6ea042dbec44c754f100984d186620d8b841ec30f1b19eda9b93a634d0\", size = 76077 },\n ]\n \n+[[package]]\n+name = \"mdurl\"\n+version = \"0.1.2\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz\", hash = \"sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba\", size = 8729 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl\", hash = \"sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8\", size = 9979 },\n+]\n+\n [[package]]\n name = \"ml-dtypes\"\n version = \"0.4.1\"\n@@ -826,6 +880,60 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/c4/f7/049e85faf6a000890e5ca0edca8e9183f8a43c9e7bba869cad871da0caba/openai-1.71.0-py3-none-any.whl\", hash = \"sha256:e1c643738f1fff1af52bce6ef06a7716c95d089281e7011777179614f32937aa\", size = 598975 },\n ]\n \n+[[package]]\n+name = \"opentelemetry-api\"\n+version = \"1.32.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"deprecated\" },\n+    { name = \"importlib-metadata\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/7b/34/e701d77900123af17a11dbaf0c9f527fa7ef94b8f02b2c55bed94477890a/opentelemetry_api-1.32.0.tar.gz\", hash = \"sha256:2623280c916f9b19cad0aa4280cb171265f19fd2909b0d47e4f06f7c83b02cb5\", size = 64134 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/fe/e8/d05fd19c1c7e7e230ab44c366791179fd64c843bc587c257a56e853893c5/opentelemetry_api-1.32.0-py3-none-any.whl\", hash = \"sha256:15df743c765078611f376037b0d9111ec5c1febf2ec9440cdd919370faa1ce55\", size = 65285 },\n+]\n+\n+[[package]]\n+name = \"opentelemetry-exporter-prometheus\"\n+version = \"0.53b0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"opentelemetry-api\" },\n+    { name = \"opentelemetry-sdk\" },\n+    { name = \"prometheus-client\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/e0/13/d1fe83281e40f050d04f45e6e865f36991e98d9c8f1c5f4614602af1431a/opentelemetry_exporter_prometheus-0.53b0.tar.gz\", hash = \"sha256:2d8dd0684b5229840974a85686028954a2b2170f5118cb36fa6497f11cb35f29\", size = 14948 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/18/f3/4f0e7e33dcb41317905db2933f9ea86ce728d5710cf4f60063ee9763d4d5/opentelemetry_exporter_prometheus-0.53b0-py3-none-any.whl\", hash = \"sha256:a202262aa96f1840e0ebff75bef5b11e6d84cafd0e6a0f82979cccda0cdcee3b\", size = 12950 },\n+]\n+\n+[[package]]\n+name = \"opentelemetry-sdk\"\n+version = \"1.32.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"opentelemetry-api\" },\n+    { name = \"opentelemetry-semantic-conventions\" },\n+    { name = \"typing-extensions\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/e8/0c/842aed73035cab0302ec70057f3180f4f023974d74bd9764ef3046f358fb/opentelemetry_sdk-1.32.0.tar.gz\", hash = \"sha256:5ff07fb371d1ab1189fa7047702e2e888b5403c5efcbb18083cae0d5aa5f58d2\", size = 161043 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/ee/6a/b8cb562234bd94bcf12ad3058ef7f31319b94a8df65130ce9cc2ff3c8d55/opentelemetry_sdk-1.32.0-py3-none-any.whl\", hash = \"sha256:ed252d035c22a15536c1f603ca089298daab60850fc2f5ddfa95d95cc1c043ea\", size = 118990 },\n+]\n+\n+[[package]]\n+name = \"opentelemetry-semantic-conventions\"\n+version = \"0.53b0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"deprecated\" },\n+    { name = \"opentelemetry-api\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/c2/c4/213d23239df175b420b74c6e25899c482701e6614822dc51f8c20dae7e2d/opentelemetry_semantic_conventions-0.53b0.tar.gz\", hash = \"sha256:05b7908e1da62d72f9bf717ed25c72f566fe005a2dd260c61b11e025f2552cf6\", size = 114343 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/7c/23/0bef11f394f828f910f32567d057f097dbaba23edf33114018a380a0d0bf/opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl\", hash = \"sha256:561da89f766ab51615c0e72b12329e0a1bc16945dbd62c8646ffc74e36a1edff\", size = 188441 },\n+]\n+\n [[package]]\n name = \"packaging\"\n version = \"24.2\"\n@@ -963,6 +1071,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/88/74/a88bf1b1efeae488a0c0b7bdf71429c313722d1fc0f377537fbe554e6180/pre_commit-4.2.0-py2.py3-none-any.whl\", hash = \"sha256:a009ca7205f1eb497d10b845e52c838a98b6cdd2102a6c8e4540e94ee75c58bd\", size = 220707 },\n ]\n \n+[[package]]\n+name = \"prometheus-client\"\n+version = \"0.21.1\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/62/14/7d0f567991f3a9af8d1cd4f619040c93b68f09a02b6d0b6ab1b2d1ded5fe/prometheus_client-0.21.1.tar.gz\", hash = \"sha256:252505a722ac04b0456be05c05f75f45d760c2911ffc45f2a06bcaed9f3ae3fb\", size = 78551 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/ff/c2/ab7d37426c179ceb9aeb109a85cda8948bb269b7561a0be870cc656eefe4/prometheus_client-0.21.1-py3-none-any.whl\", hash = \"sha256:594b45c410d6f4f8888940fe80b5cc2521b305a1fafe1c58609ef715a001f301\", size = 54682 },\n+]\n+\n [[package]]\n name = \"psutil\"\n version = \"7.0.0\"\n@@ -1048,6 +1165,35 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/0b/53/a64f03044927dc47aafe029c42a5b7aabc38dfb813475e0e1bf71c4a59d0/pydantic_settings-2.8.1-py3-none-any.whl\", hash = \"sha256:81942d5ac3d905f7f3ee1a70df5dfb62d5569c12f51a5a647defc1c3d9ee2e9c\", size = 30839 },\n ]\n \n+[[package]]\n+name = \"pydocket\"\n+version = \"0.6.3\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"cloudpickle\" },\n+    { name = \"opentelemetry-api\" },\n+    { name = \"opentelemetry-exporter-prometheus\" },\n+    { name = \"prometheus-client\" },\n+    { name = \"python-json-logger\" },\n+    { name = \"redis\" },\n+    { name = \"rich\" },\n+    { name = \"typer\" },\n+    { name = \"uuid7\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/6e/a5/925cea9bf8047c4c262f4d789c140f5bfb4a55d2e4dcfeccca1527e77403/pydocket-0.6.3.tar.gz\", hash = \"sha256:a7ffeb2c58fc8a98d5de27cdead5b5ec71f0eaae76ac4f9f0a32b7dc410057a6\", size = 86026 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/ac/8f/710e6733a51ac4a8cb3480405275fd63b2c8cf061109e8ba0d525f0ba108/pydocket-0.6.3-py3-none-any.whl\", hash = \"sha256:2d7a148bc6341e463348ee9e375b1f25f8835289218c46707744dd69a5443c63\", size = 32367 },\n+]\n+\n+[[package]]\n+name = \"pygments\"\n+version = \"2.19.1\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz\", hash = \"sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f\", size = 4968581 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl\", hash = \"sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c\", size = 1225293 },\n+]\n+\n [[package]]\n name = \"pynndescent\"\n version = \"0.5.13\"\n@@ -1134,6 +1280,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl\", hash = \"sha256:d7c01d9e2293916c18baf562d95698754b0dbbb5e74d457c45d4f6561fb9d55d\", size = 20256 },\n ]\n \n+[[package]]\n+name = \"python-json-logger\"\n+version = \"3.3.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/9e/de/d3144a0bceede957f961e975f3752760fbe390d57fbe194baf709d8f1f7b/python_json_logger-3.3.0.tar.gz\", hash = \"sha256:12b7e74b17775e7d565129296105bbe3910842d9d0eb083fc83a6a617aa8df84\", size = 16642 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/08/20/0f2523b9e50a8052bc6a8b732dfc8568abbdc42010aef03a2d750bdab3b2/python_json_logger-3.3.0-py3-none-any.whl\", hash = \"sha256:dd980fae8cffb24c13caf6e158d3d61c0d6d22342f932cb6e9deedab3d35eec7\", size = 15163 },\n+]\n+\n [[package]]\n name = \"python-ulid\"\n version = \"3.0.0\"\n@@ -1274,6 +1429,19 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl\", hash = \"sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6\", size = 64928 },\n ]\n \n+[[package]]\n+name = \"rich\"\n+version = \"14.0.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"markdown-it-py\" },\n+    { name = \"pygments\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/a1/53/830aa4c3066a8ab0ae9a9955976fb770fe9c6102117c8ec4ab3ea62d89e8/rich-14.0.0.tar.gz\", hash = \"sha256:82f1bc23a6a21ebca4ae0c45af9bdbc492ed20231dcb63f297d6d1021a9d5725\", size = 224078 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl\", hash = \"sha256:1c9491e1951aac09caffd42f448ee3d04e58923ffe14993f6e83068dc395d7e0\", size = 243229 },\n+]\n+\n [[package]]\n name = \"ruff\"\n version = \"0.11.4\"\n@@ -1415,6 +1583,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/54/21/f43f0a1fa8b06b32812e0975981f4677d28e0f3271601dc88ac5a5b83220/setuptools-78.1.0-py3-none-any.whl\", hash = \"sha256:3e386e96793c8702ae83d17b853fb93d3e09ef82ec62722e61da5cd22376dcd8\", size = 1256108 },\n ]\n \n+[[package]]\n+name = \"shellingham\"\n+version = \"1.5.4\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz\", hash = \"sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de\", size = 10310 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl\", hash = \"sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686\", size = 9755 },\n+]\n+\n [[package]]\n name = \"six\"\n version = \"1.17.0\"\n@@ -1651,6 +1828,21 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/c7/30/37a3384d1e2e9320331baca41e835e90a3767303642c7a80d4510152cbcf/triton-3.2.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\", hash = \"sha256:e5dfa23ba84541d7c0a531dfce76d8bcd19159d50a4a8b14ad01e91734a5c1b0\", size = 253154278 },\n ]\n \n+[[package]]\n+name = \"typer\"\n+version = \"0.15.2\"\n+source = { registry = \"https://pypi.org/simple\" }\n+dependencies = [\n+    { name = \"click\" },\n+    { name = \"rich\" },\n+    { name = \"shellingham\" },\n+    { name = \"typing-extensions\" },\n+]\n+sdist = { url = \"https://files.pythonhosted.org/packages/8b/6f/3991f0f1c7fcb2df31aef28e0594d8d54b05393a0e4e34c65e475c2a5d41/typer-0.15.2.tar.gz\", hash = \"sha256:ab2fab47533a813c49fe1f16b1a370fd5819099c00b119e0633df65f22144ba5\", size = 100711 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/7f/fc/5b29fea8cee020515ca82cc68e3b8e1e34bb19a3535ad854cac9257b414c/typer-0.15.2-py3-none-any.whl\", hash = \"sha256:46a499c6107d645a9c13f7ee46c5d5096cae6f5fc57dd11eccbbb9ae3e44ddfc\", size = 45061 },\n+]\n+\n [[package]]\n name = \"typing-extensions\"\n version = \"4.13.1\"\n@@ -1707,6 +1899,15 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl\", hash = \"sha256:1cee9ad369867bfdbbb48b7dd50374c0967a0bb7710050facf0dd6911440e3df\", size = 128369 },\n ]\n \n+[[package]]\n+name = \"uuid7\"\n+version = \"0.1.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/5c/19/7472bd526591e2192926247109dbf78692e709d3e56775792fec877a7720/uuid7-0.1.0.tar.gz\", hash = \"sha256:8c57aa32ee7456d3cc68c95c4530bc571646defac01895cfc73545449894a63c\", size = 14052 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/b5/77/8852f89a91453956582a85024d80ad96f30a41fed4c2b3dce0c9f12ecc7e/uuid7-0.1.0-py2.py3-none-any.whl\", hash = \"sha256:5e259bb63c8cb4aded5927ff41b444a80d0c7124e8a0ced7cf44efa1f5cccf61\", size = 7477 },\n+]\n+\n [[package]]\n name = \"uvicorn\"\n version = \"0.34.0\"\n@@ -1775,3 +1976,12 @@ wheels = [\n     { url = \"https://files.pythonhosted.org/packages/09/5e/1655cf481e079c1f22d0cabdd4e51733679932718dc23bf2db175f329b76/wrapt-1.17.2-cp313-cp313t-win_amd64.whl\", hash = \"sha256:eaf675418ed6b3b31c7a989fd007fa7c3be66ce14e5c3b27336383604c9da85c\", size = 40750 },\n     { url = \"https://files.pythonhosted.org/packages/2d/82/f56956041adef78f849db6b289b282e72b55ab8045a75abad81898c28d19/wrapt-1.17.2-py3-none-any.whl\", hash = \"sha256:b18f2d1533a71f069c7f82d524a52599053d4c7166e9dd374ae2136b7f40f7c8\", size = 23594 },\n ]\n+\n+[[package]]\n+name = \"zipp\"\n+version = \"3.21.0\"\n+source = { registry = \"https://pypi.org/simple\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/3f/50/bad581df71744867e9468ebd0bcd6505de3b275e06f202c2cb016e3ff56f/zipp-3.21.0.tar.gz\", hash = \"sha256:2c9958f6430a2040341a52eb608ed6dd93ef4392e02ffe219417c1b28b5dd1f4\", size = 24545 }\n+wheels = [\n+    { url = \"https://files.pythonhosted.org/packages/b7/1a/7e4798e9339adc931158c9d69ecc34f5e6791489d469f5e50ec15e35f458/zipp-3.21.0-py3-none-any.whl\", hash = \"sha256:ac1bbe05fd2991f160ebce24ffbac5f6d11d83dc90891255885223d42b3cd931\", size = 9630 },\n+]\n"}
{"id": 47, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis was straightforward to reproduce locally, both with and without scheduler-side queuing. Unsure why on CI it seems to only fail without queuing.\r\n\r\n![image](https://github.com/dask/distributed/assets/6213168/ae93bdb1-0624-4543-8b0d-cd003e0570f1)\r\n\r\nNow the test is rock solid (ran 200 times per CI environment).\n\n### Code Patch:\n@@ -25,6 +25,7 @@\n     assert_can_connect_from_everywhere_4_6,\n     assert_can_connect_locally_4,\n     assert_cannot_connect,\n+    async_poll_for,\n     captured_logger,\n     gen_test,\n     inc,\n@@ -746,37 +747,43 @@ def test_adapt(loop):\n             assert time() < start + 5\n \n \n-def test_adapt_then_manual(loop):\n+@gen_test()\n+async def test_adapt_then_manual():\n     \"\"\"We can revert from adaptive, back to manual\"\"\"\n-    with LocalCluster(\n-        silence_logs=False,\n-        loop=loop,\n+    async with LocalCluster(\n+        asynchronous=True,\n         dashboard_address=\":0\",\n         processes=False,\n         n_workers=8,\n+        threads_per_worker=1,\n     ) as cluster:\n-        cluster.adapt(minimum=0, maximum=4, interval=\"10ms\")\n \n-        start = time()\n-        while cluster.scheduler.workers or cluster.workers:\n-            sleep(0.01)\n-            assert time() < start + 30\n+        def wait_workers(n):\n+            return async_poll_for(\n+                lambda: len(cluster.scheduler.workers) == n\n+                and len(cluster.workers) == n,\n+                timeout=5,\n+            )\n \n-        assert not cluster.workers\n+        await wait_workers(8)\n+        cluster.adapt(minimum=0, maximum=4, interval=\"10ms\")\n+        await wait_workers(0)\n \n-        with Client(cluster) as client:\n+        async with Client(cluster, asynchronous=True) as client:\n             futures = client.map(slowinc, range(1000), delay=0.1)\n-            sleep(0.2)\n+            await wait_workers(4)\n \n             cluster._adaptive.stop()\n-            sleep(0.2)\n+            # stop() disables a PeriodicCallback. However, sometimes the callback has\n+            # already been scheduled or is halfway through (it's asynchronous).\n+            # Wait for it to finish, otherwise you'd get a race condition with\n+            # scale() below and flakiness in the test.\n+            tasks = {t for t in asyncio.all_tasks() if \"PeriodicCallback\" in str(t)}\n+            if tasks:\n+                await asyncio.wait(tasks)\n \n             cluster.scale(2)\n-\n-            start = time()\n-            while len(cluster.scheduler.workers) != 2:\n-                sleep(0.01)\n-                assert time() < start + 30\n+            await wait_workers(2)\n \n \n @pytest.mark.parametrize(\"temporary\", [True, False])\n"}
{"id": 426, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n/kind failing-test\r\n<!--\r\nAdd one of the following kinds:\r\n/kind bug\r\n/kind cleanup\r\n/kind documentation\r\n/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n/kind api-change\r\n/kind deprecation\r\n/kind failing-test\r\n/kind flake\r\n/kind regression\r\n-->\r\n\r\n#### What this PR does / why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes part of #128113 #129036\r\n\r\n#### Special notes for your reviewer:\r\nIn the PreStop hook of a pod, we continuously output \"Looping\" in a loop and sleep for 1 second. The script declared in PreStop is as follows:\r\n```\r\nsh -c '\r\ntouch /persistent/my-container.log;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Starting 0'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\n_term() { sleep 0; echo $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Exiting'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1; exit 0; };\r\ntrap _term TERM;\r\ntouch started;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Started'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Delaying 0'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\nwhile true; do\r\n  echo $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Looping'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\n  sleep 1;\r\ndone;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Exiting'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\nexit 0'\r\n```\r\nIt exhibits the following issues:\r\n\r\n1. The while loop execution time isn't consistently 1 second - it often takes slightly longer by tens of milliseconds\r\n\r\n2. The start/end times of the while loop vary across containers (not just loop execution variance, but also hundreds of milliseconds difference in when kubelet initiates preStop execution)\r\n\r\n3. When kubelet forcibly terminates containers, due to the inconsistent loop timings across containers, some loops might execute N times while others execute N+1 times\r\n\r\nTo mitigate these, I modified the loop timing to be configurable externally through ExecCommand parameters, which should help alleviate these issues.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n\n### Code Patch:\n@@ -48,6 +48,8 @@ type execCommand struct {\n \t// ContainerName is the name of the container to append the log. If empty,\n \t// the name specified in ExecCommand will be used.\n \tContainerName string\n+\t// LoopPeriod is the time interval for executing the loop.\n+\tLoopPeriod float32\n }\n \n // ExecCommand returns the command to execute in the container that implements\n@@ -88,8 +90,13 @@ func ExecCommand(name string, c execCommand) []string {\n \tif c.Delay != 0 {\n \t\tfmt.Fprint(&cmd, sleepCommand(c.Delay))\n \t}\n+\n+\tloopPeriod := float32(1)\n+\tif c.LoopPeriod != 0 {\n+\t\tloopPeriod = c.LoopPeriod\n+\t}\n \tif c.LoopForever {\n-\t\tfmt.Fprintf(&cmd, \"while true; do echo %s '%s Looping' | tee -a %s >> /proc/1/fd/1 ; sleep 1 ; done; \", timeCmd, name, containerLog)\n+\t\tfmt.Fprintf(&cmd, \"while true; do echo %s '%s Looping' | tee -a %s >> /proc/1/fd/1 ; sleep %f ; done; \", timeCmd, name, containerLog, loopPeriod)\n \t}\n \tfmt.Fprintf(&cmd, \"echo %s '%s Exiting'  | tee -a %s >> /proc/1/fd/1; \", timeCmd, name, containerLog)\n \tfmt.Fprintf(&cmd, \"exit %d\", c.ExitCode)\n\n@@ -3178,6 +3178,7 @@ var _ = SIGDescribe(nodefeature.SidecarContainers, \"Containers Lifecycle\", func(\n \t\t\t\t\t\t\t\t\tExitCode:      0,\n \t\t\t\t\t\t\t\t\tContainerName: containerName,\n \t\t\t\t\t\t\t\t\tLoopForever:   true,\n+\t\t\t\t\t\t\t\t\tLoopPeriod:    0.2,\n \t\t\t\t\t\t\t\t}),\n \t\t\t\t\t\t\t},\n \t\t\t\t\t\t},\n@@ -3280,7 +3281,7 @@ var _ = SIGDescribe(nodefeature.SidecarContainers, \"Containers Lifecycle\", func(\n \t\t\t\tps3Last, err := results.TimeOfLastLoop(prefixedName(PreStopPrefix, restartableInit3))\n \t\t\t\tframework.ExpectNoError(err)\n \n-\t\t\t\tconst simulToleration = 500 // milliseconds\n+\t\t\t\tconst simulToleration = 1000 // milliseconds\n \t\t\t\t// should all end together since they loop infinitely and exceed their grace period\n \t\t\t\tgomega.Expect(ps1Last-ps2Last).To(gomega.BeNumerically(\"~\", 0, simulToleration),\n \t\t\t\t\tfmt.Sprintf(\"expected PostStart 1 & PostStart 2 to be killed at the same time, got %s\", results))\n@@ -3290,7 +3291,7 @@ var _ = SIGDescribe(nodefeature.SidecarContainers, \"Containers Lifecycle\", func(\n \t\t\t\t\tfmt.Sprintf(\"expected PostStart 2 & PostStart 3 to be killed at the same time, got %s\", results))\n \n \t\t\t\t// 30 seconds + 2 second minimum grace for the SIGKILL\n-\t\t\t\tconst lifetimeToleration = 1000 // milliseconds\n+\t\t\t\tconst lifetimeToleration = 1500 // milliseconds\n \t\t\t\tgomega.Expect(ps1Last-ps1).To(gomega.BeNumerically(\"~\", 32000, lifetimeToleration),\n \t\t\t\t\tfmt.Sprintf(\"expected PostStart 1 to live for ~32 seconds, got %s\", results))\n \t\t\t\tgomega.Expect(ps2Last-ps2).To(gomega.BeNumerically(\"~\", 32000, lifetimeToleration),\n"}
{"id": 248, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n\r\n<!--\r\n  Thanks for sending a pull request!\r\n    1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n    2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n-->\r\n\r\n### Description of PR\r\nPlease refer to YARN-11816 for details.\r\n\r\n### How was this patch tested?\r\n\r\n\r\n### For code changes:\r\n\r\n- [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n- [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n- [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n- [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n\r\n\n\n### Code Patch:\n@@ -23,7 +23,6 @@\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n import static org.junit.jupiter.api.Assertions.fail;\n import static org.mockito.Mockito.when;\n \n@@ -35,8 +34,10 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.TimeoutException;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.thirdparty.com.google.common.collect.Iterators;\n \n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n@@ -625,7 +626,6 @@ public void testCheckRequestOnceForUnsatisfiedRequest() throws Exception {\n     // mock node tracker with 2000 nodes\n     // to simulate the scenario where there are many nodes in the cluster\n     List<FiCaSchedulerNode> mockNodes = new ArrayList<>();\n-    long ss = System.currentTimeMillis();\n     for (int i = 0; i < 2000; i++) {\n       FiCaSchedulerNode node =\n           TestUtils.getMockNode(\"host\" + i + \":1234\", \"\", 0, 10 * GB, 10);\n@@ -660,26 +660,34 @@ public List<FiCaSchedulerNode> getNodesPerPartition(String partition) {\n     // create an unsatisfied request which will reach the headroom\n     am1.allocate(\"*\", 2 * GB, 10, new ArrayList<>());\n \n-    // verify that when headroom is reached for an unsatisfied request,\n-    // scheduler should only check the request once before checking all nodes.\n-    CandidateNodeSet<FiCaSchedulerNode> candidates =\n-        new SimpleCandidateNodeSet<>(Collections.emptyMap(), \"\");\n-    int numSchedulingCycles = 10;\n-    long startTime = System.currentTimeMillis();\n-    for (int i = 0; i < numSchedulingCycles; i++) {\n-      spyCs.allocateContainersToNode(candidates, false);\n+    List<Long> elapsedMsLst = new ArrayList<>();\n+    try {\n+      GenericTestUtils.waitFor(() -> {\n+        // verify that when headroom is reached for an unsatisfied request,\n+        // scheduler should only check the request once before checking all nodes.\n+        CandidateNodeSet<FiCaSchedulerNode> candidates =\n+            new SimpleCandidateNodeSet<>(Collections.emptyMap(), \"\");\n+        int numSchedulingCycles = 10;\n+        long startTime = System.currentTimeMillis();\n+        for (int i = 0; i < numSchedulingCycles; i++) {\n+          spyCs.allocateContainersToNode(candidates, false);\n+        }\n+        long avgElapsedMs =\n+            (System.currentTimeMillis() - startTime) / numSchedulingCycles;\n+        LOG.info(\"Average elapsed time for a scheduling cycle: {} ms\",\n+            avgElapsedMs);\n+\n+        elapsedMsLst.add(avgElapsedMs);\n+        // verify that the scheduling cycle is less than 10ms,\n+        // ideally the latency should be less than 2ms.\n+        return avgElapsedMs < 10;\n+      }, 500, 3000);\n+    } catch (TimeoutException e) {\n+      fail(\"Scheduling cycle expected to be less than 10ms, \" +\n+          \"but took too long, elapsedMs:\" + elapsedMsLst);\n+    } finally {\n+      rm.stop();\n     }\n-    long avgElapsedMs =\n-        (System.currentTimeMillis() - startTime) / numSchedulingCycles;\n-    LOG.info(\"Average elapsed time for a scheduling cycle: {} ms\",\n-        avgElapsedMs);\n-    // verify that the scheduling cycle is less than 5ms,\n-    // ideally the latency should be less than 2ms.\n-    assertTrue(avgElapsedMs < 5,\n-        String.format(\"%d ms elapsed in average for a scheduling cycle, \" +\n-            \"expected to be less than 5ms.\", avgElapsedMs));\n-\n-    rm.stop();\n   }\n \n   private static void moveReservation(CapacityScheduler cs,\n"}
{"id": 241, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nSummary: `DBErrorHandlingFSTest.AtomicFlushNoSpaceError` is flaky due to seg fault during error recovery:\r\n```\r\n...\r\nframe #5: 0x00007f0b3ea0a9d6 librocksdb.so.9.10`rocksdb::VersionSet::GetObsoleteFiles(std::vector<rocksdb::ObsoleteFileInfo, std::allocator<rocksdb::ObsoleteFileInfo>>*, std::vector<rocksdb::ObsoleteBlobFileInfo, std::allocator<rocksdb::ObsoleteBlobFileInfo>>*, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>>*, unsigned long) [inlined] std::vector<rocksdb::ObsoleteFileInfo, std::allocator<rocksdb::ObsoleteFileInfo>>::begin(this=<unavailable>) at stl_vector.h:812:16\r\nframe #6: 0x00007f0b3ea0a9d6 librocksdb.so.9.10`rocksdb::VersionSet::GetObsoleteFiles(this=0x0000000000000000, files=size=0, blob_files=size=0, manifest_filenames=size=0, min_pending_output=18446744073709551615) at version_set.cc:7258:18\r\nframe #7: 0x00007f0b3e8ccbc0 librocksdb.so.9.10`rocksdb::DBImpl::FindObsoleteFiles(this=<unavailable>, job_context=<unavailable>, force=<unavailable>, no_full_scan=<unavailable>) at db_impl_files.cc:162:30\r\nframe #8: 0x00007f0b3e85e698 librocksdb.so.9.10`rocksdb::DBImpl::ResumeImpl(this=<unavailable>, context=<unavailable>) at db_impl.cc:434:20\r\nframe #9: 0x00007f0b3e921516 librocksdb.so.9.10`rocksdb::ErrorHandler::RecoverFromBGError(this=<unavailable>, is_manual=<unavailable>) at error_handler.cc:632:46\r\n```\r\n\r\nI suspect that this is due to DB being destructed and reopened during recovery. Specifically, the [ClearBGError() call](https://github.com/facebook/rocksdb/blob/c72e79a262bf696faf5f8becabf92374fc14b464/db/db_impl/db_impl.cc#L425) can release and re-acquire mutex, and DB can be closed during this time. So it's not safe to access DB states after ClearBGError(). There was a similar story in #9496. [Moving the obsolete files logic after ClearBGError()](https://github.com/facebook/rocksdb/pull/11955) probably makes the seg fault more easily triggered.\r\n\r\nThis PR updates `ClearBGError()` to guarantee that db close cannot finish until the method returns and the mutex is released. So that we can safely access DB states after calling it.\r\n\r\n\r\nTest plan: I could not trigger the seg fault locally, will just monitor future test failures.\r\n\r\n\n\n### Code Patch:\n@@ -418,26 +418,26 @@ Status DBImpl::ResumeImpl(DBRecoverContext context) {\n     }\n   }\n \n-  if (s.ok()) {\n-    // This will notify and unblock threads waiting for error recovery to\n-    // finish. Those previouly waiting threads can now proceed, which may\n-    // include closing the db.\n-    s = error_handler_.ClearBGError();\n-  } else {\n-    // NOTE: this is needed to pass ASSERT_STATUS_CHECKED\n-    // in the DBSSTTest.DBWithMaxSpaceAllowedRandomized test.\n-    // See https://github.com/facebook/rocksdb/pull/7715#issuecomment-754947952\n-    error_handler_.GetRecoveryError().PermitUncheckedError();\n-  }\n-\n   JobContext job_context(0);\n   FindObsoleteFiles(&job_context, true);\n   mutex_.Unlock();\n+  // If DB shutdown initiated here, it will wait for this ongoing recovery.\n   job_context.manifest_file_number = 1;\n   if (job_context.HaveSomethingToDelete()) {\n     PurgeObsoleteFiles(job_context);\n   }\n   job_context.Clean();\n+  mutex_.Lock();\n+\n+  if (s.ok()) {\n+    // Will notify and unblock threads waiting for error recovery to finish.\n+    s = error_handler_.ClearBGError();\n+  } else {\n+    // NOTE: this is needed to pass ASSERT_STATUS_CHECKED\n+    // in the DBSSTTest.DBWithMaxSpaceAllowedRandomized test.\n+    // See https://github.com/facebook/rocksdb/pull/7715#issuecomment-754947952\n+    error_handler_.GetRecoveryError().PermitUncheckedError();\n+  }\n \n   if (s.ok()) {\n     ROCKS_LOG_INFO(immutable_db_options_.info_log, \"Successfully resumed DB\");\n@@ -446,7 +446,6 @@ Status DBImpl::ResumeImpl(DBRecoverContext context) {\n                    s.ToString().c_str());\n   }\n \n-  mutex_.Lock();\n   // Check for shutdown again before scheduling further compactions,\n   // since we released and re-acquired the lock above\n   if (shutdown_initiated_) {\n@@ -540,8 +539,8 @@ Status DBImpl::CloseHelper() {\n   // continuing with the shutdown\n   mutex_.Lock();\n   shutdown_initiated_ = true;\n-  error_handler_.CancelErrorRecovery();\n-  while (error_handler_.IsRecoveryInProgress()) {\n+  error_handler_.CancelErrorRecoveryForShutDown();\n+  while (!error_handler_.ReadyForShutdown()) {\n     bg_cv_.Wait();\n   }\n   mutex_.Unlock();\n\n@@ -159,6 +159,7 @@ void DBImpl::FindObsoleteFiles(JobContext* job_context, bool force,\n \n   // Get obsolete files.  This function will also update the list of\n   // pending files in VersionSet().\n+  assert(versions_);\n   versions_->GetObsoleteFiles(\n       &job_context->sst_delete_files, &job_context->blob_delete_files,\n       &job_context->manifest_delete_files, job_context->min_pending_output);\n\n@@ -227,7 +227,7 @@ std::map<std::tuple<BackgroundErrorReason, bool>, Status::Severity>\n          Status::Severity::kFatalError},\n };\n \n-void ErrorHandler::CancelErrorRecovery() {\n+void ErrorHandler::CancelErrorRecoveryForShutDown() {\n   db_mutex_->AssertHeld();\n \n   // We'll release the lock before calling sfm, so make sure no new\n@@ -585,8 +585,15 @@ Status ErrorHandler::ClearBGError() {\n     recovery_error_.PermitUncheckedError();\n     recovery_in_prog_ = false;\n     soft_error_no_bg_work_ = false;\n-    EventHelpers::NotifyOnErrorRecoveryEnd(db_options_.listeners, old_bg_error,\n-                                           bg_error_, db_mutex_);\n+    if (!db_->shutdown_initiated_) {\n+      // NotifyOnErrorRecoveryEnd() may release and re-acquire db_mutex_.\n+      // Prevent DB from being closed while we notify listeners. DB close will\n+      // wait until allow_db_shutdown_ = true, see ReadyForShutdown().\n+      allow_db_shutdown_ = false;\n+      EventHelpers::NotifyOnErrorRecoveryEnd(\n+          db_options_.listeners, old_bg_error, bg_error_, db_mutex_);\n+      allow_db_shutdown_ = true;\n+    }\n   }\n   return recovery_error_;\n }\n\n@@ -44,6 +44,7 @@ class ErrorHandler {\n         auto_recovery_(false),\n         recovery_in_prog_(false),\n         soft_error_no_bg_work_(false),\n+        allow_db_shutdown_(true),\n         is_db_stopped_(false),\n         bg_error_stats_(db_options.statistics) {\n     // Clear the checked flag for uninitialized errors\n@@ -63,6 +64,12 @@ class ErrorHandler {\n \n   Status GetRecoveryError() const { return recovery_error_; }\n \n+  // REQUIREs: db mutex held\n+  //\n+  // Returns non-OK status if encountered error during recovery.\n+  // Returns OK if bg error is successfully cleared. May releases and\n+  // re-acquire db mutex to notify listeners. However, DB close (if initiated)\n+  // will be blocked until db mutex is released after return.\n   Status ClearBGError();\n \n   bool IsDBStopped() { return is_db_stopped_.load(std::memory_order_acquire); }\n@@ -79,8 +86,14 @@ class ErrorHandler {\n \n   bool IsRecoveryInProgress() { return recovery_in_prog_; }\n \n+  // REQUIRES: db mutex held\n+  bool ReadyForShutdown() {\n+    db_mutex_->AssertHeld();\n+    return !recovery_in_prog_ && allow_db_shutdown_;\n+  }\n+\n   Status RecoverFromBGError(bool is_manual = false);\n-  void CancelErrorRecovery();\n+  void CancelErrorRecoveryForShutDown();\n \n   void EndAutoRecovery();\n \n@@ -121,6 +134,8 @@ class ErrorHandler {\n   // A flag to indicate that for the soft error, we should not allow any\n   // background work except the work is from recovery.\n   bool soft_error_no_bg_work_;\n+  // Used in ClearBGError() to prevent DB from being closed.\n+  bool allow_db_shutdown_;\n \n   // Used to store the context for recover, such as flush reason.\n   DBRecoverContext recover_context_;\n"}
{"id": 153, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n**What type of PR is this?**\r\n\r\n/kind bug\r\n\r\n**What this PR does / why we need it**:\r\n\r\nPod with PVC will not be scheduled if the PVC is being deleted.\r\nThis can happen when the PVC has finalizers of storage plugins.\r\n\r\nSuch a pod becomes pending.  Unfortunately, after the finalizer\r\nfinishes and PVC is deleted, the pod remains pending forever.\r\nThe StatefulSet controller does nothing for this pending pod.\r\n\r\nThis commit prevents the StatefulSet controller from creating\r\nsuch pods when PVC is to be deleted.\r\n\r\nReprocedure:\r\n\r\n1. Create a single node cluster with [kind](https://github.com/kubernetes-sigs/kind).\r\n2. Create a StatefulSet with the following manifests:\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: test\r\n  namespace: default\r\nspec:\r\n  clusterIP: None\r\n  selector:\r\n    app: test\r\n---\r\napiVersion: apps/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  name: test\r\n  namespace: default\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: test\r\n  serviceName: test\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: test\r\n    spec:\r\n      containers:\r\n      - command:\r\n        - pause\r\n        image: quay.io/cybozu/ubuntu:18.04\r\n        name: ubuntu\r\n        volumeMounts:\r\n        - mountPath: /usr/share/nginx/html\r\n          name: www\r\n  volumeClaimTemplates:\r\n  - metadata:\r\n      name: www\r\n    spec:\r\n      accessModes:\r\n      - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      volumeMode: Filesystem\r\n```\r\n\r\n3. Once the pod gets running, edit PVC to add a finalizer `aaa.bbb/ccc`.\r\n4. Run `kubectl delete --wait=false pvc/www-test-0`.\r\n5. Run `kubectl delete --wait=false pods/test-0`.\r\n6. See a new pod is created and become pending.\r\n7. Edit the PVC to remove the `aaa.bbb/ccc` finalizer.\r\n8. See the PVC is get deleted.\r\n9. See the Pod remains pending forever.\r\n\r\n**Does this PR introduce a user-facing change?**:\r\n```release-note\r\nAdding fix to statefulset controller to wait for pvc deletion before creating pods.\r\n```\r\n\n\n### Code Patch:\n@@ -179,7 +179,7 @@ func (spc *realStatefulPodControl) recordClaimEvent(verb string, set *apps.State\n func (spc *realStatefulPodControl) createPersistentVolumeClaims(set *apps.StatefulSet, pod *v1.Pod) error {\n \tvar errs []error\n \tfor _, claim := range getPersistentVolumeClaims(set, pod) {\n-\t\t_, err := spc.pvcLister.PersistentVolumeClaims(claim.Namespace).Get(claim.Name)\n+\t\tpvc, err := spc.pvcLister.PersistentVolumeClaims(claim.Namespace).Get(claim.Name)\n \t\tswitch {\n \t\tcase apierrors.IsNotFound(err):\n \t\t\t_, err := spc.client.CoreV1().PersistentVolumeClaims(claim.Namespace).Create(&claim)\n@@ -192,6 +192,8 @@ func (spc *realStatefulPodControl) createPersistentVolumeClaims(set *apps.Statef\n \t\tcase err != nil:\n \t\t\terrs = append(errs, fmt.Errorf(\"failed to retrieve PVC %s: %s\", claim.Name, err))\n \t\t\tspc.recordClaimEvent(\"create\", set, pod, &claim, err)\n+\t\tcase pvc.DeletionTimestamp != nil:\n+\t\t\terrs = append(errs, fmt.Errorf(\"pvc %s is to be deleted\", claim.Name))\n \t\t}\n \t\t// TODO: Check resource requirements and accessmodes, update if necessary\n \t}\n\n@@ -20,6 +20,7 @@ import (\n \t\"errors\"\n \t\"strings\"\n \t\"testing\"\n+\t\"time\"\n \n \tapierrors \"k8s.io/apimachinery/pkg/api/errors\"\n \t\"k8s.io/apimachinery/pkg/runtime\"\n@@ -29,6 +30,7 @@ import (\n \t\"k8s.io/client-go/tools/record\"\n \n \t\"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n \t\"k8s.io/client-go/kubernetes/fake\"\n \tcorelisters \"k8s.io/client-go/listers/core/v1\"\n \t_ \"k8s.io/kubernetes/pkg/apis/apps/install\"\n@@ -128,6 +130,42 @@ func TestStatefulPodControlCreatePodPvcCreateFailure(t *testing.T) {\n \t\t}\n \t}\n }\n+func TestStatefulPodControlCreatePodPvcDeleting(t *testing.T) {\n+\trecorder := record.NewFakeRecorder(10)\n+\tset := newStatefulSet(3)\n+\tpod := newStatefulSetPod(set, 0)\n+\tfakeClient := &fake.Clientset{}\n+\tpvcs := getPersistentVolumeClaims(set, pod)\n+\tpvcIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})\n+\tdeleteTime := time.Date(2019, time.January, 1, 0, 0, 0, 0, time.UTC)\n+\tfor k := range pvcs {\n+\t\tpvc := pvcs[k]\n+\t\tpvc.DeletionTimestamp = &metav1.Time{Time: deleteTime}\n+\t\tpvcIndexer.Add(&pvc)\n+\t}\n+\tpvcLister := corelisters.NewPersistentVolumeClaimLister(pvcIndexer)\n+\tcontrol := NewRealStatefulPodControl(fakeClient, nil, nil, pvcLister, recorder)\n+\tfakeClient.AddReactor(\"create\", \"persistentvolumeclaims\", func(action core.Action) (bool, runtime.Object, error) {\n+\t\tcreate := action.(core.CreateAction)\n+\t\treturn true, create.GetObject(), nil\n+\t})\n+\tfakeClient.AddReactor(\"create\", \"pods\", func(action core.Action) (bool, runtime.Object, error) {\n+\t\tcreate := action.(core.CreateAction)\n+\t\treturn true, create.GetObject(), nil\n+\t})\n+\tif err := control.CreateStatefulPod(set, pod); err == nil {\n+\t\tt.Error(\"Failed to produce error on deleting PVC\")\n+\t}\n+\tevents := collectEvents(recorder.Events)\n+\tif eventCount := len(events); eventCount != 1 {\n+\t\tt.Errorf(\"Deleting PVC: got %d events, but want 1\", eventCount)\n+\t}\n+\tfor i := range events {\n+\t\tif !strings.Contains(events[i], v1.EventTypeWarning) {\n+\t\t\tt.Errorf(\"Found unexpected non-warning event %s\", events[i])\n+\t\t}\n+\t}\n+}\n \n type fakeIndexer struct {\n \tcache.Indexer\n"}
{"id": 392, "ground_truth": "OS", "predicted": "OS", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\n`test/async-hooks/test-improper-order.js`\n\n### Platform\n\nAIX\n\n### Console output\n\n```console\nnot ok 68 async-hooks/test-improper-order\n  ---\n  duration_ms: 459.21600\n  severity: fail\n  exitcode: 1\n  stack: |-\n    node:assert:94\n      throw new AssertionError(obj);\n      ^\n    \n    AssertionError [ERR_ASSERTION]: Expected values to be strictly equal:\n    \n    null !== 1\n    \n        at ChildProcess.<anonymous> (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/async-hooks/test-improper-order.js:55:12)\n        at ChildProcess.<anonymous> (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/common/index.js:437:15)\n        at ChildProcess.emit (node:events:507:28)\n        at maybeClose (node:internal/child_process:1101:16)\n        at ChildProcess._handle.onexit (node:internal/child_process:305:5) {\n      generatedMessage: true,\n      code: 'ERR_ASSERTION',\n      actual: null,\n      expected: 1,\n      operator: 'strictEqual'\n    }\n    \n    Node.js v25.0.0-pre\n  ...\n```\n\n### Build links\n\n- https://ci.nodejs.org/job/node-test-commit-aix/57496/nodes=aix72-ppc64/testReport/junit/(root)/async-hooks/test_improper_order/\n\n### Additional information\n\nThis flake is appearing in the reliability report 3 times daily from 2025-05-31 - https://github.com/nodejs/reliability/issues/1217 till today 2025-06-03 - https://github.com/nodejs/reliability/issues/1220.\n\nThe same strategy in https://github.com/nodejs/node/pull/58478 can be used to deflake this test.\n\n### Code Patch:\n@@ -51,8 +51,15 @@ if (process.argv[2] === 'child') {\n   child.stderr.on('data', (d) => { errData = Buffer.concat([ errData, d ]); });\n   child.stdout.on('data', (d) => { outData = Buffer.concat([ outData, d ]); });\n \n-  child.on('close', common.mustCall((code) => {\n-    assert.strictEqual(code, 1);\n+  child.on('close', common.mustCall((code, signal) => {\n+    if ((common.isAIX ||\n+        (common.isLinux && process.arch === 'x64')) &&\n+        signal === 'SIGABRT') {\n+      // XXX: The child process could be aborted due to unknown reasons. Work around it.\n+    } else {\n+      assert.strictEqual(signal, null);\n+      assert.strictEqual(code, 1);\n+    }\n     assert.match(outData.toString(), heartbeatMsg,\n                  'did not crash until we reached offending line of code ' +\n                  `(found ${outData})`);\n"}
{"id": 273, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nTake 3: on win32 we use empty.txt in the fixtures directory, otherwise we use a file constructed specifically for this test due to POSIX socket path length limitations, in which case we need to do appropriate cleanup. Cleaning up empty.txt in fixtures causes downstream problems for other tests using it (currently the next test is test-zlib).\n\n\n### Code Patch:\n@@ -42,19 +42,20 @@ if (process.platform === 'win32') {\n   // use common.PIPE to ensure we stay within POSIX socket path length\n   // restrictions, even on CI\n   emptyTxt = common.PIPE + '.txt';\n-}\n \n-function cleanup() {\n-  try {\n-    fs.unlinkSync(emptyTxt);\n-  } catch (e) {\n-    if (e.code != 'ENOENT')\n-      throw e;\n+  function cleanup() {\n+    try {\n+      fs.unlinkSync(emptyTxt);\n+    } catch (e) {\n+      if (e.code != 'ENOENT')\n+        throw e;\n+    }\n   }\n+  process.on('exit', cleanup);\n+  cleanup();\n+  fs.writeFileSync(emptyTxt, '');\n }\n-process.on('exit', cleanup);\n-cleanup();\n-fs.writeFileSync(emptyTxt, '');\n+\n var notSocketClient = net.createConnection(emptyTxt, function() {\n   assert.ok(false);\n });\n"}
{"id": 313, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis improves the SmokeTestDriverIntegrationTest in three ways:\n\n1) If a SmokeTestClient fails (enters a terminal ERROR state), the\nSmokeTestDriverIntegrationTest currently times out, because it keeps\nwaiting for state NOT_RUNNING. This makes debugging quite difficult.\nThis minor  change makes sure to just fail the test immediately, if a\nSmokeTestClient enters the ERROR state.\n\n2) If a test times out or fails prematurely, because a SmokeTestClient\ncrashed, the SmokeTestClients aren't shut down correctly, which will\naffect the following test runs. Therefore, I am adding clean-up logic\nthat running SmokeTestClients in `@AfterAll`.\n\n3) Finally, I found that the processingThread variation of this thread\ntriggers a subtle race condition. Since this features is currently not\nactively developed, I disabled those variations and created a ticket to\nreactivate the test.\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Chia-Ping Tsai\n <chia7712@gmail.com>, Bill Bejeck <bill@confluent.io>\n\n\n### Code Patch:\n@@ -27,6 +27,7 @@\n import org.apache.kafka.streams.tests.SmokeTestDriver;\n \n import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Tag;\n@@ -46,29 +47,43 @@\n import static org.apache.kafka.streams.tests.SmokeTestDriver.generate;\n import static org.apache.kafka.streams.tests.SmokeTestDriver.verify;\n import static org.apache.kafka.streams.utils.TestUtils.safeUniqueTestName;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n @Timeout(600)\n @Tag(\"integration\")\n public class SmokeTestDriverIntegrationTest {\n-    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(3);\n+    private static EmbeddedKafkaCluster cluster = null;\n     public TestInfo testInfo;\n+    private ArrayList<SmokeTestClient> clients = new ArrayList<>();\n \n     @BeforeAll\n     public static void startCluster() throws IOException {\n-        CLUSTER.start();\n+        cluster = new EmbeddedKafkaCluster(3);\n+        cluster.start();\n     }\n \n     @AfterAll\n     public static void closeCluster() {\n-        CLUSTER.stop();\n+        cluster.stop();\n+        cluster = null;\n     }\n \n     @BeforeEach\n     public void setUp(final TestInfo testInfo) {\n         this.testInfo = testInfo;\n     }\n \n+    @AfterEach\n+    public void shutDown(final TestInfo testInfo) {\n+        // Clean up clients in case the test failed or timed out\n+        for (final SmokeTestClient client : clients) {\n+            if (!client.closed() && !client.error()) {\n+                client.close();\n+            }\n+        }\n+    }\n+\n     private static class Driver extends Thread {\n         private final String bootstrapServers;\n         private final int numKeys;\n@@ -108,12 +123,11 @@ SmokeTestDriver.VerificationResult result() {\n     // During the new stream added and old stream left, the stream process should still complete without issue.\n     // We set 2 timeout condition to fail the test before passing the verification:\n     // (1) 10 min timeout, (2) 30 tries of polling without getting any data\n+    // The processing thread variations where disabled since they triggered a race condition, see KAFKA-19696\n     @ParameterizedTest\n     @CsvSource({\n         \"false, true\",\n-        \"false, false\",\n-        \"true, true\",\n-        \"true, false\"\n+        \"false, false\"\n     })\n     public void shouldWorkWithRebalance(\n         final boolean processingThreadsEnabled,\n@@ -126,11 +140,10 @@ public void shouldWorkWithRebalance(\n             throw new AssertionError(\"Test called halt(). code:\" + statusCode + \" message:\" + message);\n         });\n         int numClientsCreated = 0;\n-        final ArrayList<SmokeTestClient> clients = new ArrayList<>();\n \n-        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, SmokeTestDriver.topics());\n+        IntegrationTestUtils.cleanStateBeforeTest(cluster, SmokeTestDriver.topics());\n \n-        final String bootstrapServers = CLUSTER.bootstrapServers();\n+        final String bootstrapServers = cluster.bootstrapServers();\n         final Driver driver = new Driver(bootstrapServers, 10, 1000);\n         driver.start();\n         System.out.println(\"started driver\");\n@@ -144,8 +157,8 @@ public void shouldWorkWithRebalance(\n         if (streamsProtocolEnabled) {\n             props.put(StreamsConfig.GROUP_PROTOCOL_CONFIG, GroupProtocol.STREAMS.name().toLowerCase(Locale.getDefault()));\n             // decrease the session timeout so that we can trigger the rebalance soon after old client left closed\n-            CLUSTER.setGroupSessionTimeout(appId, 10000);\n-            CLUSTER.setGroupHeartbeatTimeout(appId, 1000);\n+            cluster.setGroupSessionTimeout(appId, 10000);\n+            cluster.setGroupHeartbeatTimeout(appId, 1000);\n         } else {\n             // decrease the session timeout so that we can trigger the rebalance soon after old client left closed\n             props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10000);\n@@ -167,6 +180,7 @@ public void shouldWorkWithRebalance(\n \n                 client.closeAsync();\n                 while (!client.closed()) {\n+                    assertFalse(client.error(), \"The streams application seems to have crashed.\");\n                     Thread.sleep(100);\n                 }\n             }\n@@ -184,6 +198,7 @@ public void shouldWorkWithRebalance(\n             // then, wait for them to stop\n             for (final SmokeTestClient client : clients) {\n                 while (!client.closed()) {\n+                    assertFalse(client.error(), \"The streams application seems to have crashed.\");\n                     Thread.sleep(100);\n                 }\n             }\n\n@@ -54,6 +54,7 @@ public class SmokeTestClient extends SmokeTestUtil {\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n     private volatile boolean closed;\n+    private volatile boolean error;\n \n     private static void addShutdownHook(final String name, final Runnable runnable) {\n         if (name != null) {\n@@ -71,6 +72,10 @@ public boolean closed() {\n         return closed;\n     }\n \n+    public boolean error() {\n+        return error;\n+    }\n+\n     public void start(final Properties streamsProperties) {\n         final Topology build = getTopology();\n         streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n@@ -85,6 +90,10 @@ public void start(final Properties streamsProperties) {\n             if (newState == KafkaStreams.State.NOT_RUNNING) {\n                 closed = true;\n             }\n+\n+            if (newState == KafkaStreams.State.ERROR) {\n+                error = true;\n+            }\n         });\n \n         streams.setUncaughtExceptionHandler(e -> {\n"}
{"id": 122, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Thank you for submitting a Pull Request. Please:\n* Read our Pull Request guidelines:\n  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests\n* Associate an issue with the Pull Request.\n* Ensure that the code is up-to-date with the `main` branch.\n* Include a description of the proposed changes and how to test them.\n-->\n\n\n### Code Patch:\n@@ -52,8 +52,22 @@ export class Extensions extends Viewlet {\n \n \tasync installExtension(id: string, waitUntilEnabled: boolean): Promise<void> {\n \t\tawait this.searchForExtension(id);\n-\t\tawait this.code.waitAndClick(`div.extensions-viewlet[id=\"workbench.view.extensions\"] .monaco-list-row[data-extension-id=\"${id}\"] .extension-list-item .monaco-action-bar .action-item:not(.disabled) .extension-action.install`);\n-\t\tawait this.code.waitForElement(`.extension-editor .monaco-action-bar .action-item:not(.disabled) .extension-action.uninstall`);\n+\n+\t\t// try to install extension 3 times\n+\t\tlet attempt = 1;\n+\t\twhile (true) {\n+\t\t\tawait this.code.waitAndClick(`div.extensions-viewlet[id=\"workbench.view.extensions\"] .monaco-list-row[data-extension-id=\"${id}\"] .extension-list-item .monaco-action-bar .action-item:not(.disabled) .extension-action.install`);\n+\n+\t\t\ttry {\n+\t\t\t\tawait this.code.waitForElement(`.extension-editor .monaco-action-bar .action-item:not(.disabled) .extension-action.uninstall`);\n+\t\t\t\tbreak;\n+\t\t\t} catch (err) {\n+\t\t\t\tif (attempt++ === 3) {\n+\t\t\t\t\tthrow err;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n \t\tif (waitUntilEnabled) {\n \t\t\tawait this.code.waitForElement(`.extension-editor .monaco-action-bar .action-item:not(.disabled) a[aria-label=\"Disable this extension\"]`);\n \t\t}\n"}
{"id": 253, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nticket-23842\n\n### Code Patch:\n@@ -645,18 +645,16 @@ def test_make_line(self):\n         self.assertIsNone(State.objects.aggregate(MakeLine(\"poly\"))[\"poly__makeline\"])\n         # Reference query:\n         # SELECT AsText(ST_MakeLine(geoapp_city.point)) FROM geoapp_city;\n-        ref_line = GEOSGeometry(\n-            \"LINESTRING(-95.363151 29.763374,-96.801611 32.782057,\"\n-            \"-97.521157 34.464642,174.783117 -41.315268,-104.609252 38.255001,\"\n-            \"-95.23506 38.971823,-87.650175 41.850385,-123.305196 48.462611)\",\n-            srid=4326,\n-        )\n-        # We check for equality with a tolerance of 10e-5 which is a lower bound\n-        # of the precisions of ref_line coordinates\n         line = City.objects.aggregate(MakeLine(\"point\"))[\"point__makeline\"]\n-        self.assertTrue(\n-            ref_line.equals_exact(line, tolerance=10e-5), \"%s != %s\" % (ref_line, line)\n-        )\n+        ref_points = City.objects.values_list(\"point\", flat=True)\n+        self.assertIsInstance(line, LineString)\n+        self.assertEqual(len(line), ref_points.count())\n+        # Compare pairs of manually sorted points, as the default ordering is\n+        # flaky.\n+        for (point, ref_city) in zip(sorted(line), sorted(ref_points)):\n+            point_x, point_y = point\n+            self.assertAlmostEqual(point_x, ref_city.x, 5),\n+            self.assertAlmostEqual(point_y, ref_city.y, 5),\n \n     @skipUnlessDBFeature(\"supports_union_aggr\")\n     def test_unionagg(self):\n"}
{"id": 247, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Description of PR\r\ntestDecommissionStatus keeps failing intermittently.\r\n```\r\n[ERROR] testDecommissionStatus(org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor)  Time elapsed: 3.299 s  <<< FAILURE!\r\njava.lang.AssertionError: Unexpected num under-replicated blocks expected:<4> but was:<3>\r\n\tat org.junit.Assert.fail(Assert.java:89)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:835)\r\n\tat org.junit.Assert.assertEquals(Assert.java:647)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.checkDecommissionStatus(TestDecommissioningStatus.java:169)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor.testDecommissionStatus(TestDecommissioningStatusWithBackoffMonitor.java:136)\r\n```\r\n\r\n### How was this patch tested?\r\nLocal run of unit test\r\n\r\n### For code changes:\r\n\r\n- [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n\n\n### Code Patch:\n@@ -398,6 +398,28 @@ public static void assertExceptionContains(String expectedText,\n   public static void waitFor(final Supplier<Boolean> check,\n       final long checkEveryMillis, final long waitForMillis)\n       throws TimeoutException, InterruptedException {\n+    waitFor(check, checkEveryMillis, waitForMillis, null);\n+  }\n+\n+  /**\n+   * Wait for the specified test to return true. The test will be performed\n+   * initially and then every {@code checkEveryMillis} until at least\n+   * {@code waitForMillis} time has expired. If {@code check} is null or\n+   * {@code waitForMillis} is less than {@code checkEveryMillis} this method\n+   * will throw an {@link IllegalArgumentException}.\n+   *\n+   * @param check the test to perform.\n+   * @param checkEveryMillis how often to perform the test.\n+   * @param waitForMillis the amount of time after which no more tests will be\n+   * performed.\n+   * @param errorMsg error message to provide in TimeoutException.\n+   * @throws TimeoutException if the test does not return true in the allotted\n+   * time.\n+   * @throws InterruptedException if the method is interrupted while waiting.\n+   */\n+  public static void waitFor(final Supplier<Boolean> check,\n+      final long checkEveryMillis, final long waitForMillis,\n+      final String errorMsg) throws TimeoutException, InterruptedException {\n     Objects.requireNonNull(check, ERROR_MISSING_ARGUMENT);\n     if (waitForMillis < checkEveryMillis) {\n       throw new IllegalArgumentException(ERROR_INVALID_ARGUMENT);\n@@ -412,9 +434,12 @@ public static void waitFor(final Supplier<Boolean> check,\n     }\n \n     if (!result) {\n-      throw new TimeoutException(\"Timed out waiting for condition. \" +\n-          \"Thread diagnostics:\\n\" +\n-          TimedOutTestsListener.buildThreadDiagnosticString());\n+      final String exceptionErrorMsg = \"Timed out waiting for condition. \"\n+          + (org.apache.commons.lang3.StringUtils.isNotEmpty(errorMsg)\n+          ? \"Error Message: \" + errorMsg : \"\")\n+          + \"\\nThread diagnostics:\\n\" +\n+          TimedOutTestsListener.buildThreadDiagnosticString();\n+      throw new TimeoutException(exceptionErrorMsg);\n     }\n   }\n \n\n@@ -27,7 +27,10 @@\n import java.util.Arrays;\n import java.util.List;\n \n+import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n import org.apache.commons.io.output.ByteArrayOutputStream;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.BlockLocation;\n@@ -57,11 +60,12 @@\n import org.apache.hadoop.hdfs.tools.DFSAdmin;\n import org.apache.hadoop.hdfs.util.HostsFileWriter;\n import org.apache.hadoop.test.GenericTestUtils;\n-import org.apache.log4j.Level;\n-import org.apache.log4j.Logger;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.slf4j.event.Level;\n \n /**\n  * This class tests the decommissioning of nodes.\n@@ -75,7 +79,8 @@ public class TestDecommissioningStatus {\n   private static FileSystem fileSys;\n   private static HostsFileWriter hostsFileWriter;\n   private static Configuration conf;\n-  private Logger LOG;\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestDecommissioningStatus.class);\n \n   final ArrayList<String> decommissionedNodes = new ArrayList<String>(numDatanodes);\n   \n@@ -102,8 +107,8 @@ public void setUp() throws Exception {\n     fileSys = cluster.getFileSystem();\n     cluster.getNamesystem().getBlockManager().getDatanodeManager()\n         .setHeartbeatExpireInterval(3000);\n-    Logger.getLogger(DatanodeAdminManager.class).setLevel(Level.DEBUG);\n-    LOG = Logger.getLogger(TestDecommissioningStatus.class);\n+    GenericTestUtils.setLogLevel(\n+        LoggerFactory.getLogger(DatanodeAdminManager.class), Level.DEBUG);\n   }\n \n   @After\n@@ -142,17 +147,30 @@ private void decommissionNode(String dnName)\n \n   private void checkDecommissionStatus(DatanodeDescriptor decommNode,\n       int expectedUnderRep, int expectedDecommissionOnly,\n-      int expectedUnderRepInOpenFiles) {\n-    assertEquals(\"Unexpected num under-replicated blocks\",\n-        expectedUnderRep,\n-        decommNode.getLeavingServiceStatus().getUnderReplicatedBlocks());\n-    assertEquals(\"Unexpected number of decom-only replicas\",\n-        expectedDecommissionOnly,\n-        decommNode.getLeavingServiceStatus().getOutOfServiceOnlyReplicas());\n-    assertEquals(\n-        \"Unexpected number of replicas in under-replicated open files\",\n-        expectedUnderRepInOpenFiles,\n-        decommNode.getLeavingServiceStatus().getUnderReplicatedInOpenFiles());\n+      int expectedUnderRepInOpenFiles) throws TimeoutException,\n+      InterruptedException {\n+    String errorMsg;\n+    errorMsg = \"Under replicated blocks. Expected: \"\n+        + expectedUnderRep + \" , Actual: \"\n+        + decommNode.getLeavingServiceStatus().getUnderReplicatedBlocks();\n+    GenericTestUtils.waitFor(\n+        () -> expectedUnderRep == decommNode.getLeavingServiceStatus()\n+            .getUnderReplicatedBlocks(),\n+        1000, TimeUnit.SECONDS.toMillis(10), errorMsg);\n+    errorMsg = \"OutOfService only replicas. Expected: \"\n+        + expectedDecommissionOnly + \" , Actual: \"\n+        + decommNode.getLeavingServiceStatus().getOutOfServiceOnlyReplicas();\n+    GenericTestUtils.waitFor(\n+        () -> expectedDecommissionOnly == decommNode.getLeavingServiceStatus()\n+            .getOutOfServiceOnlyReplicas(),\n+        1000, TimeUnit.SECONDS.toMillis(10), errorMsg);\n+    errorMsg = \"UnderReplicated in open files. Expected: \"\n+        + expectedUnderRepInOpenFiles + \" , Actual: \"\n+        + decommNode.getLeavingServiceStatus().getUnderReplicatedInOpenFiles();\n+    GenericTestUtils.waitFor(\n+        () -> expectedUnderRepInOpenFiles == decommNode\n+            .getLeavingServiceStatus().getUnderReplicatedInOpenFiles(),\n+        1000, TimeUnit.SECONDS.toMillis(10), errorMsg);\n   }\n \n   private void checkDFSAdminDecommissionStatus(\n@@ -247,6 +265,7 @@ public void testDecommissionStatus() throws Exception {\n \n     FSNamesystem fsn = cluster.getNamesystem();\n     final DatanodeManager dm = fsn.getBlockManager().getDatanodeManager();\n+    verifyInitialState(fsn, dm);\n     for (int iteration = 0; iteration < numDatanodes; iteration++) {\n       String downnode = decommissionNode(client, iteration);\n       dm.refreshNodes(conf);\n@@ -255,14 +274,13 @@ public void testDecommissionStatus() throws Exception {\n       // Block until the admin's monitor updates the number of tracked nodes.\n       waitForDecommissionedNodes(dm.getDatanodeAdminManager(), iteration + 1);\n       final List<DatanodeDescriptor> decommissioningNodes = dm.getDecommissioningNodes();\n+      assertEquals(decommissioningNodes.size(), iteration + 1);\n       if (iteration == 0) {\n-        assertEquals(decommissioningNodes.size(), 1);\n         DatanodeDescriptor decommNode = decommissioningNodes.get(0);\n         checkDecommissionStatus(decommNode, 3, 0, 1);\n         checkDFSAdminDecommissionStatus(decommissioningNodes.subList(0, 1),\n             fileSys, admin);\n       } else {\n-        assertEquals(decommissioningNodes.size(), 2);\n         DatanodeDescriptor decommNode1 = decommissioningNodes.get(0);\n         DatanodeDescriptor decommNode2 = decommissioningNodes.get(1);\n         // This one is still 3,3,1 since it passed over the UC block \n@@ -284,6 +302,69 @@ public void testDecommissionStatus() throws Exception {\n     AdminStatesBaseTest.cleanupFile(fileSys, file2);\n   }\n \n+  // Why do we verify initial state of DataNodes here?\n+  // Before we start actual decommission testing, we should ensure that\n+  // total 8 blocks (original 4 blocks of 2 files and 4 replicas) are\n+  // present over two Datanodes available. If we don't wait until all 8 blocks\n+  // are reported live by BlockManager, we might get to a situation\n+  // where one of the replicas might not yet been present on any of Datanodes\n+  // and we start decommissioning process, and then it would result in\n+  // flaky test because total (no of under replicated blocks, no of outOfService\n+  // only replicas, no of under replicated in open files) counts would be\n+  // incorrect.\n+  protected void verifyInitialState(FSNamesystem fsn, DatanodeManager dm)\n+      throws InterruptedException {\n+    dm.getDatanodes().forEach(datanodeDescriptor -> {\n+      try {\n+        checkDecommissionStatus(datanodeDescriptor, 0, 0, 0);\n+      } catch (TimeoutException | InterruptedException e) {\n+        throw new AssertionError(\"Datanode not in good state.\", e);\n+      }\n+    });\n+    int c = 0;\n+    int totalBlocks;\n+    long totalReplicatedBlocks;\n+    while (true) {\n+      totalBlocks = fsn.getBlockManager().getTotalBlocks();\n+      totalReplicatedBlocks = fsn.getBlockManager().getTotalReplicatedBlocks();\n+      if (totalBlocks == 4 && totalReplicatedBlocks == 4) {\n+        break;\n+      } else {\n+        if (c == 4) {\n+          throw new AssertionError(\"Unexpected Total blocks \" + totalBlocks\n+              + \" and replicated blocks \" + totalReplicatedBlocks);\n+        }\n+        Thread.sleep(3000);\n+      }\n+      c++;\n+    }\n+    c = 0;\n+    AtomicInteger total = new AtomicInteger(0);\n+    AtomicInteger sufficientBlocksSuccess = new AtomicInteger(0);\n+    while (true) {\n+      total.set(0);\n+      sufficientBlocksSuccess.set(0);\n+      dm.getDatanodes().forEach(\n+          datanodeDescriptor -> {\n+            total.addAndGet(datanodeDescriptor.numBlocks());\n+            if (datanodeDescriptor.numBlocks() == 4) {\n+              sufficientBlocksSuccess.incrementAndGet();\n+            }\n+          });\n+      if (total.get() == 8 && sufficientBlocksSuccess.get() == 2) {\n+        break;\n+      } else {\n+        if (c == 4) {\n+          throw new AssertionError(\"Unexpected Total blocks \" + total.get()\n+              + \" from Datanode Storage. 4 blocks per Datanode Storage\"\n+              + \" expected from each DataNode\");\n+        }\n+        Thread.sleep(3000);\n+      }\n+      c++;\n+    }\n+  }\n+\n   /**\n    * Verify a DN remains in DECOMMISSION_INPROGRESS state if it is marked\n    * as dead before decommission has completed. That will allow DN to resume\n@@ -367,8 +448,8 @@ public void testDecommissionStatusAfterDNRestart() throws Exception {\n    */\n   @Test(timeout=120000)\n   public void testDecommissionDeadDN() throws Exception {\n-    Logger log = Logger.getLogger(DatanodeAdminManager.class);\n-    log.setLevel(Level.DEBUG);\n+    Logger log = LoggerFactory.getLogger(DatanodeAdminManager.class);\n+    GenericTestUtils.setLogLevel(log, Level.DEBUG);\n     DatanodeID dnID = cluster.getDataNodes().get(0).getDatanodeId();\n     String dnName = dnID.getXferAddr();\n     DataNodeProperties stoppedDN = cluster.stopDataNode(0);\n"}
{"id": 286, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #10853\r\n\r\nI think the issue is that we do DNS client request to the DNS caching server, and expect the request to fail on client side, as caching server doesn't respond in time (as the server is waiting for a response).\r\n\r\nMake sure we use a short timeout on our client side, so that we time out before a server does.\r\n\n\n### Code Patch:\n@@ -90,7 +90,11 @@ func TestDNS(t *testing.T) {\n \n \t\t\t\ttime.Sleep(10 * time.Millisecond)\n \n-\t\t\t\tr, err := dnssrv.Exchange(createQuery(test.hostname), dnsAddr)\n+\t\t\t\tc := dnssrv.Client{\n+\t\t\t\t\tTimeout: time.Second,\n+\t\t\t\t}\n+\n+\t\t\t\tr, _, err := c.Exchange(createQuery(test.hostname), dnsAddr)\n \t\t\t\ttest.errCheck(t, err)\n \n \t\t\t\tif r != nil {\n"}
{"id": 389, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis fixes a failure that I have with #16965.\n\n### Code Patch:\n@@ -494,7 +494,8 @@ def test_select_multiple(self):\n             + reverse(\"autocomplete_admin:admin_views_question_add\")\n         )\n         elem = self.selenium.find_element(By.CSS_SELECTOR, \".select2-selection\")\n-        elem.click()  # Open the autocomplete dropdown.\n+        with self.select2_ajax_wait():\n+            elem.click()  # Open the autocomplete dropdown.\n         results = self.selenium.find_element(By.CSS_SELECTOR, \".select2-results\")\n         self.assertTrue(results.is_displayed())\n         option = self.selenium.find_element(By.CSS_SELECTOR, \".select2-results__option\")\n"}
{"id": 123, "ground_truth": "I/O", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n 1) VSCode Smoke Tests (Electron)\r\n       Terminal\r\n         Terminal Persistence\r\n           detach/attach\r\n             should persist buffer content:\r\n     Error: Timeout: get element '.quick-input-widget' after 20 seconds.\r\n      at Code.poll (/Users/runner/work/1/s/test/automation/out/code.js:204:23)\r\n      at async Code.waitForElement (/Users/runner/work/1/s/test/automation/out/code.js:163:16)\r\n      at async QuickInput.waitForQuickInputClosed (/Users/runner/work/1/s/test/automation/out/quickinput.js:32:9)\r\n      at async Terminal.runCommandWithValue (/Users/runner/work/1/s/test/automation/out/terminal.js:109:9)\r\n      at async Context.<anonymous> (out/areas/terminal/terminal-persistence.test.js:64:17)\r\n\r\nhttps://dev.azure.com/monacotools/Monaco/_build/results?buildId=278997&view=logs&j=260a512b-e5ca-5cf1-3f8e-20c0fa41efc9&t=dc7a7a05-ae89-521b-55db-0cb2ded34668\n\n### Code Patch:\n@@ -75,7 +75,8 @@ export function setup(options?: { skipSuite: boolean }) {\n \t\t\t\tawait terminal.assertTerminalGroups([\n \t\t\t\t\t[{ name }]\n \t\t\t\t]);\n-\t\t\t\tawait terminal.waitForTerminalText(buffer => buffer.some(e => e.includes('terminal_test_content')));\n+\t\t\t\t// There can be line wrapping, so remove newlines #216464\n+\t\t\t\tawait terminal.waitForTerminalText(buffer => buffer.some(e => e.replaceAll('\\n', '').includes('terminal_test_content')));\n \t\t\t});\n \t\t});\n \t});\n"}
{"id": 110, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nI've seen this fail a couple times\n\n```\n=== Failed\n=== FAIL: codegen/pcl TestRangeTraversalFromObjectOfObjectsDoesNotError (0.43s)\n    binder_test.go:1110: \n        \tError Trace:\t/home/runner/work/pulumi/pulumi/pkg/codegen/pcl/binder_test.go:1110\n        \tError:      \t\"unknown property 'aws_region' among [something awsRegion]\" does not contain \"unknown property 'aws_region' among [awsRegion something]\"\n        \tTest:       \tTestRangeTraversalFromObjectOfObjectsDoesNotError\n        \tMessages:   \tThe diagnostic contains the correct message about the unknown property\n```\n\nhttps://github.com/pulumi/pulumi/actions/runs/17270052509/job/49012394297?pr=20403\n\n### Code Patch:\n@@ -16,6 +16,7 @@ package model\n \n import (\n \t\"fmt\"\n+\t\"slices\"\n \n \t\"github.com/hashicorp/hcl/v2\"\n \t\"github.com/zclconf/go-cty/cty\"\n@@ -104,6 +105,7 @@ func tupleIndexOutOfRange(tupleLen int, indexRange hcl.Range) *hcl.Diagnostic {\n }\n \n func unknownObjectProperty(name string, indexRange hcl.Range, props []string) *hcl.Diagnostic {\n+\tslices.Sort(props)\n \treturn errorf(indexRange, \"unknown property '%s' among %v\", name, props)\n }\n \n"}
{"id": 256, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n#### What type of PR is this?\r\n\r\n/kind flake\r\n\r\n#### What this PR does / why we need it:\r\n\r\nThis PR fixes issue #133763 where the ResourceSlice Controller test in the DRA (Dynamic Resource Allocation) e2e tests was experiencing flakiness due to insufficient timeouts.\r\n\r\n**Root Cause**: The test was failing intermittently because:\r\n- ResourceSlice operations were taking longer than expected\r\n- Default timeouts were too short for the test environment\r\n- Network latency and resource allocation delays caused test failures\r\n\r\n**Solution**: Increased timeouts for ResourceSlice Controller test to:\r\n- Provide sufficient time for resource operations to complete\r\n- Reduce test flakiness in various network conditions\r\n- Improve test reliability and stability\r\n\r\n**Impact**: More reliable DRA e2e tests, reducing false failures and improving CI stability.\r\n\r\n#### Which issue(s) this PR is related to:\r\n\r\nFixes #133763\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThis fix addresses test reliability without changing test logic:\r\n\r\n- Increased timeouts to accommodate network and resource allocation delays\r\n- No changes to test behavior or assertions\r\n- Improves CI stability by reducing flaky test failures\r\n- Minimal change that only affects test timing\r\n\r\nThe fix is focused on improving test reliability rather than changing functionality.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\nNONE\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\nN/A\n\n### Code Patch:\n@@ -2143,7 +2143,7 @@ var _ = framework.SIGDescribe(\"node\")(framework.WithLabel(\"DRA\"), func() {\n \t\t\t})\n \n \t\t\tginkgo.By(\"Creating slices\")\n-\t\t\tmutationCacheTTL := 10 * time.Second\n+\t\t\tmutationCacheTTL := 15 * time.Second\n \t\t\tcontroller, err := resourceslice.StartController(ctx, resourceslice.Options{\n \t\t\t\tDriverName:       driverName,\n \t\t\t\tKubeClient:       f.ClientSet,\n@@ -2165,7 +2165,7 @@ var _ = framework.SIGDescribe(\"node\")(framework.WithLabel(\"DRA\"), func() {\n \t\t\t})\n \n \t\t\t// Eventually we should have all desired slices.\n-\t\t\tgomega.Eventually(ctx, listSlices).WithTimeout(3 * time.Minute).Should(gomega.HaveField(\"Items\", gomega.HaveLen(numSlices)))\n+\t\t\tgomega.Eventually(ctx, listSlices).WithTimeout(5 * time.Minute).Should(gomega.HaveField(\"Items\", gomega.HaveLen(numSlices)))\n \n \t\t\t// Verify state.\n \t\t\texpectSlices, err := listSlices(ctx)\n@@ -2188,15 +2188,15 @@ var _ = framework.SIGDescribe(\"node\")(framework.WithLabel(\"DRA\"), func() {\n \n \t\t\t// One empty slice should remain, after removing the full ones and adding the empty one.\n \t\t\temptySlice := gomega.HaveField(\"Spec.Devices\", gomega.BeEmpty())\n-\t\t\tgomega.Eventually(ctx, listSlices).WithTimeout(2 * time.Minute).Should(gomega.HaveField(\"Items\", gomega.HaveExactElements(emptySlice)))\n+\t\t\tgomega.Eventually(ctx, listSlices).WithTimeout(3 * time.Minute).Should(gomega.HaveField(\"Items\", gomega.HaveExactElements(emptySlice)))\n \t\t\texpectStats = resourceslice.Stats{NumCreates: int64(numSlices) + 1, NumDeletes: int64(numSlices)}\n \n \t\t\t// There is a window of time where the ResourceSlice exists and is\n \t\t\t// returned in a list but before that ResourceSlice is accounted for\n \t\t\t// in the controller's stats, consisting mostly of network latency\n \t\t\t// between this test process and the API server. Wait for the stats\n \t\t\t// to converge before asserting there are no further changes.\n-\t\t\tgomega.Eventually(ctx, controller.GetStats).WithTimeout(30 * time.Second).Should(gomega.Equal(expectStats))\n+\t\t\tgomega.Eventually(ctx, controller.GetStats).WithTimeout(45 * time.Second).Should(gomega.Equal(expectStats))\n \n \t\t\tgomega.Consistently(ctx, controller.GetStats).WithTimeout(2 * mutationCacheTTL).Should(gomega.Equal(expectStats))\n \t\t})\n"}
{"id": 140, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe test testShareGroupHeartbeatInitializeOnPartitionUpdate was flaky\nearlier. The shareGroupStatePartitionMetadataRecord that is created\nduring heartbeat contains 2 topics to be initialized,  but the order in\nwhich they appear in the list is not deterministic. The test is changed\nto simply see whether the contents of the record is correct instead of\ndirectly comparing it with an expected record which may contains the\ncorrect topics, but in some different order.\n\n\n### Code Patch:\n@@ -21260,16 +21260,21 @@ public void testShareGroupHeartbeatInitializeOnPartitionUpdate() {\n                 .setMemberEpoch(0)\n                 .setSubscribedTopicNames(List.of(t1Name, t2Name)));\n \n-        assertTrue(result.records().contains(\n-            newShareGroupStatePartitionMetadataRecord(groupId, mkShareGroupStateMap(List.of(\n-                    mkShareGroupStateMetadataEntry(t1Uuid, t1Name, List.of(0, 1)),\n-                    mkShareGroupStateMetadataEntry(t2Uuid, t2Name, List.of(0, 1))\n-                )),\n-                Map.of(),\n-                Map.of()\n-            ))\n+        CoordinatorRecord expected = newShareGroupStatePartitionMetadataRecord(\n+            groupId,\n+            mkShareGroupStateMap(List.of(\n+                mkShareGroupStateMetadataEntry(t1Uuid, t1Name, List.of(0, 1)),\n+                mkShareGroupStateMetadataEntry(t2Uuid, t2Name, List.of(0, 1))\n+            )),\n+            Map.of(),\n+            Map.of()\n         );\n \n+        Optional<CoordinatorRecord> actual = result.records().stream().filter(record -> record.key() instanceof ShareGroupStatePartitionMetadataKey)\n+            .findAny();\n+        assertTrue(actual.isPresent());\n+        assertRecordEquals(expected, actual.get());\n+\n         verifyShareGroupHeartbeatInitializeRequest(\n             result.response().getValue(),\n             Map.of(\n@@ -21330,18 +21335,23 @@ public void testShareGroupHeartbeatInitializeOnPartitionUpdate() {\n                 .setMemberEpoch(1)\n                 .setSubscribedTopicNames(null));\n \n-        assertTrue(result.records().contains(\n-            newShareGroupStatePartitionMetadataRecord(groupId, mkShareGroupStateMap(List.of(\n-                    mkShareGroupStateMetadataEntry(t1Uuid, t1Name, List.of(2, 3))\n-                )),\n-                mkShareGroupStateMap(List.of(\n-                    mkShareGroupStateMetadataEntry(t1Uuid, t1Name, List.of(0, 1)),\n-                    mkShareGroupStateMetadataEntry(t2Uuid, t2Name, List.of(0, 1))\n-                )),\n-                Map.of()\n-            ))\n+        expected = newShareGroupStatePartitionMetadataRecord(\n+            groupId,\n+            mkShareGroupStateMap(List.of(\n+                mkShareGroupStateMetadataEntry(t1Uuid, t1Name, List.of(2, 3))\n+            )),\n+            mkShareGroupStateMap(List.of(\n+                mkShareGroupStateMetadataEntry(t1Uuid, t1Name, List.of(0, 1)),\n+                mkShareGroupStateMetadataEntry(t2Uuid, t2Name, List.of(0, 1))\n+            )),\n+            Map.of()\n         );\n \n+        actual = result.records().stream().filter(record -> record.key() instanceof ShareGroupStatePartitionMetadataKey)\n+            .findAny();\n+        assertTrue(actual.isPresent());\n+        assertRecordEquals(expected, actual.get());\n+\n         verifyShareGroupHeartbeatInitializeRequest(\n             result.response().getValue(),\n             Map.of(\n"}
{"id": 121, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n<!-- ‚ö†Ô∏è‚ö†Ô∏è Do Not Delete This! bug_report_template ‚ö†Ô∏è‚ö†Ô∏è -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- üïÆ Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- üîé Search existing issues to avoid creating duplicates. -->\n<!-- üß™ Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- üí° Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- üîß Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- ü™ì If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- üì£ Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: \n- OS Version: \n\nMy PR is failing to be merged due to windows unit tests failing surrounding composite tokens\n\nhttps://dev.azure.com/vscode/VSCode/_build/results?buildId=152385&view=logs&j=e4714b0f-e6ff-5e04-bac1-2dcbc509f151&t=5d351641-7442-59b2-8726-6d905a792029\n\n```\n  4442 passing (37s)\n  96 pending\n  1 failing\n\n  1) Composite***\n       ‚Ä¢ equals\n         ‚Ä¢ true\n           ‚Ä¢ composite tokens:\n     Error: Method not implemented.\n      at Test***.toString (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:20:13)\n      at Base***.fullRange (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/base***.js:87:54)\n      at new Composite*** (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/composite***.js:4:21)\n      at new Test*** (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:17:7)\n      at Context.<anonymous> (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:126:24)\n      at process.processImmediate (node:internal/timers:483:21)\n\n```\n\n### Code Patch:\n@@ -228,12 +228,12 @@ suite('CompositeToken', () => {\n \t\t\t\t// ensure there is at least one composite token\n \t\t\t\tconst lastToken = tokens[tokens.length - 1];\n \t\t\t\tconst compositeToken1 = new TestToken(randomTokens(\n-\t\t\t\t\trandomInt(5, 2),\n+\t\t\t\t\trandomInt(3, 1),\n \t\t\t\t\tlastToken.range.endLineNumber,\n \t\t\t\t\tlastToken.range.endColumn,\n \t\t\t\t));\n \t\t\t\tconst compositeToken2 = new TestToken(randomTokens(\n-\t\t\t\t\trandomInt(5, 2),\n+\t\t\t\t\trandomInt(6, 4),\n \t\t\t\t\tlastToken.range.endLineNumber,\n \t\t\t\t\tlastToken.range.endColumn,\n \t\t\t\t));\n"}
{"id": 134, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\n\r\nSimplify org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.\"reattach after connection expired\" to make it more deterministic.\r\n\r\n### Why are the changes needed?\r\n\r\nThe test previously involved execution and interruption that made the test unnecessarily flaky, e.g., an exception was thrown when releasing the corresponding [execution](https://github.com/apache/spark/actions/runs/12296721038/job/34316344940), not when reattaching the execution.\r\n- The test's sole purpose is to check whether the lack of 'session' results in the correct error code.\r\n- The involvement of actual query execution only makes the test flaky and complicated.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nRepeatedly ran testOnly org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\n\r\nNo.\r\n\n\n### Code Patch:\n@@ -58,18 +58,15 @@ class ReattachableExecuteSuite extends SparkConnectServerTest {\n \n   test(\"reattach after connection expired\") {\n     withClient { client =>\n-      val iter = client.execute(buildPlan(MEDIUM_RESULTS_QUERY))\n-      val operationId = getReattachableIterator(iter).operationId\n-      // open the iterator\n-      iter.next()\n-\n-      SparkConnectService.invalidateSession(defaultUserId, defaultSessionId)\n       withRawBlockingStub { stub =>\n-        val iter2 = stub.reattachExecute(buildReattachExecuteRequest(operationId, None))\n+        // emulate session expiration\n+        SparkConnectService.invalidateSession(defaultUserId, defaultSessionId)\n \n-        // session closed, bound to fail\n+        // session closed, bound to fail immediately\n+        val operationId = UUID.randomUUID().toString\n+        val iter = stub.reattachExecute(buildReattachExecuteRequest(operationId, None))\n         val e = intercept[StatusRuntimeException] {\n-          while (iter2.hasNext) iter2.next()\n+          iter.next()\n         }\n         assert(e.getMessage.contains(\"INVALID_HANDLE.SESSION_NOT_FOUND\"))\n       }\n"}
{"id": 388, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nMy theory is that since 0ebea6e5c07485a36862e9b6e2be18d1694ad2c5, the saving of objects has become slightly slower\r\nThis caused some failures in the selenium test suite: https://github.com/django/django/actions/runs/13826117388/job/38698352291\r\n\n\n### Code Patch:\n@@ -123,13 +123,21 @@ def wait_page_loaded(self, timeout=10):\n         \"\"\"\n         Block until a new page has loaded and is ready.\n         \"\"\"\n+        from selenium.common.exceptions import WebDriverException\n         from selenium.webdriver.common.by import By\n         from selenium.webdriver.support import expected_conditions as ec\n \n         old_page = self.selenium.find_element(By.TAG_NAME, \"html\")\n         yield\n         # Wait for the next page to be loaded\n-        self.wait_until(ec.staleness_of(old_page), timeout=timeout)\n+        try:\n+            self.wait_until(ec.staleness_of(old_page), timeout=timeout)\n+        except WebDriverException:\n+            # Issue in version 113+ of Chrome driver where a WebDriverException\n+            # error is raised rather than a StaleElementReferenceException, see:\n+            # https://issues.chromium.org/issues/42323468\n+            pass\n+\n         self.wait_page_ready(timeout=timeout)\n \n     def admin_login(self, username, password, login_url=\"/admin/\"):\n\n@@ -6448,6 +6448,7 @@ def test_inline_uuid_pk_delete_with_popup(self):\n         ActionChains(self.selenium).move_to_element(delete_parent).click().perform()\n         self.wait_for_and_switch_to_popup()\n         self.selenium.find_element(By.XPATH, '//input[@value=\"Yes, I‚Äôm sure\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 1, 1)\n         self.selenium.switch_to.window(self.selenium.window_handles[0])\n         select = Select(self.selenium.find_element(By.ID, \"id_parent\"))\n         self.assertEqual(ParentWithUUIDPK.objects.count(), 0)\n@@ -6582,6 +6583,7 @@ def test_related_popup_index(self):\n \n         self.selenium.find_element(By.ID, \"id_title\").send_keys(\"test3\")\n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 3, 1)\n         self.selenium.switch_to.window(popup_window_test2)\n         select = Select(self.selenium.find_element(By.ID, \"id_next_box\"))\n         next_box_id = str(Box.objects.get(title=\"test3\").id)\n@@ -6590,6 +6592,7 @@ def test_related_popup_index(self):\n         )\n \n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 2, 1)\n         self.selenium.switch_to.window(popup_window_test)\n         select = Select(self.selenium.find_element(By.ID, \"id_next_box\"))\n         next_box_id = str(Box.objects.get(title=\"test2\").id)\n@@ -6598,6 +6601,7 @@ def test_related_popup_index(self):\n         )\n \n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 1, 1)\n         self.selenium.switch_to.window(base_window)\n         select = Select(self.selenium.find_element(By.ID, \"id_next_box\"))\n         next_box_id = str(Box.objects.get(title=\"test\").id)\n@@ -6833,15 +6837,17 @@ def test_redirect_on_add_view_add_another_button(self):\n         self.selenium.get(self.live_server_url + add_url)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input.send_keys(\"Test section 1\")\n-        self.selenium.find_element(\n-            By.XPATH, '//input[@value=\"Save and add another\"]'\n-        ).click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(\n+                By.XPATH, '//input[@value=\"Save and add another\"]'\n+            ).click()\n         self.assertEqual(Section.objects.count(), 1)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input.send_keys(\"Test section 2\")\n-        self.selenium.find_element(\n-            By.XPATH, '//input[@value=\"Save and add another\"]'\n-        ).click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(\n+                By.XPATH, '//input[@value=\"Save and add another\"]'\n+            ).click()\n         self.assertEqual(Section.objects.count(), 2)\n \n     def test_redirect_on_add_view_continue_button(self):\n@@ -6854,9 +6860,10 @@ def test_redirect_on_add_view_continue_button(self):\n         self.selenium.get(self.live_server_url + add_url)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input.send_keys(\"Test section 1\")\n-        self.selenium.find_element(\n-            By.XPATH, '//input[@value=\"Save and continue editing\"]'\n-        ).click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(\n+                By.XPATH, '//input[@value=\"Save and continue editing\"]'\n+            ).click()\n         self.assertEqual(Section.objects.count(), 1)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input_value = name_input.get_attribute(\"value\")\n\n@@ -19,6 +19,7 @@ def test_textarea_trailing_newlines(self):\n         self.selenium.get(\n             self.live_server_url + reverse(\"article_form\", args=[article.pk])\n         )\n-        self.selenium.find_element(By.ID, \"submit\").click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(By.ID, \"submit\").click()\n         article = Article.objects.get(pk=article.pk)\n         self.assertEqual(article.content, \"\\r\\nTst\\r\\n\")\n"}
{"id": 139, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[Jira: KAFKA-18537](https://issues.apache.org/jira/browse/KAFKA-18537)\nAdd a wait for cleaner thread shutdown in `testCleanerThreadShutdown` to\neliminate flakiness. After calling `cache.close()`, the test now uses\n`TestUtils.waitForCondition` to poll until the background\n‚Äúremote-log-index-cleaner‚Äù thread has fully exited before asserting that\nno cleaner threads remain. This ensures the asynchronous shutdown always\ncompletes before the final assertions.\n\nReviewers: TengYao Chi <kitingiao@gmail.com>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n\n### Code Patch:\n@@ -376,8 +376,10 @@ public void testCleanerThreadShutdown() throws IOException, InterruptedException\n         // close the cache properly\n         cache.close();\n         // verify that the thread is closed properly\n-        threads = getRunningCleanerThread();\n-        assertTrue(threads.isEmpty(), \"Found unexpected \" + threads.size() + \" threads=\" + threads.stream().map(Thread::getName).collect(Collectors.joining(\", \")));\n+        TestUtils.waitForCondition(\n+                () -> getRunningCleanerThread().isEmpty(),\n+                () -> \"Failed while waiting for cleaner threads to shutdown. Remaining threads: \" +\n+                        getRunningCleanerThread().stream().map(Thread::getName).collect(Collectors.joining(\", \")));\n         // if the thread is correctly being shutdown it will not be running\n         assertFalse(cache.cleanerScheduler().isStarted(), \"Unexpected thread state=running. Check error logs.\");\n     }\n"}
{"id": 363, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n/kind flake\r\n\r\n#### What this PR does / why we need it:\r\n\r\n**Kubelet logs:**\r\n\r\n```\r\n// The container probe is failed\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.197112     285 prober.go:153] \"Exec-Probe runProbe\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" containerName=\"test-container-subpath-configmap-h8zg\" execCommand=[\"sh\",\"-c\",\"cat /probe-volume/probe-file || test `cat /test-volume` = 'configmap-modified-value'\"]\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.905819     285 prober.go:120] \"Probe failed\" probeType=\"Liveness\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" containerName=\"test-container-subpath-configmap-h8zg\" probeResult=\"failure\" output=<\r\n\r\n// kubelet tried to stop the container via calling container runtime\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.910529     285 kuberuntime_manager.go:1148] \"computePodActions got for pod\" podActions=\"KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [0], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[{containerd 89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1}:{0xc000b59c08 test-container-subpath-configmap-h8zg Container test-container-subpath-configmap-h8zg failed liveness probe, will be restarted LivenessProbe}]\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.910732     285 kuberuntime_manager.go:1182] \"Killing unwanted container for pod\" containerName=\"test-container-subpath-configmap-h8zg\" containerID={\"Type\":\"containerd\",\"ID\":\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\"} pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.910919     285 kuberuntime_container.go:809] \"Killing container with a grace period\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" containerName=\"test-container-subpath-configmap-h8zg\" containerID=\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\" gracePeriod=2\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.912334     285 event.go:389] \"Event occurred\" object=\"subpath-2059/pod-subpath-test-configmap-h8zg\" fieldPath=\"spec.containers{test-container-subpath-configmap-h8zg}\" kind=\"Pod\" apiVersion=\"v1\" type=\"Normal\" reason=\"Killing\" message=\"Container test-container-subpath-configmap-h8zg failed liveness probe, will be restarted\"\r\n```\r\n\r\n**Containerd logs:** \r\n\r\n```\r\n# Killing container with a grace period but timeout is reached\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.912504992Z\" level=info msg=\"StopContainer for \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" with timeout 2 (s)\"\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.913797826Z\" level=info msg=\"Stop container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" with signal terminated\"\r\n\r\n# Force killing the container but the container is already exited\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.976463751Z\" level=info msg=\"received exit event container_id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" pid:225289 exited_at:{seconds:1739607183 nanos:962757445}\"\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.977725541Z\" level=info msg=\"TaskExit event in podsandbox handler container_id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" pid:225289 exited_at:{seconds:1739607183 nanos:962757445}\"\r\nFeb 15 08:13:05 kind-worker containerd[185]: time=\"2025-02-15T08:13:05.954244737Z\" level=info msg=\"Kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\"\"\r\nFeb 15 08:13:13 kind-worker containerd[185]: time=\"2025-02-15T08:13:12.797190089Z\" level=error msg=\"StopContainer for \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" failed\" error=\"rpc error: code = Unknown desc = failed to kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\": ttrpc: closed\"\r\n```\r\n\r\n**Kubelet logs:** \r\n\r\n```\r\n// kubelet fails to stop the container and the containerd returns an error\r\nFeb 15 08:13:13 kind-worker kubelet[285]: E0215 08:13:12.800578     285 kuberuntime_container.go:814] \"Container termination failed with gracePeriod\" err=\"rpc error: code = Unknown desc = failed to kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\": ttrpc: closed\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" containerName=\"test-container-subpath-configmap-h8zg\" containerID=\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\" gracePeriod=2\r\nFeb 15 08:13:13 kind-worker kubelet[285]: E0215 08:13:12.800970     285 kuberuntime_manager.go:1187] \"killContainer for pod failed\" err=\"rpc error: code = Unknown desc = failed to kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\": ttrpc: closed\" containerName=\"test-container-subpath-configmap-h8zg\" containerID={\"Type\":\"containerd\",\"ID\":\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\"} pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:13 kind-worker kubelet[285]: I0215 08:13:12.801016     285 kubelet.go:1859] \"SyncPod exit\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" isTerminal=false\r\nFeb 15 08:13:13 kind-worker kubelet[285]: E0215 08:13:12.801142     285 pod_workers.go:1301] \"Error syncing pod, skipping\" err=\"failed to \\\"KillContainer\\\" for \\\"test-container-subpath-configmap-h8zg\\\" with KillContainerError: \\\"rpc error: code = Unknown desc = failed to kill container \\\\\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\\\\\": ttrpc: closed\\\"\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\"\r\nFeb 15 08:13:13 kind-worker kubelet[285]: I0215 08:13:12.801177     285 pod_workers.go:1338] \"Processing pod event done\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" updateType=\"sync\"\r\n\r\n// The exitCode of The container `test-container-subpath-configmap-h8zg` is 0.\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.214763     285 helpers.go:104] \"Already successfully ran container, do nothing\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" containerName=\"test-container-subpath-configmap-h8zg\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.214879     285 kuberuntime_manager.go:1148] \"computePodActions got for pod\" podActions=\"KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215172     285 container_manager.go:238] \"Pod contains no container with pinned cpus\" podName=\"pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215210     285 kuberuntime_sandbox_linux.go:62] \"Enforcing CFS quota\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" unlimited=false\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215232     285 kubelet.go:1859] \"SyncPod exit\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" isTerminal=false\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215258     285 pod_workers.go:1338] \"Processing pod event done\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" updateType=\"sync\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.259606     285 status_manager.go:935] \"Patch status for pod\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" patch=\"{\\\"metadata\\\":{\\\"uid\\\":\\\"c9f2f574-b2f4-49c0-a94f-c452d6290215\\\"},\\\"status\\\":{\\\"$setElementOrder/conditions\\\":[{\\\"type\\\":\\\"PodReadyToStartContainers\\\"},{\\\"type\\\":\\\"Initialized\\\"},{\\\"type\\\":\\\"Ready\\\"},{\\\"type\\\":\\\"ContainersReady\\\"},{\\\"type\\\":\\\"PodScheduled\\\"}],\\\"conditions\\\":[{\\\"lastTransitionTime\\\":\\\"2025-02-15T08:13:14Z\\\",\\\"message\\\":\\\"containers with unready status: [test-container-subpath-configmap-h8zg]\\\",\\\"reason\\\":\\\"ContainersNotReady\\\",\\\"status\\\":\\\"False\\\",\\\"type\\\":\\\"Ready\\\"},{\\\"lastTransitionTime\\\":\\\"2025-02-15T08:13:14Z\\\",\\\"message\\\":\\\"containers with unready status: [test-container-subpath-configmap-h8zg]\\\",\\\"reason\\\":\\\"ContainersNotReady\\\",\\\"status\\\":\\\"False\\\",\\\"type\\\":\\\"ContainersReady\\\"}],\\\"containerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\",\\\"image\\\":\\\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\\\",\\\"imageID\\\":\\\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"test-container-subpath-configmap-h8zg\\\",\\\"ready\\\":false,\\\"restartCount\\\":0,\\\"started\\\":false,\\\"state\\\":{\\\"terminated\\\":{\\\"containerID\\\":\\\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\",\\\"exitCode\\\":0,\\\"finishedAt\\\":\\\"2025-02-15T08:13:03Z\\\",\\\"reason\\\":\\\"Completed\\\",\\\"startedAt\\\":\\\"2025-02-15T08:12:57Z\\\"}},\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/test-volume\\\",\\\"name\\\":\\\"test-volume\\\"},{\\\"mountPath\\\":\\\"/probe-volume\\\",\\\"name\\\":\\\"liveness-probe-volume\\\"},{\\\"mountPath\\\":\\\"/var/run/secrets/kubernetes.io/serviceaccount\\\",\\\"name\\\":\\\"kube-api-access-d7bjr\\\",\\\"readOnly\\\":true,\\\"recursiveReadOnly\\\":\\\"Disabled\\\"}]},{\\\"containerID\\\":\\\"containerd://b83d07f353e6c10960e144839d90c658f1f40aea77ea78a9483620e33f98b67b\\\",\\\"image\\\":\\\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\\\",\\\"imageID\\\":\\\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"test-container-volume-configmap-h8zg\\\",\\\"ready\\\":true,\\\"restartCount\\\":0,\\\"started\\\":true,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2025-02-15T08:12:58Z\\\"}},\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/test-volume\\\",\\\"name\\\":\\\"test-volume\\\"},{\\\"mountPath\\\":\\\"/probe-volume\\\",\\\"name\\\":\\\"liveness-probe-volume\\\"},{\\\"mountPath\\\":\\\"/var/run/secrets/kubernetes.io/serviceaccount\\\",\\\"name\\\":\\\"kube-api-access-d7bjr\\\",\\\"readOnly\\\":true,\\\"recursiveReadOnly\\\":\\\"Disabled\\\"}]}]}}\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.259762     285 status_manager.go:944] \"Status for pod updated successfully\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" statusVersion=5 status={\"phase\":\"Running\",\"conditions\":[{\"type\":\"PodReadyToStartContainers\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:12:55Z\"},{\"type\":\"Initialized\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:12:56Z\"},{\"type\":\"Ready\",\"status\":\"False\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:13:14Z\",\"reason\":\"ContainersNotReady\",\"message\":\"containers with unready status: [test-container-subpath-configmap-h8zg]\"},{\"type\":\"ContainersReady\",\"status\":\"False\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:13:14Z\",\"reason\":\"ContainersNotReady\",\"message\":\"containers with unready status: [test-container-subpath-configmap-h8zg]\"},{\"type\":\"PodScheduled\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:12:52Z\"}],\"hostIP\":\"fc00:f853:ccd:e793::3\",\"hostIPs\":[{\"ip\":\"fc00:f853:ccd:e793::3\"}],\"podIP\":\"fd00:10:244:1::27f\",\"podIPs\":[{\"ip\":\"fd00:10:244:1::27f\"}],\"startTime\":\"2025-02-15T08:12:52Z\",\"initContainerStatuses\":[{\"name\":\"init-volume-configmap-h8zg\",\"state\":{\"terminated\":{\"exitCode\":0,\"reason\":\"Completed\",\"startedAt\":\"2025-02-15T08:12:55Z\",\"finishedAt\":\"2025-02-15T08:12:55Z\",\"containerID\":\"containerd://caf4b3bafcaec61cd9836078209eb64b0f17b158ae2b7aea8bbca9038d50a330\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\",\"imageID\":\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\",\"containerID\":\"containerd://caf4b3bafcaec61cd9836078209eb64b0f17b158ae2b7aea8bbca9038d50a330\",\"started\":false,\"volumeMounts\":[{\"name\":\"test-volume\",\"mountPath\":\"/test-volume\"},{\"name\":\"liveness-probe-volume\",\"mountPath\":\"/probe-volume\"},{\"name\":\"kube-api-access-d7bjr\",\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]}],\"containerStatuses\":[{\"name\":\"test-container-subpath-configmap-h8zg\",\"state\":{\"terminated\":{\"exitCode\":0,\"reason\":\"Completed\",\"startedAt\":\"2025-02-15T08:12:57Z\",\"finishedAt\":\"2025-02-15T08:13:03Z\",\"containerID\":\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\"}},\"lastState\":{},\"ready\":false,\"restartCount\":0,\"image\":\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\",\"imageID\":\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\",\"containerID\":\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\",\"started\":false,\"volumeMounts\":[{\"name\":\"test-volume\",\"mountPath\":\"/test-volume\"},{\"name\":\"liveness-probe-volume\",\"mountPath\":\"/probe-volume\"},{\"name\":\"kube-api-access-d7bjr\",\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]},{\"name\":\"test-container-volume-configmap-h8zg\",\"state\":{\"running\":{\"startedAt\":\"2025-02-15T08:12:58Z\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\",\"imageID\":\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\",\"containerID\":\"containerd://b83d07f353e6c10960e144839d90c658f1f40aea77ea78a9483620e33f98b67b\",\"started\":true,\"volumeMounts\":[{\"name\":\"test-volume\",\"mountPath\":\"/test-volume\"},{\"name\":\"liveness-probe-volume\",\"mountPath\":\"/probe-volume\"},{\"name\":\"kube-api-access-d7bjr\",\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]}],\"qosClass\":\"BestEffort\"}\r\n\r\n// The ContainersToStart of the computed PodActions is empty, so no new container is created to replace the previous one.\r\nFeb 15 08:14:31 kind-worker kubelet[285]: I0215 08:14:31.576812     285 helpers.go:104] \"Already successfully ran container, do nothing\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" containerName=\"test-container-subpath-configmap-h8zg\"\r\nFeb 15 08:14:31 kind-worker kubelet[285]: I0215 08:14:31.576881     285 kuberuntime_manager.go:1148] \"computePodActions got for pod\" podActions=\"KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\n```\r\n\r\n**Root cause:**\r\n\r\nThe RestartPolicy of the pod is `OnFailure` and the container exited successfully.\r\n\r\n- https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/test/e2e/storage/testsuites/subpath.go#L793\r\n- https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/pkg/kubelet/container/helpers.go#L103\r\n- https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/test/e2e/framework/pod/utils.go#L34\r\n\r\n**Fix it:**\r\n\r\nDrop `trap exit TERM` from the command to make the container exit with a non-zero exit code.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #130268\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n\n### Code Patch:\n@@ -31,8 +31,20 @@ import (\n \t\"k8s.io/utils/pointer\"\n )\n \n+// This command runs an infinite loop, sleeping for 1 second in each iteration.\n+// It sets up a trap to exit gracefully when a TERM signal is received.\n+//\n+// This is useful for testing scenarios where the container is terminated\n+// with a zero exit code.\n const InfiniteSleepCommand = \"trap exit TERM; while true; do sleep 1; done\"\n \n+// This command will cause the shell to remain in a sleep state indefinitely,\n+// and it won't exit unless it receives a KILL signal.\n+//\n+// This is useful for testing scenarios where the container is terminated\n+// with a non-zero exit code.\n+const InfiniteSleepCommandWithoutGracefulShutdown = \"sleep infinity\"\n+\n // GenerateScriptCmd generates the corresponding command lines to execute a command.\n func GenerateScriptCmd(command string) []string {\n \treturn []string{\"/bin/sh\", \"-c\", command}\n\n@@ -793,10 +793,10 @@ func testPodContainerRestartWithHooks(ctx context.Context, f *framework.Framewor\n \tpod.Spec.RestartPolicy = v1.RestartPolicyOnFailure\n \n \tpod.Spec.Containers[0].Image = e2epod.GetDefaultTestImage()\n-\tpod.Spec.Containers[0].Command = e2epod.GenerateScriptCmd(e2epod.InfiniteSleepCommand)\n+\tpod.Spec.Containers[0].Command = e2epod.GenerateScriptCmd(e2epod.InfiniteSleepCommandWithoutGracefulShutdown)\n \tpod.Spec.Containers[0].Args = nil\n \tpod.Spec.Containers[1].Image = e2epod.GetDefaultTestImage()\n-\tpod.Spec.Containers[1].Command = e2epod.GenerateScriptCmd(e2epod.InfiniteSleepCommand)\n+\tpod.Spec.Containers[1].Command = e2epod.GenerateScriptCmd(e2epod.InfiniteSleepCommandWithoutGracefulShutdown)\n \tpod.Spec.Containers[1].Args = nil\n \thooks.AddLivenessProbe(pod, probeFilePath)\n \n"}
{"id": 372, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test checks if loading an app in the background will catch and re-raise an exception. It occasionally fails on pypy3. Possibly, the background loading thread does not finish before the assert line, and so there is no exception yet. Try to address this by waiting for the lock before asserting. This could still fail if the thread starts *very* slowly.\r\n\r\ncloses #4292 \n\n### Code Patch:\n@@ -334,7 +334,9 @@ def bad_load():\n     lazy = DispatchingApp(bad_load, use_eager_loading=False)\n \n     with pytest.raises(BadExc):\n-        lazy._flush_bg_loading_exception()\n+        # reduce flakiness by waiting for the internal loading lock\n+        with lazy._lock:\n+            lazy._flush_bg_loading_exception()\n \n \n def test_with_appcontext(runner):\n"}
{"id": 225, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nPreviously, ZooKeeperPersistenceEngine would crash the whole Master process if\nthere was stored data from a prior Spark version. Now, we just delete these files.\n\n\n### Code Patch:\n@@ -64,11 +64,11 @@ class ZooKeeperPersistenceEngine(serialization: Serialization, conf: SparkConf)\n   override def readPersistedData(): (Seq[ApplicationInfo], Seq[DriverInfo], Seq[WorkerInfo]) = {\n     val sortedFiles = zk.getChildren().forPath(WORKING_DIR).toList.sorted\n     val appFiles = sortedFiles.filter(_.startsWith(\"app_\"))\n-    val apps = appFiles.map(deserializeFromFile[ApplicationInfo])\n+    val apps = appFiles.map(deserializeFromFile[ApplicationInfo]).flatten\n     val driverFiles = sortedFiles.filter(_.startsWith(\"driver_\"))\n-    val drivers = driverFiles.map(deserializeFromFile[DriverInfo])\n+    val drivers = driverFiles.map(deserializeFromFile[DriverInfo]).flatten\n     val workerFiles = sortedFiles.filter(_.startsWith(\"worker_\"))\n-    val workers = workerFiles.map(deserializeFromFile[WorkerInfo])\n+    val workers = workerFiles.map(deserializeFromFile[WorkerInfo]).flatten\n     (apps, drivers, workers)\n   }\n \n@@ -78,10 +78,18 @@ class ZooKeeperPersistenceEngine(serialization: Serialization, conf: SparkConf)\n     zk.create().withMode(CreateMode.PERSISTENT).forPath(path, serialized)\n   }\n \n-  def deserializeFromFile[T](filename: String)(implicit m: Manifest[T]): T = {\n+  def deserializeFromFile[T](filename: String)(implicit m: Manifest[T]): Option[T] = {\n     val fileData = zk.getData().forPath(WORKING_DIR + \"/\" + filename)\n     val clazz = m.runtimeClass.asInstanceOf[Class[T]]\n     val serializer = serialization.serializerFor(clazz)\n-    serializer.fromBinary(fileData).asInstanceOf[T]\n+    try {\n+      Some(serializer.fromBinary(fileData).asInstanceOf[T])\n+    } catch {\n+      case e: Exception => {\n+        logWarning(\"Exception while reading persisted file, deleting\", e)\n+        zk.delete().forPath(WORKING_DIR + \"/\" + filename)\n+        None\n+      }\n+    }\n   }\n }\n"}
{"id": 282, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nAPI Key expiration value has millisecond precision as we use\r\n{@link Instant#toEpoqueMilli()} when creating the API key\r\ndocument.\r\nIt could often happen that `Instant.now()` Instant in the testCreateApiKey\r\nwas close enough to the ApiKeyService's `clock.instant()` Instant,\r\nwhen the nanos were removed from the latter ( due to the call\r\nto `toEpoqueMilli()` ) the result of comparing these two Instants\r\nwas a few nanos short of a 7 days.\r\n\r\nResolves: #47958\n\n### Code Patch:\n@@ -131,9 +131,9 @@ private void awaitApiKeysRemoverCompletion() throws Exception {\n         }\n     }\n \n-    @AwaitsFix(bugUrl = \"https://github.com/elastic/elasticsearch/issues/47958\")\n     public void testCreateApiKey() throws Exception{\n-        final Instant start = Instant.now();\n+        // Get an instant without nanoseconds as the expiration has millisecond precision\n+        final Instant start = Instant.ofEpochMilli(Instant.now().toEpochMilli());\n         final RoleDescriptor descriptor = new RoleDescriptor(\"role\", new String[] { \"monitor\" }, null, null);\n         Client client = client().filterWithHeader(Collections.singletonMap(\"Authorization\",\n             UsernamePasswordToken.basicAuthHeaderValue(SecuritySettingsSource.TEST_SUPERUSER,\n@@ -148,6 +148,7 @@ public void testCreateApiKey() throws Exception{\n         assertNotNull(response.getId());\n         assertNotNull(response.getKey());\n         Instant expiration = response.getExpiration();\n+        // Expiration has millisecond precision\n         final long daysBetween = ChronoUnit.DAYS.between(start, expiration);\n         assertThat(daysBetween, is(7L));\n \n"}
{"id": 268, "ground_truth": "Concurrency", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nReally love this repo, I've been using it to finetune CodeGen models with >2k context windows.\r\n\r\nIt's way faster than hugging face (3x) and slightly faster than Megatron for the 350M and 2.7b parameter CodeGen models but doesn't work for the 6.1B and 16B parameter models as they have a head dimension of 256.\r\n\r\n<img width=\"1006\" alt=\"Screen Shot 2022-11-01 at 5 32 47 PM\" src=\"https://user-images.githubusercontent.com/17725268/199345886-f8b7531e-9918-4fba-ab37-4ae980ec2796.png\">\r\n\r\nI would imagine CodeGen finetuning will be a solid use-case for flash attention since coding models can really benefit from long context windows. And CodeGen is basically SOTA for coding (competitive with Codex).\r\n\r\nIs this something that is even possible with flash attention?\n\n### Code Patch:\n@@ -4,6 +4,7 @@\n \n ```bash\n pip install cmake torch transformers\n+pip install flash-attn # This may take up to 10 mins.\n pip install -e .\n ```\n \n\n@@ -1,5 +1,6 @@\n from typing import List, Optional\n \n+from flash_attn.flash_attention import FlashAttention\n import torch\n import torch.nn as nn\n \n@@ -14,20 +15,7 @@ def __init__(self, scale: float) -> None:\n         super().__init__()\n         self.scale = float(scale)\n \n-    def _masked_attention(\n-        self,\n-        query: torch.Tensor,                        # [num_queries, num_heads, head_size]\n-        key: torch.Tensor,                          # [num_keys, num_heads, head_size]\n-        value: torch.Tensor,                        # [num_keys, num_heads, head_size]\n-        attn_mask: Optional[torch.Tensor] = None,   # [num_queries, num_keys]\n-    ) -> torch.Tensor:                              # [num_queries, num_heads, head_size]\n-        query = query * self.scale\n-        attn = torch.einsum('qhd,khd->hqk', query, key)\n-        if attn_mask is not None:\n-            attn = attn + attn_mask\n-        attn = torch.softmax(attn, dim=-1)\n-        out = torch.einsum('hqk,khd->qhd', attn, value)\n-        return out\n+        self.flash_attn = FlashAttention(softmax_scale=self.scale)\n \n     def multi_query_kv_attention(\n         self,\n@@ -37,21 +25,31 @@ def multi_query_kv_attention(\n         value: torch.Tensor,        # [num_prompt_tokens, num_heads, head_size]\n         prompt_lens: List[int],\n     ) -> None:\n-        # FIXME(woosuk): Replace the following with a custom op.\n-        start_idx = 0\n+        if query.dtype == torch.float:\n+            raise ValueError('The float data type is not supported by '\n+                             'FlashAttention. Use the half data type instead.')\n+        head_size = query.shape[2]\n+        if head_size > 128:\n+            raise ValueError('FlashAttention does not support head_size > 128.')\n+\n+        device = query.device\n+        prefix_sum = [0]\n         for prompt_len in prompt_lens:\n-            out = output[start_idx:start_idx + prompt_len]\n-            q = query[start_idx:start_idx + prompt_len]\n-            k = key[start_idx:start_idx + prompt_len]\n-            v = value[start_idx:start_idx + prompt_len]\n-\n-            attention_mask = torch.triu(\n-                torch.ones(q.shape[0], k.shape[0]), diagonal=1) * -1e5\n-            attention_mask = attention_mask.to(dtype=q.dtype, device=q.device)\n-            attention_out = self._masked_attention(q, k, v, attention_mask)\n-            out.copy_(attention_out, non_blocking=True)\n-\n-            start_idx += prompt_len\n+            prefix_sum.append(prefix_sum[-1] + prompt_len)\n+        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n+        max_prompt_len = max(prompt_lens)\n+\n+        # FIXME(woosuk): Unnecessary copy. Optimize this.\n+        qkv = torch.stack([query, key, value], dim=1)\n+        out = self.flash_attn(\n+            qkv,\n+            cu_seqlens=prefix_sum,\n+            max_s=max_prompt_len,\n+            causal=True,\n+        )[0]\n+        num_tokens = prefix_sum[-1]\n+        # FIXME(woosuk): Unnecessary copy. Optimize this.\n+        output[:num_tokens].copy_(out, non_blocking=True)\n \n     def single_query_cached_kv_attention(\n         self,\n@@ -61,6 +59,14 @@ def single_query_cached_kv_attention(\n         value_cache: torch.Tensor,      # [num_blocks, num_heads, head_size, block_size]\n         input_metadata: InputMetadata,\n     ) -> None:\n+        head_size = value_cache.shape[2]\n+        supported_head_sizes = [32, 64, 80, 96, 128, 160, 192, 256]\n+        if head_size not in supported_head_sizes:\n+            raise ValueError(f'head_size ({head_size}) is not supported by '\n+                             'the single_query_cached_kv_attention kernel. '\n+                             'Use one of the following head sizes: '\n+                             f'{supported_head_sizes}.')\n+\n         block_size = value_cache.shape[3]\n         attention_ops.single_query_cached_kv_attention(\n             output,\n@@ -101,8 +107,9 @@ def forward(\n         output = output.view(-1, num_heads, head_size)\n \n         # Compute the attention op for prompts.\n-        self.multi_query_kv_attention(\n-            output, query, key, value, input_metadata.prompt_lens)\n+        if input_metadata.num_prompts > 0:\n+            self.multi_query_kv_attention(\n+                output, query, key, value, input_metadata.prompt_lens)\n \n         # Wait until the cache op is done.\n         if cache_event is not None:\n\n@@ -9,10 +9,12 @@\n parser.add_argument('--model', type=str, default='facebook/opt-125m', help='model name')\n parser.add_argument('--num-nodes', type=int, default=1, help='number of nodes')\n parser.add_argument('--num-workers', type=int, default=1, help='number of workers per node')\n-parser.add_argument('--block-size', type=int, default=8, help='token block size')\n+parser.add_argument('--block-size', type=int, default=8, choices=[8, 16], help='token block size')\n # TODO(woosuk): Add an analytical model to determine the maximum number of GPU/CPU blocks.\n parser.add_argument('--num-gpu-blocks', type=int, default=1024, help='number of GPU blocks (per GPU)')\n-parser.add_argument('--num-cpu-blocks', type=int, default=256, help='number of CPU blocks (per GPU)')\n+parser.add_argument('--num-cpu-blocks', type=int, default=32, help='number of CPU blocks (per GPU)')\n+# NOTE(woosuk): If FlashAttention is used, the float data type is not supported.\n+parser.add_argument('--dtype', type=str, default='half', choices=['half', 'float'], help='data type')\n args = parser.parse_args()\n \n \n@@ -27,6 +29,7 @@ def main():\n             block_size=args.block_size,\n             num_gpu_blocks=args.num_gpu_blocks,\n             num_cpu_blocks=args.num_cpu_blocks,\n+            dtype=args.dtype,\n         )\n         controllers.append(controller)\n \n\n@@ -1,10 +1,13 @@\n import random\n from typing import Optional\n \n+from flash_attn.flash_attention import FlashAttention\n import torch\n \n from cacheflow import attention_ops\n \n+MAX_SEQ_LEN = 4096\n+\n \n def ref_masked_attention(\n     query: torch.Tensor,\n@@ -79,7 +82,7 @@ def test_single_query_cached_kv_attention(\n     value_cache = torch.randn(\n         size=(num_blocks, *value_block_shape), dtype=dtype, device='cuda')\n \n-    context_lens = [random.randint(1, 4096) for _ in range(num_tokens)] \n+    context_lens = [random.randint(1, MAX_SEQ_LEN) for _ in range(num_tokens)] \n     max_context_len = max(context_lens)\n     context_lens = torch.tensor(context_lens, dtype=torch.int, device='cuda')\n \n@@ -123,11 +126,60 @@ def test_single_query_cached_kv_attention(\n     assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n \n \n+def test_multi_query_kv_attention(\n+    num_seqs: int,\n+    num_heads: int,\n+    head_size: int,\n+    dtype: torch.dtype,\n+) -> None:\n+    seq_lens = random.sample(range(1, MAX_SEQ_LEN), num_seqs)\n+    max_seq_len = max(seq_lens)\n+    num_tokens = sum(seq_lens)\n+\n+    cu_seq_lens = [0]\n+    for seq_len in seq_lens:\n+        cu_seq_lens.append(cu_seq_lens[-1] + seq_len)\n+    cu_seq_lens = torch.tensor(cu_seq_lens, dtype=torch.int, device='cuda')\n+\n+    scale = float(1.0 / (head_size ** 0.5))\n+    query = torch.randn(\n+        num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n+    key = torch.rand_like(query)\n+    value = torch.rand_like(query)\n+\n+    qkv = torch.stack([query, key, value], dim=1)\n+    flash_attn = FlashAttention(softmax_scale=scale)\n+    output = flash_attn(\n+        qkv,\n+        cu_seqlens=cu_seq_lens,\n+        max_s=max_seq_len,\n+        causal=True,\n+    )[0]\n+\n+    ref_outputs = []\n+    for i, seq_len in enumerate(seq_lens):\n+        attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * -1e5\n+        attn_mask = attn_mask.to(dtype=dtype, device='cuda')\n+        start_idx = cu_seq_lens[i]\n+        end_idx = cu_seq_lens[i + 1]\n+        ref_output = ref_masked_attention(\n+            query[start_idx:end_idx],\n+            key[start_idx:end_idx],\n+            value[start_idx:end_idx],\n+            scale,\n+            attn_mask=attn_mask,\n+        )\n+        ref_outputs.append(ref_output)\n+    ref_output = torch.cat(ref_outputs, dim=0)\n+\n+    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n+\n+\n @torch.inference_mode()\n def test_attention() -> None:\n     for dtype in [torch.half, torch.float]:\n         for block_size in [8, 16]:\n-            for head_size in [64, 80, 96, 128, 256]:\n+            for head_size in [32, 64, 80, 96, 128, 160, 192, 256]:\n                 test_single_query_cached_kv_attention(\n                     num_tokens=37,\n                     num_heads=3,\n@@ -137,6 +189,17 @@ def test_attention() -> None:\n                     dtype=dtype,\n                 )\n \n+    # NOTE(woosuk): FlashAttention does not support FP32.\n+    for dtype in [torch.half]:\n+        # NOTE(woosuk): FlashAttention does not support head_size > 128.\n+        for head_size in [64, 80, 96, 128]:\n+            test_multi_query_kv_attention(\n+                num_seqs=11,\n+                num_heads=3,\n+                head_size=head_size,\n+                dtype=dtype,\n+            )\n+\n \n if __name__ == '__main__':\n     test_attention()\n"}
{"id": 72, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Thank you for submitting a Pull Request. Please:\n* Read our Pull Request guidelines:\n  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests\n* Associate an issue with the Pull Request.\n* Ensure that the code is up-to-date with the `main` branch.\n* Include a description of the proposed changes and how to test them.\n-->\n\n\n### Code Patch:\n@@ -52,8 +52,22 @@ export class Extensions extends Viewlet {\n \n \tasync installExtension(id: string, waitUntilEnabled: boolean): Promise<void> {\n \t\tawait this.searchForExtension(id);\n-\t\tawait this.code.waitAndClick(`div.extensions-viewlet[id=\"workbench.view.extensions\"] .monaco-list-row[data-extension-id=\"${id}\"] .extension-list-item .monaco-action-bar .action-item:not(.disabled) .extension-action.install`);\n-\t\tawait this.code.waitForElement(`.extension-editor .monaco-action-bar .action-item:not(.disabled) .extension-action.uninstall`);\n+\n+\t\t// try to install extension 3 times\n+\t\tlet attempt = 1;\n+\t\twhile (true) {\n+\t\t\tawait this.code.waitAndClick(`div.extensions-viewlet[id=\"workbench.view.extensions\"] .monaco-list-row[data-extension-id=\"${id}\"] .extension-list-item .monaco-action-bar .action-item:not(.disabled) .extension-action.install`);\n+\n+\t\t\ttry {\n+\t\t\t\tawait this.code.waitForElement(`.extension-editor .monaco-action-bar .action-item:not(.disabled) .extension-action.uninstall`);\n+\t\t\t\tbreak;\n+\t\t\t} catch (err) {\n+\t\t\t\tif (attempt++ === 3) {\n+\t\t\t\t\tthrow err;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n \t\tif (waitUntilEnabled) {\n \t\t\tawait this.code.waitForElement(`.extension-editor .monaco-action-bar .action-item:not(.disabled) a[aria-label=\"Disable this extension\"]`);\n \t\t}\n"}
{"id": 336, "ground_truth": "Randomness", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nLet's test CI..\n\n### Code Patch:\n@@ -46,7 +46,7 @@ Checks: >\n   -readability-uppercase-literal-suffix,\n   -readability-use-anyofallof,\n WarningsAsErrors: ''\n-HeaderFilterRegex: 'AK|Userland|Kernel|Tests'\n+HeaderFilterRegex: 'AK|Userland|Tests'\n FormatStyle: none\n CheckOptions:\n   - key: bugprone-dangling-handle.HandleClasses\n\n@@ -6,10 +6,8 @@\n     \"features\": {\n         \"ghcr.io/devcontainers/features/github-cli:1\": {},\n         \"ghcr.io/devcontainers-contrib/features/pre-commit:1\": {},\n-        \"./features/serenity\": {\n+        \"./features/ladybird\": {\n             \"llvm_version\": 18,\n-            \"enable_ladybird\": true,\n-            \"enable_serenity\": true\n         },\n         \"ghcr.io/devcontainers/features/desktop-lite\": {\n             \"password\": \"vscode\",\n\n@@ -0,0 +1,18 @@\n+{\n+    \"name\": \"Ladybird Development\",\n+    \"id\": \"ladybird\",\n+    \"version\": \"2.0.0\",\n+    \"description\": \"Enable development of Ladybird libraries and applications\",\n+    \"options\": {\n+        \"llvm_version\": {\n+            \"type\": \"string\",\n+            \"proposals\": [\n+                17,\n+                18,\n+                \"trunk\"\n+            ],\n+            \"default\": 18,\n+            \"description\": \"Select LLVM compiler version to use\"\n+        }\n+    }\n+}\n\n@@ -4,9 +4,6 @@ set -e\n # Feature options\n \n LLVM_VERSION=${LLVM_VERSION:-18}\n-ENABLE_LADYBIRD=${ENABLE_LADYBIRD:-true}\n-ENABLE_SERENITY=${ENABLE_SERENITY:-true}\n-\n ### Check distro\n \n if [ ! -f /etc/lsb-release ]; then\n@@ -31,13 +28,7 @@ install_llvm_key() {\n ### Install packages\n \n apt update -y\n-apt install -y build-essential cmake ninja-build ccache shellcheck\n-if [ \"${ENABLE_LADYBIRD}\" = \"true\" ]; then\n-    apt install -y libgl1-mesa-dev qt6-base-dev qt6-tools-dev-tools qt6-wayland qt6-multimedia-dev\n-fi\n-if [ \"${ENABLE_SERENITY}\" = \"true\" ]; then\n-    apt install -y curl libmpfr-dev libmpc-dev libgmp-dev e2fsprogs genext2fs qemu-system-gui qemu-system-x86 qemu-utils rsync unzip texinfo libssl-dev\n-fi\n+apt install -y build-essential cmake ninja-build ccache shellcheck libgl1-mesa-dev qt6-base-dev qt6-tools-dev-tools qt6-wayland qt6-multimedia-dev\n \n ### Ensure new enough host compiler is available\n \n\n@@ -1,30 +0,0 @@\n-{\n-    \"name\": \"Serenity Development\",\n-    \"id\": \"serenity\",\n-    \"version\": \"1.0.1\",\n-    \"description\": \"Enable development of Serenity and Lagom libraries and applications\",\n-    \"options\": {\n-        \"llvm_version\": {\n-            \"type\": \"string\",\n-            \"proposals\": [\n-                15,\n-                16,\n-                17,\n-                18,\n-                \"trunk\"\n-            ],\n-            \"default\": 18,\n-            \"description\": \"Select LLVM compiler version to use\"\n-        },\n-        \"enable_ladybird\": {\n-            \"type\": \"boolean\",\n-            \"default\": true,\n-            \"description\": \"Install Qt6 packages for Ladybird development\"\n-        },\n-        \"enable_serenity\": {\n-            \"type\": \"boolean\",\n-            \"default\": true,\n-            \"description\": \"Install packages for SerenityOS development\"\n-        }\n-    }\n-}\n\n@@ -6,6 +6,6 @@\n // do not need to install all the prerequsite dependencies from scratch,\n // getting you into your development environment faster!\n {\n-    \"name\": \"SerenityOS (Pre-Built Image)\",\n-    \"image\": \"ghcr.io/serenityos/serenity-devcontainer:base\"\n+    \"name\": \"Ladybird (Pre-Built Image)\",\n+    \"image\": \"ghcr.io/ladybirdwebbrowser/ladybird-devcontainer:base\"\n }\n\n@@ -10,6 +10,3 @@\n *.pgm binary\n *.png binary\n *.ppm binary\n-\n-# Prevent port patches from being normalized\n-/Ports/**/patches/* -text\n\n@@ -1,14 +1,9 @@\n /AK/CircularBuffer.* @timschumi\n /AK/MaybeOwned.h @timschumi\n /AK/*Stream.* @timschumi\n-/Kernel/Devices/Audio @GMTA\n /Lagom/Tools/CodeGenerators/LibWeb @AtkinsSJ\n-/Ports @timschumi\n /Tests/LibCompress @timschumi\n /Toolchain @BertalanD\n-/Userland/Applications/Spreadsheet @alimpfard\n-/Userland/BuggieBox @timschumi\n-/Userland/DynamicLoader @BertalanD\n /Userland/Libraries/LibArchive @timschumi\n /Userland/Libraries/LibCompress @timschumi\n /Userland/Libraries/LibCore/File.* @timschumi\n@@ -34,10 +29,8 @@\n /Userland/Services/RequestServer @alimpfard\n /Userland/Services/SQLServer @trflynn89\n /Userland/Services/WebDriver @trflynn89\n-/Userland/Shell @alimpfard\n /Userland/Utilities/gzip.cpp @timschumi\n /Userland/Utilities/lzcat.cpp @timschumi\n-/Userland/Utilities/readelf.cpp @BertalanD\n /Userland/Utilities/sql.cpp @trflynn89\n /Userland/Utilities/tar.cpp @timschumi\n /Userland/Utilities/unzip.cpp @timschumi\n\n@@ -27,37 +27,15 @@ inputs:\n     description: 'Path to the SerenityOS ccache directory'\n     required: false\n     default: ''\n-  toolchain_ccache_path:\n-    description: 'Path to the toolchain ccache directory'\n-    required: false\n-    default: ''\n   download_cache_path:\n     description: 'Path to the download cache directory'\n     required: false\n     default: 'caches'\n \n outputs:\n-  qemu_cache_primary_key:\n-    description: 'Primary key for the AArch64 QEMU cache'\n-    value: ${{ steps.qemu-cache.outputs.cache-primary-key }}\n-  qemu_cache_hit:\n-    description: 'Whether the AArch64 QEMU cache was hit'\n-    value: ${{ steps.qemu-cache.outputs.cache-hit }}\n   serenity_ccache_primary_key:\n     description: 'Primary key for the Serenity ccache'\n     value: ${{ steps.serenity-ccache.outputs.cache-primary-key }}\n-  toolchain_ccache_primary_key:\n-    description: 'Primary key for the Toolchain ccache'\n-    value: ${{ steps.toolchain-ccache.outputs.cache-primary-key }}\n-  toolchain_prebuilt_primary_key:\n-    description: 'Primary key for the toolchain cache'\n-    value: ${{ steps.toolchain-prebuilt.outputs.cache-primary-key }}\n-  toolchain_prebuilt_path:\n-    description: 'Path to the prebuilt toolchain'\n-    value: ${{ steps.toolchain-stamps.outputs.toolchain_path }}\n-  toolchain_prebuilt_hit:\n-    description: 'Whether the toolchain prebuilt cache was hit'\n-    value: ${{ steps.toolchain-prebuilt.outputs.cache-hit }}\n \n runs:\n   using: \"composite\"\n@@ -68,64 +46,6 @@ runs:\n       run: |\n         echo \"timestamp=$(date -u \"+%Y%m%d%H%M_%S\")\" >> \"$GITHUB_OUTPUT\"\n \n-    - name: 'Toolchain Stamps'\n-      shell: bash\n-      id: 'toolchain-stamps'\n-      if: ${{ inputs.arch != 'Lagom' }}\n-      run: |\n-        if ${{ inputs.toolchain == 'Clang' }} ; then\n-          echo \"toolchain_path=${{ github.workspace }}/Toolchain/Local/clang\" >> \"$GITHUB_OUTPUT\"\n-\n-          hashed_files='${{ hashFiles('Toolchain/BuildClang.sh', 'Toolchain/Patches/llvm/*.patch', 'Toolchain/CMake/*.cmake', 'Userland/Libraries/LibC/**/*.h') }}'\n-          echo \"toolchain_stamp=${hashed_files}\" >> \"$GITHUB_OUTPUT\"\n-\n-          source $GITHUB_WORKSPACE/Ports/llvm/package.sh\n-          echo \"toolchain_version=$(echo ${version} | cut -d'.' -f1)\" >> \"$GITHUB_OUTPUT\"\n-        elif ${{ inputs.toolchain == 'GNU' }} ; then\n-          echo \"toolchain_path=${{ github.workspace }}/Toolchain/Local/${{ inputs.arch }}\" >> $GITHUB_OUTPUT\n-\n-          hashed_files='${{ hashFiles('Toolchain/BuildGNU.sh', 'Toolchain/Patches/binutils/*.patch', 'Toolchain/Patches/gcc/*.patch', 'Userland/Libraries/LibC/**/*.h') }}'\n-          echo \"toolchain_stamp=${hashed_files}\" >> $GITHUB_OUTPUT\n-\n-          source $GITHUB_WORKSPACE/Ports/gcc/package.sh\n-          echo \"toolchain_version=$(echo ${version} | cut -d'.' -f1)\" >> $GITHUB_OUTPUT\n-        fi\n-\n-    - name: 'Toolchain Prebuilt Cache'\n-      uses: actions/cache/restore@v4\n-      id: 'toolchain-prebuilt'\n-      if: ${{ inputs.arch != 'Lagom' }}\n-      with:\n-        path: ${{ steps.toolchain-stamps.outputs.toolchain_path }}\n-        key: '\"toolchain\" | \"${{ inputs.arch }}\" | \"${{ inputs.toolchain }}\" | \"${{ steps.toolchain-stamps.outputs.toolchain_stamp }}\"'\n-\n-    # FIXME: Remove manually built QEMU when we bump QEMU to >=8.1.x\n-    - name: 'AArch64 QEMU Cache'\n-      id: 'qemu-cache'\n-      uses: actions/cache/restore@v4\n-      if: ${{ inputs.arch == 'aarch64' }}\n-      with:\n-        path: ${{ github.workspace }}/Toolchain/Local/qemu\n-        key: ${{ runner.os }}-qemu-${{ hashFiles('Ports/qemu/version.sh', 'Toolchain/BuildQemu.sh') }}\n-\n-    - name: 'Toolchain Compiler Cache'\n-      uses: actions/cache/restore@v4\n-      id: 'toolchain-ccache'\n-      if: ${{ inputs.toolchain_ccache_path != '' }}\n-      with:\n-          path: ${{ inputs.toolchain_ccache_path }}\n-          key: '\"toolchain ccache\" | \"${{ inputs.arch }}\" | \"${{ inputs.toolchain }}\" | \"${{ steps.toolchain-stamps.outputs.toolchain_version }}\" | \"${{ inputs.ccache_version }}\" | ${{ steps.date-stamp.outputs.timestamp }}'\n-          restore-keys: |\n-            \"toolchain ccache\" | \"${{ inputs.arch }}\" | \"${{ inputs.toolchain }}\" | \"${{ steps.toolchain-stamps.outputs.toolchain_version }}\" | \"${{ inputs.ccache_version }}\"\n-\n-    - name: 'Configure Toolchain ccache'\n-      if: ${{ inputs.toolchain_ccache_path != '' }}\n-      shell: bash\n-      run: |\n-          CCACHE_DIR=${{ inputs.toolchain_ccache_path }} ccache -M 0\n-          CCACHE_DIR=${{ inputs.toolchain_ccache_path }} ccache -s\n-          CCACHE_DIR=${{ inputs.toolchain_ccache_path }} ccache -z\n-\n     - name: 'Serenity Compiler Cache'\n       uses: actions/cache/restore@v4\n       id: 'serenity-ccache'\n\n@@ -7,14 +7,6 @@ inputs:\n     description: 'Target Architecture to restore caches for'\n     required: false\n     default: 'x86_64'\n-  qemu_cache_primary_key:\n-    description: 'Primary key for the AArch64 QEMU cache'\n-    required: false\n-    default: ''\n-  qemu_cache_hit:\n-    description: 'Whether the AArch64 QEMU cache was hit'\n-    required: false\n-    default: false\n   serenity_ccache_path:\n     description: 'Path to the SerenityOS ccache directory'\n     required: false\n@@ -23,54 +15,10 @@ inputs:\n     description: 'Primary key for the Serenity ccache'\n     required: false\n     default: ''\n-  toolchain_ccache_path:\n-    description: 'Path to the toolchain ccache directory'\n-    required: false\n-    default: ''\n-  toolchain_ccache_primary_key:\n-    description: 'Primary key for the Toolchain ccache'\n-    required: false\n-    default: ''\n-  toolchain_prebuilt_path:\n-    description: 'Path to the prebuilt toolchain'\n-    required: false\n-    default: ''\n-  toolchain_prebuilt_primary_key:\n-    description: 'Primary key for the toolchain cache'\n-    required: false\n-    default: ''\n-  toolchain_prebuilt_hit:\n-    description: 'Whether the toolchain prebuilt cache was hit'\n-    required: false\n-    default: false\n \n runs:\n   using: \"composite\"\n   steps:\n-    - name: 'Toolchain Prebuilt Cache'\n-      uses: actions/cache/save@v4\n-      # Do not waste time and storage space by updating the toolchain cache from a PR,\n-      # as it would be discarded after being merged anyway.\n-      if: ${{ github.event_name != 'pull_request' && !inputs.toolchain_prebuilt_hit && inputs.arch != 'Lagom' }}\n-      with:\n-        path: ${{ inputs.toolchain_prebuilt_path }}\n-        key: ${{ inputs.toolchain_prebuilt_primary_key }}\n-\n-    # FIXME: Remove manually built QEMU when we bump QEMU to >=8.1.x\n-    - name: 'AArch64 QEMU Cache'\n-      uses: actions/cache/save@v4\n-      if: ${{ github.event_name != 'pull_request' && !inputs.qemu_cache_hit && inputs.arch == 'aarch64' }}\n-      with:\n-        path: ${{ github.workspace }}/Toolchain/Local/qemu\n-        key: ${{ inputs.qemu_cache_primary_key }}\n-\n-    - name: 'Toolchain Compiler Cache'\n-      uses: actions/cache/save@v4\n-      if: ${{ github.event_name != 'pull_request' && inputs.toolchain_ccache_path != '' }}\n-      with:\n-        path: ${{ inputs.toolchain_ccache_path }}\n-        key: ${{ inputs.toolchain_ccache_primary_key }}\n-\n     - name: 'Prune obsolete ccache files'\n       shell: bash\n       if: ${{ inputs.serenity_ccache_path != '' }}\n\n@@ -42,44 +42,17 @@ runs:\n         echo \"${{ github.workspace }}/wabt-1.0.23/bin\" >> $GITHUB_PATH\n \n     - name: 'Install JS dependencies'\n-      if: ${{ inputs.os == 'Linux' || inputs.os == 'Serenity' }}\n+      if: ${{ inputs.os == 'Linux' }}\n       shell: bash\n       run: sudo npm install -g prettier@2.7.1\n \n     - name: 'Install Python dependencies'\n-      if: ${{ inputs.os == 'Linux' || inputs.os == 'Serenity' }}\n+      if: ${{ inputs.os == 'Linux' }}\n       shell: bash\n       run: |\n         python3 -m pip install --upgrade pip\n         pip3 install flake8 requests six\n \n-    - name: 'Install Dependencies'\n-      if: ${{ inputs.os == 'Serenity' }}\n-      shell: bash\n-      run: |\n-        set -e\n-\n-        wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -\n-        sudo add-apt-repository 'deb http://apt.llvm.org/jammy/ llvm-toolchain-jammy-18 main'\n-\n-        sudo add-apt-repository ppa:ubuntu-toolchain-r/test\n-\n-        sudo apt-get update\n-        sudo apt-get install clang-format-18 ccache e2fsprogs gcc-13 g++-13 libstdc++-13-dev libmpfr-dev libmpc-dev ninja-build optipng qemu-utils qemu-system-i386 unzip generate-ninja libegl1-mesa-dev\n-\n-        if ${{ inputs.arch == 'aarch64' }}; then\n-            # FIXME: Remove this when we no longer build our own Qemu binary.\n-            sudo apt-get install libgtk-3-dev libpixman-1-dev libsdl2-dev libslirp-dev\n-        fi\n-\n-    - name: Enable KVM group perms\n-      if: ${{ inputs.os == 'Serenity' }}\n-      shell: bash\n-      run: |\n-        echo 'KERNEL==\"kvm\", GROUP=\"kvm\", MODE=\"0666\", OPTIONS+=\"static_node=kvm\"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules\n-        sudo udevadm control --reload-rules\n-        sudo udevadm trigger --name-match=kvm\n-\n     - name: 'Install Dependencies'\n       if: ${{ inputs.os == 'macOS' || inputs.os == 'Android' }}\n       shell: bash\n\n@@ -7,51 +7,29 @@ concurrency:\n   cancel-in-progress: true\n \n jobs:\n-  SerenityOS:\n-    if: github.repository == 'SerenityOS/serenity'\n-\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        toolchain: ['GNU', 'Clang']\n-        os: [ubuntu-22.04]\n-        arch: ['x86_64']\n-        debug_options: ['NORMAL_DEBUG']\n-        include:\n-          - toolchain: 'GNU'\n-            os: ubuntu-22.04\n-            arch: 'aarch64'\n-            debug_options: 'NORMAL_DEBUG'\n-\n-          - toolchain: 'GNU'\n-            os: ubuntu-22.04\n-            arch: 'x86_64'\n-            debug_options: 'ALL_DEBUG'\n-\n-    uses: ./.github/workflows/serenity-template.yml\n-    with:\n-      toolchain: ${{ matrix.toolchain }}\n-      os: ${{ matrix.os }}\n-      arch: ${{ matrix.arch }}\n-      debug_options: ${{ matrix.debug_options }}\n-\n   Lagom:\n-    if: github.repository == 'SerenityOS/serenity'\n+    if: github.repository == 'LadybirdWebBrowser/ladybird'\n \n     strategy:\n       fail-fast: false\n       matrix:\n         os_name: ['Linux']\n         os: [ubuntu-22.04]\n         fuzzer: ['NO_FUZZ', 'FUZZ']\n+        toolchain: ['Clang']\n         include:\n           - os_name: 'macOS'\n             os: macos-14\n             fuzzer: 'NO_FUZZ'\n+            toolchain: 'Clang'\n+          - os_name: 'Linux'\n+            os: ubuntu-22.04\n+            fuzzer: 'NO_FUZZ'\n+            toolchain: 'GNU'\n \n     uses: ./.github/workflows/lagom-template.yml\n     with:\n-      toolchain: 'Clang'\n+      toolchain: ${{ matrix.toolchain }}\n       os_name: ${{ matrix.os_name }}\n       os: ${{ matrix.os }}\n       fuzzer: ${{ matrix.fuzzer }}\n\n@@ -17,7 +17,7 @@ permissions:\n \n jobs:\n   build:\n-    if: github.repository == 'SerenityOS/serenity'\n+    if: github.repository == 'LadybirdWebBrowser/ladybird'\n     runs-on: ubuntu-latest\n     steps:\n \n@@ -34,6 +34,6 @@ jobs:\n       - name: Build Base Dev Container Image\n         uses: devcontainers/ci@v0.3\n         with:\n-          imageName: ghcr.io/serenityos/serenity-devcontainer\n+          imageName: ghcr.io/ladybirdwebbrowser/ladybird-devcontainer\n           imageTag: base,latest\n           push: always\n\n@@ -1,17 +0,0 @@\n-name: Discord notifications\n-\n-on: [push, pull_request_target]\n-\n-jobs:\n-  notify_discord:\n-    runs-on: ubuntu-22.04\n-    if: always() && github.repository == 'SerenityOS/serenity' && (github.event_name == 'pull_request_target' || (github.event_name == 'push' && github.ref == 'refs/heads/master'))\n-\n-    steps:\n-      - name: Discord action notification\n-        env:\n-          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}\n-          CUSTOM_GITHUB_EVENT_NAME: ${{ github.event_name == 'pull_request_target' && 'pull_request' || github.event_name }} # fake the event type as discord doesn't know how to parse the special pull_request_target context\n-        uses: IdanHo/action-discord@754598254f288e6d8e9fca637832e3c163515ba8\n-        # FIXME: Find a way to notify on 'ready_for_review', but rate-limited to once/twice per day.\n-        if: ${{ (github.event['pull_request'] && github.event['action'] == 'opened') || github.event['commits'] }}\n\n@@ -12,7 +12,7 @@ on:\n jobs:\n   label_pull_request:\n     runs-on: ubuntu-22.04\n-    if: always() && github.repository == 'SerenityOS/serenity'\n+    if: always() && github.repository == 'LadybirdWebBrowser/ladybird'\n     \n     steps:\n       - uses: actions/checkout@v4\n\n@@ -178,11 +178,7 @@ jobs:\n         working-directory: ${{ github.workspace }}\n         run: |\n           set -e\n-          ./Meta/check-markdown.sh\n-          ./Meta/lint-gml-format.sh\n           git ls-files '*.ipc' | xargs ./Meta/Lagom/Build/bin/IPCMagicLinter\n         env:\n-          MARKDOWN_CHECK_BINARY: ./Meta/Lagom/Build/bin/markdown-check\n-          GML_FORMAT: ./Meta/Lagom/Build/bin/gml-format\n           ASAN_OPTIONS: 'strict_string_checks=1:check_initialization_order=1:strict_init_order=1:detect_stack_use_after_return=1:allocator_may_return_null=1'\n           UBSAN_OPTIONS: 'print_stacktrace=1:print_summary=1:halt_on_error=1'\n\n@@ -8,7 +8,8 @@ env:\n jobs:\n   run_and_update_results:\n     runs-on: self-hosted\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n+    if: false\n+    #if: always() && github.repository == 'LadybirdWebBrowser/ladybird' && github.ref == 'refs/heads/master'\n \n     concurrency: libjs-test262\n \n@@ -18,7 +19,7 @@ jobs:\n           echo \"Cleaning up previous run\"\n           rm -rf \"${{ github.workspace }}/*\"\n \n-      - name: Checkout SerenityOS/serenity\n+      - name: Checkout LadybirdWebBrowser/ladybird\n         uses: actions/checkout@v4\n \n       - name: Checkout SerenityOS/libjs-test262\n\n@@ -8,11 +8,12 @@ on: [pull_request_target]\n jobs:\n   lint_commits:\n     runs-on: ubuntu-22.04\n-    if: always() && github.repository == 'SerenityOS/serenity'\n+    if: always() && github.repository == 'LadybirdWebBrowser/ladybird'\n \n     steps:\n       - name: Lint PR commits\n         uses: actions/github-script@v7\n+        github-token: ${{ secrets.LADYBIRD_BOT_TOKEN }}\n         with:\n           script: |\n             const excludedBotIds = [\n@@ -89,6 +90,6 @@ jobs:\n         if: ${{ failure() && !github.event.pull_request.draft }}\n         uses: IdanHo/comment-on-pr@63ea2bf352997c66e524b8b5be7a79163fb3a88a\n         env:\n-          GITHUB_TOKEN: ${{ secrets.BUGGIEBOT_TOKEN }}\n+          GITHUB_TOKEN: ${{ secrets.LADYBIRD_BOT_TOKEN }}\n         with:\n-          msg: \"Hello!\\n\\nOne or more of the commit messages in this PR do not match the SerenityOS [code submission policy](https://github.com/SerenityOS/serenity/blob/master/CONTRIBUTING.md#code-submission-policy), please check the `lint_commits` CI job for more details on which commits were flagged and why.\\nPlease do not close this PR and open another, instead modify your commit message(s) with [git commit --amend](https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-commits/changing-a-commit-message) and force push those changes to update this PR.\"\n+          msg: \"Hello!\\n\\nOne or more of the commit messages in this PR do not match the SerenityOS [code submission policy](https://github.com/LadybirdWebBrowser/ladybird/blob/master/CONTRIBUTING.md#code-submission-policy), please check the `lint_commits` CI job for more details on which commits were flagged and why.\\nPlease do not close this PR and open another, instead modify your commit message(s) with [git commit --amend](https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-commits/changing-a-commit-message) and force push those changes to update this PR.\"\n\n@@ -1,28 +0,0 @@\n-name: Generate man pages\n-\n-on:\n-  push:\n-    paths:\n-      - \"Base/usr/share/man/**\"\n-      - \"Meta/Websites/man.serenityos.org/**\"\n-\n-jobs:\n-  convert_using_pandoc:\n-    runs-on: ubuntu-22.04\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n-    steps:\n-      - uses: actions/checkout@v4\n-      - uses: r-lib/actions/setup-pandoc@v2\n-        with:\n-          pandoc-version: '2.13'\n-      - name: Actually build website\n-        run: ./Meta/build-manpages-website.sh\n-      - name: Deploy to GitHub pages\n-        uses: JamesIves/github-pages-deploy-action@v4.6.1\n-        with:\n-          git-config-name: BuggieBot\n-          git-config-email: buggiebot@serenityos.org\n-          branch: master\n-          repository-name: SerenityOS/manpages-website\n-          token: ${{ secrets.BUGGIEBOT_TOKEN }}\n-          folder: output\n\n@@ -18,7 +18,7 @@ concurrency:\n jobs:\n   CI:\n     runs-on: ${{ matrix.os }}\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n+    if: always() && github.repository == 'LadybirdWebBrowser/ladybird' && github.ref == 'refs/heads/master'\n     strategy:\n       fail-fast: false\n       matrix:\n\n@@ -1,36 +0,0 @@\n-name: Nightly CI\n-\n-on:\n-  # Automatically run at the end of every day.\n-  schedule:\n-    - cron: '0 0 * * *'\n-\n-concurrency:\n-  group: ${{ github.workflow }}-${{ github.head_ref || format('{0}-{1}', github.ref, github.run_number) }}\n-  cancel-in-progress: true\n-\n-jobs:\n-  CodeCoverage:\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n-\n-    strategy:\n-      fail-fast: false\n-\n-    uses: ./.github/workflows/serenity-template.yml\n-    with:\n-      toolchain: 'Clang'\n-      os: ubuntu-22.04\n-      arch: 'x86_64'\n-      coverage: 'ON'\n-\n-  LagomGCC:\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n-\n-    strategy:\n-      fail-fast: false\n-\n-    uses: ./.github/workflows/lagom-template.yml\n-    with:\n-      toolchain: 'GNU'\n-      os_name: 'Linux'\n-      os: ubuntu-22.04\n\n@@ -9,7 +9,7 @@ env:\n jobs:\n   build-and-package:\n     runs-on: ${{ matrix.os }}\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n+    if: always() && github.repository == 'LadybirdWebBrowser/ladybird' && github.ref == 'refs/heads/master'\n     strategy:\n       fail-fast: false\n       matrix:\n@@ -26,7 +26,7 @@ jobs:\n       cancel-in-progress: true\n \n     steps:\n-      - name: Checkout SerenityOS/serenity\n+      - name: Checkout LadybirdWebBrowser/ladybird\n         uses: actions/checkout@v4\n \n       - name: \"Set up environment\"\n@@ -88,5 +88,5 @@ jobs:\n         uses: actions/upload-artifact@v4\n         with:\n           name: serenity-js-${{ matrix.package_type }}\n-          path: Build/serenity-js*.tar.gz\n+          path: Build/ladybird-js*.tar.gz\n           retention-days: 7\n\n@@ -1,209 +0,0 @@\n-name: SerenityOS Template\n-\n-on:\n-  workflow_call:\n-    inputs:\n-        toolchain:\n-          required: true\n-          type: string\n-        os:\n-          required: true\n-          type: string\n-        arch:\n-          required: true\n-          type: string\n-        debug_options:\n-          required: false\n-          type: string\n-          default: 'NORMAL_DEBUG'\n-        coverage:\n-          required: false\n-          type: string\n-          default: 'OFF'\n-\n-env:\n-  # runner.workspace = /home/runner/work/serenity\n-  # github.workspace = /home/runner/work/serenity/serenity\n-  SERENITY_SOURCE_DIR: ${{ github.workspace }}\n-  SERENITY_CCACHE_DIR: ${{ github.workspace }}/.ccache\n-  TOOLCHAIN_CCACHE_DIR: ${{ github.workspace }}/Toolchain/.ccache\n-\n-jobs:\n-  CI:\n-    runs-on: ${{ inputs.os }}\n-\n-    steps:\n-      # Pull requests can trail behind `master` and can cause breakage if merging before running the CI checks on an updated branch.\n-      # Luckily, GitHub creates and maintains a merge branch that is updated whenever the target or source branch is modified. By\n-      # checking this branch out, we gain a stabler `master` at the cost of reproducibility.\n-      - uses: actions/checkout@v4\n-        if: ${{ github.event_name != 'pull_request' }}\n-\n-      - uses: actions/checkout@v4\n-        if: ${{ github.event_name == 'pull_request' }}\n-        with:\n-          ref: refs/pull/${{ github.event.pull_request.number }}/merge\n-\n-      - name: Set Up Environment\n-        uses: ./.github/actions/setup\n-        with:\n-          os: 'Serenity'\n-          arch: ${{ inputs.arch }}\n-\n-      # === PREPARE FOR BUILDING ===\n-\n-      - name: Lint\n-        run: ${{ github.workspace }}/Meta/lint-ci.sh\n-\n-      - name: Restore Caches\n-        uses: ./.github/actions/cache-restore\n-        id: 'cache-restore'\n-        with:\n-          os: 'Serenity'\n-          arch: ${{ inputs.arch }}\n-          toolchain: ${{ inputs.toolchain }}\n-          cache_key_extra: ${{ inputs.debug_options }}-COVERAGE_${{ inputs.coverage }}\n-          serenity_ccache_path: ${{ env.SERENITY_CCACHE_DIR }}\n-          toolchain_ccache_path: ${{ env.TOOLCHAIN_CCACHE_DIR }}\n-          download_cache_path: ${{ github.workspace }}/Build/caches\n-\n-      - name: Assign Build Parameters\n-        id: 'build-parameters'\n-        run: |\n-          if ${{ inputs.toolchain == 'Clang' }} ; then\n-            echo \"build_directory=${{ github.workspace }}/Build/${{ inputs.arch }}clang\" >> \"$GITHUB_OUTPUT\"\n-          elif ${{ inputs.toolchain == 'GNU' }} ; then\n-            echo \"build_directory=${{ github.workspace }}/Build/${{ inputs.arch }}\" >> \"$GITHUB_OUTPUT\"\n-          fi\n-\n-      - name: Build Toolchain\n-        if: ${{ !steps.cache-restore.outputs.toolchain_prebuilt_hit }}\n-        run: |\n-          ${{ github.workspace }}/Toolchain/Build${{ inputs.toolchain }}.sh --ci\n-        env:\n-          ARCH: ${{ inputs.arch}}\n-          CCACHE_DIR: ${{ env.TOOLCHAIN_CCACHE_DIR }}\n-\n-      - name: Build AArch64 Qemu\n-        if: ${{ inputs.arch == 'aarch64' && !steps.cache-restore.outputs.qemu_cache_hit }}\n-        run: ${{ github.workspace }}/Toolchain/BuildQemu.sh\n-        env:\n-          CCACHE_DIR: ${{ env.TOOLCHAIN_CCACHE_DIR }}\n-\n-      - name: Create Build Environment\n-        if: ${{ inputs.debug_options == 'ALL_DEBUG' }}\n-        # Build the entire project with all available debug options turned on, to prevent code rot.\n-        # However, it is unwieldy and slow to run tests with them enabled, so we will build twice.\n-        run: |\n-          cmake -S Meta/CMake/Superbuild -B Build/superbuild -GNinja \\\n-            -DSERENITY_ARCH=${{ inputs.arch }} \\\n-            -DSERENITY_TOOLCHAIN=${{ inputs.toolchain }} \\\n-            -DBUILD_LAGOM=ON \\\n-            -DCMAKE_C_COMPILER=gcc-13 \\\n-            -DCMAKE_CXX_COMPILER=g++-13 \\\n-            -DENABLE_ALL_DEBUG_FACILITIES=ON \\\n-            -DENABLE_PCI_IDS_DOWNLOAD=OFF \\\n-            -DENABLE_USB_IDS_DOWNLOAD=OFF\n-        env:\n-          CCACHE_DIR: ${{ env.SERENITY_CCACHE_DIR }}\n-\n-      - name: Create Build Environment\n-        if: ${{ inputs.debug_options == 'NORMAL_DEBUG' }}\n-        working-directory: ${{ github.workspace }}\n-        # Note that we do not set BUILD_LAGOM for the normal debug build\n-        # We build and run the Lagom tests in a separate job, and sanitizer builds take a good while longer than non-sanitized.\n-        run:  |\n-          cmake -S Meta/CMake/Superbuild -B Build/superbuild -GNinja \\\n-            -DSERENITY_ARCH=${{ inputs.arch }} \\\n-            -DSERENITY_TOOLCHAIN=${{ inputs.toolchain }} \\\n-            -DCMAKE_C_COMPILER=gcc-13 \\\n-            -DCMAKE_CXX_COMPILER=g++-13 \\\n-            -DENABLE_UNDEFINED_SANITIZER=ON \\\n-            -DUNDEFINED_BEHAVIOR_IS_FATAL=ON \\\n-            -DENABLE_USERSPACE_COVERAGE_COLLECTION=${{ inputs.coverage }} \\\n-            -DDUMP_REGIONS_ON_CRASH=ON \\\n-            -DENABLE_PCI_IDS_DOWNLOAD=OFF \\\n-            -DENABLE_USB_IDS_DOWNLOAD=OFF\n-        env:\n-          CCACHE_DIR: ${{ env.SERENITY_CCACHE_DIR }}\n-\n-      # === BUILD ===\n-\n-      - name: Build Serenity and Tests\n-        working-directory: ${{ github.workspace }}/Build/superbuild\n-        run: cmake --build .\n-        env:\n-          CCACHE_DIR: ${{ env.SERENITY_CCACHE_DIR }}\n-\n-      - name: Save Caches\n-        uses: ./.github/actions/cache-save\n-        with:\n-          arch: ${{ inputs.arch }}\n-          qemu_cache_primary_key: ${{ steps.cache-restore.outputs.qemu_cache_primary_key }}\n-          qemu_cache_hit: ${{ steps.cache-restore.outputs.qemu_cache_hit }}\n-          serenity_ccache_path: ${{ env.SERENITY_CCACHE_DIR }}\n-          serenity_ccache_primary_key: ${{ steps.cache-restore.outputs.serenity_ccache_primary_key }}\n-          toolchain_ccache_path: ${{ env.TOOLCHAIN_CCACHE_DIR }}\n-          toolchain_ccache_primary_key: ${{ steps.cache-restore.outputs.toolchain_ccache_primary_key }}\n-          toolchain_prebuilt_path: ${{ steps.cache-restore.outputs.toolchain_prebuilt_path }}\n-          toolchain_prebuilt_primary_key: ${{ steps.cache-restore.outputs.toolchain_prebuilt_primary_key }}\n-          toolchain_prebuilt_hit: ${{ steps.cache-restore.outputs.toolchain_prebuilt_hit }}\n-\n-      # === TEST ===\n-\n-      - name: Create Serenity Rootfs\n-        if: ${{ inputs.debug_options == 'NORMAL_DEBUG' }}\n-        working-directory: ${{ steps.build-parameters.outputs.build_directory }}\n-        run: ninja install && ninja qemu-image\n-\n-      - name: Run On-Target Tests\n-        if: ${{ inputs.debug_options == 'NORMAL_DEBUG' && inputs.arch != 'aarch64' }}\n-        working-directory: ${{ steps.build-parameters.outputs.build_directory }}\n-        env:\n-          SERENITY_QEMU_CPU: \"max,vmx=off\"\n-          SERENITY_KERNEL_CMDLINE: \"graphics_subsystem_mode=off panic=shutdown system_mode=self-test\"\n-          SERENITY_RUN: \"ci\"\n-        run: |\n-          echo \"::group::ninja run # Qemu output\"\n-          ninja run\n-          echo \"::endgroup::\"\n-          echo \"::group::Verify Output File\"\n-          mkdir fsmount\n-          sudo mount -t ext2 -o loop,rw _disk_image fsmount\n-          echo \"Results: \"\n-          sudo cat fsmount/home/anon/test-results.log\n-          if ! sudo grep -q \"Failed: 0\" fsmount/home/anon/test-results.log\n-          then\n-            echo \"::error:: :^( Tests failed, failing job\"\n-            exit 1\n-          fi\n-          echo \"::endgroup::\"\n-        timeout-minutes: 60\n-\n-      - name: Print Target Logs\n-        # Extremely useful if Serenity hangs trying to run one of the tests\n-        if: ${{ !cancelled() && inputs.debug_options == 'NORMAL_DEBUG'}}\n-        working-directory: ${{ steps.build-parameters.outputs.build_directory }}\n-        run: '[ ! -e debug.log ] || cat debug.log'\n-\n-      # === COVERAGE ===\n-\n-      - name: Aggregate Coverage Results\n-        if: ${{ inputs.coverage == 'ON' }}\n-        working-directory: ${{ github.workspace }}\n-        run: ./Meta/analyze-qemu-coverage.sh\n-        env:\n-          SERENITY_TOOLCHAIN: ${{ matrix.toolchain }}\n-          SERENITY_ARCH: ${{ matrix.arch }}\n-\n-      # FIXME: Deploy the static html pages somewhere\n-      # FIXME: Alter script to also (instead?) produce a raw coverage.txt file for ingestion into sonar cloud\n-      # Note: tmp_profile_data/Coverage.profdata has the entire combined profile data, but creating the raw txt requires\n-      #       all of the instrumented binaries and the profdata file.\n-      - name: Upload Coverage Results\n-        if: ${{ inputs.coverage == 'ON' }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: serenity-coverage\n-          path: ${{ steps.build-parameters.outputs.build_directory }}/reports\n-          retention-days: 90\n\n@@ -1,41 +0,0 @@\n-name: Social media notifications\n-\n-on: [ push ]\n-\n-jobs:\n-  notify_twitter:\n-    runs-on: ubuntu-22.04\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n-\n-    steps:\n-      - uses: actions/checkout@v4\n-      - uses: actions/setup-node@v4\n-        with:\n-          node-version: '14'\n-      - run: npm i twit\n-      - run: |\n-          node ${{ github.workspace }}/Meta/tweet-commits.js << 'EOF'\n-          ${{ toJSON(github.event) }}\n-          EOF\n-        env:\n-          CONSUMER_KEY: ${{ secrets.CONSUMER_KEY }}\n-          CONSUMER_SECRET: ${{ secrets.CONSUMER_SECRET }}\n-          ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n-          ACCESS_TOKEN_SECRET: ${{ secrets.ACCESS_TOKEN_SECRET }}\n-\n-  notify_mastodon:\n-    runs-on: ubuntu-22.04\n-    if: always() && github.repository == 'SerenityOS/serenity' && github.ref == 'refs/heads/master'\n-\n-    steps:\n-      - uses: actions/checkout@v4\n-      - uses: actions/setup-node@v4\n-        with:\n-          node-version: '14'\n-      - run: npm i mastodon\n-      - run: |\n-          node ${{ github.workspace }}/Meta/toot-commits.js << 'EOF'\n-          ${{ toJSON(github.event) }}\n-          EOF\n-        env:\n-          ACCESS_TOKEN: ${{ secrets.MASTODON_ACCESS_TOKEN }}\n\n@@ -10,11 +10,11 @@ concurrency: wasm\n jobs:\n   build:\n     runs-on: ubuntu-22.04\n-    if: github.repository == 'SerenityOS/serenity'\n+    if: false\n     strategy:\n       fail-fast: false\n     steps:\n-      - name: Checkout SerenityOS/serenity\n+      - name: Checkout LadybirdWebBrowser/ladybird\n         uses: actions/checkout@v4\n \n       - name: Checkout SerenityOS/libjs-data libjs-wasm\n\n@@ -8,14 +8,6 @@ repos:\n         stages: [ commit ]\n         language: system\n \n-      - id: meta-lint-ports\n-        name: Running Meta/lint-ports.py to ensure changes will pass linting on CI\n-        entry: Meta/lint-ports.py\n-        pass_filenames: false\n-        files: ^Ports/\n-        stages: [ commit ]\n-        language: system\n-\n       - id: meta-lint-commit\n         name: Lint commit message to ensure it will pass the commit linting on CI\n         entry: Meta/lint-commit.sh\n\n@@ -41,7 +41,7 @@ set(AK_SOURCES\n     Utf8View.cpp\n     kmalloc.cpp\n )\n-# AK sources are included from many different places, such as the Kernel, LibC, and Loader\n+# AK sources are included from many different places\n list(TRANSFORM AK_SOURCES PREPEND \"${CMAKE_CURRENT_SOURCE_DIR}/\")\n \n set(AK_SOURCES ${AK_SOURCES} PARENT_SCOPE)\n\n@@ -1,38 +0,0 @@\n-[Icons]\n-asm=*.S,*.asm\n-cmake=*CMakeLists.txt,*CMakeCache.txt\n-cplusplus=*.cpp,*.cxx,*.cc,*.c++\n-c=*.c\n-font=*.font\n-form=*.frm\n-git=*.gitignore,*.gitattributes,*.gitcredentials,*.mailmap,*.gitmodules,*.gitconfig\n-header=*.h,*.hpp,*.hxx,*.hh,*.h++\n-html=*.html,*.htm\n-css=*.css\n-php=*.php,*.phtml,*.php3,*.php4,*.php5,*.phps\n-sql=*.sql,*.dsql\n-db=*.db,*.sqlite,*.sqlite3\n-ini=*.ini\n-java=*.java\n-javascript=*.js,*.mjs\n-json=*.json\n-wasm=*.wasm\n-library=*.so,*.so.*,*.a\n-markdown=*.md\n-music=*.midi\n-object=*.o,*.obj\n-gml=*.gml\n-palette=*.palette\n-pdf=*.pdf\n-presenter=*.presenter\n-python=*.py\n-ruby=*.rb\n-shell=*.sh,*.bash,*.zsh\n-sound=*.wav,*.flac,*.mp3,*.qoa\n-spreadsheet=*.sheets,*.csv\n-calendar=*.cal\n-text=*.txt\n-truetype=*.ttf\n-pixelpaint=*.pp\n-archive=*.zip,*.gz,*.tar\n-video=*.avi,*.mkv,*.mov,*.mp4,*.webm\n\n@@ -1,2 +0,0 @@\n-[Mapping]\n-Keymaps=en-us\n\n@@ -1,3 +0,0 @@\n-[DNS]\n-Nameservers=1.1.1.1,1.0.0.1\n-EnableServer=false\n"}
{"id": 342, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n\r\n(cherry picked from commit 28278debebabdd44bf69609f17d2c7c9c8a39695)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n\n### Code Patch:\n@@ -520,7 +520,9 @@ def test_scan_stale_dags(self, testing_dag_bundle):\n \n     def test_kill_timed_out_processors_kill(self):\n         manager = DagFileProcessorManager(max_runs=1, processor_timeout=5)\n-        processor, _ = self.mock_processor(start_time=16000)\n+        # Set start_time to ensure timeout occurs: start_time = current_time - (timeout + 1) = always (timeout + 1) seconds\n+        start_time = time.monotonic() - manager.processor_timeout - 1\n+        processor, _ = self.mock_processor(start_time=start_time)\n         manager._processors = {\n             DagFileInfo(\n                 bundle_name=\"testing\", rel_path=Path(\"abc.txt\"), bundle_path=TEST_DAGS_FOLDER\n"}
{"id": 127, "ground_truth": "Randomness", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe test `Win32ProgramRepositoryMustCallOnAppRenamedForLnkAppsWhenRenamedEventIsRaised` was experiencing random failures due to object identity mismatches in the repository's hash-based storage system.\r\n\r\n## Root Cause\r\n\r\nThe test was manually creating `Win32Program` objects:\r\n\r\n```csharp\r\nWin32Program olditem = new Win32Program\r\n{\r\n    Name = \"oldpath\",\r\n    ExecutableName = oldpath,\r\n    FullPath = linkingTo,\r\n};\r\n```\r\n\r\nHowever, the `DoOnAppRenamedAsync` method creates the `oldApp` object for removal using a different approach for .lnk files:\r\n\r\n```csharp\r\noldApp = new Win32Program() { \r\n    Name = Path.GetFileNameWithoutExtension(e.OldName), \r\n    ExecutableName = Path.GetFileName(e.OldName), \r\n    FullPath = newApp?.FullPath ?? oldPath \r\n};\r\n```\r\n\r\nSince the repository uses `GetHashCode()` (based on `Name`, `ExecutableName`, and `FullPath`) to identify objects for removal, any subtle differences in these properties would cause the `Remove()` operation to fail, leading to test assertion failures.\r\n\r\n## Fix\r\n\r\nChanged the test to use `Win32Program.GetAppFromPath()` instead of manual object creation:\r\n\r\n```csharp\r\nWin32Program olditem = Win32Program.GetAppFromPath(oldFullPath);\r\nWin32Program newitem = Win32Program.GetAppFromPath(newFullPath);\r\n```\r\n\r\nThis mirrors the approach used in the working `Win32ProgramRepositoryMustCallOnAppRenamedForUrlAppsWhenRenamedEventIsRaised` test and ensures that test objects are created using the same code path as the production code, eliminating hash code mismatches.\r\n\r\n## Why This Was Random\r\n\r\nThe test failure appeared random because it depended on subtle differences in object creation that could vary based on timing, mock setup, or other environmental factors. By using the same object creation method as the production code, the test becomes deterministic.\n\n### Code Patch:\n@@ -363,7 +363,7 @@ public async Task Win32ProgramRepositoryMustCallOnAppRenamedForLnkAppsWhenRename\n             RenamedEventArgs e = new RenamedEventArgs(WatcherChangeTypes.Renamed, directory, path, oldpath);\n \n             string oldFullPath = directory + \"\\\\\" + oldpath;\n-            string fullPath = directory + \"\\\\\" + path;\n+            string newFullPath = directory + \"\\\\\" + path;\n             string linkingTo = Directory.GetCurrentDirectory();\n \n             // ShellLinkHelper must be mocked for lnk applications\n@@ -372,19 +372,8 @@ public async Task Win32ProgramRepositoryMustCallOnAppRenamedForLnkAppsWhenRename\n             Win32Program.ShellLinkHelper = mockShellLink.Object;\n \n             // old item and new item are the actual items when they are in existence\n-            Win32Program olditem = new Win32Program\n-            {\n-                Name = \"oldpath\",\n-                ExecutableName = oldpath,\n-                FullPath = linkingTo,\n-            };\n-\n-            Win32Program newitem = new Win32Program\n-            {\n-                Name = \"path\",\n-                ExecutableName = path,\n-                FullPath = linkingTo,\n-            };\n+            Win32Program olditem = Win32Program.GetAppFromPath(oldFullPath);\n+            Win32Program newitem = Win32Program.GetAppFromPath(newFullPath);\n \n             win32ProgramRepository.Add(olditem);\n \n"}
{"id": 192, "ground_truth": "Resource Leak", "predicted": "Resource leak", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nhttps://github.com/square/okhttp/actions/runs/7593827185/job/20684688664\r\n\r\n\r\n```\r\nHttpOverHttp2Test > concurrentRequestWithEmptyFlowControlWindow(Protocol, MockWebServer) > [1] h2_prior_knowledge FAILED\r\n    java.lang.AssertionError: Timed out waiting for log message.\r\n        at okhttp3.TestLogHandler.take(TestLogHandler.kt:92)\r\n        at okhttp3.internal.http2.HttpOverHttp2Test.waitForDataFrames(HttpOverHttp2Test.kt:425)\r\n        at okhttp3.internal.http2.HttpOverHttp2Test.concurrentRequestWithEmptyFlowControlWindow(HttpOverHttp2Test.kt:481)\r\n```        \r\n\r\nSteps \r\n1. FakeFileSystem cache to remove that issue\r\n2. Close requests cleanly\r\n3. Verify in CI \n\n### Code Patch:\n@@ -24,7 +24,6 @@ import assertk.assertions.isFalse\n import assertk.assertions.isNull\n import assertk.assertions.isTrue\n import assertk.fail\n-import java.io.File\n import java.io.IOException\n import java.net.HttpURLConnection\n import java.net.SocketTimeoutException\n@@ -86,15 +85,16 @@ import okhttp3.tls.HandshakeCertificates\n import okio.Buffer\n import okio.BufferedSink\n import okio.GzipSink\n+import okio.Path.Companion.toPath\n import okio.buffer\n+import okio.fakefilesystem.FakeFileSystem\n import org.junit.jupiter.api.AfterEach\n import org.junit.jupiter.api.Assertions.assertArrayEquals\n import org.junit.jupiter.api.Assumptions.assumeTrue\n import org.junit.jupiter.api.Disabled\n import org.junit.jupiter.api.Tag\n import org.junit.jupiter.api.Timeout\n import org.junit.jupiter.api.extension.RegisterExtension\n-import org.junit.jupiter.api.io.TempDir\n import org.junit.jupiter.params.ParameterizedTest\n import org.junit.jupiter.params.provider.ArgumentsSource\n \n@@ -107,9 +107,6 @@ class HttpOverHttp2Test {\n     override fun arguments() = listOf(Protocol.H2_PRIOR_KNOWLEDGE, Protocol.HTTP_2)\n   }\n \n-  @TempDir\n-  lateinit var tempDir: File\n-\n   @RegisterExtension\n   val platform: PlatformRule = PlatformRule()\n \n@@ -127,7 +124,8 @@ class HttpOverHttp2Test {\n   private lateinit var server: MockWebServer\n   private lateinit var protocol: Protocol\n   private lateinit var client: OkHttpClient\n-  private lateinit var cache: Cache\n+  private val fileSystem: FakeFileSystem = FakeFileSystem()\n+  private val cache: Cache = Cache(\"/tmp/cache\".toPath(), Long.MAX_VALUE, fileSystem)\n   private lateinit var scheme: String\n \n   private fun configureClientTestRule(): OkHttpClientTestRule {\n@@ -164,10 +162,13 @@ class HttpOverHttp2Test {\n           .build()\n       scheme = \"http\"\n     }\n-    cache = Cache(tempDir, Int.MAX_VALUE.toLong())\n   }\n \n   @AfterEach fun tearDown() {\n+//    TODO reenable after https://github.com/square/okhttp/issues/8206\n+//    fileSystem.checkNoOpenFiles()\n+    cache.close()\n+\n     java.net.Authenticator.setDefault(null)\n   }\n \n"}
{"id": 152, "ground_truth": "Unordered Data", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe problem is that in the yoga example's training data there are 5 classes but when using any other amount of classes as training data the training fails. Set the output layer to the actual number of classes instead.\r\n\r\nWithout this fix, a ValueError is raised (e.g. `ValueError: Shapes (None, 7) and (None, 5) are incompatible`).\n\n### Code Patch:\n@@ -1015,7 +1015,7 @@\n         \"layer = keras.layers.Dropout(0.5)(layer)\\n\",\n         \"layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\\n\",\n         \"layer = keras.layers.Dropout(0.5)(layer)\\n\",\n-        \"outputs = keras.layers.Dense(5, activation=\\\"softmax\\\")(layer)\\n\",\n+        \"outputs = keras.layers.Dense(len(class_names), activation=\\\"softmax\\\")(layer)\\n\",\n         \"\\n\",\n         \"model = keras.Model(inputs, outputs)\\n\",\n         \"model.summary()\"\n"}
{"id": 147, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe test failed in\r\n<https://lab.llvm.org/buildbot/#/builders/162/builds/5785> due to frame variable not being in stop commands, even though the DAP log shows the command being present there. I'm pretty sure this is a race in the test the collection of the test log. I fix that by making sure we wait for the expected output, and also by increasing the timeout (1s is cutting it very close).\r\n\r\nThe arm failure link is no longer functional, but I'm fairly certain that this was the cause of those flakes as well.\n\n### Code Patch:\n@@ -116,9 +116,6 @@ def test_by_name_waitFor(self):\n \n     @skipIfDarwin\n     @skipIfNetBSD  # Hangs on NetBSD as well\n-    @skipIf(\n-        archs=[\"arm\", \"aarch64\"]\n-    )  # Example of a flaky run http://lab.llvm.org:8011/builders/lldb-aarch64-ubuntu/builds/5527/steps/test/logs/stdio\n     def test_commands(self):\n         \"\"\"\n         Tests the \"initCommands\", \"preRunCommands\", \"stopCommands\",\n@@ -152,7 +149,7 @@ def test_commands(self):\n         initCommands = [\"target list\", \"platform list\"]\n         preRunCommands = [\"image list a.out\", \"image dump sections a.out\"]\n         postRunCommands = [\"help trace\", \"help process trace\"]\n-        stopCommands = [\"frame variable\", \"bt\"]\n+        stopCommands = [\"frame variable\", \"thread backtrace\"]\n         exitCommands = [\"expr 2+3\", \"expr 3+4\"]\n         terminateCommands = [\"expr 4+2\"]\n         self.attach(\n@@ -179,7 +176,7 @@ def test_commands(self):\n         breakpoint_ids = self.set_function_breakpoints(functions)\n         self.assertEqual(len(breakpoint_ids), len(functions), \"expect one breakpoint\")\n         self.continue_to_breakpoints(breakpoint_ids)\n-        output = self.get_console(timeout=1.0)\n+        output = self.collect_console(timeout_secs=10, pattern=stopCommands[-1])\n         self.verify_commands(\"stopCommands\", output, stopCommands)\n \n         # Continue after launch and hit the \"pause()\" call and stop the target.\n@@ -189,7 +186,7 @@ def test_commands(self):\n         time.sleep(0.5)\n         self.dap_server.request_pause()\n         self.dap_server.wait_for_stopped()\n-        output = self.get_console(timeout=1.0)\n+        output = self.collect_console(timeout_secs=10, pattern=stopCommands[-1])\n         self.verify_commands(\"stopCommands\", output, stopCommands)\n \n         # Continue until the program exits\n@@ -198,7 +195,7 @@ def test_commands(self):\n         # \"exitCommands\" that were run after the second breakpoint was hit\n         # and the \"terminateCommands\" due to the debugging session ending\n         output = self.collect_console(\n-            timeout_secs=1.0,\n+            timeout_secs=10.0,\n             pattern=terminateCommands[0],\n         )\n         self.verify_commands(\"exitCommands\", output, exitCommands)\n"}
{"id": 228, "ground_truth": "Time", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nI have written integration code which would allow conditionExpressions to be xpath and evaluator for it\n\nalso test case is added to illustrate the use of xpath\n\n\n### Code Patch:\n@@ -72,6 +72,11 @@\n import bitronix.tm.TransactionManagerServices;\n import bitronix.tm.resource.jdbc.PoolingDataSource;\n \n+import org.w3c.dom.Document;\n+import javax.xml.parsers.DocumentBuilderFactory;\n+\n+import java.io.ByteArrayInputStream;\n+\n public class SimpleBPMNProcessTest extends JbpmTestCase {\n \n \tprivate PoolingDataSource ds1;\n@@ -907,6 +912,20 @@ public void testNoneIntermediateThrow() throws Exception {\n         ProcessInstance processInstance = ksession.startProcess(\"NoneIntermediateEvent\", null);\n         assertEquals(ProcessInstance.STATE_COMPLETED, processInstance.getState());\n     }\n+\n+    public void testXpathExpression() throws Exception {\n+                KnowledgeBase kbase = createKnowledgeBase(\"BPMN2-XpathExpression.bpmn2\");\n+                StatefulKnowledgeSession ksession = createKnowledgeSession(kbase);\n+        Document document = DocumentBuilderFactory.newInstance().newDocumentBuilder()\n+            .parse(new ByteArrayInputStream(\n+                \"<instanceMetadata><user approved=\\\"false\\\" id=\\\"58735964413\\\"/></instanceMetadata>\".getBytes()));\n+        Map<String, Object> params = new HashMap<String, Object>();\n+        params.put(\"instanceMetadata\", document);\n+\n+                ProcessInstance processInstance = ksession.startProcess(\"594975243920585248\", params);\n+                assertTrue(processInstance.getState() == ProcessInstance.STATE_COMPLETED);\n+    }\n+\n     \n \tprivate KnowledgeBase createKnowledgeBase(String process) throws Exception {\n \t\tKnowledgeBuilderConfiguration conf = KnowledgeBuilderFactory.newKnowledgeBuilderConfiguration();\n\n@@ -0,0 +1,32 @@\n+<bpmn2:definitions xmlns:bpmn2='http://www.omg.org/spec/BPMN/20100524/MODEL' xmlns:xrm='http://www.intalio.com/cloud/xrm' typeLanguage='http://www.java.com/javaTypes' expressionLanguage='http://www.mvel.org/2.0' id='594975243920585248' targetNamespace=''>\n+   <bpmn2:import importType='xrm' namespace='http://www.intalio.com/cloud/xrm'/>\n+   <bpmn2:itemDefinition id='objectType' structureRef='java.lang.Object'/>\n+  <bpmn2:process id='594975243920585248'>\n+  <bpmn2:property id='instanceMetadata' itemSubjectRef='objectType'/>\n+<bpmn2:property id='processsMetadata' itemSubjectRef='objectType'/>\n+<bpmn2:startEvent id='_5DF6C746-EEDF-4D39-A953-B529A0320D24' name=''/>\n+<bpmn2:exclusiveGateway id='_81070F07-C929-4C93-8B08-18FF6D319F6A' name='' gatewayDirection='Diverging'/>\n+<bpmn2:sequenceFlow id='_036CFFA5-945F-4597-BD74-DBFBA5553D7D' sourceRef='_5DF6C746-EEDF-4D39-A953-B529A0320D24' targetRef='_81070F07-C929-4C93-8B08-18FF6D319F6A'/>\n+  <bpmn2:scriptTask id='_BF09B923-BE11-4447-BECB-609BD86C59EF' name=''>\n+  <bpmn2:ioSpecification><bpmn2:inputSet/>\n+<bpmn2:outputSet/>\n+</bpmn2:ioSpecification>\n+<bpmn2:script>System.out.println(\"Task 1\");</bpmn2:script>\n+</bpmn2:scriptTask>\n+<bpmn2:sequenceFlow id='_E6BD9F03-8580-4AD8-A5CC-6D1F416E514C' sourceRef='_81070F07-C929-4C93-8B08-18FF6D319F6A' targetRef='_BF09B923-BE11-4447-BECB-609BD86C59EF'>    <conditionExpression language='http://www.w3.org/1999/XPath'>count($instanceMetadata/user[@approved='true']) = 1</conditionExpression>\n+</bpmn2:sequenceFlow>\n+  <bpmn2:scriptTask id='_7CA18647-714B-44D5-8DEB-544709C4221A' name=''>\n+  <bpmn2:ioSpecification><bpmn2:inputSet/>\n+<bpmn2:outputSet/>\n+</bpmn2:ioSpecification>\n+<bpmn2:script>System.out.println(\"Task 2\");</bpmn2:script>\n+</bpmn2:scriptTask>\n+<bpmn2:sequenceFlow id='_ED6FD301-F8C0-4241-A8FF-A1F7756652B7' sourceRef='_81070F07-C929-4C93-8B08-18FF6D319F6A' targetRef='_7CA18647-714B-44D5-8DEB-544709C4221A'>    <conditionExpression language='http://www.w3.org/1999/XPath'>count($instanceMetadata/user[@approved='false']) = 1</conditionExpression>\n+</bpmn2:sequenceFlow>\n+<bpmn2:sequenceFlow id='_2BF3E89A-B28F-40E3-9A9B-A982206BF500' sourceRef='_7CA18647-714B-44D5-8DEB-544709C4221A' targetRef='_FB90F2CD-5CA2-4F85-88C0-022B03130BAE'/>\n+<bpmn2:endEvent id='_C1D067D7-3596-45F7-9B75-69F60C3442F1' name=''/>\n+<bpmn2:sequenceFlow id='_119E4948-40CF-4C76-9988-22C706C1AB1A' sourceRef='_FB90F2CD-5CA2-4F85-88C0-022B03130BAE' targetRef='_C1D067D7-3596-45F7-9B75-69F60C3442F1'/>\n+<bpmn2:exclusiveGateway id='_FB90F2CD-5CA2-4F85-88C0-022B03130BAE' name='' gatewayDirection='Converging'/>\n+<bpmn2:sequenceFlow id='_0849EEE3-09A2-4E99-A232-E1EA8B8C4121' sourceRef='_BF09B923-BE11-4447-BECB-609BD86C59EF' targetRef='_FB90F2CD-5CA2-4F85-88C0-022B03130BAE'/>\n+</bpmn2:process>\n+</bpmn2:definitions>\n\n@@ -5,6 +5,7 @@\n \n import org.jbpm.process.builder.dialect.java.JavaProcessDialect;\n import org.jbpm.process.builder.dialect.mvel.MVELProcessDialect;\n+import org.jbpm.process.builder.dialect.xpath.XPATHProcessDialect;\n \n public class ProcessDialectRegistry {\n \t\n@@ -14,6 +15,7 @@ public class ProcessDialectRegistry {\n \t\t dialects = new HashMap<String, ProcessDialect>();\n \t\t dialects.put(\"java\", new JavaProcessDialect());\n \t\t dialects.put(\"mvel\", new MVELProcessDialect());\n+\t\t dialects.put(\"XPath\", new XPATHProcessDialect());\n \t}\n \t\n \tpublic static ProcessDialect getDialect(String dialect) {\n\n@@ -0,0 +1,47 @@\n+/*\n+  Copyright 2010 Intalio Inc\n+\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.jbpm.process.builder.dialect.xpath;\n+\n+import org.jbpm.process.builder.ActionBuilder;\n+import org.jbpm.process.builder.ProcessBuildContext;\n+import org.jbpm.process.builder.ProcessClassBuilder;\n+import org.jbpm.process.builder.ReturnValueEvaluatorBuilder;\n+import org.jbpm.process.builder.dialect.ProcessDialect;\n+\n+public class XPATHProcessDialect implements ProcessDialect {\n+\n+//\tprivate static final ActionBuilder actionBuilder = new XPATHActionBuilder();\n+\tprivate static final ReturnValueEvaluatorBuilder returnValueBuilder = new XPATHReturnValueEvaluatorBuilder();\n+\t\n+\tpublic void addProcess(final ProcessBuildContext context) {\n+        // @TODO setup line mappings\n+\t}\n+\n+\tpublic ActionBuilder getActionBuilder() {\n+//\t\treturn actionBuilder;\n+\t\tthrow new UnsupportedOperationException( \"XPATHProcessDialect.getActionBuilder is not supported\" );\n+\t}\n+\n+\tpublic ProcessClassBuilder getProcessClassBuilder() {\n+        throw new UnsupportedOperationException( \"XPATHLProcessDialect.getProcessClassBuilder is not supported\" );\n+\t}\n+\n+\tpublic ReturnValueEvaluatorBuilder getReturnValueEvaluatorBuilder() {\n+\t\treturn returnValueBuilder;\n+\t}\n+\n+}\n\n@@ -0,0 +1,58 @@\n+/*\r\n+  Copyright 2010 Intalio Inc\r\n+\r\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n+ * you may not use this file except in compliance with the License.\r\n+ * You may obtain a copy of the License at\r\n+\r\n+       http://www.apache.org/licenses/LICENSE-2.0\r\n+\r\n+  Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.jbpm.process.builder.dialect.xpath;\r\n+\r\n+import org.drools.compiler.DescrBuildError;\r\n+import org.drools.compiler.ReturnValueDescr;\r\n+import org.jbpm.process.builder.ReturnValueEvaluatorBuilder;\r\n+import org.jbpm.process.core.ContextResolver;\r\n+import org.jbpm.process.instance.impl.ReturnValueConstraintEvaluator;\r\n+import org.jbpm.process.instance.impl.XPATHReturnValueEvaluator;\r\n+import org.drools.rule.builder.PackageBuildContext;\r\n+\r\n+public class XPATHReturnValueEvaluatorBuilder\r\n+    implements\r\n+    ReturnValueEvaluatorBuilder {\r\n+\r\n+    public XPATHReturnValueEvaluatorBuilder() {\r\n+\r\n+    }\r\n+\r\n+    public void build(final PackageBuildContext context,\r\n+                      final ReturnValueConstraintEvaluator constraintNode,\r\n+                      final ReturnValueDescr descr,\r\n+                      final ContextResolver contextResolver) {\r\n+\r\n+        String text = descr.getText();\r\n+\r\n+        try {\r\n+//            XPATHReturnValueEvaluator expr = new XPATHReturnValueEvaluator( text,\r\n+//                                                                          context.getDialect( \"XPath\" ).getId() );\r\n+            XPATHReturnValueEvaluator expr = new XPATHReturnValueEvaluator( text,\r\n+                    null );\r\n+\r\n+            constraintNode.setEvaluator( expr );\r\n+            \r\n+        } catch ( final Exception e ) {\r\n+            context.getErrors().add( new DescrBuildError( context.getParentDescr(),\r\n+                                                          descr,\r\n+                                                          null,\r\n+                                                          \"Unable to build expression for 'constraint' \" + descr.getText() + \"': \" + e ) );\r\n+        }\r\n+    }\r\n+\r\n+}\r\n\n@@ -0,0 +1,101 @@\n+/*\r\n+  Copyright 2010 Intalio Inc\r\n+\r\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n+ * you may not use this file except in compliance with the License.\r\n+ * You may obtain a copy of the License at\r\n+\r\n+       http://www.apache.org/licenses/LICENSE-2.0\r\n+\r\n+  Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.jbpm.process.instance.impl;\r\n+\r\n+import java.io.Externalizable;\r\n+import java.io.IOException;\r\n+import java.io.ObjectInput;\r\n+import java.io.ObjectOutput;\r\n+\r\n+import javax.xml.namespace.QName;\r\n+import javax.xml.parsers.DocumentBuilder;\r\n+import javax.xml.parsers.DocumentBuilderFactory;\r\n+import javax.xml.xpath.XPath;\r\n+import javax.xml.xpath.XPathConstants;\r\n+import javax.xml.xpath.XPathExpression;\r\n+import javax.xml.xpath.XPathFactory;\r\n+import javax.xml.xpath.XPathVariableResolver;\r\n+\r\n+import org.drools.runtime.process.ProcessContext;\r\n+\r\n+public class XPATHReturnValueEvaluator\r\n+    implements\r\n+    ReturnValueEvaluator,\r\n+    Externalizable {\r\n+    private static final long   serialVersionUID = 510l;\r\n+\r\n+    private String              xpath;\r\n+    private String              id;\r\n+\r\n+    public XPATHReturnValueEvaluator() {\r\n+    }\r\n+\r\n+    public XPATHReturnValueEvaluator(final String xpath,\r\n+                                    final String id) {\r\n+        this.xpath = xpath;\r\n+        this.id = id;\r\n+    }\r\n+\r\n+    public void readExternal(ObjectInput in) throws IOException,\r\n+                                            ClassNotFoundException {\r\n+        id = in.readUTF();\r\n+        xpath = (String) in.readObject();\r\n+    }\r\n+\r\n+    public void writeExternal(ObjectOutput out) throws IOException {\r\n+        out.writeUTF( id );\r\n+        out.writeObject(xpath);\r\n+    }\r\n+\r\n+    public String getDialect() {\r\n+        return this.id;\r\n+    }\r\n+\r\n+    public Object evaluate(final ProcessContext context) throws Exception {        \r\n+    \tXPathFactory factory = XPathFactory.newInstance();\r\n+    \tXPath xpath = factory.newXPath();\r\n+    \txpath.setXPathVariableResolver( \r\n+    \t\t\tnew  XPathVariableResolver() {\r\n+    \t\t\t\tpublic Object resolveVariable(QName var)\r\n+    \t\t\t\t{\r\n+    \t\t\t\t\tif (var == null) {\r\n+    \t\t\t\t\t\tthrow new NullPointerException(\"The variable name cannot be null\");\r\n+    \t\t\t\t\t}\r\n+\r\n+    \t\t\t\t\tif (context.getVariable(var.getLocalPart()) != null) {\r\n+    \t\t\t\t\t\treturn context.getVariable(var.getLocalPart());\r\n+    \t\t\t\t\t}\r\n+    \t\t\t\t\telse {\r\n+    \t\t\t\t\t\treturn null;\r\n+    \t\t\t\t\t}\r\n+    \t\t\t\t}\r\n+\r\n+    \t\t\t}\r\n+    \t);\r\n+        \r\n+        XPathExpression expr \r\n+         = xpath.compile(this.xpath);\r\n+\r\n+\t\tDocumentBuilder builder = DocumentBuilderFactory.newInstance().newDocumentBuilder();\r\n+        return expr.evaluate(builder.newDocument(), XPathConstants.BOOLEAN);\r\n+    }\r\n+\r\n+    public String toString() {\r\n+        return this.xpath;\r\n+    }    \r\n+\r\n+}\r\n"}
{"id": 365, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\n    Keep PR title verbose enough and add prefix telling\r\n    about what components it touches e.g \"query:\" or \".*:\"\r\n-->\r\n\r\n<!--\r\n    Don't forget about CHANGELOG!\r\n\r\n    Changelog entry format:\r\n    - [#<PR-id>](<PR-URL>) Thanos <Component> ...\r\n\r\n    <PR-id> Id of your pull request.\r\n    <PR-URL> URL of your PR such as https://github.com/thanos-io/thanos/pull/<PR-id>\r\n    <Component> Component affected by your changes such as Query, Store, Receive.\r\n-->\r\n\r\n* [x] I added CHANGELOG entry for this change.\r\n* [x] Change is not relevant to the end user.\r\n\r\n## Changes\r\n\r\nFixes: #8115\r\n\r\n<!-- Enumerate changes you made -->\r\n\r\n## Verification\r\n\r\n<!-- How you tested it? How do you know it works? -->\r\n\n\n### Code Patch:\n@@ -12,6 +12,7 @@ We use *breaking :warning:* to mark changes that are not backward compatible (re\n \n ### Fixed\n \n+- [#8393](https://github.com/thanos-io/thanos/pull/8393) Tests: fix flaky TestCompactWithStoreGateway by adding proper readiness checks\n - [#8334](https://github.com/thanos-io/thanos/pull/8334) Query: wait for initial endpoint discovery before becoming ready\n \n ### Added\n\n@@ -469,6 +469,10 @@ func testCompactWithStoreGateway(t *testing.T, penaltyDedup bool) {\n \tq := e2ethanos.NewQuerierBuilder(e, \"1\", str.InternalEndpoint(\"grpc\")).Init()\n \ttestutil.Ok(t, e2e.StartAndWaitReady(q))\n \n+\ttestutil.Ok(t, q.WaitSumMetricsWithOptions(e2emon.Equals(1), []string{\"thanos_store_nodes_grpc_connections\"}, e2emon.WaitMissingMetrics(), e2emon.WithLabelMatchers(\n+\t\tmatchers.MustNewMatcher(matchers.MatchEqual, \"store_type\", \"store\"),\n+\t)))\n+\n \tctx, cancel = context.WithTimeout(context.Background(), 3*time.Minute)\n \tt.Cleanup(cancel)\n \n"}
{"id": 106, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n# Which issue does this PR close?\r\n\r\n<!--\r\nWe generally require a GitHub issue to be filed for all bug fixes and enhancements and this helps us generate change logs for our releases. You can link an issue to this PR using the GitHub syntax. For example `Closes #123` indicates that this PR will close issue #123.\r\n-->\r\n\r\nCloses #.\r\n\r\n# Rationale for this change\r\n \r\n<!--\r\nWhy are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\nExplaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.\r\n-->\r\n\r\nWe occasionally see this test fail as it takes more than a millisecond on very contended CI runners for the asynchronous cleanup to be performed - https://github.com/apache/arrow-rs/actions/runs/8690327829/job/23830012773\r\n\r\n# What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n# Are there any user-facing changes?\r\n\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!---\r\nIf there are any breaking changes to public APIs, please add the `breaking change` label.\r\n-->\r\n\n\n### Code Patch:\n@@ -1450,10 +1450,14 @@ mod not_wasm_tests {\n         assert_eq!(file_count, 1);\n         drop(upload);\n \n-        tokio::time::sleep(Duration::from_millis(1)).await;\n-\n-        let file_count = std::fs::read_dir(root.path()).unwrap().count();\n-        assert_eq!(file_count, 0);\n+        for _ in 0..100 {\n+            tokio::time::sleep(Duration::from_millis(1)).await;\n+            let file_count = std::fs::read_dir(root.path()).unwrap().count();\n+            if file_count == 0 {\n+                return;\n+            }\n+        }\n+        panic!(\"Failed to cleanup file in 100ms\")\n     }\n }\n \n"}
{"id": 68, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Summary\r\n\r\nThis is my guess as to the source of the resolver flake, based on information and extensive debugging from @zanieb. In short, if we rely on `self.index.packages` as a source of truth during error reporting, we open ourselves up to a source of non-determinism, because we fetch package metadata asynchronously in the background while we solve -- so packages _could_ be included in or excluded from the index depending on the order in which those requests are returned.\r\n\r\nSo, instead, we now track the set of packages that _were_ visited by the solver. Visiting a package _requires_ that we wait for its metadata to be available. By limiting analysis to those packages that were visited during solving, we are faithfully representing the state of the solver at the time of failure.\r\n\r\nCloses #863 \n\n### Code Patch:\n@@ -1,13 +1,3 @@\n [profile.default]\n # Mark tests that take longer than 10s as slow\n slow-timeout = \"10s\"\n-\n-[[profile.default.overrides]]\n-# Some of these tests have a non-determinstic snapshot\n-filter = 'binary(pip_install_scenarios)'\n-retries = 2\n-\n-[[profile.no-retry.overrides]]\n-# An optional profile to avoid retries\n-filter = 'binary(pip_install_scenarios)'\n-retries = 0\n\n@@ -2,6 +2,7 @@ use std::collections::BTreeSet;\n use std::convert::Infallible;\n use std::fmt::Formatter;\n \n+use dashmap::DashSet;\n use indexmap::IndexMap;\n use pubgrub::range::Range;\n use pubgrub::report::{DefaultStringReporter, DerivationTree, Reporter};\n@@ -167,6 +168,7 @@ impl NoSolutionError {\n     pub(crate) fn with_available_versions(\n         mut self,\n         python_requirement: &PythonRequirement,\n+        visited: &DashSet<PackageName>,\n         package_versions: &OnceMap<PackageName, VersionMap>,\n     ) -> Self {\n         let mut available_versions = IndexMap::default();\n@@ -186,15 +188,21 @@ impl NoSolutionError {\n                     );\n                 }\n                 PubGrubPackage::Package(name, ..) => {\n-                    if let Some(entry) = package_versions.get(name) {\n-                        let version_map = entry.value();\n-                        available_versions.insert(\n-                            package.clone(),\n-                            version_map\n-                                .iter()\n-                                .map(|(version, _)| version.clone())\n-                                .collect(),\n-                        );\n+                    // Avoid including available versions for packages that exist in the derivation\n+                    // tree, but were never visited during resolution. We _may_ have metadata for\n+                    // these packages, but it's non-deterministic, and omitting them ensures that\n+                    // we represent the state of the resolver at the time of failure.\n+                    if visited.contains(name) {\n+                        if let Some(entry) = package_versions.get(name) {\n+                            let version_map = entry.value();\n+                            available_versions.insert(\n+                                package.clone(),\n+                                version_map\n+                                    .iter()\n+                                    .map(|(version, _)| version.clone())\n+                                    .collect(),\n+                            );\n+                        }\n                     }\n                 }\n             }\n\n@@ -3,7 +3,7 @@\n use std::sync::Arc;\n \n use anyhow::Result;\n-use dashmap::DashMap;\n+use dashmap::{DashMap, DashSet};\n use futures::channel::mpsc::UnboundedReceiver;\n use futures::{pin_mut, FutureExt, StreamExt};\n use itertools::Itertools;\n@@ -68,6 +68,8 @@ pub struct Resolver<'a, Provider: ResolverProvider> {\n     index: &'a InMemoryIndex,\n     /// A map from [`PackageId`] to the `Requires-Python` version specifiers for that package.\n     incompatibilities: DashMap<PackageId, VersionSpecifiers>,\n+    /// The set of all registry-based packages visited during resolution.\n+    visited: DashSet<PackageName>,\n     editables: FxHashMap<PackageName, (LocalEditable, Metadata21)>,\n     reporter: Option<Arc<dyn Reporter>>,\n     provider: Provider,\n@@ -167,6 +169,7 @@ impl<'a, Provider: ResolverProvider> Resolver<'a, Provider> {\n         Self {\n             index,\n             incompatibilities: DashMap::default(),\n+            visited: DashSet::default(),\n             selector,\n             allowed_urls,\n             project: manifest.project,\n@@ -223,7 +226,7 @@ impl<'a, Provider: ResolverProvider> Resolver<'a, Provider> {\n                     if let ResolveError::NoSolution(err) = err {\n                         ResolveError::NoSolution(\n                             err\n-                            .with_available_versions(&self.python_requirement, &self.index.packages)\n+                            .with_available_versions(&self.python_requirement, &self.visited, &self.index.packages)\n                             .with_selector(self.selector.clone())\n                             .with_python_requirement(&self.python_requirement)\n                         )\n@@ -504,6 +507,7 @@ impl<'a, Provider: ResolverProvider> Resolver<'a, Provider> {\n                 // Wait for the metadata to be available.\n                 let entry = self.index.packages.wait(package_name).await?;\n                 let version_map = entry.value();\n+                self.visited.insert(package_name.clone());\n \n                 if let Some(extra) = extra {\n                     debug!(\n\n@@ -566,8 +566,8 @@ fn dependency_excludes_range_of_compatible_versions() -> Result<()> {\n \n               Because only albatross<=3.0.0 is available and albatross==3.0.0 depends on bluebird==3.0.0, we can conclude that albatross>=3.0.0 depends on bluebird==3.0.0.\n               And because we know from (2) that all versions of crow, bluebird!=1.0.0, albatross<3.0.0 are incompatible, we can conclude that all versions of crow depend on one of:\n-                  bluebird<=1.0.0\n-                  bluebird>=3.0.0\n+                  bluebird==1.0.0\n+                  bluebird==3.0.0\n \n               And because you require crow and you require bluebird>=2.0.0,<3.0.0, we can conclude that the requirements are unsatisfiable.\n         \"###);\n@@ -697,8 +697,8 @@ fn dependency_excludes_non_contiguous_range_of_compatible_versions() -> Result<(\n \n               Because only albatross<=3.0.0 is available and albatross==3.0.0 depends on bluebird==3.0.0, we can conclude that albatross>=3.0.0 depends on bluebird==3.0.0.\n               And because we know from (2) that all versions of crow, bluebird!=1.0.0, albatross<3.0.0 are incompatible, we can conclude that all versions of crow depend on one of:\n-                  bluebird<=1.0.0\n-                  bluebird>=3.0.0\n+                  bluebird==1.0.0\n+                  bluebird==3.0.0\n \n               And because you require crow and you require bluebird>=2.0.0,<3.0.0, we can conclude that the requirements are unsatisfiable.\n         \"###);\n"}
{"id": 238, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nCI test **darwin://python/ray/tests:test_multi_node** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge-macos/builds/7162#01987fcf-d2dd-40d7-ab47-675bc7df8452\n\t- https://buildkite.com/ray-project/postmerge-macos/builds/7162#01987f0b-3872-4031-a057-406c38641c46\n\nDataCaseName-darwin://python/ray/tests:test_multi_node-END\nManaged by OSS Test Policy\n\n### Code Patch:\n@@ -78,14 +78,17 @@ def f():\n     def all_workers_exited():\n         result = True\n         print(\"list of idle workers:\")\n-        for proc in psutil.process_iter():\n-            if ray_constants.WORKER_PROCESS_TYPE_IDLE_WORKER in proc.name():\n+        for proc in psutil.process_iter(attrs=[\"name\"], ad_value=None):\n+            if (\n+                proc.info[\"name\"]\n+                and ray_constants.WORKER_PROCESS_TYPE_IDLE_WORKER in proc.info[\"name\"]\n+            ):\n                 print(f\"{proc}\")\n                 result = False\n         return result\n \n     # Check that workers are eventually cleaned up.\n-    wait_for_condition(all_workers_exited, timeout=15, retry_interval_ms=1000)\n+    wait_for_condition(all_workers_exited, timeout=30, retry_interval_ms=1000)\n \n \n def test_error_isolation(call_ray_start):\n"}
{"id": 420, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #13526\n\n### Code Patch:\n@@ -210,13 +210,13 @@ public void expectMessage(BlockingQueue<Frame> framesQueue, byte expectedDataOp,\n     {\n         ByteBuffer actualPayload = ByteBuffer.allocate(expectedMessage.remaining());\n \n-        Frame frame = framesQueue.poll(1, TimeUnit.SECONDS);\n+        Frame frame = framesQueue.poll(5, TimeUnit.SECONDS);\n         assertThat(\"Initial Frame.opCode\", frame.getOpCode(), is(expectedDataOp));\n \n         actualPayload.put(frame.getPayload());\n         while (!frame.isFin())\n         {\n-            frame = framesQueue.poll(1, TimeUnit.SECONDS);\n+            frame = framesQueue.poll(5, TimeUnit.SECONDS);\n             assertThat(\"Frame.opCode\", frame.getOpCode(), is(OpCode.CONTINUATION));\n             actualPayload.put(frame.getPayload());\n         }\n@@ -234,7 +234,7 @@ public void assertExpected(BlockingQueue<Frame> framesQueue, List<Frame> expect)\n             prefix = \"Frame[\" + i + \"]\";\n \n             Frame expected = expect.get(i);\n-            Frame actual = framesQueue.poll(3, TimeUnit.SECONDS);\n+            Frame actual = framesQueue.poll(5, TimeUnit.SECONDS);\n             assertThat(prefix + \".poll\", actual, notNullValue());\n \n             if (LOG.isDebugEnabled())\n@@ -359,7 +359,7 @@ public void writeRaw(ByteBuffer buffer) throws IOException\n         {\n             try\n             {\n-                assertTrue(openLatch.await(1, TimeUnit.SECONDS));\n+                assertTrue(openLatch.await(5, TimeUnit.SECONDS));\n             }\n             catch (InterruptedException e)\n             {\n\n@@ -210,13 +210,13 @@ public void expectMessage(BlockingQueue<Frame> framesQueue, byte expectedDataOp,\n     {\n         ByteBuffer actualPayload = ByteBuffer.allocate(expectedMessage.remaining());\n \n-        Frame frame = framesQueue.poll(1, TimeUnit.SECONDS);\n+        Frame frame = framesQueue.poll(5, TimeUnit.SECONDS);\n         assertThat(\"Initial Frame.opCode\", frame.getOpCode(), is(expectedDataOp));\n \n         actualPayload.put(frame.getPayload());\n         while (!frame.isFin())\n         {\n-            frame = framesQueue.poll(1, TimeUnit.SECONDS);\n+            frame = framesQueue.poll(5, TimeUnit.SECONDS);\n             assertThat(\"Frame.opCode\", frame.getOpCode(), is(OpCode.CONTINUATION));\n             actualPayload.put(frame.getPayload());\n         }\n@@ -234,7 +234,7 @@ public void assertExpected(BlockingQueue<Frame> framesQueue, List<Frame> expect)\n             prefix = \"Frame[\" + i + \"]\";\n \n             Frame expected = expect.get(i);\n-            Frame actual = framesQueue.poll(3, TimeUnit.SECONDS);\n+            Frame actual = framesQueue.poll(5, TimeUnit.SECONDS);\n             assertThat(prefix + \".poll\", actual, notNullValue());\n \n             if (LOG.isDebugEnabled())\n@@ -359,7 +359,7 @@ public void writeRaw(ByteBuffer buffer) throws IOException\n         {\n             try\n             {\n-                assertTrue(openLatch.await(1, TimeUnit.SECONDS));\n+                assertTrue(openLatch.await(5, TimeUnit.SECONDS));\n             }\n             catch (InterruptedException e)\n             {\n\n@@ -210,13 +210,13 @@ public void expectMessage(BlockingQueue<Frame> framesQueue, byte expectedDataOp,\n     {\n         ByteBuffer actualPayload = ByteBuffer.allocate(expectedMessage.remaining());\n \n-        Frame frame = framesQueue.poll(1, TimeUnit.SECONDS);\n+        Frame frame = framesQueue.poll(5, TimeUnit.SECONDS);\n         assertThat(\"Initial Frame.opCode\", frame.getOpCode(), is(expectedDataOp));\n \n         actualPayload.put(frame.getPayload());\n         while (!frame.isFin())\n         {\n-            frame = framesQueue.poll(1, TimeUnit.SECONDS);\n+            frame = framesQueue.poll(5, TimeUnit.SECONDS);\n             assertThat(\"Frame.opCode\", frame.getOpCode(), is(OpCode.CONTINUATION));\n             actualPayload.put(frame.getPayload());\n         }\n@@ -234,7 +234,7 @@ public void assertExpected(BlockingQueue<Frame> framesQueue, List<Frame> expect)\n             prefix = \"Frame[\" + i + \"]\";\n \n             Frame expected = expect.get(i);\n-            Frame actual = framesQueue.poll(3, TimeUnit.SECONDS);\n+            Frame actual = framesQueue.poll(5, TimeUnit.SECONDS);\n             assertThat(prefix + \".poll\", actual, notNullValue());\n \n             if (LOG.isDebugEnabled())\n@@ -359,7 +359,7 @@ public void writeRaw(ByteBuffer buffer) throws IOException\n         {\n             try\n             {\n-                assertTrue(openLatch.await(1, TimeUnit.SECONDS));\n+                assertTrue(openLatch.await(5, TimeUnit.SECONDS));\n             }\n             catch (InterruptedException e)\n             {\n"}
{"id": 394, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nRefs #56190\r\n\r\nThis PR does two things in the specific `test` case that is flaky:\r\n\r\n- Make the `composedSignal` a weak ref so the test won't hold any reference that could keep that variable from being GCed.\r\n- Add `gcUntil` so it will make more calls to `global.gc`. Precisely, until that condition is reached or a maximum number of attempts is hit.\n\n### Code Patch:\n@@ -134,25 +134,30 @@ it('does not prevent source signal from being GCed if it is short-lived', (t, do\n \n it('drops settled dependant signals when signal is composite', (t, done) => {\n   const controllers = Array.from({ length: 2 }, () => new AbortController());\n-  const composedSignal1 = AbortSignal.any([controllers[0].signal]);\n-  const composedSignalRef = new WeakRef(AbortSignal.any([composedSignal1, controllers[1].signal]));\n+\n+  // Using WeakRefs to avoid this test to retain information that will make the test fail\n+  const composedSignal1 = new WeakRef(AbortSignal.any([controllers[0].signal]));\n+  const composedSignalRef = new WeakRef(AbortSignal.any([composedSignal1.deref(), controllers[1].signal]));\n \n   const kDependantSignals = Object.getOwnPropertySymbols(controllers[0].signal).find(\n     (s) => s.toString() === 'Symbol(kDependantSignals)'\n   );\n \n-  setImmediate(() => {\n-    global.gc({ execution: 'async' }).then(() => {\n-      t.assert.strictEqual(composedSignalRef.deref(), undefined);\n-      t.assert.strictEqual(controllers[0].signal[kDependantSignals].size, 2);\n-      t.assert.strictEqual(controllers[1].signal[kDependantSignals].size, 1);\n-\n-      setImmediate(() => {\n-        t.assert.strictEqual(controllers[0].signal[kDependantSignals].size, 0);\n-        t.assert.strictEqual(controllers[1].signal[kDependantSignals].size, 0);\n+  t.assert.strictEqual(controllers[0].signal[kDependantSignals].size, 2);\n+  t.assert.strictEqual(controllers[1].signal[kDependantSignals].size, 1);\n \n-        done();\n+  setImmediate(() => {\n+    global.gc({ execution: 'async' }).then(async () => {\n+      await gcUntil('all signals are GCed', () => {\n+        const totalDependantSignals = Math.max(\n+          controllers[0].signal[kDependantSignals].size,\n+          controllers[1].signal[kDependantSignals].size\n+        );\n+\n+        return composedSignalRef.deref() === undefined && totalDependantSignals === 0;\n       });\n+\n+      done();\n     });\n   });\n });\n"}
{"id": 135, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR increases the max time we wait for a connect server to come up for testing. The current threshold is too low, and is causing flakyness.\r\n\r\n### Why are the changes needed?\r\nIt makes connect tests less flaky.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nIt is test infra code.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\n\n### Code Patch:\n@@ -24,6 +24,9 @@ import java.util.concurrent.TimeUnit\n import scala.concurrent.duration.FiniteDuration\n \n import org.scalatest.{BeforeAndAfterAll, Suite}\n+import org.scalatest.concurrent.Eventually.eventually\n+import org.scalatest.concurrent.Futures.timeout\n+import org.scalatest.time.SpanSugar._\n \n import org.apache.spark.SparkBuildInfo\n import org.apache.spark.sql.SparkSession\n@@ -184,12 +187,14 @@ object SparkConnectServerUtils {\n           .port(port)\n           .retryPolicy(RetryPolicy\n             .defaultPolicy()\n-            .copy(maxRetries = Some(7), maxBackoff = Some(FiniteDuration(10, \"s\"))))\n+            .copy(maxRetries = Some(10), maxBackoff = Some(FiniteDuration(30, \"s\"))))\n           .build())\n       .create()\n \n     // Execute an RPC which will get retried until the server is up.\n-    assert(spark.version == SparkBuildInfo.spark_version)\n+    eventually(timeout(1.minute)) {\n+      assert(spark.version == SparkBuildInfo.spark_version)\n+    }\n \n     // Auto-sync dependencies.\n     SparkConnectServerUtils.syncTestDependencies(spark)\n"}
{"id": 33, "ground_truth": "Concurrency", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #143878\n* #143865\n\nSummary:\nTest erroneously assumed that input/output sizes are same and that all\nstates are matchable.\n\nFixes issue #143798\n\nTest Plan:\nTest passes\n\nReviewers\nTest passes\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k\n\n### Code Patch:\n@@ -1,5 +1,6 @@\n # Owner(s): [\"oncall: distributed\"]\n \n+import math\n import pathlib\n import sys\n \n@@ -113,14 +114,22 @@ def test_match_one_event(self):\n \n     def test_all_events(self):\n         for collective in sorted(COLLECTIVES):\n+            input_sizes = [[4, 4]]\n+            output_sizes = [[4, 4]]\n+            expectedState = MatchState.FULLY_MATCHED\n+            if collective == \"_reduce_scatter_base\":\n+                input_sizes = [[4, 4]]\n+                output_sizes = [[input_sizes[0][0] * 2]]\n+            if collective == \"all_gather\":\n+                output_sizes = [[math.prod(input_sizes[0]) * 2]]\n+            if collective == \"all_to_all\":\n+                expectedState = MatchState.UNDECIDED\n             event = create_one_event(\n-                collective, (\"0\", \"default\"), [[4, 4]], [[4, 4]], \"scheduled\", 1\n+                collective, (\"0\", \"default\"), input_sizes, output_sizes, \"scheduled\", 1\n             )\n             membership = {\"0\": {0, 1}}\n-            self.assertEqual(\n-                match_one_event(event, event, membership, \"0\"), MatchState.FULLY_MATCHED\n-            )\n-            break\n+            result = match_one_event(event, event, membership, \"0\")\n+            self.assertEqual(result, expectedState)\n \n \n if __name__ == \"__main__\":\n"}
{"id": 380, "ground_truth": "Floating point operations", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFix flaky TFRT tensor utility unit test.\n\n\n### Code Patch:\n@@ -15,6 +15,7 @@ limitations under the License.\n #include \"tensorflow/core/tfrt/utils/tensor_util.h\"\n \n #include <complex>\n+#include <cstdint>\n #include <memory>\n #include <numeric>\n \n@@ -43,7 +44,10 @@ TEST(TensorUtilTest, DHTToTFTensor) {\n   std::iota(view.begin(), view.end(), 1);\n   auto tf_tensor = *TFRTTensorToTFTensor(dht);\n   EXPECT_THAT(tf_tensor.shape().dim_sizes(), testing::ElementsAre(2, 2));\n-  EXPECT_EQ(tf_tensor.data(), dht.data());\n+\n+  const int32_t* data = tf_tensor.flat<int32_t>().data();\n+  EXPECT_THAT(std::vector<int32_t>(data, data + 4),\n+              testing::ElementsAre(1, 2, 3, 4));\n }\n \n TEST(TensorUtilTest, SHTToTFTensor) {\n"}
{"id": 114, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n6.0.0\r\n\r\nRelease version:\r\n\r\n## Summary\r\n\r\nFix some flaky tests found on master:\r\n\r\n- persistence_enabled.mqttv5.t_takeover_clean_session_with_delayed_willmsg\r\n- emqx_prometheus_data_SUITE\r\n\r\n\r\n## PR Checklist\r\n<!--\r\nPlease convert the PR to a draft if any of the following conditions are not met.\r\n-->\r\n- [ ] For internal contributor: there is a jira ticket to track this change\r\n- [x] The changes are covered with new or existing tests\r\n- [ ] n/a Change log for changes visible by users has been added to `changes/ee/(feat|perf|fix|breaking)-<PR-id>.en.md` files\r\n- [x] Schema changes are backward compatible or intentionally breaking (describe the changes and the reasoning in the summary)\r\n\r\n<!--\r\nPlease, take in account the following guidelines while working on PR:\r\n* Try to achieve reasonable coverage of the new code\r\n* Add property-based tests for code that performs complex user input validation or implements a complex algorithm\r\n* Create a PR to [emqx-docs](https://github.com/emqx/emqx-docs) if documentation update is required, or make a follow-up jira ticket\r\n* Do not squash large PRs into a single commit, try to keep comprehensive history of incremental changes\r\n* Do not squash any significant amount of review fixes into the previous commits\r\n-->\r\n\r\n<!--\r\n## Checklist for CI (.github/workflows) changes\r\n- [ ] If changed package build workflow, pass [this action](https://github.com/emqx/emqx/actions/workflows/build_packages.yaml) (manual trigger)\r\n- [ ] Change log has been added to `changes/` dir for user-facing artifacts update\r\n-->\r\n\n\n### Code Patch:\n@@ -323,7 +323,12 @@ delete(\n     #{?collection_guard := Guard, ?id := ClientId}\n ) ->\n     Opts = #{\n-        db => ?DB, shard => {auto, ClientId}, generation => Generation, timeout => trans_timeout()\n+        db => ?DB,\n+        shard => {auto, ClientId},\n+        generation => Generation,\n+        timeout => trans_timeout(),\n+        retries => trans_retries(),\n+        retry_interval => trans_retry_interval()\n     },\n     {atomic, _, _} =\n         emqx_ds:trans(\n\n@@ -627,14 +627,16 @@ unpack_iterator(Shard, Iterator = #'Iterator'{}) ->\n     emqx_ds_storage_layer_ttv:unpack_iterator(Shard, Iterator).\n \n high_watermark(DBShard = {DB, Shard}, Stream = #'Stream'{}) ->\n-    Now = current_timestamp(DB, Shard),\n-    emqx_ds_storage_layer_ttv:high_watermark(DBShard, Stream, Now).\n+    maybe\n+        {ok, Now} ?= current_timestamp(DB, Shard),\n+        emqx_ds_storage_layer_ttv:high_watermark(DBShard, Stream, Now)\n+    end.\n \n fast_forward(DBShard = {DB, Shard}, It = #'Iterator'{}, Key, BatchSize) ->\n     ?IF_SHARD_READY(\n         DBShard,\n-        begin\n-            Now = current_timestamp(DB, Shard),\n+        maybe\n+            {ok, Now} ?= current_timestamp(DB, Shard),\n             emqx_ds_storage_layer_ttv:fast_forward(DBShard, It, Key, Now, BatchSize)\n         end\n     ).\n@@ -648,9 +650,9 @@ iterator_match_context(DBShard, Iterator = #'Iterator'{}) ->\n scan_stream(DBShard = {DB, Shard}, Stream = #'Stream'{}, TopicFilter, StartMsg, BatchSize) ->\n     ?IF_SHARD_READY(\n         DBShard,\n-        begin\n+        maybe\n             %% TODO: this has been changed during refactoring. Double-check.\n-            Now = current_timestamp(DB, Shard),\n+            {ok, Now} ?= current_timestamp(DB, Shard),\n             emqx_ds_storage_layer_ttv:scan_stream(\n                 DBShard, Stream, TopicFilter, Now, StartMsg, BatchSize\n             )\n@@ -737,10 +739,14 @@ otx_get_runtime_config(DB) ->\n -doc \"\"\"\n Messages have been replicated up to this timestamp on the local replica.\n \"\"\".\n--spec current_timestamp(emqx_ds:db(), emqx_ds:shard()) -> emqx_ds:time().\n+-spec current_timestamp(emqx_ds:db(), emqx_ds:shard()) -> {ok, emqx_ds:time()} | emqx_ds:error(_).\n current_timestamp(DB, Shard) ->\n-    {ok, Val} = emqx_dsch:gvar_get(DB, Shard, ?gv_sc_replica, ?gv_timestamp),\n-    Val.\n+    case emqx_dsch:gvar_get(DB, Shard, ?gv_sc_replica, ?gv_timestamp) of\n+        {ok, _} = Ok ->\n+            Ok;\n+        undefined ->\n+            ?err_rec(replica_offline)\n+    end.\n \n %%================================================================================\n %% RPC targets\n\n@@ -485,7 +485,12 @@ local_shard(DB, Shard, IsActive, Labels0) ->\n     end.\n \n current_timestamp(DB, Shard, Ls) ->\n-    [{Ls, emqx_ds_builtin_raft:current_timestamp(DB, Shard)}].\n+    case emqx_ds_builtin_raft:current_timestamp(DB, Shard) of\n+        {ok, TS} ->\n+            [{Ls, TS}];\n+        {error, _, _} ->\n+            []\n+    end.\n \n rasrv_lifecycle(Counters, Ls, Acc) ->\n     Acc#{\n"}
{"id": 205, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## TLDR\r\n\r\nfixes a few flaky tests\r\n\r\n## Dive Deeper\r\n\r\n- Mock file size check in fileUtils test to avoid creating large files\r\n- Add retry options to directory cleanup in memoryDiscovery test\r\n- Create test directories in parallel for better performance\r\n\r\n## Reviewer Test Plan\r\n\r\nrun the tests and they shouldn't flake\r\n\r\n## Testing Matrix\r\n\r\n|          | üçè  | ü™ü  | üêß  |\r\n| -------- | --- | --- | --- |\r\n| npm run  | ‚ùì  | ‚ùì  | ‚úÖ  |\r\n| npx      | ‚ùì  | ‚ùì  | ‚ùì  |\r\n| Docker   | ‚ùì  | ‚ùì  | ‚ùì  |\r\n| Podman   | ‚ùì  | -   | -   |\r\n| Seatbelt | ‚ùì  | -   | -   |\r\n\r\n## Linked issues / bugs\r\n\r\n- Fixes #8345\r\n- Fixes #8351\n\n### Code Patch:\n@@ -15,6 +15,7 @@ import {\n } from 'vitest';\n \n import * as actualNodeFs from 'node:fs'; // For setup/teardown\n+import fs from 'node:fs';\n import fsPromises from 'node:fs/promises';\n import path from 'node:path';\n import os from 'node:os';\n@@ -954,22 +955,30 @@ describe('fileUtils', () => {\n     });\n \n     it('should return an error if the file size exceeds 20MB', async () => {\n-      // Create a file just over 20MB\n-      const twentyOneMB = 21 * 1024 * 1024;\n-      const buffer = Buffer.alloc(twentyOneMB, 0x61); // Fill with 'a'\n-      actualNodeFs.writeFileSync(testTextFilePath, buffer);\n-\n-      const result = await processSingleFileContent(\n-        testTextFilePath,\n-        tempRootDir,\n-        new StandardFileSystemService(),\n-      );\n-\n-      expect(result.error).toContain('File size exceeds the 20MB limit');\n-      expect(result.returnDisplay).toContain(\n-        'File size exceeds the 20MB limit',\n-      );\n-      expect(result.llmContent).toContain('File size exceeds the 20MB limit');\n+      // Create a small test file\n+      actualNodeFs.writeFileSync(testTextFilePath, 'test content');\n+\n+      // Spy on fs.promises.stat to return a large file size\n+      const statSpy = vi.spyOn(fs.promises, 'stat').mockResolvedValueOnce({\n+        size: 21 * 1024 * 1024,\n+        isDirectory: () => false,\n+      } as fs.Stats);\n+\n+      try {\n+        const result = await processSingleFileContent(\n+          testTextFilePath,\n+          tempRootDir,\n+          new StandardFileSystemService(),\n+        );\n+\n+        expect(result.error).toContain('File size exceeds the 20MB limit');\n+        expect(result.returnDisplay).toContain(\n+          'File size exceeds the 20MB limit',\n+        );\n+        expect(result.llmContent).toContain('File size exceeds the 20MB limit');\n+      } finally {\n+        statSpy.mockRestore();\n+      }\n     });\n   });\n });\n\n@@ -63,7 +63,13 @@ describe('loadServerHierarchicalMemory', () => {\n     // Some tests set this to a different value.\n     setGeminiMdFilename(DEFAULT_CONTEXT_FILENAME);\n     // Clean up the temporary directory to prevent resource leaks.\n-    await fsPromises.rm(testRootDir, { recursive: true, force: true });\n+    // Use maxRetries option for robust cleanup without race conditions\n+    await fsPromises.rm(testRootDir, {\n+      recursive: true,\n+      force: true,\n+      maxRetries: 3,\n+      retryDelay: 10,\n+    });\n   });\n \n   describe('when untrusted', () => {\n@@ -354,9 +360,11 @@ describe('loadServerHierarchicalMemory', () => {\n       .spyOn(console, 'debug')\n       .mockImplementation(() => {});\n \n-    for (let i = 0; i < 100; i++) {\n-      await createEmptyDir(path.join(cwd, `deep_dir_${i}`));\n-    }\n+    // Create directories in parallel for better performance\n+    const dirPromises = Array.from({ length: 2 }, (_, i) =>\n+      createEmptyDir(path.join(cwd, `deep_dir_${i}`)),\n+    );\n+    await Promise.all(dirPromises);\n \n     // Pass the custom limit directly to the function\n     await loadServerHierarchicalMemory(\n@@ -371,12 +379,12 @@ describe('loadServerHierarchicalMemory', () => {\n         respectGitIgnore: true,\n         respectGeminiIgnore: true,\n       },\n-      50, // maxDirs\n+      1, // maxDirs\n     );\n \n     expect(consoleDebugSpy).toHaveBeenCalledWith(\n       expect.stringContaining('[DEBUG] [BfsFileSearch]'),\n-      expect.stringContaining('Scanning [50/50]:'),\n+      expect.stringContaining('Scanning [1/1]:'),\n     );\n \n     vi.mocked(console.debug).mockRestore();\n"}
{"id": 290, "ground_truth": "Test order dependency", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFollow-up of #10811 to ignore other types of `FileSystemException` like `DirectoryNotEmptyException` as well\r\n\r\n```\r\nTestDataFrameWrites > testFaultToleranceOnWrite() > format = parquet FAILED\r\n    java.nio.file.DirectoryNotEmptyException: /tmp/junit-7768099913831474039/parquet/test\r\n        at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:289)\r\n        at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:104)\r\n        at java.base/java.nio.file.Files.delete(Files.java:1152)\r\n        at org.apache.commons.io.FileUtils.delete(FileUtils.java:1222)\r\n        at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1242)\r\n        at org.apache.iceberg.spark.source.TestDataFrameWrites.testFaultToleranceOnWrite(TestDataFrameWrites.java:427)\r\n```\r\n\r\ncc @Fokko @nastra \n\n### Code Patch:\n@@ -28,15 +28,13 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n-import java.nio.file.NoSuchFileException;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n import org.apache.avro.generic.GenericData.Record;\n-import org.apache.commons.io.FileUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Parameter;\n@@ -76,6 +74,7 @@\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n @ExtendWith(ParameterizedTestExtension.class)\n public class TestDataFrameWrites extends ParameterizedAvroDataTest {\n@@ -88,6 +87,8 @@ public static Collection<String> parameters() {\n \n   @Parameter private String format;\n \n+  @TempDir private File location;\n+\n   private static SparkSession spark = null;\n   private static JavaSparkContext sc = null;\n \n@@ -140,47 +141,37 @@ public static void stopSpark() {\n \n   @Override\n   protected void writeAndValidate(Schema schema) throws IOException {\n-    File location = createTableFolder();\n-    Table table = createTable(schema, location);\n-    writeAndValidateWithLocations(table, location, new File(location, \"data\"));\n+    Table table = createTable(schema);\n+    writeAndValidateWithLocations(table, new File(location, \"data\"));\n   }\n \n   @TestTemplate\n   public void testWriteWithCustomDataLocation() throws IOException {\n-    File location = createTableFolder();\n     File tablePropertyDataLocation = temp.resolve(\"test-table-property-data-dir\").toFile();\n-    Table table = createTable(new Schema(SUPPORTED_PRIMITIVES.fields()), location);\n+    Table table = createTable(new Schema(SUPPORTED_PRIMITIVES.fields()));\n     table\n         .updateProperties()\n         .set(TableProperties.WRITE_DATA_LOCATION, tablePropertyDataLocation.getAbsolutePath())\n         .commit();\n-    writeAndValidateWithLocations(table, location, tablePropertyDataLocation);\n-  }\n-\n-  private File createTableFolder() throws IOException {\n-    File parent = temp.resolve(\"parquet\").toFile();\n-    File location = new File(parent, \"test\");\n-    assertThat(location.mkdirs()).as(\"Mkdir should succeed\").isTrue();\n-    return location;\n+    writeAndValidateWithLocations(table, tablePropertyDataLocation);\n   }\n \n-  private Table createTable(Schema schema, File location) {\n+  private Table createTable(Schema schema) {\n     HadoopTables tables = new HadoopTables(CONF);\n     return tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n   }\n \n-  private void writeAndValidateWithLocations(Table table, File location, File expectedDataDir)\n-      throws IOException {\n+  private void writeAndValidateWithLocations(Table table, File expectedDataDir) throws IOException {\n     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned\n \n     table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n \n     Iterable<Record> expected = RandomData.generate(tableSchema, 100, 0L);\n-    writeData(expected, tableSchema, location.toString());\n+    writeData(expected, tableSchema);\n \n     table.refresh();\n \n-    List<Row> actual = readTable(location.toString());\n+    List<Row> actual = readTable();\n \n     Iterator<Record> expectedIter = expected.iterator();\n     Iterator<Row> actualIter = actual.iterator();\n@@ -204,21 +195,20 @@ private void writeAndValidateWithLocations(Table table, File location, File expe\n                     .startsWith(expectedDataDir.getAbsolutePath()));\n   }\n \n-  private List<Row> readTable(String location) {\n-    Dataset<Row> result = spark.read().format(\"iceberg\").load(location);\n+  private List<Row> readTable() {\n+    Dataset<Row> result = spark.read().format(\"iceberg\").load(location.toString());\n \n     return result.collectAsList();\n   }\n \n-  private void writeData(Iterable<Record> records, Schema schema, String location)\n-      throws IOException {\n+  private void writeData(Iterable<Record> records, Schema schema) throws IOException {\n     Dataset<Row> df = createDataset(records, schema);\n     DataFrameWriter<?> writer = df.write().format(\"iceberg\").mode(\"append\");\n-    writer.save(location);\n+    writer.save(location.toString());\n   }\n \n-  private void writeDataWithFailOnPartition(\n-      Iterable<Record> records, Schema schema, String location) throws IOException, SparkException {\n+  private void writeDataWithFailOnPartition(Iterable<Record> records, Schema schema)\n+      throws IOException, SparkException {\n     final int numPartitions = 10;\n     final int partitionToFail = new Random().nextInt(numPartitions);\n     MapPartitionsFunction<Row, Row> failOnFirstPartitionFunc =\n@@ -241,7 +231,7 @@ private void writeDataWithFailOnPartition(\n     // Setting \"check-nullability\" option to \"false\" doesn't help as it fails at Spark analyzer.\n     Dataset<Row> convertedDf = df.sqlContext().createDataFrame(df.rdd(), convert(schema));\n     DataFrameWriter<?> writer = convertedDf.write().format(\"iceberg\").mode(\"append\");\n-    writer.save(location);\n+    writer.save(location.toString());\n   }\n \n   private Dataset<Row> createDataset(Iterable<Record> records, Schema schema) throws IOException {\n@@ -287,7 +277,6 @@ public void testNullableWithWriteOption() throws IOException {\n         .as(\"Spark 3 rejects writing nulls to a required column\")\n         .startsWith(\"2\");\n \n-    File location = temp.resolve(\"parquet\").resolve(\"test\").toFile();\n     String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location);\n     String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location);\n \n@@ -341,7 +330,6 @@ public void testNullableWithSparkSqlOption() throws IOException {\n         .as(\"Spark 3 rejects writing nulls to a required column\")\n         .startsWith(\"2\");\n \n-    File location = temp.resolve(\"parquet\").resolve(\"test\").toFile();\n     String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location);\n     String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location);\n \n@@ -397,37 +385,28 @@ public void testNullableWithSparkSqlOption() throws IOException {\n \n   @TestTemplate\n   public void testFaultToleranceOnWrite() throws IOException {\n-    File location = createTableFolder();\n     Schema schema = new Schema(SUPPORTED_PRIMITIVES.fields());\n-    Table table = createTable(schema, location);\n+    Table table = createTable(schema);\n \n     Iterable<Record> records = RandomData.generate(schema, 100, 0L);\n-    writeData(records, schema, location.toString());\n+    writeData(records, schema);\n \n     table.refresh();\n \n     Snapshot snapshotBeforeFailingWrite = table.currentSnapshot();\n-    List<Row> resultBeforeFailingWrite = readTable(location.toString());\n+    List<Row> resultBeforeFailingWrite = readTable();\n \n     Iterable<Record> records2 = RandomData.generate(schema, 100, 0L);\n \n-    assertThatThrownBy(() -> writeDataWithFailOnPartition(records2, schema, location.toString()))\n+    assertThatThrownBy(() -> writeDataWithFailOnPartition(records2, schema))\n         .isInstanceOf(SparkException.class);\n \n     table.refresh();\n \n     Snapshot snapshotAfterFailingWrite = table.currentSnapshot();\n-    List<Row> resultAfterFailingWrite = readTable(location.toString());\n+    List<Row> resultAfterFailingWrite = readTable();\n \n     assertThat(snapshotBeforeFailingWrite).isEqualTo(snapshotAfterFailingWrite);\n     assertThat(resultBeforeFailingWrite).isEqualTo(resultAfterFailingWrite);\n-\n-    while (location.exists()) {\n-      try {\n-        FileUtils.deleteDirectory(location);\n-      } catch (NoSuchFileException e) {\n-        // ignore NoSuchFileException when a file is already deleted\n-      }\n-    }\n   }\n }\n"}
{"id": 353, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis issue can be caused by a non-existing path but also a misunderstanding from the config file. A short example will help the user.\n\n\n### Code Patch:\n@@ -101,7 +101,9 @@ log.cleanup.interval.mins=1\n # This is a comma separated host:port pairs, each corresponding to a zk\n # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n # You can also append an optional chroot string to the urls to specify the\n-# root directory for all kafka znodes.\n+# root directory for all kafka znodes, e.g.\n+# \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/kafka\". If you use a chroot\n+# string, be sure to manualy create this namespace in zookeeper.\n zk.connect=localhost:2181\n \n # Timeout in ms for connecting to zookeeper\n"}
{"id": 326, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nSet `getNumHomes: 1` to enable the 567 ms or 2 seconds of wait for the server to be ready in `TestServerPathEncodingIssues/Unicode_paths` and `TestServerPathEncodingIssues/Windows_multilingual_404`.\r\n\r\nFixes #10332\r\n\r\n---\r\n\r\nHi @bep, I hope this is the right fix.  Besides waiting for the GitHub Actions CI test result, I'll be uploading 0.104.2-2 to Debian shortly and will report back if these tests pass the very first time on all architectures.\n\n### Code Patch:\n@@ -25,7 +25,6 @@ import (\n \n \t\"github.com/gohugoio/hugo/config\"\n \t\"github.com/gohugoio/hugo/helpers\"\n-\t\"github.com/gohugoio/hugo/htesting\"\n \t\"golang.org/x/net/context\"\n \t\"golang.org/x/sync/errgroup\"\n \n@@ -69,20 +68,14 @@ func TestServer404(t *testing.T) {\n }\n \n func TestServerPathEncodingIssues(t *testing.T) {\n-\tif htesting.IsGitHubAction() {\n-\t\t// This test is flaky on CI for some reason. Run it on Windows only for now.\n-\t\t// TODO(bep)\n-\t\tif runtime.GOOS != \"windows\" {\n-\t\t\tt.Skip(\"Skipping test on CI\")\n-\t\t}\n-\t}\n \tc := qt.New(t)\n \n \t// Issue 10287\n \tc.Run(\"Unicode paths\", func(c *qt.C) {\n \t\tr := runServerTest(c,\n \t\t\tserverTestOptions{\n-\t\t\t\tpathsToGet: []string{\"h√ºg√∂/\"},\n+\t\t\t\tpathsToGet:  []string{\"h√ºg√∂/\"},\n+\t\t\t\tgetNumHomes: 1,\n \t\t\t},\n \t\t)\n \n@@ -123,8 +116,9 @@ status = 404\n `\n \t\tr := runServerTest(c,\n \t\t\tserverTestOptions{\n-\t\t\t\tconfig:     config,\n-\t\t\t\tpathsToGet: []string{\"en/this/does/not/exist\", \"es/this/does/not/exist\"},\n+\t\t\t\tconfig:      config,\n+\t\t\t\tpathsToGet:  []string{\"en/this/does/not/exist\", \"es/this/does/not/exist\"},\n+\t\t\t\tgetNumHomes: 1,\n \t\t\t},\n \t\t)\n \n"}
{"id": 129, "ground_truth": "Time", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Enter a brief description/summary of your PR here. What does it fix/what does it change/how was it tested (even manually, if necessary)? -->\r\n## Summary of the Pull Request\r\n\r\nAnother attempt for trying to fix flaky WebView2 CI ü§û \r\n\r\n<!-- Please review the items on the PR checklist before submitting-->\r\n## PR Checklist\r\n\r\n- [ ] **Closes:** #xxx\r\n- [ ] **Communication:** I've discussed this with core contributors already. If work hasn't been agreed, this work might be rejected\r\n- [ ] **Tests:** Added/updated and all pass\r\n- [ ] **Localization:** All end user facing strings can be localized\r\n- [ ] **Dev docs:** Added/updated\r\n- [ ] **New binaries:** Added on the required places\r\n   - [ ] [JSON for signing](https://github.com/microsoft/PowerToys/blob/main/.pipelines/ESRPSigning_core.json) for new binaries\r\n   - [ ] [WXS for installer](https://github.com/microsoft/PowerToys/blob/main/installer/PowerToysSetup/Product.wxs) for new binaries and localization folder\r\n   - [ ] [YML for CI pipeline](https://github.com/microsoft/PowerToys/blob/main/.pipelines/ci/templates/build-powertoys-steps.yml) for new test projects\r\n   - [ ] [YML for signed pipeline](https://github.com/microsoft/PowerToys/blob/main/.pipelines/release.yml)\r\n- [ ] **Documentation updated:** If checked, please file a pull request on [our docs repo](https://github.com/MicrosoftDocs/windows-uwp/tree/docs/hub/powertoys) and link it here: #xxx\r\n\r\n<!-- Provide a more detailed description of the PR, other things fixed or any additional comments/features here -->\r\n## Detailed Description of the Pull Request / Additional comments\r\n\r\n<!-- Describe how you validated the behavior. Add automated tests wherever possible, but list manual validation steps taken as well -->\r\n## Validation Steps Performed\r\n\r\n\n\n### Code Patch:\n@@ -17,7 +17,7 @@ namespace MarkdownPreviewHandlerUnitTests\n     public class MarkdownPreviewHandlerTest\n     {\n         // A long timeout is needed. WebView2 can take a long time to load the first time in some CI systems.\n-        private static readonly int HardTimeoutInMilliseconds = 30000;\n+        private static readonly int HardTimeoutInMilliseconds = 60000;\n         private static readonly int SleepTimeInMilliseconds = 200;\n \n         [TestMethod]\n\n@@ -4,9 +4,7 @@\n \n using System;\n using System.Drawing;\n-using System.Runtime.InteropServices;\n using System.Runtime.InteropServices.ComTypes;\n-using System.Text;\n using System.Threading;\n using System.Windows.Forms;\n using Microsoft.PowerToys.PreviewHandler.Svg;\n@@ -22,7 +20,7 @@ namespace SvgPreviewHandlerUnitTests\n     public class SvgPreviewControlTests\n     {\n         // A long timeout is needed. WebView2 can take a long time to load the first time in some CI systems.\n-        private static readonly int HardTimeoutInMilliseconds = 30000;\n+        private static readonly int HardTimeoutInMilliseconds = 60000;\n         private static readonly int SleepTimeInMilliseconds = 200;\n \n         [TestMethod]\n"}
{"id": 250, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nProperly shutdown broker for each test and speed up tests by sending less messages\n\n### Code Patch:\n@@ -66,13 +66,14 @@\n @RunWith(Parameterized.class)\n public class AdvisoryTests {\n \n-    protected static final int MESSAGE_COUNT = 2000;\n+    protected static final int MESSAGE_COUNT = 100;\n     protected BrokerService broker;\n     protected Connection connection;\n     protected String bindAddress = ActiveMQConnectionFactory.DEFAULT_BROKER_BIND_URL;\n     protected final boolean includeBodyForAdvisory;\n     protected final boolean persistent;\n     protected final int EXPIRE_MESSAGE_PERIOD = 3000;\n+    protected final int DEFAULT_PREFETCH = 50;\n \n     @Parameters(name = \"includeBodyForAdvisory={0}, persistent={1}\")\n     public static Collection<Object[]> data() {\n@@ -126,7 +127,7 @@ public void testQueueSlowConsumerAdvisory() throws Exception {\n \n     @Test(timeout = 60000)\n     public void testTopicSlowConsumerAdvisory() throws Exception {\n-        broker.getDestinationPolicy().getDefaultEntry().setTopicPrefetch(500);\n+        broker.getDestinationPolicy().getDefaultEntry().setTopicPrefetch(10);\n         broker.getDestinationPolicy().getDefaultEntry().setPendingMessageLimitStrategy(null);\n         testSlowConsumerAdvisory(new ActiveMQTopic(getClass().getName()));\n     }\n@@ -597,8 +598,7 @@ public void testMessageDiscardedAdvisory() throws Exception {\n         MessageConsumer advisoryConsumer = s.createConsumer(advisoryTopic);\n         // start throwing messages at the consumer\n         MessageProducer producer = s.createProducer(topic);\n-        int count = (new ActiveMQPrefetchPolicy().getTopicPrefetch() * 2);\n-        for (int i = 0; i < count; i++) {\n+        for (int i = 0; i < MESSAGE_COUNT; i++) {\n             BytesMessage m = s.createBytesMessage();\n             m.writeBytes(new byte[1024]);\n             producer.send(m);\n@@ -672,7 +672,11 @@ public void setUp() throws Exception {\n \n     @After\n     public void tearDown() throws Exception {\n-        connection.close();\n+        try {\n+            connection.close();\n+        } catch (Exception e) {\n+            //swallow exception so we can still stop the broker even on error\n+        }\n         if (broker != null) {\n             broker.stop();\n         }\n@@ -704,6 +708,10 @@ protected void configureBroker(BrokerService answer) throws Exception {\n         policy.setAdvisoryWhenFull(true);\n         policy.setIncludeBodyForAdvisory(includeBodyForAdvisory);\n         policy.setProducerFlowControl(false);\n+        policy.setDurableTopicPrefetch(DEFAULT_PREFETCH);\n+        policy.setTopicPrefetch(DEFAULT_PREFETCH);\n+        policy.setQueuePrefetch(DEFAULT_PREFETCH);\n+\n         ConstantPendingMessageLimitStrategy strategy = new ConstantPendingMessageLimitStrategy();\n         strategy.setLimit(10);\n         policy.setPendingMessageLimitStrategy(strategy);\n"}
{"id": 38, "ground_truth": "Environment", "predicted": "Floating point operations", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n```\r\nFalsifying example: test_gpu_data_iterator(\r\n    n_samples_per_batch=57, n_features=1, n_batches=9, subsample=False,\r\n)\r\n```\r\n\r\nIt's caused by one different element in the generated cuts.  We need to change the test to adapt to the differences in the models.\n\n### Code Patch:\n@@ -19,7 +19,7 @@ def test_gpu_single_batch() -> None:\n @given(\n     strategies.integers(0, 1024),\n     strategies.integers(1, 7),\n-    strategies.integers(0, 13),\n+    strategies.integers(0, 8),\n     strategies.booleans(),\n )\n @settings(deadline=None, print_blob=True)\n\n@@ -92,6 +92,12 @@ def run_data_iterator(\n         assert non_increasing(results_from_it[\"Train\"][\"rmse\"])\n \n     X, y, w = it.as_arrays()\n+    if use_cupy:\n+        _y = y.get()\n+    else:\n+        _y = y\n+    np.testing.assert_allclose(Xy.get_label(), _y)\n+\n     Xy = xgb.DMatrix(X, y, weight=w)\n     assert Xy.num_row() == n_samples_per_batch * n_batches\n     assert Xy.num_col() == n_features\n\n@@ -12,6 +12,7 @@\n import pytest\n import gc\n import xgboost as xgb\n+from xgboost.core import ArrayLike\n import numpy as np\n from scipy import sparse\n import platform\n@@ -212,13 +213,16 @@ def reset(self) -> None:\n \n     def as_arrays(\n         self,\n-    ) -> Tuple[Union[np.ndarray, sparse.csr_matrix], np.ndarray, np.ndarray]:\n+    ) -> Tuple[Union[np.ndarray, sparse.csr_matrix], ArrayLike, ArrayLike]:\n         if isinstance(self.X[0], sparse.csr_matrix):\n             X = sparse.vstack(self.X, format=\"csr\")\n         else:\n             X = np.concatenate(self.X, axis=0)\n         y = np.concatenate(self.y, axis=0)\n-        w = np.concatenate(self.w, axis=0)\n+        if self.w:\n+            w = np.concatenate(self.w, axis=0)\n+        else:\n+            w = None\n         return X, y, w\n \n \n"}
{"id": 203, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\ntrust center tests need to create their own users because of top level restrictions on number of trust centers\n\n### Code Patch:\n@@ -851,8 +851,6 @@ func TestMutationUpdateOrganization(t *testing.T) {\n }\n \n func TestMutationDeleteOrganization(t *testing.T) {\n-\tt.Parallel()\n-\n \t// create another user for this test\n \t// so it doesn't interfere with the other tests\n \torgUser := suite.userBuilder(context.Background(), t)\n\n@@ -14,11 +14,16 @@ import (\n \n func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t// Create test data\n-\ttrustCenter := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n+\ttestUser := suite.userBuilder(context.Background(), t)\n+\tom := (&OrgMemberBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tviewOnlyUserCtx := auth.NewTestContextWithOrgID(om.UserID, testUser.OrganizationID)\n+\n+\t// Create test data\n+\ttrustCenter := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n \ttrustCenter2 := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser2.UserCtx, t)\n \n \t// Create subprocessors for testing\n-\tsubprocessor1 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n+\tsubprocessor1 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n \tsubprocessor2 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser2.UserCtx, t)\n \n \ttestCases := []struct {\n@@ -37,7 +42,7 @@ func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCountries:      []string{\"US\", \"CA\"},\n \t\t\t},\n \t\t\tclient: suite.client.api,\n-\t\t\tctx:    testUser1.UserCtx,\n+\t\t\tctx:    testUser.UserCtx,\n \t\t},\n \t\t{\n \t\t\tname: \"not authorized - view only user cannot create trust center subprocessor\",\n@@ -48,7 +53,7 @@ func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCountries:      []string{\"US\"},\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         viewOnlyUser.UserCtx,\n+\t\t\tctx:         viewOnlyUserCtx,\n \t\t\texpectedErr: notAuthorizedErrorMsg,\n \t\t},\n \t\t{\n@@ -72,7 +77,7 @@ func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCountries:      []string{\"US\"},\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         testUser1.UserCtx,\n+\t\t\tctx:         testUser.UserCtx,\n \t\t\texpectedErr: notFoundErrorMsg,\n \t\t},\n \t\t{\n@@ -84,7 +89,7 @@ func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCountries:      []string{\"US\"},\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         testUser1.UserCtx,\n+\t\t\tctx:         testUser.UserCtx,\n \t\t\texpectedErr: notFoundErrorMsg,\n \t\t},\n \t\t{\n@@ -95,7 +100,7 @@ func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCountries:      []string{\"US\"},\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         testUser1.UserCtx,\n+\t\t\tctx:         testUser.UserCtx,\n \t\t\texpectedErr: \"value is less than the required length\",\n \t\t},\n \t}\n@@ -124,15 +129,13 @@ func TestMutationCreateTrustCenterSubprocessor(t *testing.T) {\n \t}\n \n \t// Clean up test data\n-\t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter.ID}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter.ID}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter2.ID}).MustDelete(testUser2.UserCtx, t)\n-\t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessor1.ID}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessor1.ID}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessor2.ID}).MustDelete(testUser2.UserCtx, t)\n }\n \n func TestMutationCreateTrustCenterSubprocessorAsAnonymousUser(t *testing.T) {\n-\tt.Parallel()\n-\n \t// create new test users\n \ttestUser := suite.userBuilder(context.Background(), t)\n \n@@ -183,11 +186,15 @@ func TestMutationCreateTrustCenterSubprocessorAsAnonymousUser(t *testing.T) {\n \n func TestQueryTrustCenterSubprocessorByID(t *testing.T) {\n \t// Create test data\n-\ttrustCenter := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n+\ttestUser := suite.userBuilder(context.Background(), t)\n+\tom := (&OrgMemberBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tviewOnlyUserCtx := auth.NewTestContextWithOrgID(om.UserID, testUser.OrganizationID)\n+\n+\ttrustCenter := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n \n \t// Create a trust center subprocessor using GraphQL mutation\n-\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\tSubprocessorID: subprocessor.ID,\n \t\tTrustCenterID:  &trustCenter.ID,\n \t\tCategory:       \"Data Processing\",\n@@ -220,21 +227,21 @@ func TestQueryTrustCenterSubprocessorByID(t *testing.T) {\n \t\t\tname:             \"happy path - get trust center subprocessor\",\n \t\t\tqueryID:          tcSubprocessor.ID,\n \t\t\tclient:           suite.client.api,\n-\t\t\tctx:              testUser1.UserCtx,\n+\t\t\tctx:              testUser.UserCtx,\n \t\t\texpectedCategory: \"Data Processing\",\n \t\t},\n \t\t{\n \t\t\tname:             \"happy path - view only user can get trust center subprocessor\",\n \t\t\tqueryID:          tcSubprocessor.ID,\n \t\t\tclient:           suite.client.api,\n-\t\t\tctx:              viewOnlyUser.UserCtx,\n+\t\t\tctx:              viewOnlyUserCtx,\n \t\t\texpectedCategory: \"Data Processing\",\n \t\t},\n \t\t{\n \t\t\tname:             \"happy path - anon user\",\n \t\t\tqueryID:          tcSubprocessor.ID,\n \t\t\tclient:           suite.client.api,\n-\t\t\tctx:              createAnonymousTrustCenterContext(trustCenter.ID, testUser1.OrganizationID),\n+\t\t\tctx:              createAnonymousTrustCenterContext(trustCenter.ID, testUser.OrganizationID),\n \t\t\texpectedCategory: \"Data Processing\",\n \t\t},\n \t\t{\n@@ -255,7 +262,7 @@ func TestQueryTrustCenterSubprocessorByID(t *testing.T) {\n \t\t\tname:     \"not found - non-existent ID\",\n \t\t\tqueryID:  \"non-existent-id\",\n \t\t\tclient:   suite.client.api,\n-\t\t\tctx:      testUser1.UserCtx,\n+\t\t\tctx:      testUser.UserCtx,\n \t\t\terrorMsg: notFoundErrorMsg,\n \t\t},\n \t}\n@@ -278,23 +285,28 @@ func TestQueryTrustCenterSubprocessorByID(t *testing.T) {\n \t}\n \n \t// Clean up\n-\t(&Cleanup[*generated.TrustCenterSubprocessorDeleteOne]{client: suite.client.db.TrustCenterSubprocessor, ID: tcSubprocessor.ID}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.TrustCenterSubprocessorDeleteOne]{client: suite.client.db.TrustCenterSubprocessor, ID: tcSubprocessor.ID}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.TrustCenterSubprocessorDeleteOne]{client: suite.client.db.TrustCenterSubprocessor, ID: tcSubprocessor2.ID}).MustDelete(testUser2.UserCtx, t)\n-\t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter.ID}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter.ID}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter2.ID}).MustDelete(testUser2.UserCtx, t)\n-\t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessor.ID}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessor.ID}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessor2.ID}).MustDelete(testUser2.UserCtx, t)\n }\n \n func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t// Create test data\n-\ttrustCenter := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor1 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor2 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor3 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor4 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor5 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\tsubprocessor6 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n+\ttestUser := suite.userBuilder(context.Background(), t)\n+\tom := (&OrgMemberBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tviewOnlyUserCtx := auth.NewTestContextWithOrgID(om.UserID, testUser.OrganizationID)\n+\n+\t// Create test data\n+\ttrustCenter := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor1 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor2 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor3 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor4 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor5 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\tsubprocessor6 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n \n \t// Create another trust center subprocessor for different org\n \ttrustCenter2 := (&TrustCenterBuilder{client: suite.client}).MustNew(testUser2.UserCtx, t)\n@@ -322,7 +334,7 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t{\n \t\t\tname: \"happy path - update category and countries\",\n \t\t\tsetupFunc: func() string {\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessor1.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -336,12 +348,12 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCountries: newCountries,\n \t\t\t},\n \t\t\tclient: suite.client.api,\n-\t\t\tctx:    testUser1.UserCtx,\n+\t\t\tctx:    testUser.UserCtx,\n \t\t},\n \t\t{\n \t\t\tname: \"happy path - update subprocessor\",\n \t\t\tsetupFunc: func() string {\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessor2.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -354,12 +366,12 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tSubprocessorID: &subprocessor3.ID,\n \t\t\t},\n \t\t\tclient: suite.client.api,\n-\t\t\tctx:    testUser1.UserCtx,\n+\t\t\tctx:    testUser.UserCtx,\n \t\t},\n \t\t{\n \t\t\tname: \"happy path - append countries\",\n \t\t\tsetupFunc: func() string {\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessor4.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -372,12 +384,12 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tAppendCountries: []string{\"MX\"},\n \t\t\t},\n \t\t\tclient: suite.client.api,\n-\t\t\tctx:    testUser1.UserCtx,\n+\t\t\tctx:    testUser.UserCtx,\n \t\t},\n \t\t{\n \t\t\tname: \"happy path - clear countries\",\n \t\t\tsetupFunc: func() string {\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessor5.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -390,12 +402,12 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tClearCountries: lo.ToPtr(true),\n \t\t\t},\n \t\t\tclient: suite.client.api,\n-\t\t\tctx:    testUser1.UserCtx,\n+\t\t\tctx:    testUser.UserCtx,\n \t\t},\n \t\t{\n \t\t\tname: \"not authorized - view only user cannot update\",\n \t\t\tsetupFunc: func() string {\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessor6.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -408,14 +420,14 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCategory: &newCategory,\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         viewOnlyUser.UserCtx,\n+\t\t\tctx:         viewOnlyUserCtx,\n \t\t\texpectedErr: notAuthorizedErrorMsg,\n \t\t},\n \t\t{\n \t\t\tname: \"not authorized - anon user cannot update\",\n \t\t\tsetupFunc: func() string {\n-\t\t\t\tsubprocessoranon := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tsubprocessoranon := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessoranon.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -428,15 +440,15 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCategory: &newCategory,\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         createAnonymousTrustCenterContext(trustCenter.ID, testUser1.OrganizationID),\n+\t\t\tctx:         createAnonymousTrustCenterContext(trustCenter.ID, testUser.OrganizationID),\n \t\t\texpectedErr: couldNotFindUser,\n \t\t},\n \t\t{\n \t\t\tname: \"not authorized - different org user cannot update\",\n \t\t\tsetupFunc: func() string {\n \t\t\t\t// Create a separate subprocessor for this test to avoid conflicts\n-\t\t\t\tsubprocessor7 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser1.UserCtx, t)\n-\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser1.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n+\t\t\t\tsubprocessor7 := (&SubprocessorBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n+\t\t\t\tcreateResp, err := suite.client.api.CreateTrustCenterSubprocessor(testUser.UserCtx, testclient.CreateTrustCenterSubprocessorInput{\n \t\t\t\t\tSubprocessorID: subprocessor7.ID,\n \t\t\t\t\tTrustCenterID:  &trustCenter.ID,\n \t\t\t\t\tCategory:       \"Data Processing\",\n@@ -459,7 +471,7 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \t\t\t\tCategory: &newCategory,\n \t\t\t},\n \t\t\tclient:      suite.client.api,\n-\t\t\tctx:         testUser1.UserCtx,\n+\t\t\tctx:         testUser.UserCtx,\n \t\t\texpectedErr: notFoundErrorMsg,\n \t\t},\n \t}\n@@ -498,18 +510,16 @@ func TestMutationUpdateTrustCenterSubprocessor(t *testing.T) {\n \n \t// Clean up created trust center subprocessors\n \tif len(createdIDs) > 0 {\n-\t\t(&Cleanup[*generated.TrustCenterSubprocessorDeleteOne]{client: suite.client.db.TrustCenterSubprocessor, IDs: createdIDs}).MustDelete(testUser1.UserCtx, t)\n+\t\t(&Cleanup[*generated.TrustCenterSubprocessorDeleteOne]{client: suite.client.db.TrustCenterSubprocessor, IDs: createdIDs}).MustDelete(testUser.UserCtx, t)\n \t}\n \t(&Cleanup[*generated.TrustCenterSubprocessorDeleteOne]{client: suite.client.db.TrustCenterSubprocessor, ID: tcSubprocessor2.ID}).MustDelete(testUser2.UserCtx, t)\n-\t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter.ID}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter.ID}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.TrustCenterDeleteOne]{client: suite.client.db.TrustCenter, ID: trustCenter2.ID}).MustDelete(testUser2.UserCtx, t)\n-\t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, IDs: []string{subprocessor1.ID, subprocessor2.ID, subprocessor3.ID, subprocessor4.ID, subprocessor5.ID, subprocessor6.ID}}).MustDelete(testUser1.UserCtx, t)\n+\t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, IDs: []string{subprocessor1.ID, subprocessor2.ID, subprocessor3.ID, subprocessor4.ID, subprocessor5.ID, subprocessor6.ID}}).MustDelete(testUser.UserCtx, t)\n \t(&Cleanup[*generated.SubprocessorDeleteOne]{client: suite.client.db.Subprocessor, ID: subprocessorOtherOrg.ID}).MustDelete(testUser2.UserCtx, t)\n }\n \n func TestMutationDeleteTrustCenterSubprocessor(t *testing.T) {\n-\tt.Parallel()\n-\n \t// Create test data\n \ttestUser := suite.userBuilder(context.Background(), t)\n \tom := (&OrgMemberBuilder{client: suite.client}).MustNew(testUser.UserCtx, t)\n"}
{"id": 16, "ground_truth": "OS", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nGiven my home-directory is `/Users/rocko` - and I have a file named\n`npm.json` in it and also a repository with name `npm`, which is a\nfolder for the node-module.\n\nWhen try to require the `/Users/rocko/npm/index.js` two direcotry\nlevels down in the npm folder (e.g. `/Users/rocko/npm/test/tap`)\nwith require(\"../../\") node will load `/Users/rocko/npm/index.json`.\n\nWhen I use require(\"../..\") node will load `/Users/rocko/npm.json`\nwhich is fixed by this commit.\n\nOriginal PR: https://github.com/joyent/node/pull/7094\n\n\n### Code Patch:\n@@ -172,19 +172,20 @@ Module._findPath = function(request, paths) {\n     var filename;\n \n     if (!trailingSlash) {\n+      var stats = statPath(basePath);\n       // try to join the request to the path\n       filename = tryFile(basePath);\n \n-      if (!filename && !trailingSlash) {\n+      if (!filename && stats && stats.isDirectory()) {\n+        filename = tryPackage(basePath, exts);\n+      }\n+\n+      if (!filename) {\n         // try it with each of the extensions\n         filename = tryExtensions(basePath, exts);\n       }\n     }\n \n-    if (!filename) {\n-      filename = tryPackage(basePath, exts);\n-    }\n-\n     if (!filename) {\n       // try it with each of the extensions at \"index\"\n       filename = tryExtensions(path.resolve(basePath, 'index'), exts);\n\n@@ -0,0 +1 @@\n+{\"rocko\": \"artischocko\"}\n\n@@ -0,0 +1,22 @@\n+// Copyright Joyent, Inc. and other Node contributors.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a\n+// copy of this software and associated documentation files (the\n+// \"Software\"), to deal in the Software without restriction, including\n+// without limitation the rights to use, copy, modify, merge, publish,\n+// distribute, sublicense, and/or sell copies of the Software, and to permit\n+// persons to whom the Software is furnished to do so, subject to the\n+// following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included\n+// in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n+// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n+// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n+// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n+// USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+module.exports = \"hello from module-stub!\"\n\n@@ -0,0 +1,22 @@\n+// Copyright Joyent, Inc. and other Node contributors.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a\n+// copy of this software and associated documentation files (the\n+// \"Software\"), to deal in the Software without restriction, including\n+// without limitation the rights to use, copy, modify, merge, publish,\n+// distribute, sublicense, and/or sell copies of the Software, and to permit\n+// persons to whom the Software is furnished to do so, subject to the\n+// following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included\n+// in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n+// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n+// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n+// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n+// USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+module.exports = require('../../');\n\n@@ -0,0 +1,22 @@\n+// Copyright Joyent, Inc. and other Node contributors.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a\n+// copy of this software and associated documentation files (the\n+// \"Software\"), to deal in the Software without restriction, including\n+// without limitation the rights to use, copy, modify, merge, publish,\n+// distribute, sublicense, and/or sell copies of the Software, and to permit\n+// persons to whom the Software is furnished to do so, subject to the\n+// following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included\n+// in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n+// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n+// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n+// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n+// USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+module.exports = require('../..');\n\n@@ -0,0 +1,7 @@\n+{\n+  \"version\": \"1.1.12\",\n+  \"name\": \"module-stub\",\n+  \"main\": \"./index.js\",\n+  \"description\": \"A stub for node tests\",\n+  \"author\": \"Robert Kowalski <rok@kowalski.gd>\"\n+}\n\n@@ -0,0 +1,29 @@\n+// Copyright Joyent, Inc. and other Node contributors.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a\n+// copy of this software and associated documentation files (the\n+// \"Software\"), to deal in the Software without restriction, including\n+// without limitation the rights to use, copy, modify, merge, publish,\n+// distribute, sublicense, and/or sell copies of the Software, and to permit\n+// persons to whom the Software is furnished to do so, subject to the\n+// following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included\n+// in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n+// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n+// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n+// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n+// USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+var common = require('../common');\n+var assert = require('assert');\n+\n+var content = require(common.fixturesDir +\n+  '/json-with-directory-name-module/module-stub/one-trailing-slash/two/three.js');\n+\n+assert.notEqual(content.rocko, 'artischocko');\n+assert.equal(content, 'hello from module-stub!');\n\n@@ -0,0 +1,29 @@\n+// Copyright Joyent, Inc. and other Node contributors.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining a\n+// copy of this software and associated documentation files (the\n+// \"Software\"), to deal in the Software without restriction, including\n+// without limitation the rights to use, copy, modify, merge, publish,\n+// distribute, sublicense, and/or sell copies of the Software, and to permit\n+// persons to whom the Software is furnished to do so, subject to the\n+// following conditions:\n+//\n+// The above copyright notice and this permission notice shall be included\n+// in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n+// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n+// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n+// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n+// USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+var common = require('../common');\n+var assert = require('assert');\n+\n+var content = require(common.fixturesDir +\n+  '/json-with-directory-name-module/module-stub/one/two/three.js');\n+\n+assert.notEqual(content.rocko, 'artischocko');\n+assert.equal(content, 'hello from module-stub!');\n"}
{"id": 142, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nRemove hardcoded `start_time` with relative calculation to prevent test\r\nfailures in CI. The old pattern used `start_time=16000`\r\nwhich could result in negative durations when we calculate durations time.monotonic() < 16000,\r\ncausing the timeout logic to fail.\r\n\r\nI repkace that with populating start_time relative to current monotonic time,\r\nto guarantee the processor always runs for exactly\r\n(timeout + 1) seconds, to ensure consistent timeout behaviour across\r\nall environments.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n\n### Code Patch:\n@@ -518,10 +518,11 @@ def test_scan_stale_dags(self, testing_dag_bundle):\n             # SerializedDagModel gives history about Dags\n             assert serialized_dag_count == 1\n \n-    @time_machine.travel(datetime(2000, 1, 1, 0, 0, 0), tick=False)\n     def test_kill_timed_out_processors_kill(self):\n         manager = DagFileProcessorManager(max_runs=1, processor_timeout=5)\n-        processor, _ = self.mock_processor(start_time=16000)\n+        # Set start_time to ensure timeout occurs: start_time = current_time - (timeout + 1) = always (timeout + 1) seconds\n+        start_time = time.monotonic() - manager.processor_timeout - 1\n+        processor, _ = self.mock_processor(start_time=start_time)\n         manager._processors = {\n             DagFileInfo(\n                 bundle_name=\"testing\", rel_path=Path(\"abc.txt\"), bundle_path=TEST_DAGS_FOLDER\n"}
{"id": 103, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n**JIRA**: [JBPM-9895](https://issues.redhat.com/browse/JBPM-9895)\r\n\r\nCherry-pick from adc8aaf884bb967b2d25bb0072b4063b3ddcdf96\n\n### Code Patch:\n@@ -114,9 +114,13 @@ public void testDelayedReassignmentOnDeadline() throws Exception {\n         assertEquals(1, tasks.size());\n         \n         AuditTask auditTask = tasks.get(0);\n-        assertEquals(Status.Reserved.toString(), auditTask.getStatus());\n-        assertEquals(\"Tony Stark\", auditTask.getActualOwner());\n-    \n+        \n+        //Check if countdown listener has not expired yet\n+        if (countDownListener.latchCount() != 0) {\n+            assertEquals(Status.Reserved.toString(), auditTask.getStatus());\n+            assertEquals(\"Tony Stark\", auditTask.getActualOwner());\n+        }\n+        \n         // should have re-assigned by now\n         countDownListener.waitTillCompleted();\n         \n@@ -171,8 +175,12 @@ public void testDelayedReassignmentOnDeadlineISOFormat() throws Exception {\n         assertEquals(1, tasks.size());\n \n         AuditTask auditTask = tasks.get(0);\n-        assertEquals(Status.Reserved.toString(), auditTask.getStatus());\n-        assertEquals(\"Tony Stark\", auditTask.getActualOwner());\n+        \n+        //Check if countdown listener has not expired yet\n+        if (countDownListener.latchCount() != 0) {\n+            assertEquals(Status.Reserved.toString(), auditTask.getStatus());\n+            assertEquals(\"Tony Stark\", auditTask.getActualOwner());\n+        }\n \n         // should have re-assigned by now\n         countDownListener.waitTillCompleted();\n\n@@ -292,6 +292,10 @@ public void afterTaskOutputVariableChangedEvent(TaskEvent event, Map<String, Obj\n         \n     }\n     \n+    public long latchCount() {\n+        return latch.getCount();\n+    }\n+    \n     protected void countDown() {\n         try {\n             TransactionManager tm = TransactionManagerFactory.get().newTransactionManager();\n"}
{"id": 187, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\n!!!ATTENTION!!!\r\n\r\nIf you are fixing *any* crash or *any* potential security issue, *do not*\r\nopen a pull request in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.\r\nThank you in advance for helping to keep Envoy secure.\r\n\r\n!!!ATTENTION!!!\r\n\r\nFor an explanation of how to fill out the fields, please see the relevant section\r\nin [PULL_REQUESTS.md](https://github.com/envoyproxy/envoy/blob/main/PULL_REQUESTS.md)\r\n-->\r\n\r\nFix #29595 \r\n\r\nIt is possible that we will get data + rst from the wrapped transport socket since there is a `do {} while` loop. We should trigger the `onRead` if there is any processed data.\r\n\r\nDo not flake anymore.\r\n```\r\nbazel test test/integration:protocol_integration_test --test_filter=\"*LargeRequestMethod*\" --test_arg=\"-l trace\" -c opt --runs_per_test=1000\r\n```\r\n\r\nCommit Message:\r\nAdditional Description:\r\nRisk Level:\r\nTesting:\r\nDocs Changes:\r\nRelease Notes:\r\nPlatform Specific Features:\r\n[Optional Runtime guard:]\r\n[Optional Fixes #Issue]\r\n[Optional Fixes commit #PR or SHA]\r\n[Optional Deprecated:]\r\n[Optional [API Considerations](https://github.com/envoyproxy/envoy/blob/main/api/review_checklist.md):]\r\n\n\n### Code Patch:\n@@ -685,6 +685,9 @@ void ConnectionImpl::onReadReady() {\n   if (enable_rst_detect_send_ && result.err_code_.has_value() &&\n       result.err_code_ == Api::IoError::IoErrorCode::ConnectionReset) {\n     ENVOY_CONN_LOG(trace, \"read: rst close from peer\", *this);\n+    if (result.bytes_processed_ != 0) {\n+      onRead(new_buffer_size);\n+    }\n     setDetectedCloseType(DetectedCloseType::RemoteReset);\n     closeSocket(ConnectionEvent::RemoteClose);\n     return;\n\n@@ -2794,6 +2794,45 @@ TEST_F(MockTransportConnectionImplTest, ResumeWhileAndAfterReadDisable) {\n             connection_->readDisable(false));\n }\n \n+// Test the connection correctly handle the transport socket read (data + RST) case.\n+TEST_F(MockTransportConnectionImplTest, ServerLargeReadResetClose) {\n+  InSequence s;\n+\n+  std::shared_ptr<MockReadFilter> read_filter(new StrictMock<MockReadFilter>());\n+  connection_->addReadFilter(read_filter);\n+\n+  EXPECT_CALL(*transport_socket_, doRead(_))\n+      .WillOnce(Invoke([](Buffer::Instance& buffer) -> IoResult {\n+        buffer.add(\"01234\");\n+        return {PostIoAction::KeepOpen, 5, false};\n+      }));\n+\n+  EXPECT_CALL(*read_filter, onNewConnection()).WillOnce(Return(FilterStatus::Continue));\n+  EXPECT_CALL(*read_filter, onData(_, _))\n+      .WillOnce(Invoke([&](Buffer::Instance& data, bool) -> FilterStatus {\n+        EXPECT_EQ(5, data.length());\n+        data.drain(data.length());\n+        return FilterStatus::Continue;\n+      }));\n+  file_ready_cb_(Event::FileReadyType::Read);\n+\n+  // This simulates the socket do {...} while read when there is processed data\n+  // with the last rest flag.\n+  EXPECT_CALL(*transport_socket_, doRead(_))\n+      .WillOnce(Invoke([](Buffer::Instance& buffer) -> IoResult {\n+        buffer.add(\"5678\");\n+        return {PostIoAction::Close, 4, false, Api::IoError::IoErrorCode::ConnectionReset};\n+      }));\n+\n+  EXPECT_CALL(*read_filter, onData(_, _))\n+      .WillOnce(Invoke([&](Buffer::Instance& data, bool) -> FilterStatus {\n+        EXPECT_EQ(4, data.length());\n+        data.drain(data.length());\n+        return FilterStatus::Continue;\n+      }));\n+  file_ready_cb_(Event::FileReadyType::Read);\n+}\n+\n // Test that BytesSentCb is invoked at the correct times\n TEST_F(MockTransportConnectionImplTest, BytesSentCallback) {\n   uint64_t bytes_sent = 0;\n"}
{"id": 164, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe URL www.squirrelchasers.com is actually existing, so we should not access it in our tests. Hope this make the test more stable. \n\n### Code Patch:\n@@ -398,25 +398,16 @@ def test_http_timeout(monkeypatch, sentry_init, capture_envelopes):\n \n     envelopes = capture_envelopes()\n \n-    with start_transaction(op=\"op\", name=\"name\"):\n-        try:\n-            conn = HTTPSConnection(\"www.squirrelchasers.com\")\n-            conn.request(\"GET\", \"/top-chasers\")\n+    with pytest.raises(TimeoutError):\n+        with start_transaction(op=\"op\", name=\"name\"):\n+            conn = HTTPSConnection(\"www.example.com\")\n+            conn.request(\"GET\", \"/bla\")\n             conn.getresponse()\n-        except Exception:\n-            pass\n-\n-    items = [\n-        item\n-        for envelope in envelopes\n-        for item in envelope.items\n-        if item.type == \"transaction\"\n-    ]\n-    assert len(items) == 1\n-\n-    transaction = items[0].payload.json\n+\n+    (transaction_envelope,) = envelopes\n+    transaction = transaction_envelope.get_transaction_event()\n     assert len(transaction[\"spans\"]) == 1\n \n     span = transaction[\"spans\"][0]\n     assert span[\"op\"] == \"http.client\"\n-    assert span[\"description\"] == \"GET https://www.squirrelchasers.com/top-chasers\"\n+    assert span[\"description\"] == \"GET https://www.example.com/bla\"\n"}
{"id": 362, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n/kind bug\r\n/kind cleanup\r\n/kind documentation\r\n/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n/kind api-change\r\n/kind deprecation\r\n/kind failing-test\r\n/kind flake\r\n/kind regression\r\n-->\r\n\r\n#### What this PR does / why we need it:\r\nfix flaky test on dra manager_test.go\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #119701\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n\n### Code Patch:\n@@ -93,7 +93,7 @@ func setupFakeDRADriverGRPCServer(shouldTimeout bool) (string, tearDown, error)\n \t\tdriverName: driverName,\n \t}\n \tif shouldTimeout {\n-\t\ttimeout := plugin.PluginClientTimeout + time.Millisecond\n+\t\ttimeout := plugin.PluginClientTimeout + 10*time.Millisecond\n \t\tfakeDRADriverGRPCServer.timeout = &timeout\n \t}\n \n"}
{"id": 375, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n<!-- ‚ö†Ô∏è‚ö†Ô∏è Do Not Delete This! bug_report_template ‚ö†Ô∏è‚ö†Ô∏è -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- üïÆ Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- üîé Search existing issues to avoid creating duplicates. -->\n<!-- üß™ Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- üí° Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- üîß Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- ü™ì If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- üì£ Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: \n- OS Version: \n\nMy PR is failing to be merged due to windows unit tests failing surrounding composite tokens\n\nhttps://dev.azure.com/vscode/VSCode/_build/results?buildId=152385&view=logs&j=e4714b0f-e6ff-5e04-bac1-2dcbc509f151&t=5d351641-7442-59b2-8726-6d905a792029\n\n```\n  4442 passing (37s)\n  96 pending\n  1 failing\n\n  1) Composite***\n       ‚Ä¢ equals\n         ‚Ä¢ true\n           ‚Ä¢ composite tokens:\n     Error: Method not implemented.\n      at Test***.toString (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:20:13)\n      at Base***.fullRange (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/base***.js:87:54)\n      at new Composite*** (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/composite***.js:4:21)\n      at new Test*** (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:17:7)\n      at Context.<anonymous> (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:126:24)\n      at process.processImmediate (node:internal/timers:483:21)\n\n```\n\n### Code Patch:\n@@ -228,12 +228,12 @@ suite('CompositeToken', () => {\n \t\t\t\t// ensure there is at least one composite token\n \t\t\t\tconst lastToken = tokens[tokens.length - 1];\n \t\t\t\tconst compositeToken1 = new TestToken(randomTokens(\n-\t\t\t\t\trandomInt(5, 2),\n+\t\t\t\t\trandomInt(3, 1),\n \t\t\t\t\tlastToken.range.endLineNumber,\n \t\t\t\t\tlastToken.range.endColumn,\n \t\t\t\t));\n \t\t\t\tconst compositeToken2 = new TestToken(randomTokens(\n-\t\t\t\t\trandomInt(5, 2),\n+\t\t\t\t\trandomInt(6, 4),\n \t\t\t\t\tlastToken.range.endLineNumber,\n \t\t\t\t\tlastToken.range.endColumn,\n \t\t\t\t));\n"}
{"id": 186, "ground_truth": "Test Order Dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n‚Ä¶ot specifying topic\n\n\n### Code Patch:\n@@ -239,10 +239,22 @@ object ConsoleConsumer extends Logging {\n     var groupIdPassed = true\n     val options: OptionSet = tryParse(parser, args)\n     val useNewConsumer = options.has(useNewConsumerOpt)\n-    val filterOpt = List(whitelistOpt, blacklistOpt).filter(options.has)\n-    val topicOrFilterOpt = List(topicIdOpt, whitelistOpt, blacklistOpt).filter(options.has)\n-    val topicArg = options.valueOf(topicOrFilterOpt.head)\n-    val filterSpec = if (options.has(blacklistOpt)) new Blacklist(topicArg) else new Whitelist(topicArg)\n+\n+    // If using old consumer, exactly one of whitelist/blacklist/topic is required.\n+    // If using new consumer, topic must be specified.\n+    var topicArg: String = null\n+    var filterSpec: TopicFilter = null\n+    if (useNewConsumer) {\n+      if (!options.has(topicIdOpt))\n+        CommandLineUtils.printUsageAndDie(parser, \"Topic must be specified.\")\n+      topicArg = options.valueOf(topicIdOpt)\n+    } else {\n+      val topicOrFilterOpt = List(topicIdOpt, whitelistOpt, blacklistOpt).filter(options.has)\n+      if (topicOrFilterOpt.size != 1)\n+        CommandLineUtils.printUsageAndDie(parser, \"Exactly one of whitelist/blacklist/topic is required.\")\n+      topicArg = options.valueOf(topicOrFilterOpt.head)\n+      filterSpec = if (options.has(blacklistOpt)) new Blacklist(topicArg) else new Whitelist(topicArg)\n+    }\n     val consumerProps = if (options.has(consumerConfigOpt))\n       Utils.loadProps(options.valueOf(consumerConfigOpt))\n     else\n@@ -262,9 +274,6 @@ object ConsoleConsumer extends Logging {\n \n     CommandLineUtils.checkRequiredArgs(parser, options, if (useNewConsumer) bootstrapServerOpt else zkConnectOpt)\n \n-    if (!useNewConsumer && topicOrFilterOpt.size != 1)\n-      CommandLineUtils.printUsageAndDie(parser, \"Exactly one of whitelist/blacklist/topic is required.\")\n-\n     if (options.has(csvMetricsReporterEnabledOpt)) {\n       val csvReporterProps = new Properties()\n       csvReporterProps.put(\"kafka.metrics.polling.interval.secs\", \"5\")\n"}
{"id": 180, "ground_truth": "Resource Leak", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe `render_template_fields` method of mapped operator needs to use database session object to render mapped fields, but it cannot get the session passed by @provide_session decorator, because it is used in derived classes and we cannot change the signature without impacting those classes.\r\n\r\nSo far it was done by creating new session in mapped_operator, but it has the drawback of creating an extra session while one is already created (remnder_template_fields is always run in the context of task run and it always has a session created already in _run_raw_task). It also causes problems in our tests where two opened database session accessed database at the same time and it cases sqlite exception on concurrent access and mysql error on running operations out of sync - likely when the same object was modified in both sessions.\r\n\r\nThis PR changes the approach - rather than creating a new session in the mapped_operator, we are retrieving the session from one stored by the _run_raw_task. It is done by context manager and adequate protection has been added to make sure that:\r\n\r\na) the call is made within the context manager\r\nb) context manageer is never initialized twice in the same\r\n   call stack\r\n\r\nAfter this change, resources used by running task will be smaller, and mapped tasks will not always open 2 DB sesions.\r\n\r\nFixes: #33178\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n\n### Code Patch:\n@@ -67,6 +67,7 @@\n from airflow.utils.providers_configuration_loader import providers_configuration_loaded\n from airflow.utils.session import NEW_SESSION, create_session, provide_session\n from airflow.utils.state import DagRunState\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n \n if TYPE_CHECKING:\n     from sqlalchemy.orm.session import Session\n@@ -649,7 +650,8 @@ def task_render(args, dag: DAG | None = None) -> None:\n     ti, _ = _get_ti(\n         task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, create_if_necessary=\"memory\"\n     )\n-    ti.render_templates()\n+    with create_session() as session, set_current_task_instance_session(session=session):\n+        ti.render_templates()\n     for attr in task.template_fields:\n         print(\n             textwrap.dedent(\n\n@@ -26,7 +26,6 @@\n \n import attr\n \n-from airflow import settings\n from airflow.compat.functools import cache\n from airflow.exceptions import AirflowException, UnmappableOperator\n from airflow.models.abstractoperator import (\n@@ -54,6 +53,7 @@\n from airflow.typing_compat import Literal\n from airflow.utils.context import context_update_for_unmapped\n from airflow.utils.helpers import is_container, prevent_duplicates\n+from airflow.utils.task_instance_session import get_current_task_instance_session\n from airflow.utils.types import NOTSET\n from airflow.utils.xcom import XCOM_RETURN_KEY\n \n@@ -720,12 +720,13 @@ def render_template_fields(\n         if not jinja_env:\n             jinja_env = self.get_template_env()\n \n-        # Ideally we'd like to pass in session as an argument to this function,\n-        # but we can't easily change this function signature since operators\n-        # could override this. We can't use @provide_session since it closes and\n-        # expunges everything, which we don't want to do when we are so \"deep\"\n-        # in the weeds here. We don't close this session for the same reason.\n-        session = settings.Session()\n+        # We retrieve the session here, stored by _run_raw_task in set_current_task_session\n+        # context manager - we cannot pass the session via @provide_session because the signature\n+        # of render_template_fields is defined by BaseOperator and there are already many subclasses\n+        # overriding it, so changing the signature is not an option. However render_template_fields is\n+        # always executed within \"_run_raw_task\" so we make sure that _run_raw_task uses the\n+        # set_current_task_session context manager to store the session in the current task.\n+        session = get_current_task_instance_session()\n \n         mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)\n         unmapped_task = self.unmap(mapped_kwargs)\n\n@@ -118,6 +118,7 @@\n )\n from airflow.utils.state import DagRunState, JobState, State, TaskInstanceState\n from airflow.utils.task_group import MappedTaskGroup\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.timeout import timeout\n from airflow.utils.xcom import XCOM_RETURN_KEY\n \n@@ -1511,98 +1512,98 @@ def _run_raw_task(\n                 count=0,\n                 tags={**self.stats_tags, \"state\": str(state)},\n             )\n+        with set_current_task_instance_session(session=session):\n+            self.task = self.task.prepare_for_execution()\n+            context = self.get_template_context(ignore_param_exceptions=False)\n \n-        self.task = self.task.prepare_for_execution()\n-        context = self.get_template_context(ignore_param_exceptions=False)\n-\n-        try:\n-            if not mark_success:\n-                self._execute_task_with_callbacks(context, test_mode, session=session)\n-            if not test_mode:\n-                self.refresh_from_db(lock_for_update=True, session=session)\n-            self.state = TaskInstanceState.SUCCESS\n-        except TaskDeferred as defer:\n-            # The task has signalled it wants to defer execution based on\n-            # a trigger.\n-            self._defer_task(defer=defer, session=session)\n-            self.log.info(\n-                \"Pausing task as DEFERRED. dag_id=%s, task_id=%s, execution_date=%s, start_date=%s\",\n-                self.dag_id,\n-                self.task_id,\n-                self._date_or_empty(\"execution_date\"),\n-                self._date_or_empty(\"start_date\"),\n-            )\n-            if not test_mode:\n-                session.add(Log(self.state, self))\n-                session.merge(self)\n-                session.commit()\n-            return TaskReturnCode.DEFERRED\n-        except AirflowSkipException as e:\n-            # Recording SKIP\n-            # log only if exception has any arguments to prevent log flooding\n-            if e.args:\n-                self.log.info(e)\n-            if not test_mode:\n-                self.refresh_from_db(lock_for_update=True, session=session)\n-            self.state = TaskInstanceState.SKIPPED\n-        except AirflowRescheduleException as reschedule_exception:\n-            self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, session=session)\n-            session.commit()\n-            return None\n-        except (AirflowFailException, AirflowSensorTimeout) as e:\n-            # If AirflowFailException is raised, task should not retry.\n-            # If a sensor in reschedule mode reaches timeout, task should not retry.\n-            self.handle_failure(e, test_mode, context, force_fail=True, session=session)\n-            session.commit()\n-            raise\n-        except AirflowException as e:\n-            if not test_mode:\n-                self.refresh_from_db(lock_for_update=True, session=session)\n-            # for case when task is marked as success/failed externally\n-            # or dagrun timed out and task is marked as skipped\n-            # current behavior doesn't hit the callbacks\n-            if self.state in State.finished:\n-                self.clear_next_method_args()\n-                session.merge(self)\n+            try:\n+                if not mark_success:\n+                    self._execute_task_with_callbacks(context, test_mode, session=session)\n+                if not test_mode:\n+                    self.refresh_from_db(lock_for_update=True, session=session)\n+                self.state = TaskInstanceState.SUCCESS\n+            except TaskDeferred as defer:\n+                # The task has signalled it wants to defer execution based on\n+                # a trigger.\n+                self._defer_task(defer=defer, session=session)\n+                self.log.info(\n+                    \"Pausing task as DEFERRED. dag_id=%s, task_id=%s, execution_date=%s, start_date=%s\",\n+                    self.dag_id,\n+                    self.task_id,\n+                    self._date_or_empty(\"execution_date\"),\n+                    self._date_or_empty(\"start_date\"),\n+                )\n+                if not test_mode:\n+                    session.add(Log(self.state, self))\n+                    session.merge(self)\n+                    session.commit()\n+                return TaskReturnCode.DEFERRED\n+            except AirflowSkipException as e:\n+                # Recording SKIP\n+                # log only if exception has any arguments to prevent log flooding\n+                if e.args:\n+                    self.log.info(e)\n+                if not test_mode:\n+                    self.refresh_from_db(lock_for_update=True, session=session)\n+                self.state = TaskInstanceState.SKIPPED\n+            except AirflowRescheduleException as reschedule_exception:\n+                self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, session=session)\n                 session.commit()\n                 return None\n-            else:\n+            except (AirflowFailException, AirflowSensorTimeout) as e:\n+                # If AirflowFailException is raised, task should not retry.\n+                # If a sensor in reschedule mode reaches timeout, task should not retry.\n+                self.handle_failure(e, test_mode, context, force_fail=True, session=session)\n+                session.commit()\n+                raise\n+            except AirflowException as e:\n+                if not test_mode:\n+                    self.refresh_from_db(lock_for_update=True, session=session)\n+                # for case when task is marked as success/failed externally\n+                # or dagrun timed out and task is marked as skipped\n+                # current behavior doesn't hit the callbacks\n+                if self.state in State.finished:\n+                    self.clear_next_method_args()\n+                    session.merge(self)\n+                    session.commit()\n+                    return None\n+                else:\n+                    self.handle_failure(e, test_mode, context, session=session)\n+                    session.commit()\n+                    raise\n+            except (Exception, KeyboardInterrupt) as e:\n                 self.handle_failure(e, test_mode, context, session=session)\n                 session.commit()\n                 raise\n-        except (Exception, KeyboardInterrupt) as e:\n-            self.handle_failure(e, test_mode, context, session=session)\n-            session.commit()\n-            raise\n-        finally:\n-            Stats.incr(f\"ti.finish.{self.dag_id}.{self.task_id}.{self.state}\", tags=self.stats_tags)\n-            # Same metric with tagging\n-            Stats.incr(\"ti.finish\", tags={**self.stats_tags, \"state\": str(self.state)})\n-\n-        # Recording SKIPPED or SUCCESS\n-        self.clear_next_method_args()\n-        self.end_date = timezone.utcnow()\n-        self._log_state()\n-        self.set_duration()\n+            finally:\n+                Stats.incr(f\"ti.finish.{self.dag_id}.{self.task_id}.{self.state}\", tags=self.stats_tags)\n+                # Same metric with tagging\n+                Stats.incr(\"ti.finish\", tags={**self.stats_tags, \"state\": str(self.state)})\n+\n+            # Recording SKIPPED or SUCCESS\n+            self.clear_next_method_args()\n+            self.end_date = timezone.utcnow()\n+            self._log_state()\n+            self.set_duration()\n+\n+            # run on_success_callback before db committing\n+            # otherwise, the LocalTaskJob sees the state is changed to `success`,\n+            # but the task_runner is still running, LocalTaskJob then treats the state is set externally!\n+            self._run_finished_callback(self.task.on_success_callback, context, \"on_success\")\n \n-        # run on_success_callback before db committing\n-        # otherwise, the LocalTaskJob sees the state is changed to `success`,\n-        # but the task_runner is still running, LocalTaskJob then treats the state is set externally!\n-        self._run_finished_callback(self.task.on_success_callback, context, \"on_success\")\n-\n-        if not test_mode:\n-            session.add(Log(self.state, self))\n-            session.merge(self).task = self.task\n-            if self.state == TaskInstanceState.SUCCESS:\n-                self._register_dataset_changes(session=session)\n+            if not test_mode:\n+                session.add(Log(self.state, self))\n+                session.merge(self).task = self.task\n+                if self.state == TaskInstanceState.SUCCESS:\n+                    self._register_dataset_changes(session=session)\n \n-            session.commit()\n-            if self.state == TaskInstanceState.SUCCESS:\n-                get_listener_manager().hook.on_task_instance_success(\n-                    previous_state=TaskInstanceState.RUNNING, task_instance=self, session=session\n-                )\n+                session.commit()\n+                if self.state == TaskInstanceState.SUCCESS:\n+                    get_listener_manager().hook.on_task_instance_success(\n+                        previous_state=TaskInstanceState.RUNNING, task_instance=self, session=session\n+                    )\n \n-        return None\n+            return None\n \n     def _register_dataset_changes(self, *, session: Session) -> None:\n         for obj in self.task.outlets or []:\n\n@@ -0,0 +1,60 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import contextlib\n+import logging\n+import traceback\n+from typing import TYPE_CHECKING\n+\n+from airflow.utils.session import create_session\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+__current_task_instance_session: Session | None = None\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def get_current_task_instance_session() -> Session:\n+    global __current_task_instance_session\n+    if not __current_task_instance_session:\n+        log.warning(\"No task session set for this task. Continuing but this likely causes a resource leak.\")\n+        log.warning(\"Please report this and stacktrace below to https://github.com/apache/airflow/issues\")\n+        for filename, line_number, name, line in traceback.extract_stack():\n+            log.warning('File: \"%s\", %s , in %s', filename, line_number, name)\n+            if line:\n+                log.warning(\"  %s\", line.strip())\n+        __current_task_instance_session = create_session()\n+    return __current_task_instance_session\n+\n+\n+@contextlib.contextmanager\n+def set_current_task_instance_session(session: Session):\n+    global __current_task_instance_session\n+    if __current_task_instance_session:\n+        raise RuntimeError(\n+            \"Session already set for this task. \"\n+            \"You can only have one 'set_current_task_session' context manager active at a time.\"\n+        )\n+    __current_task_instance_session = session\n+    try:\n+        yield\n+    finally:\n+        __current_task_instance_session = None\n\n@@ -36,6 +36,7 @@\n from airflow.utils import timezone\n from airflow.utils.state import State\n from airflow.utils.task_group import TaskGroup\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.trigger_rule import TriggerRule\n from airflow.utils.types import DagRunType\n from airflow.utils.xcom import XCOM_RETURN_KEY\n@@ -747,36 +748,37 @@ def test_mapped_render_template_fields(dag_maker, session):\n     def fn(arg1, arg2):\n         ...\n \n-    with dag_maker(session=session):\n-        task1 = BaseOperator(task_id=\"op1\")\n-        mapped = fn.partial(arg2=\"{{ ti.task_id }}\").expand(arg1=task1.output)\n+    with set_current_task_instance_session(session=session):\n+        with dag_maker(session=session):\n+            task1 = BaseOperator(task_id=\"op1\")\n+            mapped = fn.partial(arg2=\"{{ ti.task_id }}\").expand(arg1=task1.output)\n \n-    dr = dag_maker.create_dagrun()\n-    ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n-\n-    ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n-\n-    session.add(\n-        TaskMap(\n-            dag_id=dr.dag_id,\n-            task_id=task1.task_id,\n-            run_id=dr.run_id,\n-            map_index=-1,\n-            length=1,\n-            keys=None,\n+        dr = dag_maker.create_dagrun()\n+        ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+\n+        ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n+\n+        session.add(\n+            TaskMap(\n+                dag_id=dr.dag_id,\n+                task_id=task1.task_id,\n+                run_id=dr.run_id,\n+                map_index=-1,\n+                length=1,\n+                keys=None,\n+            )\n         )\n-    )\n-    session.flush()\n+        session.flush()\n \n-    mapped_ti: TaskInstance = dr.get_task_instance(mapped.operator.task_id, session=session)\n-    mapped_ti.map_index = 0\n+        mapped_ti: TaskInstance = dr.get_task_instance(mapped.operator.task_id, session=session)\n+        mapped_ti.map_index = 0\n \n-    assert isinstance(mapped_ti.task, MappedOperator)\n-    mapped.operator.render_template_fields(context=mapped_ti.get_template_context(session=session))\n-    assert isinstance(mapped_ti.task, BaseOperator)\n+        assert isinstance(mapped_ti.task, MappedOperator)\n+        mapped.operator.render_template_fields(context=mapped_ti.get_template_context(session=session))\n+        assert isinstance(mapped_ti.task, BaseOperator)\n \n-    assert mapped_ti.task.op_kwargs[\"arg1\"] == \"{{ ds }}\"\n-    assert mapped_ti.task.op_kwargs[\"arg2\"] == \"fn\"\n+        assert mapped_ti.task.op_kwargs[\"arg1\"] == \"{{ ds }}\"\n+        assert mapped_ti.task.op_kwargs[\"arg2\"] == \"fn\"\n \n \n def test_task_decorator_has_wrapped_attr():\n\n@@ -39,6 +39,7 @@\n from airflow.operators.python import PythonOperator\n from airflow.utils.state import TaskInstanceState\n from airflow.utils.task_group import TaskGroup\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.trigger_rule import TriggerRule\n from airflow.utils.xcom import XCOM_RETURN_KEY\n from tests.models import DEFAULT_DATE\n@@ -403,109 +404,114 @@ def test_mapped_expand_against_params(dag_maker, dag_params, task_params, expect\n \n \n def test_mapped_render_template_fields_validating_operator(dag_maker, session):\n-    class MyOperator(BaseOperator):\n-        template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n-        template_ext = (\".ext\",)\n-\n-        def __init__(\n-            self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n-        ):\n-            for value in [partial_template, partial_static, map_template, map_static, file_template]:\n-                assert isinstance(value, str), \"value should have been resolved before unmapping\"\n-            super().__init__(**kwargs)\n-            self.partial_template = partial_template\n-            self.partial_static = partial_static\n-            self.map_template = map_template\n-            self.map_static = map_static\n-            self.file_template = file_template\n+    with set_current_task_instance_session(session=session):\n+\n+        class MyOperator(BaseOperator):\n+            template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n+            template_ext = (\".ext\",)\n+\n+            def __init__(\n+                self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n+            ):\n+                for value in [partial_template, partial_static, map_template, map_static, file_template]:\n+                    assert isinstance(value, str), \"value should have been resolved before unmapping\"\n+                    super().__init__(**kwargs)\n+                    self.partial_template = partial_template\n+                self.partial_static = partial_static\n+                self.map_template = map_template\n+                self.map_static = map_static\n+                self.file_template = file_template\n \n         def execute(self, context):\n             pass\n \n-    with dag_maker(session=session):\n-        task1 = BaseOperator(task_id=\"op1\")\n-        output1 = task1.output\n-        mapped = MyOperator.partial(\n-            task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n-        ).expand(map_template=output1, map_static=output1, file_template=[\"/path/to/file.ext\"])\n+        with dag_maker(session=session):\n+            task1 = BaseOperator(task_id=\"op1\")\n+            output1 = task1.output\n+            mapped = MyOperator.partial(\n+                task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n+            ).expand(map_template=output1, map_static=output1, file_template=[\"/path/to/file.ext\"])\n+\n+        dr = dag_maker.create_dagrun()\n+        ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+\n+        ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n+\n+        session.add(\n+            TaskMap(\n+                dag_id=dr.dag_id,\n+                task_id=task1.task_id,\n+                run_id=dr.run_id,\n+                map_index=-1,\n+                length=1,\n+                keys=None,\n+            )\n+        )\n+        session.flush()\n \n-    dr = dag_maker.create_dagrun()\n-    ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+        mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n+        mapped_ti.map_index = 0\n \n-    ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)\n+        assert isinstance(mapped_ti.task, MappedOperator)\n+        with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n+            \"os.path.isfile\", return_value=True\n+        ), patch(\"os.path.getmtime\", return_value=0):\n+            mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n+        assert isinstance(mapped_ti.task, MyOperator)\n \n-    session.add(\n-        TaskMap(\n-            dag_id=dr.dag_id,\n-            task_id=task1.task_id,\n-            run_id=dr.run_id,\n-            map_index=-1,\n-            length=1,\n-            keys=None,\n-        )\n-    )\n-    session.flush()\n+        assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n+        assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.map_template == \"{{ ds }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n \n-    mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n-    mapped_ti.map_index = 0\n \n-    assert isinstance(mapped_ti.task, MappedOperator)\n-    with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n-        \"os.path.isfile\", return_value=True\n-    ), patch(\"os.path.getmtime\", return_value=0):\n-        mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n-    assert isinstance(mapped_ti.task, MyOperator)\n+def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):\n \n-    assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n-    assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.map_template == \"{{ ds }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n+    with set_current_task_instance_session(session=session):\n \n+        class MyOperator(BaseOperator):\n+            template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n+            template_ext = (\".ext\",)\n \n-def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):\n-    class MyOperator(BaseOperator):\n-        template_fields = (\"partial_template\", \"map_template\", \"file_template\")\n-        template_ext = (\".ext\",)\n-\n-        def __init__(\n-            self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n-        ):\n-            for value in [partial_template, partial_static, map_template, map_static, file_template]:\n-                assert isinstance(value, str), \"value should have been resolved before unmapping\"\n-            super().__init__(**kwargs)\n-            self.partial_template = partial_template\n-            self.partial_static = partial_static\n-            self.map_template = map_template\n-            self.map_static = map_static\n-            self.file_template = file_template\n+            def __init__(\n+                self, partial_template, partial_static, map_template, map_static, file_template, **kwargs\n+            ):\n+                for value in [partial_template, partial_static, map_template, map_static, file_template]:\n+                    assert isinstance(value, str), \"value should have been resolved before unmapping\"\n+                super().__init__(**kwargs)\n+                self.partial_template = partial_template\n+                self.partial_static = partial_static\n+                self.map_template = map_template\n+                self.map_static = map_static\n+                self.file_template = file_template\n \n-        def execute(self, context):\n-            pass\n+            def execute(self, context):\n+                pass\n \n-    with dag_maker(session=session):\n-        mapped = MyOperator.partial(\n-            task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n-        ).expand_kwargs(\n-            [{\"map_template\": \"{{ ds }}\", \"map_static\": \"{{ ds }}\", \"file_template\": \"/path/to/file.ext\"}]\n-        )\n+        with dag_maker(session=session):\n+            mapped = MyOperator.partial(\n+                task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"\n+            ).expand_kwargs(\n+                [{\"map_template\": \"{{ ds }}\", \"map_static\": \"{{ ds }}\", \"file_template\": \"/path/to/file.ext\"}]\n+            )\n \n-    dr = dag_maker.create_dagrun()\n+        dr = dag_maker.create_dagrun()\n \n-    mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)\n+        mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)\n \n-    assert isinstance(mapped_ti.task, MappedOperator)\n-    with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n-        \"os.path.isfile\", return_value=True\n-    ), patch(\"os.path.getmtime\", return_value=0):\n-        mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n-    assert isinstance(mapped_ti.task, MyOperator)\n+        assert isinstance(mapped_ti.task, MappedOperator)\n+        with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(\n+            \"os.path.isfile\", return_value=True\n+        ), patch(\"os.path.getmtime\", return_value=0):\n+            mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))\n+        assert isinstance(mapped_ti.task, MyOperator)\n \n-    assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n-    assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.map_template == \"2016-01-01\", \"Should be templated!\"\n-    assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n-    assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n+        assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"\n+        assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.map_template == \"2016-01-01\", \"Should be templated!\"\n+        assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"\n+        assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"\n \n \n def test_mapped_render_nested_template_fields(dag_maker, session):\n@@ -607,35 +613,36 @@ def test_expand_kwargs_mapped_task_instance(dag_maker, session, num_existing_tis\n     ],\n )\n def test_expand_kwargs_render_template_fields_validating_operator(dag_maker, session, map_index, expected):\n-    with dag_maker(session=session):\n-        task1 = BaseOperator(task_id=\"op1\")\n-        mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)\n-\n-    dr = dag_maker.create_dagrun()\n-    ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n-\n-    ti.xcom_push(key=XCOM_RETURN_KEY, value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}], session=session)\n-\n-    session.add(\n-        TaskMap(\n-            dag_id=dr.dag_id,\n-            task_id=task1.task_id,\n-            run_id=dr.run_id,\n-            map_index=-1,\n-            length=2,\n-            keys=None,\n+    with set_current_task_instance_session(session=session):\n+        with dag_maker(session=session):\n+            task1 = BaseOperator(task_id=\"op1\")\n+            mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)\n+\n+        dr = dag_maker.create_dagrun()\n+        ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)\n+\n+        ti.xcom_push(key=XCOM_RETURN_KEY, value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}], session=session)\n+\n+        session.add(\n+            TaskMap(\n+                dag_id=dr.dag_id,\n+                task_id=task1.task_id,\n+                run_id=dr.run_id,\n+                map_index=-1,\n+                length=2,\n+                keys=None,\n+            )\n         )\n-    )\n-    session.flush()\n-\n-    ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n-    ti.refresh_from_task(mapped)\n-    ti.map_index = map_index\n-    assert isinstance(ti.task, MappedOperator)\n-    mapped.render_template_fields(context=ti.get_template_context(session=session))\n-    assert isinstance(ti.task, MockOperator)\n-    assert ti.task.arg1 == expected\n-    assert ti.task.arg2 == \"a\"\n+        session.flush()\n+\n+        ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)\n+        ti.refresh_from_task(mapped)\n+        ti.map_index = map_index\n+        assert isinstance(ti.task, MappedOperator)\n+        mapped.render_template_fields(context=ti.get_template_context(session=session))\n+        assert isinstance(ti.task, MockOperator)\n+        assert ti.task.arg1 == expected\n+        assert ti.task.arg2 == \"a\"\n \n \n def test_xcomarg_property_of_mapped_operator(dag_maker):\n\n@@ -29,6 +29,7 @@\n from airflow.models import Variable\n from airflow.models.renderedtifields import RenderedTaskInstanceFields as RTIF\n from airflow.operators.bash import BashOperator\n+from airflow.utils.task_instance_session import set_current_task_instance_session\n from airflow.utils.timezone import datetime\n from tests.test_utils.asserts import assert_queries_count\n from tests.test_utils.db import clear_db_dags, clear_db_runs, clear_rendered_ti_fields\n@@ -153,44 +154,46 @@ def test_get_templated_fields(self, templated_field, expected_rendered_field, da\n         ],\n     )\n     def test_delete_old_records(\n-        self, rtif_num, num_to_keep, remaining_rtifs, expected_query_count, dag_maker\n+        self, rtif_num, num_to_keep, remaining_rtifs, expected_query_count, dag_maker, session\n     ):\n         \"\"\"\n         Test that old records are deleted from rendered_task_instance_fields table\n         for a given task_id and dag_id.\n         \"\"\"\n-        session = settings.Session()\n-        with dag_maker(\"test_delete_old_records\") as dag:\n-            task = BashOperator(task_id=\"test\", bash_command=\"echo {{ ds }}\")\n-        rtif_list = []\n-        for num in range(rtif_num):\n-            dr = dag_maker.create_dagrun(run_id=str(num), execution_date=dag.start_date + timedelta(days=num))\n-            ti = dr.task_instances[0]\n-            ti.task = task\n-            rtif_list.append(RTIF(ti))\n-\n-        session.add_all(rtif_list)\n-        session.flush()\n-\n-        result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n-\n-        for rtif in rtif_list:\n-            assert rtif in result\n-\n-        assert rtif_num == len(result)\n-\n-        # Verify old records are deleted and only 'num_to_keep' records are kept\n-        # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n-        expected_query_count_based_on_db = (\n-            expected_query_count + 1\n-            if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n-            else expected_query_count\n-        )\n+        with set_current_task_instance_session(session=session):\n+            with dag_maker(\"test_delete_old_records\") as dag:\n+                task = BashOperator(task_id=\"test\", bash_command=\"echo {{ ds }}\")\n+            rtif_list = []\n+            for num in range(rtif_num):\n+                dr = dag_maker.create_dagrun(\n+                    run_id=str(num), execution_date=dag.start_date + timedelta(days=num)\n+                )\n+                ti = dr.task_instances[0]\n+                ti.task = task\n+                rtif_list.append(RTIF(ti))\n+\n+            session.add_all(rtif_list)\n+            session.flush()\n+\n+            result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n+\n+            for rtif in rtif_list:\n+                assert rtif in result\n+\n+            assert rtif_num == len(result)\n+\n+            # Verify old records are deleted and only 'num_to_keep' records are kept\n+            # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n+            expected_query_count_based_on_db = (\n+                expected_query_count + 1\n+                if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n+                else expected_query_count\n+            )\n \n-        with assert_queries_count(expected_query_count_based_on_db):\n-            RTIF.delete_old_records(task_id=task.task_id, dag_id=task.dag_id, num_to_keep=num_to_keep)\n-        result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n-        assert remaining_rtifs == len(result)\n+            with assert_queries_count(expected_query_count_based_on_db):\n+                RTIF.delete_old_records(task_id=task.task_id, dag_id=task.dag_id, num_to_keep=num_to_keep)\n+            result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()\n+            assert remaining_rtifs == len(result)\n \n     @pytest.mark.parametrize(\n         \"num_runs, num_to_keep, remaining_rtifs, expected_query_count\",\n@@ -207,40 +210,41 @@ def test_delete_old_records_mapped(\n         Test that old records are deleted from rendered_task_instance_fields table\n         for a given task_id and dag_id with mapped tasks.\n         \"\"\"\n-        with dag_maker(\"test_delete_old_records\", session=session) as dag:\n-            mapped = BashOperator.partial(task_id=\"mapped\").expand(bash_command=[\"a\", \"b\"])\n-        for num in range(num_runs):\n-            dr = dag_maker.create_dagrun(\n-                run_id=f\"run_{num}\", execution_date=dag.start_date + timedelta(days=num)\n+        with set_current_task_instance_session(session=session):\n+            with dag_maker(\"test_delete_old_records\", session=session) as dag:\n+                mapped = BashOperator.partial(task_id=\"mapped\").expand(bash_command=[\"a\", \"b\"])\n+            for num in range(num_runs):\n+                dr = dag_maker.create_dagrun(\n+                    run_id=f\"run_{num}\", execution_date=dag.start_date + timedelta(days=num)\n+                )\n+\n+                mapped.expand_mapped_task(dr.run_id, session=dag_maker.session)\n+                session.refresh(dr)\n+                for ti in dr.task_instances:\n+                    ti.task = dag.get_task(ti.task_id)\n+                    session.add(RTIF(ti))\n+            session.flush()\n+\n+            result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id).all()\n+            assert len(result) == num_runs * 2\n+\n+            # Verify old records are deleted and only 'num_to_keep' records are kept\n+            # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n+            expected_query_count_based_on_db = (\n+                expected_query_count + 1\n+                if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n+                else expected_query_count\n             )\n \n-            mapped.expand_mapped_task(dr.run_id, session=dag_maker.session)\n-            session.refresh(dr)\n-            for ti in dr.task_instances:\n-                ti.task = dag.get_task(ti.task_id)\n-                session.add(RTIF(ti))\n-        session.flush()\n-\n-        result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id).all()\n-        assert len(result) == num_runs * 2\n-\n-        # Verify old records are deleted and only 'num_to_keep' records are kept\n-        # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records\n-        expected_query_count_based_on_db = (\n-            expected_query_count + 1\n-            if session.bind.dialect.name == \"mssql\" and expected_query_count != 0\n-            else expected_query_count\n-        )\n-\n-        with assert_queries_count(expected_query_count_based_on_db):\n-            RTIF.delete_old_records(\n-                task_id=mapped.task_id, dag_id=dr.dag_id, num_to_keep=num_to_keep, session=session\n-            )\n-        result = session.query(RTIF).filter_by(dag_id=dag.dag_id, task_id=mapped.task_id).all()\n-        rtif_num_runs = Counter(rtif.run_id for rtif in result)\n-        assert len(rtif_num_runs) == remaining_rtifs\n-        # Check that we have _all_ the data for each row\n-        assert len(result) == remaining_rtifs * 2\n+            with assert_queries_count(expected_query_count_based_on_db):\n+                RTIF.delete_old_records(\n+                    task_id=mapped.task_id, dag_id=dr.dag_id, num_to_keep=num_to_keep, session=session\n+                )\n+            result = session.query(RTIF).filter_by(dag_id=dag.dag_id, task_id=mapped.task_id).all()\n+            rtif_num_runs = Counter(rtif.run_id for rtif in result)\n+            assert len(rtif_num_runs) == remaining_rtifs\n+            # Check that we have _all_ the data for each row\n+            assert len(result) == remaining_rtifs * 2\n \n     def test_write(self, dag_maker):\n         \"\"\"\n\n@@ -41,7 +41,7 @@ def pull(value):\n     # The function passed to \"map\" is *NOT* a task.\n     assert set(dag.task_dict) == {\"push\", \"pull\"}\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -79,7 +79,7 @@ def c_to_none(v):\n \n         pull.expand(value=push().map(c_to_none))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -113,7 +113,7 @@ def c_to_none(v):\n \n         pull.expand_kwargs(push().map(c_to_none))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -158,7 +158,7 @@ def does_not_work_with_c(v):\n \n         pull.expand_kwargs(push().map(does_not_work_with_c))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # The \"push\" task should not fail.\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -211,7 +211,7 @@ def skip_c(v):\n \n         collect(value=forward.expand_kwargs(push().map(skip_c)))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -246,7 +246,7 @@ def pull(value):\n         converted = push().map(lambda v: v * 2).map(lambda v: {\"value\": v})\n         pull.expand_kwargs(converted)\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n@@ -289,7 +289,7 @@ def convert_zipped(zipped):\n \n         pull.expand(value=combined.map(convert_zipped))\n \n-    dr = dag_maker.create_dagrun()\n+    dr = dag_maker.create_dagrun(session=session)\n \n     # Run \"push_letters\" and \"push_numbers\".\n     decision = dr.task_instance_scheduling_decisions(session=session)\n"}
{"id": 71, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nI hereby agree to the terms of the [RisingWave Labs, Inc. Contributor License Agreement](https://raw.githubusercontent.com/risingwavelabs/risingwave/17af8a747593ebdbfa826691daf75bdab7d14fa0/.github/contributor-license-agreement.txt).\r\n\r\nissue: resolve #22872 \r\n\r\n## What's changed and what's your intention?\r\n\r\nTests under src/storage/src/vector/distance.rs used a signed difference to compare floating-point values:\r\n`($first - $second) < FLOAT_ALLOWED_BIAS`\r\n\r\nThis is asymmetric and can let large negative differences pass. After upgrading CMake I also observed a drift (~1.2e-5) that slightly exceeds the current 1e-5 epsilon causing the test to fail locally.\r\n\r\nObserved failure: `Expected: 31.531513, Actual: 31.531525  (|Œî| ‚âà 1.2e-5)`\r\n\r\n**This PR:**\r\n- Fixes the logic to compare absolute difference.\r\n- Uses a combined absolute + relative tolerance to remain strict yet robust across SIMD/FMA/BLAS/FAISS code paths.\r\n\r\n**Changes:**\r\n- Replace the signed comparison with abs() on the difference, this measures the magnitude of the difference, so both positive and negative deviations are caught.\r\n- Introduce FLOAT_ABS_EPS = 2e-5 and FLOAT_REL_EPS = 1e-6 to allow for expected noise while still detecting genuine errors.\r\n- The tolerance is computed as the larger of the absolute floor (FLOAT_ABS_EPS) and a scaled relative term (FLOAT_REL_EPS * max(|a|, |b|)). This relative component adjusts the allowed difference based on the magnitude of the values, ensuring the comparison remains fair and consistent across both small and large numbers.\r\n\r\n<!--\r\n\r\n**Please do not leave this empty!**\r\n\r\nPlease explain **IN DETAIL** what the changes are in this PR and why they are needed:\r\n\r\n- Summarize your change (**mandatory**)\r\n- How does this PR work? Need a brief introduction for the changed logic (optional)\r\n- Describe clearly one logical change and avoid lazy messages (optional)\r\n- Describe any limitations of the current code (optional)\r\n- Refer to a related PR or issue link (optional)\r\n\r\n-->\r\n\r\n## Checklist\r\n\r\n- [ ] I have written necessary rustdoc comments.\r\n- [ ] <!-- OPTIONAL --> I have added necessary unit tests and integration tests.\r\n- [ ] <!-- OPTIONAL --> I have added test labels as necessary. <!-- See https://github.com/risingwavelabs/risingwave/blob/main/docs/developer-guide.md#ci-labels-guide) -->\r\n- [ ] <!-- OPTIONAL --> I have added fuzzing tests or opened an issue to track them. <!-- Recommended for new SQL features, see #7934 -->\r\n- [ ] <!-- OPTIONAL --> My PR contains breaking changes. <!-- If it deprecates some features, please create a tracking issue to remove them in the future -->\r\n- [ ] <!-- OPTIONAL --> My PR changes performance-critical code, so I will run (micro) benchmarks and present the results. <!-- To manually trigger a benchmark, please check out [Notion](https://www.notion.so/risingwave-labs/Manually-trigger-nexmark-performance-dashboard-test-b784f1eae1cf48889b2645d020b6b7d3). -->\r\n- [ ] <!-- OPTIONAL --> I have checked the [Release Timeline](https://github.com/risingwavelabs/rw-commits-history/blob/main/release_timeline.md) and [Currently Supported Versions](https://docs.risingwave.com/changelog/release-support-policy#support-end-dates-for-recent-releases) to determine which release branches I need to cherry-pick this PR into. <!-- Please check out the [details](https://github.com/risingwavelabs/risingwave/blob/main/CONTRIBUTING.md) -->\r\n\r\n\r\n## Documentation\r\n\r\n- [ ] <!-- OPTIONAL --> My PR needs documentation updates. <!-- Please use the **Release note** section below to summarize the impact on users -->\r\n\r\n<details>\r\n<summary><b>Release note</b></summary>\r\n\r\n<!--\r\nIf this PR includes changes that directly affect users or other significant modifications relevant to the community, kindly draft a release note to provide a concise summary of these changes.\r\n\r\nPlease prioritize highlighting the impact these changes will have on users.\r\nDiscuss technical details in the \"What's changed\" section, and focus on the impact on users in the release note.\r\n\r\nYou should also mention the environment or conditions where the impact may occur.\r\n-->\r\n\r\n</details>\r\n\n\n### Code Patch:\n@@ -235,17 +235,24 @@ mod tests {\n         0.22877127, 0.97690505, 0.44438475,\n     ];\n \n-    const FLOAT_ALLOWED_BIAS: f32 = 1e-5;\n+    const FLOAT_ABS_EPS: f32 = 2e-5;\n+    const FLOAT_REL_EPS: f32 = 1e-6;\n \n     macro_rules! assert_eq_float {\n-        ($first:expr, $second:expr) => {\n+        ($first:expr, $second:expr) => {{\n+            let a: f32 = $first;\n+            let b: f32 = $second;\n+            let diff = (a - b).abs();\n+            let tol = FLOAT_ABS_EPS.max(FLOAT_REL_EPS * a.abs().max(b.abs()));\n             assert!(\n-                ($first - $second) < FLOAT_ALLOWED_BIAS,\n-                \"Expected: {}, Actual: {}\",\n-                $second,\n-                $first\n+                diff <= tol,\n+                \"Expected: {}, Actual: {}, |Œî|={} > tol={}\",\n+                b,\n+                a,\n+                diff,\n+                tol\n             );\n-        };\n+        }};\n     }\n \n     #[test]\n"}
{"id": 254, "ground_truth": "OS", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nNoticed `test_basic_series_frame_alignment` and `test_raw_roundtrip` can occasitionally fail in the CI. Marking as xfail strict=False and clearing the clipboard respectively\n\n### Code Patch:\n@@ -860,6 +860,19 @@ def test_basic_frame_series_alignment(\n     def test_basic_series_frame_alignment(\n         self, request, engine, parser, index_name, r_idx_type, c_idx_type\n     ):\n+        if (\n+            engine == \"numexpr\"\n+            and parser == \"pandas\"\n+            and index_name == \"index\"\n+            and r_idx_type == \"i\"\n+            and c_idx_type == \"s\"\n+        ):\n+            reason = (\n+                f\"Flaky column ordering when engine={engine}, \"\n+                f\"parser={parser}, index_name={index_name}, \"\n+                f\"r_idx_type={r_idx_type}, c_idx_type={c_idx_type}\"\n+            )\n+            request.node.add_marker(pytest.mark.xfail(reason=reason, strict=False))\n         df = tm.makeCustomDataframe(\n             10, 7, data_gen_f=f, r_idx_type=r_idx_type, c_idx_type=c_idx_type\n         )\n\n@@ -1,10 +1,15 @@\n import os\n+import subprocess\n from textwrap import dedent\n \n import numpy as np\n import pytest\n \n-from pandas.compat import is_platform_mac\n+from pandas.compat import (\n+    is_ci_environment,\n+    is_platform_linux,\n+    is_platform_mac,\n+)\n from pandas.errors import (\n     PyperclipException,\n     PyperclipWindowsException,\n@@ -409,6 +414,9 @@ def test_raw_roundtrip(self, data):\n         # PR #25040 wide unicode wasn't copied correctly on PY3 on windows\n         clipboard_set(data)\n         assert data == clipboard_get()\n+        if is_ci_environment() and is_platform_linux():\n+            # Clipboard can sometimes keep previous param causing flaky CI failures\n+            subprocess.run([\"xsel\", \"--delete\", \"--clipboard\"], check=True)\n \n     @pytest.mark.parametrize(\"dtype_backend\", [\"pandas\", \"pyarrow\"])\n     @pytest.mark.parametrize(\"engine\", [\"c\", \"python\"])\n"}
{"id": 48, "ground_truth": "Unordered Data", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\nHey! You're PRing? Cool!\r\nPlease be sure to check out our contribution guide (https://github.com/python-telegram-bot/python-telegram-bot/blob/master/.github/CONTRIBUTING.rst).\r\nEspecially, please have a look at the check list for PRs (https://github.com/python-telegram-bot/python-telegram-bot/blob/master/.github/CONTRIBUTING.rst#check-list-for-prs). Feel free to copy (parts of) the checklist to the PR description to remind you or the maintainers of open points or if you have questions on anything.\r\n-->\r\n\r\n\n\n### Code Patch:\n@@ -2368,11 +2368,15 @@ async def test_delete_message_old_message(self, bot, chat_id):\n     # test modules. No need to duplicate here.\n \n     async def test_delete_messages(self, bot, chat_id):\n-        msg1 = await bot.send_message(chat_id, text=\"will be deleted\")\n-        msg2 = await bot.send_message(chat_id, text=\"will be deleted\")\n-        await asyncio.sleep(2)\n+        msg1, msg2 = await asyncio.gather(\n+            bot.send_message(chat_id, text=\"will be deleted\"),\n+            bot.send_message(chat_id, text=\"will be deleted\"),\n+        )\n \n-        assert await bot.delete_messages(chat_id=chat_id, message_ids=(msg1.id, msg2.id)) is True\n+        assert (\n+            await bot.delete_messages(chat_id=chat_id, message_ids=sorted((msg1.id, msg2.id)))\n+            is True\n+        )\n \n     async def test_send_venue(self, bot, chat_id):\n         longitude = -46.788279\n@@ -3915,7 +3919,7 @@ async def test_copy_messages(self, bot, chat_id):\n         msg1, msg2 = await tasks\n \n         copy_messages = await bot.copy_messages(\n-            chat_id, from_chat_id=chat_id, message_ids=(msg1.message_id, msg2.message_id)\n+            chat_id, from_chat_id=chat_id, message_ids=sorted((msg1.message_id, msg2.message_id))\n         )\n         assert isinstance(copy_messages, tuple)\n \n\n@@ -234,6 +234,11 @@ async def test_max_message_length(self, bot, chat_id):\n             return_exceptions=True,\n         )\n         good_msg, bad_msg = await tasks\n+\n+        if isinstance(good_msg, BaseException):\n+            # handling xfails\n+            raise good_msg\n+\n         assert good_msg.text == good_text\n         assert isinstance(bad_msg, BadRequest)\n         assert \"Message is too long\" in str(bad_msg)\n@@ -247,6 +252,11 @@ async def test_max_caption_length(self, bot, chat_id):\n             return_exceptions=True,\n         )\n         good_msg, bad_msg = await tasks\n+\n+        if isinstance(good_msg, BaseException):\n+            # handling xfails\n+            raise good_msg\n+\n         assert good_msg.caption == good_caption\n         assert isinstance(bad_msg, BadRequest)\n         assert \"Message caption is too long\" in str(bad_msg)\n"}
{"id": 320, "ground_truth": "Time", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n\n### Code Patch:\n@@ -400,7 +400,7 @@ repos:\n         name: Check and update common.sql API stubs\n         entry: ./scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py\n         language: python\n-        files: ^scripts/ci/pre_commit/pre_commit_update_common_sql_api\\.py|^airflow/providers/common/sql/.*\\.py[i]$\n+        files: ^scripts/ci/pre_commit/pre_commit_update_common_sql_api\\.py|^airflow/providers/common/sql/.*\\.pyi?$\n         additional_dependencies: ['rich>=12.4.4', 'mypy==0.971', 'black==22.3.0']\n         pass_filenames: false\n         require_serial: true\n"}
{"id": 312, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR aims to fix flaky tests in `SparkConnectServiceSuite` which are caused by `executorHolder` [undefined](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L908).\r\nThe conditions to reproduce this issue are:\r\n\r\n(1) The operation finishes before its `executeHolder` is set in [MockSparkListener#onOtherEvent](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L961).\r\n(2) `executeHolder` is accessed through calling `verifyEvents.onComplete` after the operation finishes. \r\n\r\n`SparkListenerConnectOperationStarted` is posted asynchronously with the corresponding operation so the condition (1) can be met. After an operation finishes, `executeHolder` is [removed from a map](https://github.com/apache/spark/blob/af16aa8e11c223642f928b0b9893854a851d70bb/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectExecutionManager.scala#L153) so if the condition (1) is met, `executeHolder` is never set because `SparkConnectService.executionManager.getExecuteHolder` consistently returns `None`.\r\n\r\nOne example of the test affected by this issue is `SPARK-43923: commands send events - get_resources_command`.\r\nYou can easily reproduce this issue by inserting sleep into `MockSparkListener#onOtherEvent` like as follows.\r\n\r\n```\r\n   val executeKey =\r\n     ExecuteKey(sessionHolder.userId, sessionHolder.sessionId, e.operationId)\r\n+  Thread.sleep(1000)\r\n   executeHolder = SparkConnectService.executionManager.getExecuteHolder(executeKey)\r\n```\r\n\r\nAnd then, run test.\r\n```\r\n$ build/sbt 'connect/testOnly org.apache.spark.sql.connect.planner.SparkConnectServiceSuite -- -z \"get_resources_command\"'\r\n```\r\nTo resolve this issue, this PR proposes:\r\n\r\n* Change `VerifyEvents#onCompleted` just to assert `executeHolder.eventsManager.getProducedRowCount == producedRowCount`\r\n* Call `VerifyEvents#onCompleted` from `StreamObserver#onCompleted`\r\n* Add `VerifyEvents#assertClosed` to check if the status is `Closed`\r\n\r\n### Why are the changes needed?\r\nFor test stability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nInserting `Thread.sleep(1000)` like mentioned above and then run `SparkConnectServiceSuite`.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\n\n### Code Patch:\n@@ -192,10 +192,11 @@ class SparkConnectServiceSuite\n           }\n \n           override def onCompleted(): Unit = {\n+            verifyEvents.onCompleted(Some(100))\n             done = true\n           }\n         })\n-      verifyEvents.onCompleted(Some(100))\n+      verifyEvents.assertClosed()\n       // The current implementation is expected to be blocking. This is here to make sure it is.\n       assert(done)\n \n@@ -293,10 +294,11 @@ class SparkConnectServiceSuite\n           }\n \n           override def onCompleted(): Unit = {\n+            verifyEvents.onCompleted(Some(6))\n             done = true\n           }\n         })\n-      verifyEvents.onCompleted(Some(6))\n+      verifyEvents.assertClosed()\n       // The current implementation is expected to be blocking. This is here to make sure it is.\n       assert(done)\n \n@@ -529,10 +531,11 @@ class SparkConnectServiceSuite\n           }\n \n           override def onCompleted(): Unit = {\n+            verifyEvents.onCompleted(producedNumRows)\n             done = true\n           }\n         })\n-      verifyEvents.onCompleted(producedNumRows)\n+      verifyEvents.assertClosed()\n       // The current implementation is expected to be blocking.\n       // This is here to make sure it is.\n       assert(done)\n@@ -620,7 +623,7 @@ class SparkConnectServiceSuite\n           }\n         })\n       thread.join()\n-      verifyEvents.onCompleted()\n+      verifyEvents.assertClosed()\n     }\n   }\n \n@@ -683,7 +686,7 @@ class SparkConnectServiceSuite\n           }\n         })\n       assert(failures.isEmpty, s\"this should have no failures but got $failures\")\n-      verifyEvents.onCompleted()\n+      verifyEvents.assertClosed()\n     }\n   }\n \n@@ -925,6 +928,8 @@ class SparkConnectServiceSuite\n     }\n     def onCompleted(producedRowCount: Option[Long] = None): Unit = {\n       assert(executeHolder.eventsManager.getProducedRowCount == producedRowCount)\n+    }\n+    def assertClosed(): Unit = {\n       // The eventsManager is closed asynchronously\n       Eventually.eventually(EVENT_WAIT_TIMEOUT) {\n         assert(\n"}
{"id": 295, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n**Build Scans:**\n- [elasticsearch-intake #27235 / part4](https://gradle-enterprise.elastic.co/s/gf3w65zgpsugy)\n- [elasticsearch-pull-request #87067 / part-4](https://gradle-enterprise.elastic.co/s/ptir5haak4lju)\n- [elasticsearch-pull-request #87067 / part-4-fips](https://gradle-enterprise.elastic.co/s/2inhl3fgi7sae)\n\n**Reproduction Line:**\n```\n./gradlew \":x-pack:plugin:security:test\" --tests \"org.elasticsearch.xpack.security.authc.AuthenticationServiceTests.testInvalidToken\" -Dtests.seed=3801949404B71D88 -Dtests.locale=nl-SX -Dtests.timezone=Etc/Universal -Druntime.java=24\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nN/A\n\n**Failure History:**\n[See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.xpack.security.authc.AuthenticationServiceTests),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!('testInvalidToken'),title:'Test',type:optionsListControl)))))\n\n**Failure Message:**\n```\njava.lang.Exception: Test abandoned because suite timeout was reached.\n```\n\n**Issue Reasons:**\n- [main] 3 failures in test testInvalidToken (1.3% fail rate in 228 executions)\n\n**Note:**\nThis issue was created using new test triage automation. Please report issues or feedback to es-delivery.\n\n### Code Patch:\n@@ -1980,6 +1980,11 @@ public void testInvalidToken() throws Exception {\n             when(projectIndex.getUnavailableReason(any())).thenReturn(new ElasticsearchException(getTestName()));\n         } else {\n             when(projectIndex.isAvailable(any())).thenReturn(true);\n+            doAnswer(invocationOnMock -> {\n+                Runnable runnable = (Runnable) invocationOnMock.getArguments()[1];\n+                runnable.run();\n+                return null;\n+            }).when(projectIndex).checkIndexVersionThenExecute(anyConsumer(), any(Runnable.class));\n             doAnswer(inv -> {\n                 final GetRequest request = inv.getArgument(0);\n                 final ActionListener<GetResponse> listener = inv.getArgument(1);\n"}
{"id": 208, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe test was flaking because instead of receiving an ENTER, we received a PRESENT. This happens when the presence entry arrives whilst a SYNC is in progress.\r\n\r\nWe wait for presence.get() to return, ensuring that the sync is complete prior to proceeding.\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n* **Tests**\n  * Improved reliability of presence-related integration tests by ensuring deterministic synchronization before component mount.\n  * Added detailed debug logging around presence events to aid diagnosis when tests fail.\n  * Reduced test flakiness by capturing and validating events consistently.\n\n* **Chores**\n  * Minor test harness adjustments to streamline debugging and maintainability without affecting user-facing behavior.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->\n\n### Code Patch:\n@@ -20,6 +20,7 @@ describe('usePresence', () => {\n     // create new clients\n     const chatClientOne = newChatClient();\n     const chatClientTwo = newChatClient();\n+    const logger = chatClientTwo.logger;\n \n     // create a second room and attach it, so we can listen for presence events\n     const roomName = randomRoomName();\n@@ -28,10 +29,19 @@ describe('usePresence', () => {\n \n     // start listening for presence events on room two\n     const presenceEventsRoomTwo: PresenceEvent[] = [];\n-    roomTwo.presence.subscribe((presenceEvent) => presenceEventsRoomTwo.push(presenceEvent));\n+    roomTwo.presence.subscribe((presenceEvent) => {\n+      logger.debug('received presence event', presenceEvent);\n+      presenceEventsRoomTwo.push(presenceEvent);\n+    });\n \n     let isPresentState = false;\n \n+    // Before we mount the component, we're going to call presence.get() to force\n+    // the SYNC to complete. If we don't do this, then events later may be either\n+    // present OR enter, which is brittle to assert on. This guarantees that we get enter\n+    // by not entering presence until sync is complete.\n+    await roomTwo.presence.get();\n+\n     const TestComponent = ({ initialData }: { initialData: PresenceData }) => {\n       const { update, myPresenceState } = usePresence({ initialData });\n \n"}
{"id": 10, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis PR allows SparkQA (i.e. Jenkins) to report in its posts to GitHub what category of test failed, if one can be determined.\n\nThe failure categories are:\n- general failure\n- RAT checks failed\n- Scala style checks failed\n- Python style checks failed\n- Build failed\n- Spark unit tests failed\n- PySpark unit tests failed\n- MiMa checks failed\n\nThis PR also fixes the diffing logic used to determine if a patch introduces new classes.\n\n\n### Code Patch:\n@@ -24,6 +24,16 @@ cd \"$FWDIR\"\n # Remove work directory\n rm -rf ./work\n \n+source \"$FWDIR/dev/run-tests-codes.sh\"\n+\n+CURRENT_BLOCK=$BLOCK_GENERAL\n+\n+function handle_error () {\n+  echo \"[error] Got a return code of $? on line $1 of the run-tests script.\"\n+  exit $CURRENT_BLOCK\n+}\n+\n+\n # Build against the right verison of Hadoop.\n {\n   if [ -n \"$AMPLAB_JENKINS_BUILD_PROFILE\" ]; then\n@@ -91,33 +101,43 @@ if [ -n \"$AMPLAB_JENKINS\" ]; then\n   fi\n fi\n \n-# Fail fast\n-set -e\n set -o pipefail\n+trap 'handle_error $LINENO' ERR\n \n echo \"\"\n echo \"=========================================================================\"\n echo \"Running Apache RAT checks\"\n echo \"=========================================================================\"\n+\n+CURRENT_BLOCK=$BLOCK_RAT\n+\n ./dev/check-license\n \n echo \"\"\n echo \"=========================================================================\"\n echo \"Running Scala style checks\"\n echo \"=========================================================================\"\n+\n+CURRENT_BLOCK=$BLOCK_SCALA_STYLE\n+\n ./dev/lint-scala\n \n echo \"\"\n echo \"=========================================================================\"\n echo \"Running Python style checks\"\n echo \"=========================================================================\"\n+\n+CURRENT_BLOCK=$BLOCK_PYTHON_STYLE\n+\n ./dev/lint-python\n \n echo \"\"\n echo \"=========================================================================\"\n echo \"Building Spark\"\n echo \"=========================================================================\"\n \n+CURRENT_BLOCK=$BLOCK_BUILD\n+\n {\n   # We always build with Hive because the PySpark Spark SQL tests need it.\n   BUILD_MVN_PROFILE_ARGS=\"$SBT_MAVEN_PROFILES_ARGS -Phive\"\n@@ -141,6 +161,8 @@ echo \"=========================================================================\"\n echo \"Running Spark unit tests\"\n echo \"=========================================================================\"\n \n+CURRENT_BLOCK=$BLOCK_SPARK_UNIT_TESTS\n+\n {\n   # If the Spark SQL tests are enabled, run the tests with the Hive profiles enabled.\n   # This must be a single argument, as it is.\n@@ -175,10 +197,16 @@ echo \"\"\n echo \"=========================================================================\"\n echo \"Running PySpark tests\"\n echo \"=========================================================================\"\n+\n+CURRENT_BLOCK=$BLOCK_PYSPARK_UNIT_TESTS\n+\n ./python/run-tests\n \n echo \"\"\n echo \"=========================================================================\"\n echo \"Detecting binary incompatibilites with MiMa\"\n echo \"=========================================================================\"\n+\n+CURRENT_BLOCK=$BLOCK_MIMA\n+\n ./dev/mima\n\n@@ -0,0 +1,27 @@\n+#!/usr/bin/env bash\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+readonly BLOCK_GENERAL=10\n+readonly BLOCK_RAT=11\n+readonly BLOCK_SCALA_STYLE=12\n+readonly BLOCK_PYTHON_STYLE=13\n+readonly BLOCK_BUILD=14\n+readonly BLOCK_SPARK_UNIT_TESTS=15\n+readonly BLOCK_PYSPARK_UNIT_TESTS=16\n+readonly BLOCK_MIMA=17\n\n@@ -26,9 +26,23 @@\n FWDIR=\"$(cd `dirname $0`/..; pwd)\"\n cd \"$FWDIR\"\n \n+source \"$FWDIR/dev/run-tests-codes.sh\"\n+\n COMMENTS_URL=\"https://api.github.com/repos/apache/spark/issues/$ghprbPullId/comments\"\n PULL_REQUEST_URL=\"https://github.com/apache/spark/pull/$ghprbPullId\"\n \n+# Important Environment Variables\n+# ---\n+# $ghprbActualCommit\n+#+  This is the hash of the most recent commit in the PR.\n+#+  The merge-base of this and master is the commit from which the PR was branched.\n+# $sha1\n+#+  If the patch merges cleanly, this is a reference to the merge commit hash\n+#+    (e.g. \"origin/pr/2606/merge\").\n+#+  If the patch does not merge cleanly, it is equal to $ghprbActualCommit.\n+#+  The merge-base of this and master in the case of a clean merge is the most recent commit\n+#+    against master.\n+\n COMMIT_URL=\"https://github.com/apache/spark/commit/${ghprbActualCommit}\"\n # GitHub doesn't auto-link short hashes when submitted via the API, unfortunately. :(\n SHORT_COMMIT_HASH=\"${ghprbActualCommit:0:7}\"\n@@ -84,42 +98,46 @@ function post_message () {\n   fi\n }\n \n+\n+# We diff master...$ghprbActualCommit because that gets us changes introduced in the PR\n+#+ and not anything else added to master since the PR was branched.\n+\n # check PR merge-ability and check for new public classes\n {\n   if [ \"$sha1\" == \"$ghprbActualCommit\" ]; then\n-    merge_note=\" * This patch **does not** merge cleanly!\"\n+    merge_note=\" * This patch **does not merge cleanly**.\"\n   else\n     merge_note=\" * This patch merges cleanly.\"\n+  fi\n+  \n+  source_files=$(\n+      git diff master...$ghprbActualCommit --name-only  `# diff patch against master from branch point` \\\n+    | grep -v -e \"\\/test\"                               `# ignore files in test directories` \\\n+    | grep -e \"\\.py$\" -e \"\\.java$\" -e \"\\.scala$\"        `# include only code files` \\\n+    | tr \"\\n\" \" \"\n+  )\n+  new_public_classes=$(\n+      git diff master...$ghprbActualCommit ${source_files}      `# diff patch against master from branch point` \\\n+    | grep \"^\\+\"                              `# filter in only added lines` \\\n+    | sed -r -e \"s/^\\+//g\"                    `# remove the leading +` \\\n+    | grep -e \"trait \" -e \"class \"            `# filter in lines with these key words` \\\n+    | grep -e \"{\" -e \"(\"                      `# filter in lines with these key words, too` \\\n+    | grep -v -e \"\\@\\@\" -e \"private\"          `# exclude lines with these words` \\\n+    | grep -v -e \"^// \" -e \"^/\\*\" -e \"^ \\* \"  `# exclude comment lines` \\\n+    | sed -r -e \"s/\\{.*//g\"                   `# remove from the { onwards` \\\n+    | sed -r -e \"s/\\}//g\"                     `# just in case, remove }; they mess the JSON` \\\n+    | sed -r -e \"s/\\\"/\\\\\\\\\\\"/g\"               `# escape double quotes; they mess the JSON` \\\n+    | sed -r -e \"s/^(.*)$/\\`\\1\\`/g\"           `# surround with backticks for style` \\\n+    | sed -r -e \"s/^/  \\* /g\"                 `# prepend '  *' to start of line` \\\n+    | sed -r -e \"s/$/\\\\\\n/g\"                  `# append newline to end of line` \\\n+    | tr -d \"\\n\"                              `# remove actual LF characters`\n+  )\n \n-    source_files=$(\n-        git diff master... --name-only              `# diff patch against master from branch point` \\\n-      | grep -v -e \"\\/test\"                         `# ignore files in test directories` \\\n-      | grep -e \"\\.py$\" -e \"\\.java$\" -e \"\\.scala$\"  `# include only code files` \\\n-      | tr \"\\n\" \" \"\n-    )\n-    new_public_classes=$(\n-        git diff master... ${source_files}      `# diff patch against master from branch point` \\\n-      | grep \"^\\+\"                              `# filter in only added lines` \\\n-      | sed -r -e \"s/^\\+//g\"                    `# remove the leading +` \\\n-      | grep -e \"trait \" -e \"class \"            `# filter in lines with these key words` \\\n-      | grep -e \"{\" -e \"(\"                      `# filter in lines with these key words, too` \\\n-      | grep -v -e \"\\@\\@\" -e \"private\"          `# exclude lines with these words` \\\n-      | grep -v -e \"^// \" -e \"^/\\*\" -e \"^ \\* \"  `# exclude comment lines` \\\n-      | sed -r -e \"s/\\{.*//g\"                   `# remove from the { onwards` \\\n-      | sed -r -e \"s/\\}//g\"                     `# just in case, remove }; they mess the JSON` \\\n-      | sed -r -e \"s/\\\"/\\\\\\\\\\\"/g\"               `# escape double quotes; they mess the JSON` \\\n-      | sed -r -e \"s/^(.*)$/\\`\\1\\`/g\"           `# surround with backticks for style` \\\n-      | sed -r -e \"s/^/  \\* /g\"                 `# prepend '  *' to start of line` \\\n-      | sed -r -e \"s/$/\\\\\\n/g\"                  `# append newline to end of line` \\\n-      | tr -d \"\\n\"                              `# remove actual LF characters`\n-    )\n-\n-    if [ \"$new_public_classes\" == \"\" ]; then\n-      public_classes_note=\" * This patch adds no public classes.\"\n-    else\n-      public_classes_note=\" * This patch adds the following public classes _(experimental)_:\"\n-      public_classes_note=\"${public_classes_note}\\n${new_public_classes}\"\n-    fi\n+  if [ -z \"$new_public_classes\" ]; then\n+    public_classes_note=\" * This patch adds no public classes.\"\n+  else\n+    public_classes_note=\" * This patch adds the following public classes _(experimental)_:\"\n+    public_classes_note=\"${public_classes_note}\\n${new_public_classes}\"\n   fi\n }\n \n@@ -147,12 +165,30 @@ function post_message () {\n \n     post_message \"$fail_message\"\n     exit $test_result\n+  elif [ \"$test_result\" -eq \"0\" ]; then\n+    test_result_note=\" * This patch **passes all tests**.\"\n   else\n-    if [ \"$test_result\" -eq \"0\" ]; then\n-      test_result_note=\" * This patch **passes** unit tests.\"\n+    if [ \"$test_result\" -eq \"$BLOCK_GENERAL\" ]; then\n+      failing_test=\"some tests\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_RAT\" ]; then\n+      failing_test=\"RAT tests\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_SCALA_STYLE\" ]; then\n+      failing_test=\"Scala style tests\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_PYTHON_STYLE\" ]; then\n+      failing_test=\"Python style tests\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_BUILD\" ]; then\n+      failing_test=\"to build\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_SPARK_UNIT_TESTS\" ]; then\n+      failing_test=\"Spark unit tests\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_PYSPARK_UNIT_TESTS\" ]; then\n+      failing_test=\"PySpark unit tests\"\n+    elif [ \"$test_result\" -eq \"$BLOCK_MIMA\" ]; then\n+      failing_test=\"MiMa tests\"\n     else\n-      test_result_note=\" * This patch **fails** unit tests.\"\n+      failing_test=\"some tests\"\n     fi\n+    \n+    test_result_note=\" * This patch **fails $failing_test**.\"\n   fi\n }\n \n"}
{"id": 232, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nWhen playing around with it, I noticed some flakiness in this test across sessions.\r\n \r\nAfter debugging, turns out the heavy sync primitives that I was calling (like `nvshmem_quiet()` or `nvshmem_fence()`) from inside Triton kernels was causing deadlocks. The original test tried to guarantee ordering: `put(data) -> fence/quiet -> put(flag)`. But the GPU thread got stuck in `quiet()` waiting for network confirmation while holding the SM, creating a deadlock.\r\n\r\nThe fix was realizing `wait_until` already provides all the sync you need. Just do:\r\n- PE A: `nvshmem_wait_until(&ivar, ...)`  \r\n- PE B: `nvshmem_put(&ivar_on_PE_A, ...)`\r\n\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #159788\n* #159756\n* #159755\n* #159734\n* #159701\n* __->__ #159215\n* #159136\n* #158718\n* #158515\n\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta\n\n### Code Patch:\n@@ -172,6 +172,11 @@ def barrier_test_kernel(\n         tl.store(p_dst, received + 1)\n \n \n+@triton.jit\n+def barrier_all_kernel():\n+    nvshmem.barrier_all()\n+\n+\n @triton.jit\n def sync_test_kernel(\n     dst_ptr,\n@@ -530,66 +535,49 @@ def test_triton_wait_until(self) -> None:\n \n         rank = self.rank\n         peer = (self.world_size - 1) - rank\n-        NVSHMEM_CMP_EQ = 0  # from nvshmem.h\n-\n-        # Allocate symmetric buffers\n-        msg_size_bytes = 8\n-        dtype = torch.int8\n-        numel = msg_size_bytes // dtype.itemsize\n-        val = 13\n-        flag_val = 21\n+        NVSHMEM_CMP_EQ = 0  # equal comparison\n+        FLAG_INITIAL_VALUE = 0\n+        FLAG_FINAL_VALUE = 42\n \n-        inp = symm_mem.empty(numel, dtype=dtype, device=self.device).fill_(val)\n-        out = symm_mem.empty(numel, dtype=dtype, device=self.device).fill_(-1)\n+        # Use a single int64 symmetric tensor as our synchronization flag.\n+        flag = symm_mem.empty(1, dtype=torch.int64, device=self.device).fill_(\n+            FLAG_INITIAL_VALUE\n+        )\n+        flag_hdl = symm_mem.rendezvous(flag, group=group_name)\n \n-        inp_hdl = symm_mem.rendezvous(inp, group=group_name)\n-        out_hdl = symm_mem.rendezvous(out, group=group_name)\n+        barrier_all_kernel[(1,)](extern_libs=nvshmem_lib)\n \n         if rank == 0:\n-            # Rank 0 waits for the flag to be set by Rank 1, then checks the data\n-            ivar_ptr = out_hdl.signal_pad_ptrs[rank]\n-\n-            wait_until_kernel[(1, 1, 1)](\n+            # Rank 0 (the waiter)\n+            ivar_ptr = flag_hdl.buffer_ptrs[rank]\n+            wait_until_kernel[(1,)](\n                 ivar_ptr,\n                 cmp_op=NVSHMEM_CMP_EQ,\n-                cmp_val=flag_val,\n+                cmp_val=FLAG_FINAL_VALUE,\n                 extern_libs=nvshmem_lib,\n             )\n \n+            # Verification\n             torch.testing.assert_close(\n-                out,\n-                val * torch.ones(numel, dtype=dtype, device=self.device),\n+                flag,\n+                torch.tensor([FLAG_FINAL_VALUE], dtype=torch.int64, device=self.device),\n             )\n \n         if rank == 1:\n-            # Rank 1 puts data into Rank 0's output buffer\n-            dst_ptr = out_hdl.buffer_ptrs[peer]\n-            src_ptr = inp_hdl.buffer_ptrs[rank]\n-\n-            putmem_block_kernel[(1, 1, 1)](\n-                dst_ptr,\n-                src_ptr,\n-                size_bytes=msg_size_bytes,\n-                peer=peer,\n-                extern_libs=nvshmem_lib,\n+            # Rank 1 (the signaler)\n+            val_to_put = torch.tensor(\n+                [FLAG_FINAL_VALUE], dtype=torch.int64, device=self.device\n             )\n \n-            # Fence to order data put before flag put\n-            @triton.jit\n-            def fence_kernel():\n-                nvshmem.fence()\n-\n-            fence_kernel[(1, 1, 1)](extern_libs=nvshmem_lib)\n+            # The destination is Rank 0's flag buffer.\n+            dst_ptr = flag_hdl.buffer_ptrs[rank]\n \n-            # Put the flag value (do not use signal_op here)\n-            flag_src = torch.tensor([flag_val], dtype=torch.int64, device=self.device)\n-            flag_dst_ptr = out_hdl.signal_pad_ptrs[peer]\n-\n-            putmem_block_kernel[(1, 1, 1)](\n-                flag_dst_ptr,\n-                flag_src.data_ptr(),\n-                size_bytes=8,  # 8 bytes for int64\n-                peer=peer,\n+            # Launch a kernel to put the value to Rank 0.\n+            putmem_block_kernel[(1,)](\n+                dst_ptr,  # Destination pointer on the remote PE\n+                val_to_put.data_ptr(),  # Source data pointer (local)\n+                size_bytes=8,  # Size of one int64\n+                peer=peer,  # The target PE (Rank 0)\n                 extern_libs=nvshmem_lib,\n             )\n \n"}
{"id": 119, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Description:\r\nI observed a previous suggested fix injected some delay to fix the flaky test, but I believe that fix might be unstable in a CI environment or when run on different machines, given the dependency on some constant wait time. I suggest a new way to fix the test by adding some synchronization for the test execution only. I at first identify the source code location whose slow execution (from Line 436 to 442) leads to the flaky test failure. Hence, I introduce one variable in this test class  that is only there to provide some synchronization. Basically, until these statements are executed, I force the thread that shuts down the services. The waiting location is at Line 447 of this test class.\r\n\r\n### Failure:\r\nRunning org.fluentd.logger.TestFluentLogger\r\n2023-03-31 10:11:42,340 DEBUG [pool-2-thread-1] Started MockFluentd port:33845\r\nException in thread \"pool-3-thread-15\" java.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\njava.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\nException in thread \"pool-3-thread-7\" java.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\nException in thread \"pool-3-thread-10\" java.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\n2023-03-31 10:16:50,774 ERROR [main] Timed out\r\n2023-03-31 10:16:50,774 DEBUG [pool-2-thread-1] Terminated MockFluentd port:33845\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 308.585 sec <<< FAILURE!\r\n\r\n### Results :\r\n\r\nFailed tests:\r\ntestInMultiThreading(org.fluentd.logger.TestFluentLogger): expected:<210000> but was:<209591>\r\n\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0\n\n### Code Patch:\n@@ -27,6 +27,7 @@\n import static org.junit.Assert.*;\n \n public class TestFluentLogger {\n+    private static volatile boolean hasExecuted = false;\n     private Logger _logger = LoggerFactory.getLogger(TestFluentLogger.class);\n \n     class FixedThreadManager {\n@@ -439,9 +440,13 @@ public void run() {\n                             logger.flush();\n                     }\n                     logger.flush();\n+                    hasExecuted = true;\n                 }\n             });\n         }\n+        while (!hasExecuted) { \n+            Thread.yield(); \n+        }\n         Thread.sleep(1000);\n         executorService.shutdown();\n         executorService.awaitTermination(300, TimeUnit.SECONDS);\n"}
{"id": 125, "ground_truth": "I/O", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Description\r\n\r\nThis PR addresses the issue of flaky test results when executing the `run` command test (`verifies that a melos script can call another script containing steps, and ensures all commands in those steps are executed successfully`) in Linux CI/CD environments, as reported in issue #670. \r\n\r\nCloses #670\r\n\r\n## Type of Change\r\n\r\n<!--- Put an `x` in all the boxes that apply: -->\r\n\r\n- [ ] ‚ú® `feat` -- New feature (non-breaking change which adds functionality)\r\n- [x] üõ†Ô∏è `fix` -- Bug fix (non-breaking change which fixes an issue)\r\n- [ ] ‚ùå `!` -- Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] üßπ `refactor` -- Code refactor\r\n- [ ] ‚úÖ `ci` -- Build configuration change\r\n- [ ] üìù `docs` -- Documentation\r\n- [ ] üóëÔ∏è `chore` -- Chore\r\n\n\n### Code Patch:\n@@ -594,7 +594,7 @@ melos run hello_script\n \n       expect(\n         logger.output.normalizeNewLines(),\n-        ignoringAnsii(\n+        ignoringDependencyMessages(\n           '''\n melos run hello_script\n   ‚îî> test_script\n\n@@ -26,6 +26,25 @@ import 'package:path/path.dart' as p;\n import 'package:test/test.dart';\n import 'package:yaml/yaml.dart';\n \n+/// Resolve flaky test in linux environments\n+Matcher ignoringDependencyMessages(String expected) {\n+  return predicate(\n+    (actual) {\n+      final normalizedActual = actual\n+          .toString()\n+          .split('\\n')\n+          .where(\n+            (line) =>\n+                !line.startsWith('Resolving dependencies...') &&\n+                !line.startsWith('Got dependencies!'),\n+          )\n+          .join('\\n');\n+      return ignoringAnsii(expected).matches(normalizedActual, {});\n+    },\n+    'ignores dependency resolution messages',\n+  );\n+}\n+\n Matcher packageNamed(Object? matcher) => _PackageNameMatcher(matcher);\n \n Matcher ignoringAnsii(Object? matcher) {\n\n@@ -110,7 +110,7 @@ The packages that caused the problem are:\n       );\n \n       expect(result.exitCode, 0);\n-      expect(result.stdout, 'a\\n');\n+      expect(result.stdout, ignoringDependencyMessages('a\\n'));\n     });\n \n     test(\n"}
{"id": 390, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\nparallel/test-http-keep-alive-empty-line\n\n### Platform\n\nLinux ARM64\n\n### Console output\n\n```console\nnot ok 1876 parallel/test-http-keep-alive-empty-line\n  ---\n  duration_ms: 3582.76000\n  severity: fail\n  exitcode: 1\n  stack: |-\n    node:events:486\n          throw er; // Unhandled 'error' event\n          ^\n    \n    Error [ERR_STREAM_WRITE_AFTER_END]: write after end\n        at _write (node:internal/streams/writable:487:11)\n        at Writable.write (node:internal/streams/writable:508:10)\n        at Timeout._onTimeout (file:///home/iojs/build/workspace/node-test-commit-arm-debug/test/parallel/test-http-keep-alive-empty-line.mjs:32:14)\n        at listOnTimeout (node:internal/timers:608:17)\n        at process.processTimers (node:internal/timers:543:7)\n    Emitted 'error' event on Socket instance at:\n        at emitErrorNT (node:internal/streams/destroy:170:8)\n        at emitErrorCloseNT (node:internal/streams/destroy:129:3)\n        at process.processTicksAndRejections (node:internal/process/task_queues:90:21) {\n      code: 'ERR_STREAM_WRITE_AFTER_END'\n    }\n    \n    Node.js v25.0.0-pre\n  ...\n```\n\n### Build links\n\n| Reason | <code>parallel/test-http-keep-alive-empty-line</code> |\n| - | :- |\n| Type | JS_TEST_FAILURE |\n| Failed PR | 7 ([https://github.com/nodejs/node/pull/59507/](https://ci.nodejs.org/job/node-test-pull-request/68737/), [https://github.com/nodejs/node/pull/59350/](https://ci.nodejs.org/job/node-test-pull-request/68767/), [https://github.com/nodejs/node/pull/59550/](https://ci.nodejs.org/job/node-test-pull-request/68771/), [https://github.com/nodejs/node/pull/59464/](https://ci.nodejs.org/job/node-test-pull-request/68774/), [https://github.com/nodejs/node/pull/59527/](https://ci.nodejs.org/job/node-test-pull-request/68779/), [https://github.com/nodejs/node/pull/59570/](https://ci.nodejs.org/job/node-test-pull-request/68790/), [https://github.com/nodejs/node/pull/59558/](https://ci.nodejs.org/job/node-test-pull-request/68798/)) |\n| Appeared | [test-equinix-ubuntu2204_container-arm64-4](https://ci.nodejs.org/job/node-test-commit-arm-debug/nodes=ubuntu2204_debug-arm64/20010/console), [test-equinix-rhel8_container-arm64-2](https://ci.nodejs.org/job/node-test-commit-arm/nodes=rhel8-arm64/60027/console), [test-equinix-ubuntu2204_container-arm64-6](https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu2204-arm64/60019/console) |\n| First CI | https://ci.nodejs.org/job/node-test-pull-request/68737/ |\n| Last CI | https://ci.nodejs.org/job/node-test-pull-request/68798/ |\n\n### Additional information\n\nThis test was added in https://github.com/nodejs/node/pull/58178 and has been flaky some time after that.\n\n### Code Patch:\n@@ -3,6 +3,10 @@ import assert from 'node:assert';\n import { createServer } from 'node:http';\n import { connect } from 'node:net';\n \n+// This test ensures that data like an empty line (`\\r\\n`) recevied by the\n+// server after a request, does not reset the keep-alive timeout. See\n+// https://github.com/nodejs/node/issues/58140.\n+\n const server = createServer({\n   connectionsCheckingInterval: 100,\n   headersTimeout: 100,\n@@ -28,23 +32,24 @@ server.listen(0, () => {\n           '\\r\\n'\n     );\n \n-    setTimeout(() => {\n-      client.write('\\r\\n');\n-    }, 100);\n-\n-    let responseBuffer = '';\n+    let response = '';\n+    let responseReceived = false;\n \n+    client.setEncoding('utf-8');\n     client.on('data', (chunk) => {\n-      responseBuffer += chunk.toString();\n+      response += chunk;\n \n       // Check if we've received the full header (ending with \\r\\n\\r\\n)\n-      if (responseBuffer.includes('\\r\\n\\r\\n')) {\n-        const statusLine = responseBuffer.split('\\r\\n')[0];\n+      if (response.includes('\\r\\n\\r\\n')) {\n+        responseReceived = true;\n+        const statusLine = response.split('\\r\\n')[0];\n         const status = statusLine.split(' ')[1];\n         assert.strictEqual(status, '404');\n-        client.end();\n+        client.write('\\r\\n');\n       }\n     });\n-    client.on('end', common.mustCall());\n+    client.on('end', common.mustCall(() => {\n+      assert.ok(responseReceived);\n+    }));\n   });\n });\n"}
{"id": 105, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## What does this PR do?\r\n\r\n`ExpressionVisitorTest#testTraverseExpression` relies on ordered traversal. However, the traversal depends on the order of `getDeclaredFields()` to be deterministic (see below). The Java specification explicitly marks `getDeclaredFields` as a function that can return field names in any order. In the wild, this is most likely to manifest on different architectures with different JVM versions.\r\n\r\nhttps://github.com/apache/fury/blob/54b62fb6ab5d7e557131efe07c7402c885f6e7c4/java/fury-core/src/main/java/org/apache/fury/reflect/ReflectionUtils.java#L368-L381\r\n\r\nUsing [NonDex](https://github.com/TestingResearchIllinois/NonDex), we can replicate the flaky behavior with the following command:\r\n\r\n```\r\nmvn edu.illinois:nondex-maven-plugin:2.1.7:nondex -pl fury-core/ -Dcheckstyle.skip -Dmaven.javadoc.skip -Dtest=org.apache.fury.codegen.ExpressionVisitorTest\r\n```\r\n\r\nThe fix, in this case, is to simply use HashSet equality instead of an ordered List equality. The above NonDex command should succeed after applying this patch.\r\n\r\nI do, however, note that there are other flaky tests (found by running NonDex on the entire `fury-core` project) that fail due to reliance on e.g. `PriorityQueue#toArray`, which is also explicity marked to return nondeterministically ordered arrays.\r\n\r\n## Does this PR introduce any user-facing change?\r\n\r\nNo\n\n### Code Patch:\n@@ -26,7 +26,9 @@\n import java.lang.reflect.Method;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.HashSet;\n import java.util.List;\n+import java.util.Set;\n import org.apache.fury.codegen.Expression.Literal;\n import org.apache.fury.reflect.ReflectionUtils;\n import org.apache.fury.reflect.TypeRef;\n@@ -63,7 +65,11 @@ public void testTraverseExpression() throws InvocationTargetException, IllegalAc\n     assertEquals(serializedLambda.getCapturedArgCount(), 1);\n     ExpressionVisitor.ExprHolder exprHolder =\n         (ExpressionVisitor.ExprHolder) (serializedLambda.getCapturedArg(0));\n-    assertEquals(\n-        expressions, Arrays.asList(forLoop, start, end, step, e1, ref, exprHolder.get(\"e2\")));\n+\n+    // Traversal relies on getDeclaredFields(), nondeterministic order.\n+    Set<Expression> expressionsSet = new HashSet<>(expressions);\n+    Set<Expression> expressionsSet2 =\n+        new HashSet<>(Arrays.asList(forLoop, e1, ref, exprHolder.get(\"e2\"), end, start, step));\n+    assertEquals(expressionsSet, expressionsSet2);\n   }\n }\n"}
{"id": 231, "ground_truth": "Time", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews\r\n3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed\r\n-->\r\n\r\n**What this PR does / why we need it**:\r\nDelete \"hugetlb\" from whitelistControllers\r\n\r\n**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #50770\r\n\r\n**Special notes for your reviewer**:\r\n\r\n**Release note**:\r\n<!--  Steps to write your release note:\r\n1. Use the release-note-* labels to set the release note state (if you have access)\r\n2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.\r\n-->\r\n```\r\nNONE\r\n```\r\n\n\n### Code Patch:\n@@ -233,7 +233,7 @@ func (m *cgroupManagerImpl) Exists(name CgroupName) bool {\n \t// scoped to the set control groups it understands.  this is being discussed\n \t// in https://github.com/opencontainers/runc/issues/1440\n \t// once resolved, we can remove this code.\n-\twhitelistControllers := sets.NewString(\"cpu\", \"cpuacct\", \"cpuset\", \"memory\", \"hugetlb\", \"systemd\")\n+\twhitelistControllers := sets.NewString(\"cpu\", \"cpuacct\", \"cpuset\", \"memory\", \"systemd\")\n \n \t// If even one cgroup path doesn't exist, then the cgroup doesn't exist.\n \tfor controller, path := range cgroupPaths {\n"}
{"id": 112, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nI moved this in a previous PR, but I think firstly it's only actually required by one of the tests, and more importantly it might be reloading the object before the page has finished submitting.\n\n### Code Patch:\n@@ -35,11 +35,12 @@\n   context do\n     before do\n       click_button \"Close case\"\n-      enforcement.reload\n     end\n \n     it \"can be closed\", :capybara do\n       expect(page).to have_content(\"Case successfully closed\")\n+\n+      enforcement.reload\n       expect(enforcement.status).to eq(\"closed\")\n       expect(enforcement.closed_reason).to eq(\"Because I want to\")\n     end\n"}
{"id": 364, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Changes\r\n\r\n- Increased timeout delay for the flaky test to reduce flakiness.\r\n- Adjusted the waiting logic in `e2e_test.go` to ensure proper container health check completion.\r\n\r\n## Verification\r\n\r\n- Ran the test suite multiple times locally in Gitpod and observed no flakiness.\r\n- Confirmed that the timeout handles the observed race condition.\r\n\r\n* [x] I added CHANGELOG entry for this change.\r\n\r\n## Changelog\r\n\r\n- [#8114](https://github.com/thanos-io/thanos/pull/8114) E2E: Increased test timeout delay to reduce flakiness in container startup checks.\r\n\r\n\n\n### Code Patch:\n@@ -464,7 +464,11 @@ func TestReloader_ConfigDirApply(t *testing.T) {\n \t\t\t// ‚îú‚îÄ rule3-001.yaml -> rule3-source.yaml\n \t\t\t// ‚îî‚îÄ rule3-source.yaml\n \t\t\ttestutil.Ok(t, os.Symlink(path.Join(dir2, \"rule3-001.yaml\"), path.Join(dir2, \"rule3.yaml\")))\n-\t\t\ttestutil.Ok(t, os.Rename(path.Join(dir2, \"rule3.yaml\"), path.Join(dir, \"rule3.yaml\")))\n+\n+// ADD THIS DELAY TO FIX THE FLAKINESS\n+time.Sleep(200 * time.Millisecond) // üëà Insert this line\n+\n+testutil.Ok(t, os.Rename(path.Join(dir2, \"rule3.yaml\"), path.Join(dir, \"rule3.yaml\")))\n \t\t\t// out1\n \t\t\t// ‚îú‚îÄ rule1.yaml\n \t\t\t// ‚îú‚îÄ rule2.yaml\n"}
{"id": 159, "ground_truth": "Test Order Dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes random failures in the killSkipmeYesNo test by asserting that at least one client is killed, instead of exactly one.\r\n\r\nCloses #4165 \r\n\n\n### Code Patch:\n@@ -1,5 +1,8 @@\n package redis.clients.jedis.commands.jedis;\n \n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n import static org.junit.jupiter.api.Assertions.assertArrayEquals;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n@@ -178,7 +181,9 @@ public void killSkipmeNo() {\n   public void killSkipmeYesNo() {\n     jedis.clientKill(new ClientKillParams().type(ClientType.NORMAL).skipMe(SkipMe.YES));\n     assertDisconnected(client);\n-    assertEquals(1, jedis.clientKill(new ClientKillParams().type(ClientType.NORMAL).skipMe(SkipMe.NO)));\n+\n+    ClientKillParams skipmeNo = new ClientKillParams().type(ClientType.NORMAL).skipMe(SkipMe.NO);\n+    assertThat(jedis.clientKill(skipmeNo), greaterThanOrEqualTo(1L));\n     assertDisconnected(jedis);\n   }\n \n"}
{"id": 281, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Failure cluster [9ddf28290cbb0b1d7335](https://go.k8s.io/triage#9ddf28290cbb0b1d7335)\n\n##### Error text:\n```\nFailed\n=== RUN   TestApply/optional-list-map-key\napply_test.go:221: ERROR: \n        \tError Trace:\t/home/prow/go/src/k8s.io/kubernetes/test/integration/apimachinery/apply/apply_test.go:93\n        \t            \t\t\t\t/home/prow/go/src/k8s.io/kubernetes/test/integration/apimachinery/apply/apply_test.go:127\n        \t            \t\t\t\t/home/prow/go/src/k8s.io/kubernetes/test/integration/apimachinery/apply/apply_test.go:221\n        \t            \t\t\t\t/home/prow/go/src/k8s.io/kubernetes/test/utils/ktesting/tcontext.go:383\n        \tError:      \tNot equal: \n        \t            \texpected: \"- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"x\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status1\\n  operation: Apply\\n  subresource: status\\n- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n        k:{\\\"a\\\":2,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status2\\n  operation: Apply\\n  subresource: status\\n\"\n        \t            \tactual  : \"- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n        k:{\\\"a\\\":2,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status2\\n  operation: Apply\\n  subresource: status\\n- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"x\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status1\\n  operation: Apply\\n  subresource: status\\n\"\n        \t            \t\n        \t            \tDiff:\n        \t            \t--- Expected\n        \t            \t+++ Actual\n        \t            \t@@ -1,14 +1 @@\n        \t            \t-- apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t-  fieldsType: FieldsV1\n        \t            \t-  fieldsV1:\n        \t            \t-    f:status:\n        \t            \t-      f:infos:\n        \t            \t-        k:{\"a\":1,\"b\":\"x\"}:\n        \t            \t-          .: {}\n        \t            \t-          f:a: {}\n        \t            \t-          f:b: {}\n        \t            \t-          f:data: {}\n        \t            \t-  manager: status1\n        \t            \t-  operation: Apply\n        \t            \t-  subresource: status\n        \t            \t - apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t@@ -31,2 +18,15 @@\n        \t            \t   subresource: status\n        \t            \t+- apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t+  fieldsType: FieldsV1\n        \t            \t+  fieldsV1:\n        \t            \t+    f:status:\n        \t            \t+      f:infos:\n        \t            \t+        k:{\"a\":1,\"b\":\"x\"}:\n        \t            \t+          .: {}\n        \t            \t+          f:a: {}\n        \t            \t+          f:b: {}\n        \t            \t+          f:data: {}\n        \t            \t+  manager: status1\n        \t            \t+  operation: Apply\n        \t            \t+  subresource: status\n        \t            \t \n        \tTest:       \tTestApply/optional-list-map-key\n        \tMessages:   \tremove status #1:\n        \t            \tapiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \tkind: Carp\n        \t            \tmetadata:\n        \t            \t  creationTimestamp: \"2025-08-29T07:24:13Z\"\n        \t            \t  managedFields:\n        \t            \t  - apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t    fieldsType: FieldsV1\n        \t            \t    fieldsV1:\n        \t            \t      f:status:\n        \t            \t        f:infos:\n        \t            \t          k:{\"a\":1,\"b\":\"y\"}:\n        \t            \t            .: {}\n        \t            \t            f:a: {}\n        \t            \t            f:b: {}\n        \t            \t            f:data: {}\n        \t            \t          k:{\"a\":2,\"b\":\"y\"}:\n        \t            \t            .: {}\n        \t            \t            f:a: {}\n        \t            \t            f:b: {}\n        \t            \t            f:data: {}\n        \t            \t    manager: status2\n        \t            \t    operation: Apply\n        \t            \t    subresource: status\n        \t            \t    time: \"2025-08-29T07:24:13Z\"\n        \t            \t  - apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t    fieldsType: FieldsV1\n        \t            \t    fieldsV1:\n        \t            \t      f:status:\n        \t            \t        f:infos:\n        \t            \t          k:{\"a\":1,\"b\":\"x\"}:\n        \t            \t            .: {}\n        \t            \t            f:a: {}\n        \t            \t            f:b: {}\n        \t            \t            f:data: {}\n        \t            \t    manager: status1\n        \t            \t    operation: Apply\n        \t            \t    subresource: status\n        \t            \t    time: \"2025-08-29T07:24:14Z\"\n        \t            \t  name: test-carp\n        \t            \t  namespace: testapply-optional-list-map-key-tvqfh\n        \t            \t  resourceVersion: \"605\"\n        \t            \t  uid: a34ec7bb-a425-468a-b298-9f8ab731f2de\n        \t            \tspec: {}\n        \t            \tstatus:\n        \t            \t  infos:\n        \t            \t  - a: 1\n        \t            \t    b: x\n        \t            \t    data: status1_a1_bx\n        \t            \t  - a: 1\n        \t            \t    b: \"y\"\n        \t            \t    data: status2_a1_by\n        \t            \t  - a: 2\n        \t            \t    b: \"y\"\n        \t            \t    data: status2_a2_by\n        \n--- FAIL: TestApply/optional-list-map-key (0.75s)\n```\n\nIt looks like the order of the list entries is non-deterministic, causing a flake when the order is different than expected.\n\n\n#### Recent failures:\n[8/27/2025, 7:46:41 PM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1960760579014529024)\n[8/26/2025, 7:11:13 AM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1960208069166108672)\n[8/23/2025, 1:44:09 AM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1959038602079899648)\n[8/21/2025, 1:19:07 AM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1958307527783354368)\n\n\n/kind failing-test\n/kind flake\n/sig api-machinery\n\n\n\n### Code Patch:\n@@ -83,12 +83,20 @@ func testOptionalListMapKey(tCtx ktesting.TContext) {\n \trequireManagedFields := func(what string, obj *unstructured.Unstructured, expectedManagedFields any) {\n \t\ttCtx.Helper()\n \t\tactualManagedFields, _, _ := unstructured.NestedFieldCopy(obj.Object, \"metadata\", \"managedFields\")\n-\t\t// Strip non-deterministic time.\n \t\tif actualManagedFields != nil {\n \t\t\tmanagers := actualManagedFields.([]any)\n \t\t\tfor i := range managers {\n+\t\t\t\t// Strip non-deterministic time.\n \t\t\t\tunstructured.RemoveNestedField(managers[i].(map[string]any), \"time\")\n \t\t\t}\n+\t\t\t// Sort by manager. There should be at most one entry per manager, so\n+\t\t\t// no need for a tie breaker. Semantically the order is irrelevant,\n+\t\t\t// so we don't need to expect a specific one here.\n+\t\t\t//\n+\t\t\t// The order turned out to be non-deterministic (test flake!) without this.\n+\t\t\tslices.SortFunc(managers, func(a, b any) int {\n+\t\t\t\treturn strings.Compare(a.(map[string]any)[\"manager\"].(string), b.(map[string]any)[\"manager\"].(string))\n+\t\t\t})\n \t\t}\n \t\trequire.Equal(tCtx, dump(expectedManagedFields), dump(actualManagedFields), \"%s:\\n%s\", what, dump(obj))\n \t}\n"}
{"id": 154, "ground_truth": "Unordered Data", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n- Added default blurb for help text to show the 3 kinds of requests possible\n- Generate request-type specific help text (e.g., trying 'cloudcfg run' will provide usage helptext on the run command.)\n\n```\n$ ./output/go/cloudcfg\nusage: cloudcfg -h <host> [-c config/file.json] [-p <hostPort>:<containerPort>,..., <hostPort-n>:<containerPort-n>] <method> <methodArgs>\n\n  Kubernetes REST API:\n  cloudcfg [OPTIONS] get|list|create|delete|update <url>\n\n  Run an image:\n  cloudcfg [OPTIONS] run <image> <replicas> <name>\n\n  Manage replication controllers:\n  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>\n\n  Options:\n  -auth=\"/Users/tlow/.kubernetes_auth\": Path to the auth info file.  If missing, prompt the user\n  -c=\"\": Path to the config file.\n  -h=\"\": The host to connect to.\n  -json=false: If true, print raw JSON for responses\n  -l=\"\": Label query to use for listing\n  -p=\"\": The port spec, comma-separated list of <external>:<internal>,...\n  -s=-1: If positive, create and run a corresponding service on this port, only used with 'run'\n  -u=1m0s: Update interarrival period\n  -v=false: Print the version number.\n  -yaml=false: If true, print raw YAML for responses\n\n$ ./output/go/cloudcfg rollingupdate\n2014/06/12 11:13:18 usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>\n$ ./output/go/cloudcfg get\n2014/06/12 11:13:22 usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>\n$ ./output/go/cloudcfg run\n2014/06/12 11:13:25 usage: cloudcfg [OPTIONS] run <image> <replicas> <name>\n```\n\n\n### Code Patch:\n@@ -49,7 +49,19 @@ var (\n )\n \n func usage() {\n-\tlog.Fatal(\"Usage: cloudcfg -h <host> [-c config/file.json] [-p <hostPort>:<containerPort>,..., <hostPort-n>:<containerPort-n> <method> <path>\")\n+\tfmt.Fprint(os.Stderr, `usage: cloudcfg -h [-c config/file.json] [-p :,..., :] <method>\n+\n+  Kubernetes REST API:\n+  cloudcfg [OPTIONS] get|list|create|delete|update <url>\n+\n+  Manage replication controllers:\n+  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>\n+  cloudcfg [OPTIONS] run <image> <replicas> <controller>\n+  cloudcfg [OPTIONS] resize <controller> <replicas>\n+\n+  Options:\n+`)\n+\tflag.PrintDefaults()\n }\n \n // Reads & parses config file. On error, calls log.Fatal().\n@@ -71,15 +83,20 @@ func readConfig(storage string) []byte {\n \n // CloudCfg command line tool.\n func main() {\n+\tflag.Usage = func() {\n+\t\tusage()\n+\t}\n+\n \tflag.Parse() // Scan the arguments list\n \n \tif *versionFlag {\n \t\tfmt.Println(\"Version:\", APP_VERSION)\n \t\tos.Exit(0)\n \t}\n \n-\tif len(flag.Args()) < 2 {\n+\tif len(flag.Args()) < 1 {\n \t\tusage()\n+\t\tos.Exit(1)\n \t}\n \tmethod := flag.Arg(0)\n \tsecure := true\n@@ -90,18 +107,6 @@ func main() {\n \tif parsedUrl.Scheme != \"\" && parsedUrl.Scheme != \"https\" {\n \t\tsecure = false\n \t}\n-\tstorage := strings.Trim(flag.Arg(1), \"/\")\n-\turl := *httpServer + path.Join(\"/api/v1beta1\", storage)\n-\tvar request *http.Request\n-\n-\tvar printer cloudcfg.ResourcePrinter\n-\tif *json {\n-\t\tprinter = &cloudcfg.IdentityPrinter{}\n-\t} else if *yaml {\n-\t\tprinter = &cloudcfg.YAMLPrinter{}\n-\t} else {\n-\t\tprinter = &cloudcfg.HumanReadablePrinter{}\n-\t}\n \n \tvar auth *kube_client.AuthInfo\n \tif secure {\n@@ -111,80 +116,116 @@ func main() {\n \t\t}\n \t}\n \n+\tmatchFound := executeAPIRequest(method, auth) || executeControllerRequest(method, auth)\n+\tif matchFound == false {\n+\t\tlog.Fatalf(\"Unknown command %s\", method)\n+\t}\n+}\n+\n+// Attempts to execute an API request\n+func executeAPIRequest(method string, auth *kube_client.AuthInfo) bool {\n+\tparseStorage := func() string {\n+\t\tif len(flag.Args()) != 2 {\n+\t\t\tlog.Fatal(\"usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>\")\n+\t\t}\n+\t\treturn strings.Trim(flag.Arg(1), \"/\")\n+\t}\n+\n+\treadUrl := func(storage string) string {\n+\t\treturn *httpServer + path.Join(\"/api/v1beta1\", storage)\n+\t}\n+\n+\tvar request *http.Request\n+\tvar err error\n \tswitch method {\n \tcase \"get\", \"list\":\n+\t\turl := readUrl(parseStorage())\n \t\tif len(*labelQuery) > 0 && method == \"list\" {\n \t\t\turl = url + \"?labels=\" + *labelQuery\n \t\t}\n \t\trequest, err = http.NewRequest(\"GET\", url, nil)\n \tcase \"delete\":\n-\t\trequest, err = http.NewRequest(\"DELETE\", url, nil)\n+\t\trequest, err = http.NewRequest(\"DELETE\", readUrl(parseStorage()), nil)\n \tcase \"create\":\n-\t\trequest, err = cloudcfg.RequestWithBodyData(readConfig(storage), url, \"POST\")\n+\t\tstorage := parseStorage()\n+\t\trequest, err = cloudcfg.RequestWithBodyData(readConfig(storage), readUrl(storage), \"POST\")\n \tcase \"update\":\n-\t\trequest, err = cloudcfg.RequestWithBodyData(readConfig(storage), url, \"PUT\")\n+\t\tstorage := parseStorage()\n+\t\trequest, err = cloudcfg.RequestWithBodyData(readConfig(storage), readUrl(storage), \"PUT\")\n+\tdefault:\n+\t\treturn false\n+\t}\n+\n+\tvar printer cloudcfg.ResourcePrinter\n+\tif *json {\n+\t\tprinter = &cloudcfg.IdentityPrinter{}\n+\t} else if *yaml {\n+\t\tprinter = &cloudcfg.YAMLPrinter{}\n+\t} else {\n+\t\tprinter = &cloudcfg.HumanReadablePrinter{}\n+\t}\n+\n+\tvar body string\n+\tif body, err = cloudcfg.DoRequest(request, auth); err == nil {\n+\t\tif err = printer.Print(body, os.Stdout); err != nil {\n+\t\t\tlog.Fatalf(\"Failed to print: %#v\\nRaw received text:\\n%v\\n\", err, string(body))\n+\t\t}\n+\t\tfmt.Print(\"\\n\")\n+\t} else {\n+\t\tlog.Fatalf(\"Error: %#v %s\", err, body)\n+\t}\n+\n+\treturn true\n+}\n+\n+// Attempts to execute a replicationController request\n+func executeControllerRequest(method string, auth *kube_client.AuthInfo) bool {\n+\tparseController := func() string {\n+\t\tif len(flag.Args()) != 2 {\n+\t\t\tlog.Fatal(\"usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>\")\n+\t\t}\n+\t\treturn flag.Arg(1)\n+\t}\n+\n+\tvar err error\n+\tswitch method {\n+\tcase \"stop\":\n+\t\terr = cloudcfg.StopController(parseController(), kube_client.Client{Host: *httpServer, Auth: auth})\n+\tcase \"rm\":\n+\t\terr = cloudcfg.DeleteController(parseController(), kube_client.Client{Host: *httpServer, Auth: auth})\n \tcase \"rollingupdate\":\n \t\tclient := &kube_client.Client{\n \t\t\tHost: *httpServer,\n \t\t\tAuth: auth,\n \t\t}\n-\t\tcloudcfg.Update(flag.Arg(1), client, *updatePeriod)\n+\t\terr = cloudcfg.Update(parseController(), client, *updatePeriod)\n \tcase \"run\":\n-\t\targs := flag.Args()\n-\t\tif len(args) < 4 {\n-\t\t\tlog.Fatal(\"usage: cloudcfg -h <host> run <image> <replicas> <name>\")\n+\t\tif len(flag.Args()) != 4 {\n+\t\t\tlog.Fatal(\"usage: cloudcfg [OPTIONS] run <image> <replicas> <controller>\")\n \t\t}\n-\t\timage := args[1]\n-\t\treplicas, err := strconv.Atoi(args[2])\n-\t\tname := args[3]\n+\t\timage := flag.Arg(1)\n+\t\treplicas, err := strconv.Atoi(flag.Arg(2))\n+\t\tname := flag.Arg(3)\n \t\tif err != nil {\n \t\t\tlog.Fatalf(\"Error parsing replicas: %#v\", err)\n \t\t}\n \t\terr = cloudcfg.RunController(image, name, replicas, kube_client.Client{Host: *httpServer, Auth: auth}, *portSpec, *servicePort)\n-\t\tif err != nil {\n-\t\t\tlog.Fatalf(\"Error: %#v\", err)\n-\t\t}\n-\t\treturn\n-\tcase \"stop\":\n-\t\terr = cloudcfg.StopController(flag.Arg(1), kube_client.Client{Host: *httpServer, Auth: auth})\n-\t\tif err != nil {\n-\t\t\tlog.Fatalf(\"Error: %#v\", err)\n-\t\t}\n-\t\treturn\n \tcase \"resize\":\n \t\targs := flag.Args()\n \t\tif len(args) < 3 {\n-\t\t\tlog.Fatal(\"usage: cloudcfg resize <name> <replicas>\")\n+\t\t\tlog.Fatal(\"usage: cloudcfg resize <controller> <replicas>\")\n \t\t}\n \t\tname := args[1]\n \t\treplicas, err := strconv.Atoi(args[2])\n \t\tif err != nil {\n \t\t\tlog.Fatalf(\"Error parsing replicas: %#v\", err)\n \t\t}\n \t\terr = cloudcfg.ResizeController(name, replicas, kube_client.Client{Host: *httpServer, Auth: auth})\n-\t\tif err != nil {\n-\t\t\tlog.Fatalf(\"Error: %#v\", err)\n-\t\t}\n-\t\treturn\n-\tcase \"rm\":\n-\t\terr = cloudcfg.DeleteController(flag.Arg(1), kube_client.Client{Host: *httpServer, Auth: auth})\n-\t\tif err != nil {\n-\t\t\tlog.Fatalf(\"Error: %#v\", err)\n-\t\t}\n-\t\treturn\n \tdefault:\n-\t\tlog.Fatalf(\"Unknown command: %s\", method)\n+\t\treturn false\n \t}\n \tif err != nil {\n \t\tlog.Fatalf(\"Error: %#v\", err)\n \t}\n-\tbody, err := cloudcfg.DoRequest(request, auth)\n-\tif err != nil {\n-\t\tlog.Fatalf(\"Error: %#v %s\", err, body)\n-\t}\n-\terr = printer.Print(body, os.Stdout)\n-\tif err != nil {\n-\t\tlog.Fatalf(\"Failed to print: %#v\\nRaw received text:\\n%v\\n\", err, string(body))\n-\t}\n-\tfmt.Print(\"\\n\")\n+\treturn true\n }\n"}
{"id": 413, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--- Note to EXTERNAL Contributors -->\r\n<!-- Thanks for opening a PR! \r\nIf it is a significant code change, please **make sure there is an open issue** for this. \r\nWe work best with you when we have accepted the idea first before you code. -->\r\n\r\n<!--- For ALL Contributors üëá -->\r\n\r\n## What was changed\r\n<!-- Describe what has changed in this PR -->\r\n\r\nBumped to Go 1.25 to use the new `synctest.Test` primitive to run deterministic timing tests.\r\n\r\nTip: use `Hide Whitespace` option for reviewing.\r\n\r\n## Why?\r\n<!-- Tell your future self why have you made these changes -->\r\n\r\n`TestRunDurationWithoutTimeout` is flaky - example:\r\nhttps://github.com/temporalio/omes/actions/runs/17052148233/job/48342124492?pr=179#step:4:125\r\n\r\n## Checklist\r\n<!--- add/delete as needed --->\r\n\r\n1. Closes <!-- add issue number here -->\r\n\r\n2. How was this tested:\r\n<!--- Please describe how you tested your changes/how we can test them -->\r\n\r\n3. Any docs updates needed?\r\n<!--- update README if applicable\r\n      or point out where to update docs.temporal.io -->\r\n\n\n### Code Patch:\n@@ -9,7 +9,7 @@ Omes (pronounced oh-mess) is the Hebrew word for \"load\" (◊¢◊ï◊û◊°).\n \n ## Prerequisites\n \n-- [Go](https://golang.org/) 1.24+\n+- [Go](https://golang.org/) 1.25+\n   - `protoc` + `protoc-gen-go` or [mise](https://mise.jdx.dev/) for [Kitchen Sink Workflow](#kitchen-sink-workflow)\n - [Java](https://openjdk.org/) 8+\n - TypeScript: [Node](https://nodejs.org) 16+\n@@ -134,7 +134,7 @@ go run ./cmd cleanup-scenario --scenario workflow_with_single_noop_activity --ru\n ### Running a specific version of the SDK\n \n The `--version` flag can be used to specify a version of the SDK to use, it accepts either\n-a version number lik `v1.24.0` or you can also pass a local path to use a local SDK version.\n+a version number like `v1.24.0` or you can also pass a local path to use a local SDK version.\n This is useful while testing unreleased or in-development versions of the SDK.\n \n ```sh\n\n@@ -1,6 +1,6 @@\n # Build in a full featured container\n ARG TARGETARCH\n-FROM --platform=linux/$TARGETARCH golang:1.24 AS build\n+FROM --platform=linux/$TARGETARCH golang:1.25 AS build\n \n WORKDIR /app\n \n\n@@ -1,6 +1,6 @@\n module github.com/temporalio/omes\n \n-go 1.24.5\n+go 1.25.0\n \n require (\n \tgithub.com/gogo/protobuf v1.3.2\n\n@@ -5,6 +5,7 @@ import (\n \t\"errors\"\n \t\"sync\"\n \t\"testing\"\n+\t\"testing/synctest\"\n \t\"time\"\n \n \t\"github.com/stretchr/testify/require\"\n@@ -109,143 +110,166 @@ func execute(executor *GenericExecutor) error {\n }\n \n func TestRunHappyPathIterations(t *testing.T) {\n-\ttracker := newIterationTracker()\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{Iterations: 5},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{Iterations: 5},\n+\t\t})\n+\t\trequire.NoError(t, err)\n+\t\ttracker.assertSeen(t, 5)\n \t})\n-\trequire.NoError(t, err)\n-\ttracker.assertSeen(t, 5)\n }\n \n func TestRunFailIterations(t *testing.T) {\n-\ttracker := newIterationTracker()\n-\tconcurrency := 3\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\t// Start this short timer to allow all concurrent routines to be spawned\n-\t\t\t<-time.After(time.Millisecond)\n-\t\t\tif run.Iteration == 2 {\n-\t\t\t\treturn errors.New(\"deliberate fail from test\")\n-\t\t\t}\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{MaxConcurrent: concurrency, Iterations: 50},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\tconcurrency := 3\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\tif run.Iteration == 2 {\n+\t\t\t\t\treturn errors.New(\"deliberate fail from test\")\n+\t\t\t\t}\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{MaxConcurrent: concurrency, Iterations: 50},\n+\t\t})\n+\t\trequire.ErrorContains(t, err, \"run finished with error\")\n+\t\ttracker.assertSeen(t, 2)\n \t})\n-\trequire.ErrorContains(t, err, \"run finished with error\")\n-\ttracker.assertSeen(t, 2)\n }\n \n func TestRunHappyPathDuration(t *testing.T) {\n-\ttracker := newIterationTracker()\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\ttime.Sleep(time.Millisecond * 20)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{Duration: 100 * time.Millisecond},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\ttime.Sleep(time.Millisecond * 20)\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{Duration: 100 * time.Millisecond},\n+\t\t})\n+\t\trequire.NoError(t, err)\n+\t\ttracker.assertSeen(t, DefaultMaxConcurrent*2)\n \t})\n-\trequire.NoError(t, err)\n-\ttracker.assertSeen(t, DefaultMaxConcurrent*2)\n }\n \n func TestRunFailDuration(t *testing.T) {\n-\ttracker := newIterationTracker()\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\tif run.Iteration == 2 {\n-\t\t\t\treturn errors.New(\"deliberate fail from test\")\n-\t\t\t}\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{Duration: 200 * time.Millisecond},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\tif run.Iteration == 2 {\n+\t\t\t\t\treturn errors.New(\"deliberate fail from test\")\n+\t\t\t\t}\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{Duration: 200 * time.Millisecond},\n+\t\t})\n+\t\trequire.ErrorContains(t, err, \"run finished with error\")\n+\t\ttracker.assertSeen(t, 2)\n \t})\n-\trequire.ErrorContains(t, err, \"run finished with error\")\n-\ttracker.assertSeen(t, 2)\n }\n \n func TestRunDurationWithTimeout(t *testing.T) {\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttime.Sleep(time.Millisecond * 20)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{\n-\t\t\tDuration: 100 * time.Millisecond,\n-\t\t\tTimeout:  10 * time.Millisecond},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\t<-ctx.Done()\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{\n+\t\t\t\tDuration: 100 * time.Millisecond,\n+\t\t\t\tTimeout:  10 * time.Millisecond,\n+\t\t\t},\n+\t\t})\n+\t\trequire.Error(t, err)\n+\t\trequire.ErrorContains(t, err, \"timed out\")\n+\t\ttracker.assertSeen(t, 5)\n \t})\n-\trequire.Error(t, err)\n-\trequire.ErrorContains(t, err, \"timed out\")\n }\n \n func TestRunIterationsWithTimeout(t *testing.T) {\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttime.Sleep(time.Millisecond * 20)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{\n-\t\t\tIterations: 5,\n-\t\t\tTimeout:    10 * time.Millisecond,\n-\t\t},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\t<-ctx.Done()\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{\n+\t\t\t\tIterations: 5,\n+\t\t\t\tTimeout:    10 * time.Millisecond,\n+\t\t\t},\n+\t\t})\n+\t\trequire.Error(t, err)\n+\t\trequire.ErrorContains(t, err, \"timed out\")\n+\t\ttracker.assertSeen(t, 2)\n \t})\n-\trequire.Error(t, err)\n-\trequire.ErrorContains(t, err, \"timed out\")\n }\n \n func TestRunDurationWithoutTimeout(t *testing.T) {\n-\ttracker := newIterationTracker()\n-\tstartTime := time.Now()\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\ttime.Sleep(time.Millisecond * 20)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{Duration: 1 * time.Millisecond},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\tstartTime := time.Now()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\ttime.Sleep(time.Millisecond * 20)\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{Duration: 1 * time.Millisecond},\n+\t\t})\n+\t\trequire.Equal(t, time.Millisecond*20, time.Since(startTime))\n+\t\trequire.NoError(t, err)\n+\t\ttracker.assertSeen(t, DefaultMaxConcurrent)\n \t})\n-\trequire.GreaterOrEqual(t, time.Since(startTime), time.Millisecond*20)\n-\trequire.NoError(t, err)\n-\ttracker.assertSeen(t, DefaultMaxConcurrent)\n }\n \n func TestRunIterationsWithoutTimeout(t *testing.T) {\n-\ttracker := newIterationTracker()\n-\tstartTime := time.Now()\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\ttime.Sleep(time.Millisecond * 20)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{Iterations: 5},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\ttracker := newIterationTracker()\n+\t\tstartTime := time.Now()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\ttime.Sleep(time.Millisecond * 20)\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{Iterations: 5},\n+\t\t})\n+\t\trequire.Equal(t, time.Millisecond*20, time.Since(startTime))\n+\t\trequire.NoError(t, err)\n+\t\ttracker.assertSeen(t, 5)\n \t})\n-\trequire.GreaterOrEqual(t, time.Since(startTime), time.Millisecond*20)\n-\trequire.NoError(t, err)\n-\ttracker.assertSeen(t, 5)\n }\n \n func TestRunIterationsWithRateLimit(t *testing.T) {\n-\tstartTime := time.Now()\n-\ttracker := newIterationTracker()\n-\terr := execute(&GenericExecutor{\n-\t\tExecute: func(ctx context.Context, run *Run) error {\n-\t\t\ttracker.track(run.Iteration)\n-\t\t\treturn nil\n-\t\t},\n-\t\tDefaultConfiguration: RunConfiguration{\n-\t\t\tIterations:             4,\n-\t\t\tMaxConcurrent:          1,\n-\t\t\tMaxIterationsPerSecond: 4.0,\n-\t\t},\n+\tsynctest.Test(t, func(t *testing.T) {\n+\t\tstartTime := time.Now()\n+\t\ttracker := newIterationTracker()\n+\t\terr := execute(&GenericExecutor{\n+\t\t\tExecute: func(ctx context.Context, run *Run) error {\n+\t\t\t\ttracker.track(run.Iteration)\n+\t\t\t\treturn nil\n+\t\t\t},\n+\t\t\tDefaultConfiguration: RunConfiguration{\n+\t\t\t\tIterations:             4,\n+\t\t\t\tMaxConcurrent:          1,\n+\t\t\t\tMaxIterationsPerSecond: 4.0,\n+\t\t\t},\n+\t\t})\n+\t\trequire.NoError(t, err)\n+\t\trequire.Equal(t, time.Second, time.Since(startTime))\n+\t\ttracker.assertSeen(t, 4)\n \t})\n-\trequire.NoError(t, err)\n-\trequire.GreaterOrEqual(t, time.Since(startTime), time.Second)\n-\ttracker.assertSeen(t, 4)\n }\n\n@@ -1,6 +1,6 @@\n # Tool versions\n DOTNET_VERSION=8.0.100\n-GO_VERSION=1.24.5\n+GO_VERSION=1.25.0\n GRADLE_VERSION=8.5\n JAVA_VERSION=11\n NODE_VERSION=22\n\n@@ -1,6 +1,6 @@\n module github.com/temporalio/omes/workers/go\n \n-go 1.24.5\n+go 1.25.0\n \n require github.com/temporalio/omes v1.0.0\n \n"}
{"id": 109, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n````\nFAILED providers/microsoft/azure/tests/unit/microsoft/azure/triggers/test_wasb.py::TestWasbBlobSensorTrigger::test_success - AssertionError: assert False is True\n +  where False = <built-in method done of _asyncio.Task object at 0xffe8138b6b00>()\n +    where <built-in method done of _asyncio.Task object at 0xffe8138b6b00> = <Task pending name='Task-2479' coro=<<async_generator_asend without __name__>()> wait_for=<Future pending cb=[shield.<locals>._outer_done_callback() at /usr/python/lib/python3.12/asyncio/tasks.py:922, Task.task_wakeup()]>>.done\n```\n\nFor example https://github.com/apache/airflow/actions/runs/17534123692/job/49795194833#step:8:2322\n\n### Code Patch:\n@@ -23,6 +23,7 @@\n \n import pytest\n \n+from airflow.models import Connection\n from airflow.providers.microsoft.azure.triggers.wasb import (\n     WasbBlobSensorTrigger,\n     WasbPrefixSensorTrigger,\n@@ -35,7 +36,17 @@\n TEST_WASB_CONN_ID = \"wasb_default\"\n POKE_INTERVAL = 5.0\n \n-pytestmark = pytest.mark.db_test\n+\n+@pytest.fixture(autouse=True)\n+def setup_connections(create_connection_without_db):\n+    create_connection_without_db(\n+        Connection(\n+            conn_id=TEST_WASB_CONN_ID,\n+            conn_type=\"wasb\",\n+            login=\"test_account_name\",\n+            password=\"test_account_key\",\n+        )\n+    )\n \n \n class TestWasbBlobSensorTrigger:\n"}
{"id": 293, "ground_truth": "Test order dependency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Rationale for this change\nFixing flaky bloom filter memory check test\n\n### What changes are included in this PR?\nAdded an additional runtime.GC() call to ensure the releases are called.\n\n\n\n### Code Patch:\n@@ -52,6 +52,7 @@ func (suite *BloomFilterBuilderSuite) SetupTest() {\n \n func (suite *BloomFilterBuilderSuite) TearDownTest() {\n \truntime.GC() // we use setfinalizer to clean up the buffers, so run the GC\n+\truntime.GC()\n \tsuite.mem.AssertSize(suite.T(), 0)\n }\n \n"}
{"id": 120, "ground_truth": "Network", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nCurrently, there's a clause that sends no response when the connection\r\nstaet in the channel is `idle`, the initial state.\r\n\r\nhttps://github.com/emqx/emqx/blob/e861f3963e9eee6b16e685aa4c7b57ed428d041b/apps/emqx/src/emqx_channel.erl#L490\r\n\r\nHere, we attempt to publish a simple message before sending the large\r\none to ensure that the channel's connection state is connected.\n\n### Code Patch:\n@@ -92,8 +92,17 @@ def test_maximum_packet_size(base_wait_for, base_sleep, base_socket_timeout):\n   connack = aclient.connect(host=host, port=port, cleanstart=True,\n                             socket_timeout=9 * base_socket_timeout)\n   assert hasattr(connack.properties, \"MaximumPacketSize\")\n-  payload = b\".\" * (int(connack.properties.MaximumPacketSize) + 1)\n+\n+  # publish a simple message to ensure that the channel connection\n+  # state is not idle\n+  aclient.subscribe([topics[0]], [MQTTV5.SubscribeOptions(2)])\n+  time.sleep(2 * base_sleep)\n+  aclient.publish(topics[0], b\".\", 0)\n   time.sleep(2 * base_sleep)\n+  waitfor(callback.messages, 1, 5 * base_wait_for)\n+  callback.clear()\n+\n+  payload = b\".\" * (int(connack.properties.MaximumPacketSize) + 1)\n   aclient.publish(topics[0], payload, 0)\n   # should get back a disconnect with packet size too big\n   waitfor(callback.disconnects, 1, 9 * base_wait_for)\n"}
{"id": 31, "ground_truth": "Async wait", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe StringComparator now works on serialized data.\n\nTo this end new string read/write/copy/compare methods were introduced, which use a variable-length encoding for the characters.\n\n~~key-points:~~\n  ~~\\- The most significant bits are written/read first.~~\n  ~~\\- The first 2 bits of the character are used to encode the size of the character.~~\n ~~\\- A character is at most 3 Bytes big.~~\n\nAdditionally, the StringSerializer now has full unicode support. ~~i couldn't find a unicode character that ~~uses more than 22 bits, as such 3 Bytes should be sufficient.~~\n\n\n### Code Patch:\n@@ -39,9 +39,7 @@ public StringComparator(boolean ascending) {\n \n \t@Override\n \tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException {\n-\t\tString s1 = StringValue.readString(firstSource);\n-\t\tString s2 = StringValue.readString(secondSource);\n-\t\tint comp = s1.compareTo(s2); \n+\t\tint comp = StringValue.compareUnicodeString(firstSource, secondSource);\n \t\treturn ascendingComparison ? comp : -comp;\n \t}\n \n\n@@ -57,16 +57,16 @@ public int getLength() {\n \n \t@Override\n \tpublic void serialize(String record, DataOutputView target) throws IOException {\n-\t\tStringValue.writeString(record, target);\n+\t\tStringValue.writeUnicodeString(record, target);\n \t}\n \n \t@Override\n \tpublic String deserialize(String record, DataInputView source) throws IOException {\n-\t\treturn StringValue.readString(source);\n+\t\treturn StringValue.readUnicodeString(source);\n \t}\n \n \t@Override\n \tpublic void copy(DataInputView source, DataOutputView target) throws IOException {\n-\t\tStringValue.copyString(source, target);\n+\t\tStringValue.copyUnicodeString(source, target);\n \t}\n }\n\n@@ -46,11 +46,12 @@ public class StringValue implements NormalizableKey<StringValue>, CharSequence,\n \t\n \tprivate static final int HIGH_BIT = 0x1 << 7;\n \t\n+\tprivate static final int END_BYTE = 0x60;\n+\t\n \tprivate static final int HIGH_BIT2 = 0x1 << 13;\n \t\n \tprivate static final int HIGH_BIT2_MASK = 0x3 << 6;\n \t\n-\t\n \tprivate char[] value;\t\t// character value of the string value, not necessarily completely filled\n \t\n \tprivate int len;\t\t\t// length of the string value\n@@ -835,4 +836,198 @@ public static final void copyString(DataInput in, DataOutput out) throws IOExcep\n \t\t\t}\n \t\t}\n \t}\n-}\n+\t\n+\t/**\n+\t Writes a CharSequence as a variable-length encoded Unicode String.\n+\t Supports Unicode characters up to 22 bits.\n+\t @param cs CharSequence to write\n+\t @param out output channel\n+\t @throws IOException \n+\t */\n+\tpublic static final void writeUnicodeString(CharSequence cs, DataOutput out) throws IOException {\n+\t\tif (cs == null) {\n+\t\t\twriteLength(0, out);\n+\t\t} else {\n+\t\t\twriteLength(Character.codePointCount(cs, 0, cs.length()) + 1, out);\n+\t\t\tfor (int i = 0; i < cs.length(); i++) {\n+\t\t\t\tint c = Character.codePointAt(cs, i);\n+\t\t\t\tif (c >= 65536) {\n+\t\t\t\t\t//Non-BMP Unicode character, two characters are treated as one\n+\t\t\t\t\ti++;\n+\t\t\t\t}\n+\t\t\t\twriteUnicodeChar(c, out);\n+\t\t\t}\n+\t\t}\n+\t\t//end-of-string byte\n+\t\tout.write(END_BYTE);\n+\t}\n+\t\n+\t/**\n+\tWrites the given int variable-length encoded to the given DataOutput. NOT adjusted for null offset.\n+\t@param lenToWrite int to write\n+\t@param out output\n+\t@throws IOException \n+\t*/\n+\tprivate static void writeLength(int lenToWrite, DataOutput out) throws IOException {\n+\t\tif (lenToWrite < 0) {\n+\t\t\tthrow new IllegalArgumentException(\"CharSequence is too long.\");\n+\t\t}\n+\n+\t\t// write the length, variable-length encoded\n+\t\twhile (lenToWrite >= HIGH_BIT) {\n+\t\t\tout.write(lenToWrite | HIGH_BIT);\n+\t\t\tlenToWrite >>>= 7;\n+\t\t}\n+\t\tout.write(lenToWrite);\n+\t}\n+\n+\t/**\n+\t Writes a variable-length encoded Unicode Character.\n+\t @param c char to write\n+\t @param out output channel\n+\t @return Unicode Character\n+\t @throws IOException \n+\t */\n+\tprivate static void writeUnicodeChar(int c, DataOutput out) throws IOException {\n+\t\tint shift = 0;\n+\n+\t\twhile (c >= (HIGH_BIT << shift)) {\n+\t\t\tshift += 5;\n+\t\t}\n+\n+\t\twhile (shift >= 0) {\n+\t\t\tswitch (shift) {\n+\t\t\t\tcase 0:\n+\t\t\t\t\tout.write(c | 0x80);\n+\t\t\t\t\tshift -= 1;\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase 5:\n+\t\t\t\t\tout.write((c >> shift + 2) & 0x1F);\n+\t\t\t\t\tshift -= 5;\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase 10:\n+\t\t\t\t\tout.write(((c >> shift + 2) | 0x20) & 0x3F);\n+\t\t\t\t\tshift -= 5;\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase 15:\n+\t\t\t\t\tout.write(((c >> shift + 2) | 0x40) & 0x5F);\n+\t\t\t\t\tshift -= 5;\n+\t\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t Reads and returns a variable-length encoded Unicode String.\n+\t @param in input channel\n+\t @return Unicode String\n+\t @throws IOException \n+\t */\n+\tpublic static final String readUnicodeString(DataInput in) throws IOException {\n+\t\tint len = readLength(in);\n+\t\t\n+\t\tfinal int[] data = new int[len];\n+\t\t\n+\t\tfor (int i = 0; i < len; i++) {\n+\t\t\tdata[i] = readUnicodeChar(in);\n+\t\t}\n+\t\t//end-of-string byte\n+\t\tin.readUnsignedByte();\n+\t\treturn new String(data, 0, len);\n+\t}\n+\t\n+\t/**\n+\tReads a variable-length encoded int from the given DataInput. Adjusted for null offset.\n+\t@param in input\n+\t@return read int\n+\t@throws IOException \n+\t*/\n+\tprivate static int readLength(DataInput in) throws IOException {\n+\t\t// the length we read is offset by one, because a length of zero indicates a null value\n+\t\tint len = in.readUnsignedByte();\n+\n+\t\tif (len == 0) {\n+\t\t\treturn 0;\n+\t\t}\n+\n+\t\tif (len >= HIGH_BIT) {\n+\t\t\tint shift = 7;\n+\t\t\tint curr;\n+\t\t\tlen = len & 0x7f;\n+\t\t\twhile ((curr = in.readUnsignedByte()) >= HIGH_BIT) {\n+\t\t\t\tlen |= (curr & 0x7f) << shift;\n+\t\t\t\tshift += 7;\n+\t\t\t}\n+\t\t\tlen |= curr << shift;\n+\t\t}\n+\n+\t\t// subtract one for the null length\n+\t\tlen -= 1;\n+\n+\t\treturn len;\n+\t}\n+\t\n+\t/**\n+\tReads and returns a variable-length encoded Unicode Character.\n+\t@param in input channel\n+\t@return Unicode Character\n+\t@throws IOException \n+\t*/\n+\tprivate static int readUnicodeChar(DataInput in) throws IOException {\n+\t\tint r = 0;\n+\t\tint c;\n+\t\twhile ((c = in.readUnsignedByte()) < HIGH_BIT) {\n+\t\t\tr |= (c & 0x1F);\n+\t\t\tr <<= 5;\n+\t\t}\n+\t\tr <<= 2;\n+\t\tr |= (c & 0x7F);\n+\t\treturn r;\n+\t}\n+\t\n+\t/**\n+\t Copies a serialized variable-length encoded Unicode String.\n+\t @param in input channel\n+\t @param out output channel\n+\t @throws IOException \n+\t */\n+\tpublic static final void copyUnicodeString(DataInput in, DataOutput out) throws IOException {\n+\t\t//copy length\n+\t\tint length = readLength(in);\n+\t\t// the length we write is offset by one, because a length of zero indicates a null value\n+\t\twriteLength(length + 1, out);\n+\n+\t\t//copy data\n+\t\tfor (int i = 0; i < length; i++) {\n+\t\t\twriteUnicodeChar(readUnicodeChar(in),out);\n+\t\t}\n+\t\t//end-of-string byte\n+\t\tin.readUnsignedByte();\n+\t\tout.writeByte(END_BYTE);\n+\t}\n+\n+\t/**\n+\t Compares two serialized variable-length encoded Unicode String.\n+\t @param first input channel\n+\t @param second input channel\n+\t @return A negative value if the first String is less than the second, 0 if equal, a positive value if greater.\n+\t @throws IOException \n+\t */\n+\tpublic static final int compareUnicodeString(DataInputView first, DataInputView second) throws IOException {\n+\t\tint lengthFirst = readLength(first);\n+\t\tint lengthSecond = readLength(second);\n+\t\tbyte c1, c2;\n+\t\tint cmp = 0;\n+\t\tfor(\n+\t\t\t\tint x = 0;\n+\t\t\t\t(c1=first.readByte()) < END_BYTE && \n+\t\t\t\t(c2=second.readByte()) < END_BYTE && \n+\t\t\t\t(cmp=c1-c2) == 0; \n+\t\t\t\tx++);\n+\t\tif (cmp!=0){\n+\t\t\treturn cmp;\n+\t\t}\n+\t\t//the first min(lengthFirst, lengthSecond) characters are equal, longer String > shorter String\n+\t\treturn lengthFirst - lengthSecond;\n+\t}\n+}\n\\ No newline at end of file\n\n@@ -17,8 +17,6 @@\n import eu.stratosphere.api.common.typeutils.ComparatorTestBase;\n import eu.stratosphere.api.common.typeutils.TypeComparator;\n import eu.stratosphere.api.common.typeutils.TypeSerializer;\n-import eu.stratosphere.api.common.typeutils.base.StringComparator;\n-import eu.stratosphere.api.common.typeutils.base.StringSerializer;\n \n public class StringComparatorTest extends ComparatorTestBase<String> {\n \n@@ -42,7 +40,10 @@ protected String[] getSortedTestData() {\n \t\t\t\"abce\",\n \t\t\t\"abdd\",\n \t\t\t\"accd\",\n-\t\t\t\"bbcd\"\n+\t\t\t\"bbcd\",\n+\t\t\t\"bbcde\",\n+\t\t\t((char)128)+\"\"+((char)32896),\n+\t\t\t((char)128)+\"\"+((char)32897)\n \t\t};\n \t}\n }\n\n@@ -16,6 +16,8 @@\n \n import eu.stratosphere.api.common.typeutils.SerializerTestBase;\n import eu.stratosphere.api.common.typeutils.TypeSerializer;\n+import eu.stratosphere.util.StringUtils;\n+import java.util.Random;\n \n /**\n  * A test for the {@link StringSerializer}.\n@@ -39,6 +41,12 @@ protected Class<String> getTypeClass() {\n \t\n \t@Override\n \tprotected String[] getTestData() {\n-\t\treturn new String[] {\"a\", \"\", \"bcd\", \"jbmbmner8 jhk hj \\n \\t √º√§√ü√ü@¬µ\", \"\", \"non-empty\"};\n+\t\tRandom rnd = new Random(289347567856686223L);\n+\t\treturn new String[] {\n+\t\t\tStringUtils.getRandomString(rnd, 300, 350), new String(Character.toChars(127315)), \n+\t\t\t(char)128+(char)32896+\"\", \"\"+(char)24640+(char)24640+(char)24640+(char)24640+(char)24640,\n+\t\t\t\"\"+(char)65535+(char)65535+(char)65535+(char)65535+(char)65535,\n+\t\t\t\"\"+(char)0,\n+\t\t\t\"a\", \"\", \"bcd\", \"jbmbmner8 jhk hj \\n \\t √º√§√ü√ü@¬µ\", \"\", \"non-empty\"};\n \t}\n }\n"}
{"id": 243, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nRefs #943\r\n\r\n* We've already got a test that ensures `response.elapsed > datetime.timedelta(0)`, so let's not try to enforce a \"this should end up as more than one second\" case.\r\n* Let's bump the slow response default to a more reliably noticable 1 second delay.\r\n\n\n### Code Patch:\n@@ -146,10 +146,3 @@ def test_merge_url():\n \n     assert url.scheme == \"https\"\n     assert url.is_ssl\n-\n-\n-def test_elapsed_delay(server):\n-    url = server.url.copy_with(path=\"/slow_response/100\")\n-    with httpx.Client() as client:\n-        response = client.get(url)\n-    assert response.elapsed.total_seconds() > 0.0\n\n@@ -102,12 +102,7 @@ async def hello_world(scope, receive, send):\n \n \n async def slow_response(scope, receive, send):\n-    delay_ms_str: str = scope[\"path\"].replace(\"/slow_response/\", \"\")\n-    try:\n-        delay_ms = float(delay_ms_str)\n-    except ValueError:\n-        delay_ms = 100\n-    await sleep(delay_ms / 1000.0)\n+    await sleep(1.0)\n     await send(\n         {\n             \"type\": \"http.response.start\",\n"}
{"id": 311, "ground_truth": "Unordered collections", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n* remove casts\r\n* generalize `is_valid_sample_rate` to return the validated float\r\n* added a `validate_scopes` helper method to validate the scopes entry on the context and return the narrowed type\r\n* added a `get_typed_attribute` helper method to narrow the type on getting attributes from otel\r\n* fix a flaky test on gevent\n\n### Code Patch:\n@@ -1,4 +1,5 @@\n-from typing import cast, TYPE_CHECKING\n+from __future__ import annotations\n+from typing import TYPE_CHECKING\n \n from opentelemetry.trace import get_current_span, set_span_in_context\n from opentelemetry.trace.span import INVALID_SPAN\n@@ -13,36 +14,37 @@\n     SENTRY_USE_CURRENT_SCOPE_KEY,\n     SENTRY_USE_ISOLATION_SCOPE_KEY,\n )\n+from sentry_sdk.opentelemetry.scope import PotelScope, validate_scopes\n \n if TYPE_CHECKING:\n-    from typing import Optional\n     from contextvars import Token\n-    import sentry_sdk.opentelemetry.scope as scope\n \n \n class SentryContextVarsRuntimeContext(ContextVarsRuntimeContext):\n-    def attach(self, context):\n-        # type: (Context) -> Token[Context]\n-        scopes = get_value(SENTRY_SCOPES_KEY, context)\n+    def attach(self, context: Context) -> Token[Context]:\n+        scopes = validate_scopes(get_value(SENTRY_SCOPES_KEY, context))\n \n-        should_fork_isolation_scope = context.pop(\n-            SENTRY_FORK_ISOLATION_SCOPE_KEY, False\n+        should_fork_isolation_scope = bool(\n+            context.pop(SENTRY_FORK_ISOLATION_SCOPE_KEY, False)\n         )\n-        should_fork_isolation_scope = cast(\"bool\", should_fork_isolation_scope)\n \n         should_use_isolation_scope = context.pop(SENTRY_USE_ISOLATION_SCOPE_KEY, None)\n-        should_use_isolation_scope = cast(\n-            \"Optional[scope.PotelScope]\", should_use_isolation_scope\n+        should_use_isolation_scope = (\n+            should_use_isolation_scope\n+            if isinstance(should_use_isolation_scope, PotelScope)\n+            else None\n         )\n \n         should_use_current_scope = context.pop(SENTRY_USE_CURRENT_SCOPE_KEY, None)\n-        should_use_current_scope = cast(\n-            \"Optional[scope.PotelScope]\", should_use_current_scope\n+        should_use_current_scope = (\n+            should_use_current_scope\n+            if isinstance(should_use_current_scope, PotelScope)\n+            else None\n         )\n \n         if scopes:\n-            scopes = cast(\"tuple[scope.PotelScope, scope.PotelScope]\", scopes)\n-            (current_scope, isolation_scope) = scopes\n+            current_scope = scopes[0]\n+            isolation_scope = scopes[1]\n         else:\n             current_scope = sentry_sdk.get_current_scope()\n             isolation_scope = sentry_sdk.get_isolation_scope()\n\n@@ -1,4 +1,4 @@\n-from typing import cast\n+from __future__ import annotations\n \n from opentelemetry import trace\n from opentelemetry.context import (\n@@ -37,21 +37,25 @@\n     extract_sentrytrace_data,\n     should_propagate_trace,\n )\n+from sentry_sdk.opentelemetry.scope import validate_scopes\n \n from typing import TYPE_CHECKING\n \n if TYPE_CHECKING:\n     from typing import Optional, Set\n-    import sentry_sdk.opentelemetry.scope as scope\n \n \n class SentryPropagator(TextMapPropagator):\n     \"\"\"\n     Propagates tracing headers for Sentry's tracing system in a way OTel understands.\n     \"\"\"\n \n-    def extract(self, carrier, context=None, getter=default_getter):\n-        # type: (CarrierT, Optional[Context], Getter[CarrierT]) -> Context\n+    def extract(\n+        self,\n+        carrier: CarrierT,\n+        context: Optional[Context] = None,\n+        getter: Getter[CarrierT] = default_getter,\n+    ) -> Context:\n         if context is None:\n             context = get_current()\n \n@@ -93,13 +97,16 @@ def extract(self, carrier, context=None, getter=default_getter):\n         modified_context = trace.set_span_in_context(span, context)\n         return modified_context\n \n-    def inject(self, carrier, context=None, setter=default_setter):\n-        # type: (CarrierT, Optional[Context], Setter[CarrierT]) -> None\n-        scopes = get_value(SENTRY_SCOPES_KEY, context)\n+    def inject(\n+        self,\n+        carrier: CarrierT,\n+        context: Optional[Context] = None,\n+        setter: Setter[CarrierT] = default_setter,\n+    ) -> None:\n+        scopes = validate_scopes(get_value(SENTRY_SCOPES_KEY, context))\n         if not scopes:\n             return\n \n-        scopes = cast(\"tuple[scope.PotelScope, scope.PotelScope]\", scopes)\n         (current_scope, _) = scopes\n \n         span = current_scope.span\n@@ -114,6 +121,5 @@ def inject(self, carrier, context=None, setter=default_setter):\n             setter.set(carrier, key, value)\n \n     @property\n-    def fields(self):\n-        # type: () -> Set[str]\n+    def fields(self) -> Set[str]:\n         return {SENTRY_TRACE_HEADER_NAME, BAGGAGE_HEADER_NAME}\n\n@@ -1,5 +1,5 @@\n+from __future__ import annotations\n from decimal import Decimal\n-from typing import cast\n \n from opentelemetry import trace\n from opentelemetry.sdk.trace.sampling import Sampler, SamplingResult, Decision\n@@ -21,15 +21,16 @@\n from typing import TYPE_CHECKING\n \n if TYPE_CHECKING:\n-    from typing import Any, Optional, Sequence, Union\n+    from typing import Any, Optional, Sequence\n     from opentelemetry.context import Context\n     from opentelemetry.trace import Link, SpanKind\n     from opentelemetry.trace.span import SpanContext\n     from opentelemetry.util.types import Attributes\n \n \n-def get_parent_sampled(parent_context, trace_id):\n-    # type: (Optional[SpanContext], int) -> Optional[bool]\n+def get_parent_sampled(\n+    parent_context: Optional[SpanContext], trace_id: int\n+) -> Optional[bool]:\n     if parent_context is None:\n         return None\n \n@@ -54,8 +55,9 @@ def get_parent_sampled(parent_context, trace_id):\n     return None\n \n \n-def get_parent_sample_rate(parent_context, trace_id):\n-    # type: (Optional[SpanContext], int) -> Optional[float]\n+def get_parent_sample_rate(\n+    parent_context: Optional[SpanContext], trace_id: int\n+) -> Optional[float]:\n     if parent_context is None:\n         return None\n \n@@ -74,8 +76,9 @@ def get_parent_sample_rate(parent_context, trace_id):\n     return None\n \n \n-def get_parent_sample_rand(parent_context, trace_id):\n-    # type: (Optional[SpanContext], int) -> Optional[Decimal]\n+def get_parent_sample_rand(\n+    parent_context: Optional[SpanContext], trace_id: int\n+) -> Optional[Decimal]:\n     if parent_context is None:\n         return None\n \n@@ -91,8 +94,12 @@ def get_parent_sample_rand(parent_context, trace_id):\n     return None\n \n \n-def dropped_result(span_context, attributes, sample_rate=None, sample_rand=None):\n-    # type: (SpanContext, Attributes, Optional[float], Optional[Decimal]) -> SamplingResult\n+def dropped_result(\n+    span_context: SpanContext,\n+    attributes: Attributes,\n+    sample_rate: Optional[float] = None,\n+    sample_rand: Optional[Decimal] = None,\n+) -> SamplingResult:\n     \"\"\"\n     React to a span getting unsampled and return a DROP SamplingResult.\n \n@@ -129,8 +136,12 @@ def dropped_result(span_context, attributes, sample_rate=None, sample_rand=None)\n     )\n \n \n-def sampled_result(span_context, attributes, sample_rate=None, sample_rand=None):\n-    # type: (SpanContext, Attributes, Optional[float], Optional[Decimal]) -> SamplingResult\n+def sampled_result(\n+    span_context: SpanContext,\n+    attributes: Attributes,\n+    sample_rate: Optional[float] = None,\n+    sample_rand: Optional[Decimal] = None,\n+) -> SamplingResult:\n     \"\"\"\n     React to a span being sampled and return a sampled SamplingResult.\n \n@@ -151,8 +162,12 @@ def sampled_result(span_context, attributes, sample_rate=None, sample_rand=None)\n     )\n \n \n-def _update_trace_state(span_context, sampled, sample_rate=None, sample_rand=None):\n-    # type: (SpanContext, bool, Optional[float], Optional[Decimal]) -> TraceState\n+def _update_trace_state(\n+    span_context: SpanContext,\n+    sampled: bool,\n+    sample_rate: Optional[float] = None,\n+    sample_rand: Optional[Decimal] = None,\n+) -> TraceState:\n     trace_state = span_context.trace_state\n \n     sampled = \"true\" if sampled else \"false\"\n@@ -175,15 +190,14 @@ def _update_trace_state(span_context, sampled, sample_rate=None, sample_rand=Non\n class SentrySampler(Sampler):\n     def should_sample(\n         self,\n-        parent_context,  # type: Optional[Context]\n-        trace_id,  # type: int\n-        name,  # type: str\n-        kind=None,  # type: Optional[SpanKind]\n-        attributes=None,  # type: Attributes\n-        links=None,  # type: Optional[Sequence[Link]]\n-        trace_state=None,  # type: Optional[TraceState]\n-    ):\n-        # type: (...) -> SamplingResult\n+        parent_context: Optional[Context],\n+        trace_id: int,\n+        name: str,\n+        kind: Optional[SpanKind] = None,\n+        attributes: Attributes = None,\n+        links: Optional[Sequence[Link]] = None,\n+        trace_state: Optional[TraceState] = None,\n+    ) -> SamplingResult:\n         client = sentry_sdk.get_client()\n \n         parent_span_context = trace.get_current_span(parent_context).get_span_context()\n@@ -209,13 +223,12 @@ def should_sample(\n             sample_rand = parent_sample_rand\n         else:\n             # We are the head SDK and we need to generate a new sample_rand\n-            sample_rand = cast(Decimal, _generate_sample_rand(str(trace_id), (0, 1)))\n+            sample_rand = _generate_sample_rand(str(trace_id), (0, 1))\n \n         # Explicit sampled value provided at start_span\n-        custom_sampled = cast(\n-            \"Optional[bool]\", attributes.get(SentrySpanAttribute.CUSTOM_SAMPLED)\n-        )\n-        if custom_sampled is not None:\n+        custom_sampled = attributes.get(SentrySpanAttribute.CUSTOM_SAMPLED)\n+\n+        if custom_sampled is not None and isinstance(custom_sampled, bool):\n             if is_root_span:\n                 sample_rate = float(custom_sampled)\n                 if sample_rate > 0:\n@@ -262,7 +275,8 @@ def should_sample(\n                 sample_rate_to_propagate = sample_rate\n \n         # If the sample rate is invalid, drop the span\n-        if not is_valid_sample_rate(sample_rate, source=self.__class__.__name__):\n+        sample_rate = is_valid_sample_rate(sample_rate, source=self.__class__.__name__)\n+        if sample_rate is None:\n             logger.warning(\n                 f\"[Tracing.Sampler] Discarding {name} because of invalid sample rate.\"\n             )\n@@ -275,7 +289,6 @@ def should_sample(\n                 sample_rate_to_propagate = sample_rate\n \n         # Compare sample_rand to sample_rate to make the final sampling decision\n-        sample_rate = float(cast(\"Union[bool, float, int]\", sample_rate))\n         sampled = sample_rand < Decimal.from_float(sample_rate)\n \n         if sampled:\n@@ -307,9 +320,13 @@ def get_description(self) -> str:\n         return self.__class__.__name__\n \n \n-def create_sampling_context(name, attributes, parent_span_context, trace_id):\n-    # type: (str, Attributes, Optional[SpanContext], int) -> dict[str, Any]\n-    sampling_context = {\n+def create_sampling_context(\n+    name: str,\n+    attributes: Attributes,\n+    parent_span_context: Optional[SpanContext],\n+    trace_id: int,\n+) -> dict[str, Any]:\n+    sampling_context: dict[str, Any] = {\n         \"transaction_context\": {\n             \"name\": name,\n             \"op\": attributes.get(SentrySpanAttribute.OP) if attributes else None,\n@@ -318,7 +335,7 @@ def create_sampling_context(name, attributes, parent_span_context, trace_id):\n             ),\n         },\n         \"parent_sampled\": get_parent_sampled(parent_span_context, trace_id),\n-    }  # type: dict[str, Any]\n+    }\n \n     if attributes is not None:\n         sampling_context.update(attributes)\n\n@@ -1,4 +1,4 @@\n-from typing import cast\n+from __future__ import annotations\n from contextlib import contextmanager\n import warnings\n \n@@ -24,9 +24,6 @@\n     SENTRY_USE_ISOLATION_SCOPE_KEY,\n     TRACESTATE_SAMPLED_KEY,\n )\n-from sentry_sdk.opentelemetry.contextvars_context import (\n-    SentryContextVarsRuntimeContext,\n-)\n from sentry_sdk.opentelemetry.utils import trace_state_from_baggage\n from sentry_sdk.scope import Scope, ScopeType\n from sentry_sdk.tracing import Span\n@@ -38,52 +35,46 @@\n \n class PotelScope(Scope):\n     @classmethod\n-    def _get_scopes(cls):\n-        # type: () -> Optional[Tuple[PotelScope, PotelScope]]\n+    def _get_scopes(cls) -> Optional[Tuple[PotelScope, PotelScope]]:\n         \"\"\"\n         Returns the current scopes tuple on the otel context. Internal use only.\n         \"\"\"\n-        return cast(\n-            \"Optional[Tuple[PotelScope, PotelScope]]\", get_value(SENTRY_SCOPES_KEY)\n-        )\n+        return validate_scopes(get_value(SENTRY_SCOPES_KEY))\n \n     @classmethod\n-    def get_current_scope(cls):\n-        # type: () -> PotelScope\n+    def get_current_scope(cls) -> PotelScope:\n         \"\"\"\n         Returns the current scope.\n         \"\"\"\n         return cls._get_current_scope() or _INITIAL_CURRENT_SCOPE\n \n     @classmethod\n-    def _get_current_scope(cls):\n-        # type: () -> Optional[PotelScope]\n+    def _get_current_scope(cls) -> Optional[PotelScope]:\n         \"\"\"\n         Returns the current scope without creating a new one. Internal use only.\n         \"\"\"\n         scopes = cls._get_scopes()\n         return scopes[0] if scopes else None\n \n     @classmethod\n-    def get_isolation_scope(cls):\n-        # type: () -> PotelScope\n+    def get_isolation_scope(cls) -> PotelScope:\n         \"\"\"\n         Returns the isolation scope.\n         \"\"\"\n         return cls._get_isolation_scope() or _INITIAL_ISOLATION_SCOPE\n \n     @classmethod\n-    def _get_isolation_scope(cls):\n-        # type: () -> Optional[PotelScope]\n+    def _get_isolation_scope(cls) -> Optional[PotelScope]:\n         \"\"\"\n         Returns the isolation scope without creating a new one. Internal use only.\n         \"\"\"\n         scopes = cls._get_scopes()\n         return scopes[1] if scopes else None\n \n     @contextmanager\n-    def continue_trace(self, environ_or_headers):\n-        # type: (Dict[str, Any]) -> Generator[None, None, None]\n+    def continue_trace(\n+        self, environ_or_headers: Dict[str, Any]\n+    ) -> Generator[None, None, None]:\n         \"\"\"\n         Sets the propagation context from environment or headers to continue an incoming trace.\n         Any span started within this context manager will use the same trace_id, parent_span_id\n@@ -98,8 +89,7 @@ def continue_trace(self, environ_or_headers):\n             with use_span(NonRecordingSpan(span_context)):\n                 yield\n \n-    def _incoming_otel_span_context(self):\n-        # type: () -> Optional[SpanContext]\n+    def _incoming_otel_span_context(self) -> Optional[SpanContext]:\n         if self._propagation_context is None:\n             return None\n         # If sentry-trace extraction didn't have a parent_span_id, we don't have an upstream header\n@@ -132,8 +122,7 @@ def _incoming_otel_span_context(self):\n \n         return span_context\n \n-    def start_transaction(self, **kwargs):\n-        # type: (Any) -> Span\n+    def start_transaction(self, **kwargs: Any) -> Span:\n         \"\"\"\n         .. deprecated:: 3.0.0\n             This function is deprecated and will be removed in a future release.\n@@ -146,17 +135,15 @@ def start_transaction(self, **kwargs):\n         )\n         return self.start_span(**kwargs)\n \n-    def start_span(self, **kwargs):\n-        # type: (Any) -> Span\n+    def start_span(self, **kwargs: Any) -> Span:\n         return Span(**kwargs)\n \n \n _INITIAL_CURRENT_SCOPE = PotelScope(ty=ScopeType.CURRENT)\n _INITIAL_ISOLATION_SCOPE = PotelScope(ty=ScopeType.ISOLATION)\n \n \n-def setup_initial_scopes():\n-    # type: () -> None\n+def setup_initial_scopes() -> None:\n     global _INITIAL_CURRENT_SCOPE, _INITIAL_ISOLATION_SCOPE\n     _INITIAL_CURRENT_SCOPE = PotelScope(ty=ScopeType.CURRENT)\n     _INITIAL_ISOLATION_SCOPE = PotelScope(ty=ScopeType.ISOLATION)\n@@ -165,17 +152,18 @@ def setup_initial_scopes():\n     attach(set_value(SENTRY_SCOPES_KEY, scopes))\n \n \n-def setup_scope_context_management():\n-    # type: () -> None\n+def setup_scope_context_management() -> None:\n     import opentelemetry.context\n+    from sentry_sdk.opentelemetry.contextvars_context import (\n+        SentryContextVarsRuntimeContext,\n+    )\n \n     opentelemetry.context._RUNTIME_CONTEXT = SentryContextVarsRuntimeContext()\n     setup_initial_scopes()\n \n \n @contextmanager\n-def isolation_scope():\n-    # type: () -> Generator[PotelScope, None, None]\n+def isolation_scope() -> Generator[PotelScope, None, None]:\n     context = set_value(SENTRY_FORK_ISOLATION_SCOPE_KEY, True)\n     token = attach(context)\n     try:\n@@ -185,8 +173,7 @@ def isolation_scope():\n \n \n @contextmanager\n-def new_scope():\n-    # type: () -> Generator[PotelScope, None, None]\n+def new_scope() -> Generator[PotelScope, None, None]:\n     token = attach(get_current())\n     try:\n         yield PotelScope.get_current_scope()\n@@ -195,8 +182,7 @@ def new_scope():\n \n \n @contextmanager\n-def use_scope(scope):\n-    # type: (PotelScope) -> Generator[PotelScope, None, None]\n+def use_scope(scope: PotelScope) -> Generator[PotelScope, None, None]:\n     context = set_value(SENTRY_USE_CURRENT_SCOPE_KEY, scope)\n     token = attach(context)\n \n@@ -207,12 +193,25 @@ def use_scope(scope):\n \n \n @contextmanager\n-def use_isolation_scope(isolation_scope):\n-    # type: (PotelScope) -> Generator[PotelScope, None, None]\n+def use_isolation_scope(\n+    isolation_scope: PotelScope,\n+) -> Generator[PotelScope, None, None]:\n     context = set_value(SENTRY_USE_ISOLATION_SCOPE_KEY, isolation_scope)\n     token = attach(context)\n \n     try:\n         yield isolation_scope\n     finally:\n         detach(token)\n+\n+\n+def validate_scopes(scopes: Any) -> Optional[Tuple[PotelScope, PotelScope]]:\n+    if (\n+        isinstance(scopes, tuple)\n+        and len(scopes) == 2\n+        and isinstance(scopes[0], PotelScope)\n+        and isinstance(scopes[1], PotelScope)\n+    ):\n+        return scopes\n+    else:\n+        return None\n\n@@ -1,5 +1,5 @@\n+from __future__ import annotations\n from collections import deque, defaultdict\n-from typing import cast\n \n from opentelemetry.trace import (\n     format_trace_id,\n@@ -52,30 +52,24 @@ class SentrySpanProcessor(SpanProcessor):\n     Converts OTel spans into Sentry spans so they can be sent to the Sentry backend.\n     \"\"\"\n \n-    def __new__(cls):\n-        # type: () -> SentrySpanProcessor\n+    def __new__(cls) -> SentrySpanProcessor:\n         if not hasattr(cls, \"instance\"):\n             cls.instance = super().__new__(cls)\n \n         return cls.instance\n \n-    def __init__(self):\n-        # type: () -> None\n-        self._children_spans = defaultdict(\n-            list\n-        )  # type: DefaultDict[int, List[ReadableSpan]]\n-        self._dropped_spans = defaultdict(lambda: 0)  # type: DefaultDict[int, int]\n+    def __init__(self) -> None:\n+        self._children_spans: DefaultDict[int, List[ReadableSpan]] = defaultdict(list)\n+        self._dropped_spans: DefaultDict[int, int] = defaultdict(lambda: 0)\n \n-    def on_start(self, span, parent_context=None):\n-        # type: (Span, Optional[Context]) -> None\n+    def on_start(self, span: Span, parent_context: Optional[Context] = None) -> None:\n         if is_sentry_span(span):\n             return\n \n         self._add_root_span(span, get_current_span(parent_context))\n         self._start_profile(span)\n \n-    def on_end(self, span):\n-        # type: (ReadableSpan) -> None\n+    def on_end(self, span: ReadableSpan) -> None:\n         if is_sentry_span(span):\n             return\n \n@@ -88,18 +82,15 @@ def on_end(self, span):\n             self._append_child_span(span)\n \n     # TODO-neel-potel not sure we need a clear like JS\n-    def shutdown(self):\n-        # type: () -> None\n+    def shutdown(self) -> None:\n         pass\n \n     # TODO-neel-potel change default? this is 30 sec\n     # TODO-neel-potel call this in client.flush\n-    def force_flush(self, timeout_millis=30000):\n-        # type: (int) -> bool\n+    def force_flush(self, timeout_millis: int = 30000) -> bool:\n         return True\n \n-    def _add_root_span(self, span, parent_span):\n-        # type: (Span, AbstractSpan) -> None\n+    def _add_root_span(self, span: Span, parent_span: AbstractSpan) -> None:\n         \"\"\"\n         This is required to make Span.root_span work\n         since we can't traverse back to the root purely with otel efficiently.\n@@ -112,8 +103,7 @@ def _add_root_span(self, span, parent_span):\n             # root span points to itself\n             set_sentry_meta(span, \"root_span\", span)\n \n-    def _start_profile(self, span):\n-        # type: (Span) -> None\n+    def _start_profile(self, span: Span) -> None:\n         try_autostart_continuous_profiler()\n \n         profiler_id = get_profiler_id()\n@@ -148,14 +138,12 @@ def _start_profile(self, span):\n                 span.set_attribute(SPANDATA.PROFILER_ID, profiler_id)\n             set_sentry_meta(span, \"continuous_profile\", continuous_profile)\n \n-    def _stop_profile(self, span):\n-        # type: (ReadableSpan) -> None\n+    def _stop_profile(self, span: ReadableSpan) -> None:\n         continuous_profiler = get_sentry_meta(span, \"continuous_profile\")\n         if continuous_profiler:\n             continuous_profiler.stop()\n \n-    def _flush_root_span(self, span):\n-        # type: (ReadableSpan) -> None\n+    def _flush_root_span(self, span: ReadableSpan) -> None:\n         transaction_event = self._root_span_to_transaction_event(span)\n         if not transaction_event:\n             return\n@@ -176,8 +164,7 @@ def _flush_root_span(self, span):\n         sentry_sdk.capture_event(transaction_event)\n         self._cleanup_references([span] + collected_spans)\n \n-    def _append_child_span(self, span):\n-        # type: (ReadableSpan) -> None\n+    def _append_child_span(self, span: ReadableSpan) -> None:\n         if not span.parent:\n             return\n \n@@ -192,14 +179,13 @@ def _append_child_span(self, span):\n         else:\n             self._dropped_spans[span.parent.span_id] += 1\n \n-    def _collect_children(self, span):\n-        # type: (ReadableSpan) -> tuple[List[ReadableSpan], int]\n+    def _collect_children(self, span: ReadableSpan) -> tuple[List[ReadableSpan], int]:\n         if not span.context:\n             return [], 0\n \n         children = []\n         dropped_spans = 0\n-        bfs_queue = deque()  # type: Deque[int]\n+        bfs_queue: Deque[int] = deque()\n         bfs_queue.append(span.context.span_id)\n \n         while bfs_queue:\n@@ -215,8 +201,7 @@ def _collect_children(self, span):\n \n     # we construct the event from scratch here\n     # and not use the current Transaction class for easier refactoring\n-    def _root_span_to_transaction_event(self, span):\n-        # type: (ReadableSpan) -> Optional[Event]\n+    def _root_span_to_transaction_event(self, span: ReadableSpan) -> Optional[Event]:\n         if not span.context:\n             return None\n \n@@ -250,23 +235,20 @@ def _root_span_to_transaction_event(self, span):\n             }\n         )\n \n-        profile = cast(\"Optional[Profile]\", get_sentry_meta(span, \"profile\"))\n-        if profile:\n+        profile = get_sentry_meta(span, \"profile\")\n+        if profile is not None and isinstance(profile, Profile):\n             profile.__exit__(None, None, None)\n             if profile.valid():\n                 event[\"profile\"] = profile\n \n         return event\n \n-    def _span_to_json(self, span):\n-        # type: (ReadableSpan) -> Optional[dict[str, Any]]\n+    def _span_to_json(self, span: ReadableSpan) -> Optional[dict[str, Any]]:\n         if not span.context:\n             return None\n \n-        # This is a safe cast because dict[str, Any] is a superset of Event\n-        span_json = cast(\n-            \"dict[str, Any]\", self._common_span_transaction_attributes_as_json(span)\n-        )\n+        # need to ignore the type here due to TypedDict nonsense\n+        span_json: Optional[dict[str, Any]] = self._common_span_transaction_attributes_as_json(span)  # type: ignore\n         if span_json is None:\n             return None\n \n@@ -299,29 +281,28 @@ def _span_to_json(self, span):\n \n         return span_json\n \n-    def _common_span_transaction_attributes_as_json(self, span):\n-        # type: (ReadableSpan) -> Optional[Event]\n+    def _common_span_transaction_attributes_as_json(\n+        self, span: ReadableSpan\n+    ) -> Optional[Event]:\n         if not span.start_time or not span.end_time:\n             return None\n \n-        common_json = {\n+        common_json: Event = {\n             \"start_timestamp\": convert_from_otel_timestamp(span.start_time),\n             \"timestamp\": convert_from_otel_timestamp(span.end_time),\n-        }  # type: Event\n+        }\n \n         tags = extract_span_attributes(span, SentrySpanAttribute.TAG)\n         if tags:\n             common_json[\"tags\"] = tags\n \n         return common_json\n \n-    def _cleanup_references(self, spans):\n-        # type: (List[ReadableSpan]) -> None\n+    def _cleanup_references(self, spans: List[ReadableSpan]) -> None:\n         for span in spans:\n             delete_sentry_meta(span)\n \n-    def _log_debug_info(self):\n-        # type: () -> None\n+    def _log_debug_info(self) -> None:\n         import pprint\n \n         pprint.pprint(\n\n@@ -1,3 +1,4 @@\n+from __future__ import annotations\n from opentelemetry import trace\n from opentelemetry.propagate import set_global_textmap\n from opentelemetry.sdk.trace import TracerProvider, Span, ReadableSpan\n@@ -10,25 +11,22 @@\n from sentry_sdk.utils import logger\n \n \n-def patch_readable_span():\n-    # type: () -> None\n+def patch_readable_span() -> None:\n     \"\"\"\n     We need to pass through sentry specific metadata/objects from Span to ReadableSpan\n     to work with them consistently in the SpanProcessor.\n     \"\"\"\n     old_readable_span = Span._readable_span\n \n-    def sentry_patched_readable_span(self):\n-        # type: (Span) -> ReadableSpan\n+    def sentry_patched_readable_span(self: Span) -> ReadableSpan:\n         readable_span = old_readable_span(self)\n         readable_span._sentry_meta = getattr(self, \"_sentry_meta\", {})  # type: ignore[attr-defined]\n         return readable_span\n \n     Span._readable_span = sentry_patched_readable_span  # type: ignore[method-assign]\n \n \n-def setup_sentry_tracing():\n-    # type: () -> None\n+def setup_sentry_tracing() -> None:\n     # TracerProvider can only be set once. If we're the first ones setting it,\n     # there's no issue. If it already exists, we need to patch it.\n     from opentelemetry.trace import _TRACER_PROVIDER\n\n@@ -1,5 +1,5 @@\n+from __future__ import annotations\n import re\n-from typing import cast\n from datetime import datetime, timezone\n \n from urllib3.util import parse_url as urlparse\n@@ -30,9 +30,11 @@\n from sentry_sdk._types import TYPE_CHECKING\n \n if TYPE_CHECKING:\n-    from typing import Any, Optional, Mapping, Sequence, Union\n+    from typing import Any, Optional, Mapping, Sequence, Union, Type, TypeVar\n     from sentry_sdk._types import OtelExtractedSpanData\n \n+    T = TypeVar(\"T\")\n+\n \n GRPC_ERROR_MAP = {\n     \"1\": SPANSTATUS.CANCELLED,\n@@ -54,8 +56,7 @@\n }\n \n \n-def is_sentry_span(span):\n-    # type: (ReadableSpan) -> bool\n+def is_sentry_span(span: ReadableSpan) -> bool:\n     \"\"\"\n     Break infinite loop:\n     HTTP requests to Sentry are caught by OTel and send again to Sentry.\n@@ -65,10 +66,8 @@ def is_sentry_span(span):\n     if not span.attributes:\n         return False\n \n-    span_url = span.attributes.get(SpanAttributes.HTTP_URL, None)\n-    span_url = cast(\"Optional[str]\", span_url)\n-\n-    if not span_url:\n+    span_url = get_typed_attribute(span.attributes, SpanAttributes.HTTP_URL, str)\n+    if span_url is None:\n         return False\n \n     dsn_url = None\n@@ -89,48 +88,46 @@ def is_sentry_span(span):\n     return False\n \n \n-def convert_from_otel_timestamp(time):\n-    # type: (int) -> datetime\n+def convert_from_otel_timestamp(time: int) -> datetime:\n     \"\"\"Convert an OTel nanosecond-level timestamp to a datetime.\"\"\"\n     return datetime.fromtimestamp(time / 1e9, timezone.utc)\n \n \n-def convert_to_otel_timestamp(time):\n-    # type: (Union[datetime, float]) -> int\n+def convert_to_otel_timestamp(time: Union[datetime, float]) -> int:\n     \"\"\"Convert a datetime to an OTel timestamp (with nanosecond precision).\"\"\"\n     if isinstance(time, datetime):\n         return int(time.timestamp() * 1e9)\n     return int(time * 1e9)\n \n \n-def extract_transaction_name_source(span):\n-    # type: (ReadableSpan) -> tuple[Optional[str], Optional[str]]\n+def extract_transaction_name_source(\n+    span: ReadableSpan,\n+) -> tuple[Optional[str], Optional[str]]:\n     if not span.attributes:\n         return (None, None)\n     return (\n-        cast(\"Optional[str]\", span.attributes.get(SentrySpanAttribute.NAME)),\n-        cast(\"Optional[str]\", span.attributes.get(SentrySpanAttribute.SOURCE)),\n+        get_typed_attribute(span.attributes, SentrySpanAttribute.NAME, str),\n+        get_typed_attribute(span.attributes, SentrySpanAttribute.SOURCE, str),\n     )\n \n \n-def extract_span_data(span):\n-    # type: (ReadableSpan) -> OtelExtractedSpanData\n+def extract_span_data(span: ReadableSpan) -> OtelExtractedSpanData:\n     op = span.name\n     description = span.name\n     status, http_status = extract_span_status(span)\n     origin = None\n     if span.attributes is None:\n         return (op, description, status, http_status, origin)\n \n-    attribute_op = cast(\"Optional[str]\", span.attributes.get(SentrySpanAttribute.OP))\n+    attribute_op = get_typed_attribute(span.attributes, SentrySpanAttribute.OP, str)\n     op = attribute_op or op\n-    description = cast(\n-        \"str\", span.attributes.get(SentrySpanAttribute.DESCRIPTION) or description\n+    description = (\n+        get_typed_attribute(span.attributes, SentrySpanAttribute.DESCRIPTION, str)\n+        or description\n     )\n-    origin = cast(\"Optional[str]\", span.attributes.get(SentrySpanAttribute.ORIGIN))\n+    origin = get_typed_attribute(span.attributes, SentrySpanAttribute.ORIGIN, str)\n \n-    http_method = span.attributes.get(SpanAttributes.HTTP_METHOD)\n-    http_method = cast(\"Optional[str]\", http_method)\n+    http_method = get_typed_attribute(span.attributes, SpanAttributes.HTTP_METHOD, str)\n     if http_method:\n         return span_data_for_http_method(span)\n \n@@ -165,11 +162,10 @@ def extract_span_data(span):\n     return (op, description, status, http_status, origin)\n \n \n-def span_data_for_http_method(span):\n-    # type: (ReadableSpan) -> OtelExtractedSpanData\n+def span_data_for_http_method(span: ReadableSpan) -> OtelExtractedSpanData:\n     span_attributes = span.attributes or {}\n \n-    op = cast(\"Optional[str]\", span_attributes.get(SentrySpanAttribute.OP))\n+    op = get_typed_attribute(span_attributes, SentrySpanAttribute.OP, str)\n     if op is None:\n         op = \"http\"\n \n@@ -184,10 +180,9 @@ def span_data_for_http_method(span):\n     peer_name = span_attributes.get(SpanAttributes.NET_PEER_NAME)\n \n     # TODO-neel-potel remove description completely\n-    description = span_attributes.get(\n-        SentrySpanAttribute.DESCRIPTION\n-    ) or span_attributes.get(SentrySpanAttribute.NAME)\n-    description = cast(\"Optional[str]\", description)\n+    description = get_typed_attribute(\n+        span_attributes, SentrySpanAttribute.DESCRIPTION, str\n+    ) or get_typed_attribute(span_attributes, SentrySpanAttribute.NAME, str)\n     if description is None:\n         description = f\"{http_method}\"\n \n@@ -199,7 +194,7 @@ def span_data_for_http_method(span):\n             description = f\"{http_method} {peer_name}\"\n         else:\n             url = span_attributes.get(SpanAttributes.HTTP_URL)\n-            url = cast(\"Optional[str]\", url)\n+            url = get_typed_attribute(span_attributes, SpanAttributes.HTTP_URL, str)\n \n             if url:\n                 parsed_url = urlparse(url)\n@@ -210,28 +205,24 @@ def span_data_for_http_method(span):\n \n     status, http_status = extract_span_status(span)\n \n-    origin = cast(\"Optional[str]\", span_attributes.get(SentrySpanAttribute.ORIGIN))\n+    origin = get_typed_attribute(span_attributes, SentrySpanAttribute.ORIGIN, str)\n \n     return (op, description, status, http_status, origin)\n \n \n-def span_data_for_db_query(span):\n-    # type: (ReadableSpan) -> OtelExtractedSpanData\n+def span_data_for_db_query(span: ReadableSpan) -> OtelExtractedSpanData:\n     span_attributes = span.attributes or {}\n \n-    op = cast(\"str\", span_attributes.get(SentrySpanAttribute.OP, OP.DB))\n-\n-    statement = span_attributes.get(SpanAttributes.DB_STATEMENT, None)\n-    statement = cast(\"Optional[str]\", statement)\n+    op = get_typed_attribute(span_attributes, SentrySpanAttribute.OP, str) or OP.DB\n+    statement = get_typed_attribute(span_attributes, SpanAttributes.DB_STATEMENT, str)\n \n     description = statement or span.name\n-    origin = cast(\"Optional[str]\", span_attributes.get(SentrySpanAttribute.ORIGIN))\n+    origin = get_typed_attribute(span_attributes, SentrySpanAttribute.ORIGIN, str)\n \n     return (op, description, None, None, origin)\n \n \n-def extract_span_status(span):\n-    # type: (ReadableSpan) -> tuple[Optional[str], Optional[int]]\n+def extract_span_status(span: ReadableSpan) -> tuple[Optional[str], Optional[int]]:\n     span_attributes = span.attributes or {}\n     status = span.status or None\n \n@@ -266,8 +257,19 @@ def extract_span_status(span):\n         return (SPANSTATUS.UNKNOWN_ERROR, None)\n \n \n-def infer_status_from_attributes(span_attributes):\n-    # type: (Mapping[str, str | bool | int | float | Sequence[str] | Sequence[bool] | Sequence[int] | Sequence[float]]) -> tuple[Optional[str], Optional[int]]\n+def infer_status_from_attributes(\n+    span_attributes: Mapping[\n+        str,\n+        str\n+        | bool\n+        | int\n+        | float\n+        | Sequence[str]\n+        | Sequence[bool]\n+        | Sequence[int]\n+        | Sequence[float],\n+    ],\n+) -> tuple[Optional[str], Optional[int]]:\n     http_status = get_http_status_code(span_attributes)\n \n     if http_status:\n@@ -280,10 +282,23 @@ def infer_status_from_attributes(span_attributes):\n     return (None, None)\n \n \n-def get_http_status_code(span_attributes):\n-    # type: (Mapping[str, str | bool | int | float | Sequence[str] | Sequence[bool] | Sequence[int] | Sequence[float]]) -> Optional[int]\n+def get_http_status_code(\n+    span_attributes: Mapping[\n+        str,\n+        str\n+        | bool\n+        | int\n+        | float\n+        | Sequence[str]\n+        | Sequence[bool]\n+        | Sequence[int]\n+        | Sequence[float],\n+    ],\n+) -> Optional[int]:\n     try:\n-        http_status = span_attributes.get(SpanAttributes.HTTP_RESPONSE_STATUS_CODE)\n+        http_status = get_typed_attribute(\n+            span_attributes, SpanAttributes.HTTP_RESPONSE_STATUS_CODE, int\n+        )\n     except AttributeError:\n         # HTTP_RESPONSE_STATUS_CODE was added in 1.21, so if we're on an older\n         # OTel version SpanAttributes.HTTP_RESPONSE_STATUS_CODE will throw an\n@@ -292,19 +307,18 @@ def get_http_status_code(span_attributes):\n \n     if http_status is None:\n         # Fall back to the deprecated attribute\n-        http_status = span_attributes.get(SpanAttributes.HTTP_STATUS_CODE)\n-\n-    http_status = cast(\"Optional[int]\", http_status)\n+        http_status = get_typed_attribute(\n+            span_attributes, SpanAttributes.HTTP_STATUS_CODE, int\n+        )\n \n     return http_status\n \n \n-def extract_span_attributes(span, namespace):\n-    # type: (ReadableSpan, str) -> dict[str, Any]\n+def extract_span_attributes(span: ReadableSpan, namespace: str) -> dict[str, Any]:\n     \"\"\"\n     Extract Sentry-specific span attributes and make them look the way Sentry expects.\n     \"\"\"\n-    extracted_attrs = {}  # type: dict[str, Any]\n+    extracted_attrs: dict[str, Any] = {}\n \n     for attr, value in (span.attributes or {}).items():\n         if attr.startswith(namespace):\n@@ -314,8 +328,9 @@ def extract_span_attributes(span, namespace):\n     return extracted_attrs\n \n \n-def get_trace_context(span, span_data=None):\n-    # type: (ReadableSpan, Optional[OtelExtractedSpanData]) -> dict[str, Any]\n+def get_trace_context(\n+    span: ReadableSpan, span_data: Optional[OtelExtractedSpanData] = None\n+) -> dict[str, Any]:\n     if not span.context:\n         return {}\n \n@@ -328,13 +343,13 @@ def get_trace_context(span, span_data=None):\n \n     (op, _, status, _, origin) = span_data\n \n-    trace_context = {\n+    trace_context: dict[str, Any] = {\n         \"trace_id\": trace_id,\n         \"span_id\": span_id,\n         \"parent_span_id\": parent_span_id,\n         \"op\": op,\n         \"origin\": origin or DEFAULT_SPAN_ORIGIN,\n-    }  # type: dict[str, Any]\n+    }\n \n     if status:\n         trace_context[\"status\"] = status\n@@ -350,8 +365,7 @@ def get_trace_context(span, span_data=None):\n     return trace_context\n \n \n-def trace_state_from_baggage(baggage):\n-    # type: (Baggage) -> TraceState\n+def trace_state_from_baggage(baggage: Baggage) -> TraceState:\n     items = []\n     for k, v in baggage.sentry_items.items():\n         key = Baggage.SENTRY_PREFIX + quote(k)\n@@ -360,22 +374,19 @@ def trace_state_from_baggage(baggage):\n     return TraceState(items)\n \n \n-def baggage_from_trace_state(trace_state):\n-    # type: (TraceState) -> Baggage\n+def baggage_from_trace_state(trace_state: TraceState) -> Baggage:\n     return Baggage(dsc_from_trace_state(trace_state))\n \n \n-def serialize_trace_state(trace_state):\n-    # type: (TraceState) -> str\n+def serialize_trace_state(trace_state: TraceState) -> str:\n     sentry_items = []\n     for k, v in trace_state.items():\n         if Baggage.SENTRY_PREFIX_REGEX.match(k):\n             sentry_items.append((k, v))\n     return \",\".join(key + \"=\" + value for key, value in sentry_items)\n \n \n-def dsc_from_trace_state(trace_state):\n-    # type: (TraceState) -> dict[str, str]\n+def dsc_from_trace_state(trace_state: TraceState) -> dict[str, str]:\n     dsc = {}\n     for k, v in trace_state.items():\n         if Baggage.SENTRY_PREFIX_REGEX.match(k):\n@@ -384,16 +395,14 @@ def dsc_from_trace_state(trace_state):\n     return dsc\n \n \n-def has_incoming_trace(trace_state):\n-    # type: (TraceState) -> bool\n+def has_incoming_trace(trace_state: TraceState) -> bool:\n     \"\"\"\n     The existence of a sentry-trace_id in the baggage implies we continued an upstream trace.\n     \"\"\"\n     return (Baggage.SENTRY_PREFIX + \"trace_id\") in trace_state\n \n \n-def get_trace_state(span):\n-    # type: (Union[AbstractSpan, ReadableSpan]) -> TraceState\n+def get_trace_state(span: Union[AbstractSpan, ReadableSpan]) -> TraceState:\n     \"\"\"\n     Get the existing trace_state with sentry items\n     or populate it if we are the head SDK.\n@@ -451,34 +460,45 @@ def get_trace_state(span):\n         return trace_state\n \n \n-def get_sentry_meta(span, key):\n-    # type: (Union[AbstractSpan, ReadableSpan], str) -> Any\n+def get_sentry_meta(span: Union[AbstractSpan, ReadableSpan], key: str) -> Any:\n     sentry_meta = getattr(span, \"_sentry_meta\", None)\n     return sentry_meta.get(key) if sentry_meta else None\n \n \n-def set_sentry_meta(span, key, value):\n-    # type: (Union[AbstractSpan, ReadableSpan], str, Any) -> None\n+def set_sentry_meta(\n+    span: Union[AbstractSpan, ReadableSpan], key: str, value: Any\n+) -> None:\n     sentry_meta = getattr(span, \"_sentry_meta\", {})\n     sentry_meta[key] = value\n     span._sentry_meta = sentry_meta  # type: ignore[union-attr]\n \n \n-def delete_sentry_meta(span):\n-    # type: (Union[AbstractSpan, ReadableSpan]) -> None\n+def delete_sentry_meta(span: Union[AbstractSpan, ReadableSpan]) -> None:\n     try:\n         del span._sentry_meta  # type: ignore[union-attr]\n     except AttributeError:\n         pass\n \n \n-def get_profile_context(span):\n-    # type: (ReadableSpan) -> Optional[dict[str, str]]\n+def get_profile_context(span: ReadableSpan) -> Optional[dict[str, str]]:\n     if not span.attributes:\n         return None\n \n-    profiler_id = cast(\"Optional[str]\", span.attributes.get(SPANDATA.PROFILER_ID))\n+    profiler_id = get_typed_attribute(span.attributes, SPANDATA.PROFILER_ID, str)\n     if profiler_id is None:\n         return None\n \n     return {\"profiler_id\": profiler_id}\n+\n+\n+def get_typed_attribute(\n+    attributes: Mapping[str, Any], key: str, type: Type[T]\n+) -> Optional[T]:\n+    \"\"\"\n+    helper method to coerce types of attribute values\n+    \"\"\"\n+    value = attributes.get(key)\n+    if value is not None and isinstance(value, type):\n+        return value\n+    else:\n+        return None\n\n@@ -281,7 +281,8 @@ def _set_initial_sampling_decision(self, sampling_context):\n             self.sampled = False\n             return\n \n-        if not is_valid_sample_rate(sample_rate, source=\"Profiling\"):\n+        sample_rate = is_valid_sample_rate(sample_rate, source=\"Profiling\")\n+        if sample_rate is None:\n             logger.warning(\n                 \"[Profiling] Discarding profile because of invalid sample rate.\"\n             )\n@@ -291,14 +292,14 @@ def _set_initial_sampling_decision(self, sampling_context):\n         # Now we roll the dice. random.random is inclusive of 0, but not of 1,\n         # so strict < is safe here. In case sample_rate is a boolean, cast it\n         # to a float (True becomes 1.0 and False becomes 0.0)\n-        self.sampled = random.random() < float(sample_rate)\n+        self.sampled = random.random() < sample_rate\n \n         if self.sampled:\n             logger.debug(\"[Profiling] Initializing profile\")\n         else:\n             logger.debug(\n                 \"[Profiling] Discarding profile because it's not included in the random sample (sample rate = {sample_rate})\".format(\n-                    sample_rate=float(sample_rate)\n+                    sample_rate=sample_rate\n                 )\n             )\n \n\n@@ -752,7 +752,7 @@ def get_current_span(\n def _generate_sample_rand(\n     trace_id: Optional[str],\n     interval: tuple[float, float] = (0.0, 1.0),\n-) -> Optional[Decimal]:\n+) -> Decimal:\n     \"\"\"Generate a sample_rand value from a trace ID.\n \n     The generated value will be pseudorandomly chosen from the provided\n\n@@ -1561,10 +1561,11 @@ def parse_url(url: str, sanitize: bool = True) -> ParsedUrl:\n     )\n \n \n-def is_valid_sample_rate(rate: Any, source: str) -> bool:\n+def is_valid_sample_rate(rate: Any, source: str) -> Optional[float]:\n     \"\"\"\n     Checks the given sample rate to make sure it is valid type and value (a\n     boolean or a number between 0 and 1, inclusive).\n+    Returns the final float value to use if valid.\n     \"\"\"\n \n     # both booleans and NaN are instances of Real, so a) checking for Real\n@@ -1576,7 +1577,7 @@ def is_valid_sample_rate(rate: Any, source: str) -> bool:\n                 source=source, rate=rate, type=type(rate)\n             )\n         )\n-        return False\n+        return None\n \n     # in case rate is a boolean, it will get cast to 1 if it's True and 0 if it's False\n     rate = float(rate)\n@@ -1586,9 +1587,9 @@ def is_valid_sample_rate(rate: Any, source: str) -> bool:\n                 source=source, rate=rate\n             )\n         )\n-        return False\n+        return None\n \n-    return True\n+    return rate\n \n \n def match_regex_list(\n\n@@ -259,8 +259,8 @@ def test_logging_captured_warnings(sentry_init, capture_events, recwarn):\n     assert events[1][\"logentry\"][\"params\"] == []\n \n     # Using recwarn suppresses the \"third\" warning in the test output\n-    assert len(recwarn) == 1\n-    assert str(recwarn[0].message) == \"third\"\n+    third_warnings = [w for w in recwarn if str(w.message) == \"third\"]\n+    assert len(third_warnings) == 1\n \n \n def test_ignore_logger(sentry_init, capture_events):\n\n@@ -493,7 +493,7 @@ def test_accepts_valid_sample_rate(rate):\n     with mock.patch.object(logger, \"warning\", mock.Mock()):\n         result = is_valid_sample_rate(rate, source=\"Testing\")\n         assert logger.warning.called is False\n-        assert result is True\n+        assert result == float(rate)\n \n \n @pytest.mark.parametrize(\n@@ -514,7 +514,7 @@ def test_warns_on_invalid_sample_rate(rate, StringContaining):  # noqa: N803\n     with mock.patch.object(logger, \"warning\", mock.Mock()):\n         result = is_valid_sample_rate(rate, source=\"Testing\")\n         logger.warning.assert_any_call(StringContaining(\"Given sample rate is invalid\"))\n-        assert result is False\n+        assert result is None\n \n \n @pytest.mark.parametrize(\n"}
{"id": 288, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #983 \r\n\r\n### Motivation\r\n\r\nOld TestMaxPendingChunkMessages() uses the concurrent message publish to make the consumer discard unavailable chunk. And it's flaky.\r\n\r\nSo the `sendSingeChunk()` is introduced to manual create scenarios where old chunks should be discarded.\r\n\r\n### Modifications\r\n\r\n- Fix `TestMaxPendingChunkMessages()`\r\n\r\n### Verifying this change\r\n\r\n- [x] Make sure that the change passes the CI checks.\r\n\n\n### Code Patch:\n@@ -24,10 +24,13 @@ import (\n \t\"math/rand\"\n \t\"net/http\"\n \t\"strings\"\n-\t\"sync\"\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/apache/pulsar-client-go/pulsar/internal\"\n+\n+\t\"google.golang.org/protobuf/proto\"\n+\n \t\"github.com/stretchr/testify/assert\"\n )\n \n@@ -148,92 +151,59 @@ func TestLargeMessage(t *testing.T) {\n }\n \n func TestMaxPendingChunkMessages(t *testing.T) {\n-\trand.Seed(time.Now().Unix())\n-\n \tclient, err := NewClient(ClientOptions{\n \t\tURL: lookupURL,\n \t})\n \tassert.Nil(t, err)\n \tdefer client.Close()\n \n \ttopic := newTopicName()\n+\tproducer, err := client.CreateProducer(ProducerOptions{\n+\t\tTopic:               topic,\n+\t\tDisableBatching:     true,\n+\t\tEnableChunking:      true,\n+\t\tChunkMaxMessageSize: 10,\n+\t})\n+\tassert.NoError(t, err)\n+\tassert.NotNil(t, producer)\n \n-\ttotalProducers := 5\n-\tproducers := make([]Producer, 0, 20)\n-\tdefer func() {\n-\t\tfor _, p := range producers {\n-\t\t\tp.Close()\n-\t\t}\n-\t}()\n-\n-\tclients := make([]Client, 0, 20)\n-\tdefer func() {\n-\t\tfor _, c := range clients {\n-\t\t\tc.Close()\n-\t\t}\n-\t}()\n-\n-\tfor j := 0; j < totalProducers; j++ {\n-\t\tpc, err := NewClient(ClientOptions{\n-\t\t\tURL: lookupURL,\n-\t\t})\n-\t\tassert.Nil(t, err)\n-\t\tclients = append(clients, pc)\n-\t\tproducer, err := pc.CreateProducer(ProducerOptions{\n-\t\t\tTopic:               topic,\n-\t\t\tDisableBatching:     true,\n-\t\t\tEnableChunking:      true,\n-\t\t\tChunkMaxMessageSize: 10,\n-\t\t})\n-\t\tassert.NoError(t, err)\n-\t\tassert.NotNil(t, producer)\n-\t\tproducers = append(producers, producer)\n-\t}\n-\n-\tconsumer, err := client.Subscribe(ConsumerOptions{\n+\tc, err := client.Subscribe(ConsumerOptions{\n \t\tTopic:                    topic,\n \t\tType:                     Exclusive,\n \t\tSubscriptionName:         \"chunk-subscriber\",\n \t\tMaxPendingChunkedMessage: 1,\n \t})\n \tassert.NoError(t, err)\n-\tassert.NotNil(t, consumer)\n-\tdefer consumer.Close()\n+\tassert.NotNil(t, c)\n+\tdefer c.Close()\n+\tpc := c.(*consumer).consumers[0]\n \n-\ttotalMsgs := 40\n-\twg := sync.WaitGroup{}\n-\twg.Add(totalMsgs * totalProducers)\n-\tfor i := 0; i < totalMsgs; i++ {\n-\t\tfor j := 0; j < totalProducers; j++ {\n-\t\t\tp := producers[j]\n-\t\t\tgo func() {\n-\t\t\t\tID, err := p.Send(context.Background(), &ProducerMessage{\n-\t\t\t\t\tPayload: createTestMessagePayload(50),\n-\t\t\t\t})\n-\t\t\t\tassert.NoError(t, err)\n-\t\t\t\tassert.NotNil(t, ID)\n-\t\t\t\twg.Done()\n-\t\t\t}()\n-\t\t}\n-\t}\n-\twg.Wait()\n+\tsendSingleChunk(producer, \"0\", 0, 2)\n+\t// MaxPendingChunkedMessage is 1, the chunked message with uuid 0 will be discarded\n+\tsendSingleChunk(producer, \"1\", 0, 2)\n \n-\treceived := 0\n-\tfor i := 0; i < totalMsgs*totalProducers; i++ {\n-\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*10)\n-\t\tmsg, err := consumer.Receive(ctx)\n-\t\tcancel()\n-\t\tif msg == nil || (err != nil && errors.Is(err, context.DeadlineExceeded)) {\n-\t\t\tbreak\n-\t\t}\n+\t// chunkedMsgCtx with uuid 0 should be discarded\n+\tretryAssert(t, 3, 200, func() {}, func(t assert.TestingT) bool {\n+\t\tpc.chunkedMsgCtxMap.mu.Lock()\n+\t\tdefer pc.chunkedMsgCtxMap.mu.Unlock()\n+\t\treturn assert.Equal(t, 1, len(pc.chunkedMsgCtxMap.chunkedMsgCtxs))\n+\t})\n \n-\t\treceived++\n+\tsendSingleChunk(producer, \"1\", 1, 2)\n \n-\t\terr = consumer.Ack(msg)\n-\t\tassert.NoError(t, err)\n-\t}\n+\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n+\tmsg, err := c.Receive(ctx)\n+\tcancel()\n \n-\tassert.NotEqual(t, totalMsgs*totalProducers, received)\n+\tassert.NoError(t, err)\n+\tassert.Equal(t, \"chunk-1-0|chunk-1-1|\", string(msg.Payload()))\n+\n+\t// Ensure that the chunked message of uuid 0 is discarded.\n+\tsendSingleChunk(producer, \"0\", 1, 2)\n+\tctx, cancel = context.WithTimeout(context.Background(), 5*time.Second)\n+\tmsg, err = c.Receive(ctx)\n+\tcancel()\n+\tassert.True(t, errors.Is(err, context.DeadlineExceeded))\n }\n \n func TestExpireIncompleteChunks(t *testing.T) {\n@@ -576,3 +546,32 @@ func createTestMessagePayload(size int) []byte {\n \t}\n \treturn payload\n }\n+\n+//nolint:all\n+func sendSingleChunk(p Producer, uuid string, chunkID int, totalChunks int) {\n+\tmsg := &ProducerMessage{\n+\t\tPayload: []byte(fmt.Sprintf(\"chunk-%s-%d|\", uuid, chunkID)),\n+\t}\n+\tproducerImpl := p.(*producer).producers[0].(*partitionProducer)\n+\tmm := producerImpl.genMetadata(msg, len(msg.Payload), time.Now())\n+\tmm.Uuid = proto.String(uuid)\n+\tmm.NumChunksFromMsg = proto.Int32(int32(totalChunks))\n+\tmm.TotalChunkMsgSize = proto.Int32(int32(len(msg.Payload)))\n+\tmm.ChunkId = proto.Int32(int32(chunkID))\n+\tproducerImpl.updateMetadataSeqID(mm, msg)\n+\n+\tdoneCh := make(chan struct{})\n+\tproducerImpl.internalSingleSend(\n+\t\tmm,\n+\t\tmsg.Payload,\n+\t\t&sendRequest{\n+\t\t\tcallback: func(id MessageID, producerMessage *ProducerMessage, err error) {\n+\t\t\t\tclose(doneCh)\n+\t\t\t},\n+\t\t\tmsg: msg,\n+\t\t},\n+\t\tuint32(internal.MaxMessageSize),\n+\t)\n+\n+\t<-doneCh\n+}\n"}
{"id": 189, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\n!!!ATTENTION!!!\r\n\r\nIf you are fixing *any* crash or *any* potential security issue, *do not*\r\nopen a pull request in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.\r\nThank you in advance for helping to keep Envoy secure.\r\n\r\n!!!ATTENTION!!!\r\n\r\nFor an explanation of how to fill out the fields, please see the relevant section\r\nin [PULL_REQUESTS.md](https://github.com/envoyproxy/envoy/blob/main/PULL_REQUESTS.md)\r\n-->\r\n\r\nCommit Message: aws: fix test flake when no IMDS can be found\r\nAdditional Description: Lack of IMDS (169.254.169.254 address) can cause a race condition and crash during testing due to the shutdown of cluster manager. This scenario should not occur normally, as cluster manager will still exist and the lack of IMDS is handled gracefully.\r\nRisk Level: Low\r\nTesting: N/A\r\nDocs Changes:\r\nRelease Notes:\r\nPlatform Specific Features:\r\n[Optional Runtime guard:]\r\n[Optional Fixes #Issue]\r\n[Optional Fixes commit #PR or SHA]\r\n[Optional Deprecated:]\r\n[Optional [API Considerations](https://github.com/envoyproxy/envoy/blob/main/api/review_checklist.md):]\r\n\n\n### Code Patch:\n@@ -52,6 +52,12 @@ class MetadataFetcherImpl : public MetadataFetcher,\n     ASSERT(!request_);\n     complete_ = false;\n     receiver_ = makeOptRef(receiver);\n+\n+    // Stop processing if we are shutting down\n+    if (cm_.isShutdown()) {\n+      return;\n+    }\n+\n     const auto thread_local_cluster = cm_.getThreadLocalCluster(cluster_name_);\n     if (thread_local_cluster == nullptr) {\n       ENVOY_LOG(error, \"{} AWS Metadata failed: [cluster = {}] not found\", __func__, cluster_name_);\n\n@@ -77,6 +77,20 @@ MATCHER_P(OptionsHasRetryPolicy, policyMatcher, \"\") {\n class MetadataFetcherTest : public testing::Test {\n public:\n   void setupFetcher() {\n+\n+    mock_factory_ctx_.server_factory_context_.cluster_manager_.initializeThreadLocalClusters(\n+        {\"cluster_name\"});\n+    fetcher_ = MetadataFetcher::create(mock_factory_ctx_.server_factory_context_.cluster_manager_,\n+                                       \"cluster_name\");\n+    EXPECT_TRUE(fetcher_ != nullptr);\n+  }\n+\n+  void setupFetcherShutDown() {\n+    ON_CALL(mock_factory_ctx_.server_factory_context_.cluster_manager_, getThreadLocalCluster(_))\n+        .WillByDefault(Return(nullptr));\n+    ON_CALL(mock_factory_ctx_.server_factory_context_.cluster_manager_, isShutdown())\n+        .WillByDefault(Return(true));\n+\n     mock_factory_ctx_.server_factory_context_.cluster_manager_.initializeThreadLocalClusters(\n         {\"cluster_name\"});\n     fetcher_ = MetadataFetcher::create(mock_factory_ctx_.server_factory_context_.cluster_manager_,\n@@ -103,6 +117,18 @@ TEST_F(MetadataFetcherTest, TestGetSuccess) {\n   fetcher_->fetch(message, parent_span_, receiver);\n }\n \n+TEST_F(MetadataFetcherTest, TestClusterShutdown) {\n+  // Setup\n+  setupFetcherShutDown();\n+  Http::RequestMessageImpl message;\n+  MockMetadataReceiver receiver;\n+  EXPECT_CALL(receiver, onMetadataSuccess(_)).Times(0);\n+  EXPECT_CALL(receiver, onMetadataError(_)).Times(0);\n+\n+  // Act\n+  fetcher_->fetch(message, parent_span_, receiver);\n+}\n+\n TEST_F(MetadataFetcherTest, TestRequestMatchAndSpanPassedDown) {\n   // Setup\n   setupFetcher();\n"}
{"id": 431, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThere's a TOCTOU issue that can happen when selecting unused ports for the server to use (we get assigned an unused port by the OS, and between then and when the server actually binds to the port another test steals it). Improve this by checking if the server existed soon after setup, and if so we retry starting it. Client connection can also fail spuriously (in local testing) so added a retry mechanism.\r\n\r\nThis also fixes a hang, where if the server exited (almost always due to the issue described above) before we connected to it, attempting to connect our client ZMQ sockets to it would just hang. To resolve this, I added a timeout so we can't wait forever. \n\n### Code Patch:\n@@ -191,7 +191,24 @@ async fn connect_socket<S: zeromq::Socket>(\n ) -> S {\n   let addr = spec.endpoint(port);\n   let mut socket = S::new();\n-  socket.connect(&addr).await.unwrap();\n+  let mut connected = false;\n+  for _ in 0..5 {\n+    match timeout(Duration::from_secs(5), socket.connect(&addr)).await {\n+      Ok(Ok(_)) => {\n+        connected = true;\n+        break;\n+      }\n+      Ok(Err(e)) => {\n+        eprintln!(\"Failed to connect to {addr}: {e}\");\n+      }\n+      Err(e) => {\n+        eprintln!(\"Timed out connecting to {addr}: {e}\");\n+      }\n+    }\n+  }\n+  if !connected {\n+    panic!(\"Failed to connect to {addr}\");\n+  }\n   socket\n }\n \n@@ -369,27 +386,48 @@ impl Drop for JupyterServerProcess {\n   }\n }\n \n-fn setup_server() -> (TestContext, ConnectionSpec, JupyterServerProcess) {\n+async fn setup_server() -> (TestContext, ConnectionSpec, JupyterServerProcess) {\n   let context = TestContextBuilder::new().use_temp_cwd().build();\n-  let conn = ConnectionSpec::default();\n+  let mut conn = ConnectionSpec::default();\n   let conn_file = context.temp_dir().path().join(\"connection.json\");\n   conn_file.write_json(&conn);\n-  let process = context\n-    .new_command()\n-    .piped_output()\n-    .args_vec(vec![\n-      \"jupyter\",\n-      \"--kernel\",\n-      \"--conn\",\n-      conn_file.to_string().as_str(),\n-    ])\n-    .spawn()\n-    .unwrap();\n+\n+  let start_process = |conn_file: &test_util::PathRef| {\n+    context\n+      .new_command()\n+      .args_vec(vec![\n+        \"jupyter\",\n+        \"--kernel\",\n+        \"--conn\",\n+        conn_file.to_string().as_str(),\n+      ])\n+      .spawn()\n+      .unwrap()\n+  };\n+\n+  // try to start the server, retrying up to 5 times\n+  // (this can happen due to TOCTOU errors with selecting unused TCP ports)\n+  let mut process = start_process(&conn_file);\n+  tokio::time::sleep(Duration::from_millis(1000)).await;\n+\n+  for _ in 0..5 {\n+    if process.try_wait().unwrap().is_none() {\n+      break;\n+    } else {\n+      conn = ConnectionSpec::default();\n+      conn_file.write_json(&conn);\n+      process = start_process(&conn_file);\n+      tokio::time::sleep(Duration::from_millis(1000)).await;\n+    }\n+  }\n+  if process.try_wait().unwrap().is_some() {\n+    panic!(\"Failed to start Jupyter server\");\n+  }\n   (context, conn, JupyterServerProcess(Some(process)))\n }\n \n async fn setup() -> (TestContext, JupyterClient, JupyterServerProcess) {\n-  let (context, conn, process) = setup_server();\n+  let (context, conn, process) = setup_server().await;\n   let client = JupyterClient::new(&conn).await;\n   client.io_subscribe(\"\").await.unwrap();\n \n"}
{"id": 20, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThere was a couple of errors on the dev script preventing it to work properly. I also merged this with the open pr I had for adding `next/css`\n\n\n### Code Patch:\n@@ -3,7 +3,7 @@\n import { resolve } from 'path'\n import parseArgs from 'minimist'\n import Server from '../server'\n-import build from '../server/build'\n+import build from '../server/build/index'\n \n const argv = parseArgs(process.argv.slice(2), {\n   alias: {\n\n@@ -2,12 +2,14 @@ import React from 'react'\n import ReactDOM from 'react-dom'\n import App from '../lib/app'\n import Link from '../lib/link'\n+import Css from '../lib/css'\n \n const modules = new Map([\n   ['react', React],\n   ['react-dom', ReactDOM],\n   ['next/app', App],\n-  ['next/link', Link]\n+  ['next/link', Link],\n+  ['next/css', Css]\n ])\n \n /**\n\n@@ -14,7 +14,8 @@ export default function bundle (src, dst) {\n       'react-dom',\n       {\n         [require.resolve('react')]: 'react',\n-        [require.resolve('../lib/link')]: 'next/link'\n+        [require.resolve('../../lib/link')]: 'next/link',\n+        [require.resolve('../../lib/css')]: 'next/css'\n       }\n     ],\n     resolveLoader: {\n\n@@ -25,7 +25,8 @@ const babelOptions = {\n       [\n         { src: `npm:${babelRuntimePath}`, expose: 'babel-runtime' },\n         { src: `npm:${require.resolve('react')}`, expose: 'react' },\n-        { src: `npm:${require.resolve('../lib/link')}`, expose: 'next/link' }\n+        { src: `npm:${require.resolve('../../lib/link')}`, expose: 'next/link' },\n+        { src: `npm:${require.resolve('../../lib/css')}`, expose: 'next/css' }\n       ]\n     ]\n   ],\n"}
{"id": 271, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nIntroduced by 3d707be950b387552585451071928e7b39cdfa53.\r\n\r\nFrom docs buildbot: http://buildbot.python.org/all/builders/Docs%203.x/builds/399/steps/lint/logs/stdio\r\n\r\n```\r\npython3 tools/rstlint.py -i tools -i venv\r\n[1] faq/windows.rst:303: trailing whitespace\r\n[1] faq/windows.rst:305: trailing whitespace\r\n2 problems with severity 1 found.\r\n```\n\n### Code Patch:\n@@ -300,9 +300,9 @@ this respect, and is easily configured to use spaces: Take :menuselection:`Tools\n --> Options --> Tabs`, and for file type \"Default\" set \"Tab size\" and \"Indent\n size\" to 4, and select the \"Insert spaces\" radio button.\n \n-Python raises :exc:`IndentationError` or :exc:`TabError` if mixed tabs \n+Python raises :exc:`IndentationError` or :exc:`TabError` if mixed tabs\n and spaces are causing problems in leading whitespace.\n-You may also run the :mod:`tabnanny` module to check a directory tree \n+You may also run the :mod:`tabnanny` module to check a directory tree\n in batch mode.\n \n \n"}
{"id": 74, "ground_truth": "Network", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nhttps://dev.azure.com/monacotools/Monaco/_build/results?buildId=314193&view=logs&j=9833cdaa-2e92-5f8d-56a8-239fd18fd628&t=efbaf268-cf98-50c2-bb3c-7950770fd6bc&l=749\n\n```\n  1) vscode API - terminal\n       Terminal\n         environmentVariableCollection\n           should have collection variables apply to terminals immediately after setting:\n     Error: Timeout: b1~b2~ should be printed after 20 seconds.\nDid not pass accept function\n      at poll (/Users/runner/work/1/s/extensions/vscode-api-tests/src/utils.ts:176:10)\n      at Context.<anonymous> (/Users/runner/work/1/s/extensions/vscode-api-tests/src/singlefolder-tests/terminal.test.ts:775:5)\n\n  2) vscode API - terminal\n       Terminal\n         environmentVariableCollection\n           should have collection variables apply to environment variables that don't exist:\n     Error: Timeout: ~b2~ should be printed after 20 seconds.\nDid not pass accept function\n      at poll (/Users/runner/work/1/s/extensions/vscode-api-tests/src/utils.ts:176:10)\n      at Context.<anonymous> (/Users/runner/work/1/s/extensions/vscode-api-tests/src/singlefolder-tests/terminal.test.ts:820:5)\n\n  3) vscode API - terminal\n       Terminal\n         environmentVariableCollection\n           should respect clearing entries:\n     Error: Timeout: ~b1~ should be printed after 20 seconds.\nDid not pass accept function\n      at poll (/Users/runner/work/1/s/extensions/vscode-api-tests/src/utils.ts:176:10)\n      at Context.<anonymous> (/Users/runner/work/1/s/extensions/vscode-api-tests/src/singlefolder-tests/terminal.test.ts:862:5)\n\n\n```\n\n### Code Patch:\n@@ -26,6 +26,8 @@ import { assertNoRpc, poll } from '../utils';\n \t\tawait config.update('gpuAcceleration', 'off', ConfigurationTarget.Global);\n \t\t// Disable env var relaunch for tests to prevent terminals relaunching themselves\n \t\tawait config.update('environmentChangesRelaunch', false, ConfigurationTarget.Global);\n+\t\t// Disable local echo in case it causes any problems in remote tests\n+\t\tawait config.update('localEchoEnabled', \"off\", ConfigurationTarget.Global);\n \t\tawait config.update('shellIntegration.enabled', false);\n \t});\n \n@@ -761,14 +763,9 @@ import { assertNoRpc, poll } from '../utils';\n \t\t\t\t\tdata += sanitizeData(e.data);\n \t\t\t\t}));\n \n-\t\t\t\t// Run both PowerShell and sh commands, errors don't matter we're just looking for\n-\t\t\t\t// the correct output\n-\t\t\t\tterminal.sendText('$env:A');\n-\t\t\t\tterminal.sendText('echo $A');\n-\t\t\t\tterminal.sendText('$env:B');\n-\t\t\t\tterminal.sendText('echo $B');\n-\t\t\t\tterminal.sendText('$env:C');\n-\t\t\t\tterminal.sendText('echo $C');\n+\t\t\t\t// Run sh commands, if this is ever enabled on Windows we would also want to run\n+\t\t\t\t// the pwsh equivalent\n+\t\t\t\tterminal.sendText('echo \"$A $B $C\"');\n \n \t\t\t\t// Poll for the echo results to show up\n \t\t\t\ttry {\n@@ -811,19 +808,19 @@ import { assertNoRpc, poll } from '../utils';\n \t\t\t\t\tdata += sanitizeData(e.data);\n \t\t\t\t}));\n \n-\t\t\t\t// Run both PowerShell and sh commands, errors don't matter we're just looking for\n-\t\t\t\t// the correct output\n-\t\t\t\tterminal.sendText('$env:A');\n-\t\t\t\tterminal.sendText('echo $A');\n-\t\t\t\tterminal.sendText('$env:B');\n-\t\t\t\tterminal.sendText('echo $B');\n-\t\t\t\tterminal.sendText('$env:C');\n-\t\t\t\tterminal.sendText('echo $C');\n+\t\t\t\t// Run sh commands, if this is ever enabled on Windows we would also want to run\n+\t\t\t\t// the pwsh equivalent\n+\t\t\t\tterminal.sendText('echo \"$A $B $C\"');\n \n \t\t\t\t// Poll for the echo results to show up\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~a2~'), '~a2~ should be printed');\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~b2~'), '~b2~ should be printed');\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~c2~'), '~c2~ should be printed');\n+\t\t\t\ttry {\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~a2~'), '~a2~ should be printed');\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~b2~'), '~b2~ should be printed');\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~c2~'), '~c2~ should be printed');\n+\t\t\t\t} catch (err) {\n+\t\t\t\t\tconsole.error('DATA UP UNTIL NOW:', data);\n+\t\t\t\t\tthrow err;\n+\t\t\t\t}\n \n \t\t\t\t// Wait for terminal to be disposed\n \t\t\t\tawait new Promise<void>(r => {\n@@ -855,16 +852,18 @@ import { assertNoRpc, poll } from '../utils';\n \t\t\t\t\tdata += sanitizeData(e.data);\n \t\t\t\t}));\n \n-\t\t\t\t// Run both PowerShell and sh commands, errors don't matter we're just looking for\n-\t\t\t\t// the correct output\n-\t\t\t\tterminal.sendText('$env:A');\n-\t\t\t\tterminal.sendText('echo $A');\n-\t\t\t\tterminal.sendText('$env:B');\n-\t\t\t\tterminal.sendText('echo $B');\n+\t\t\t\t// Run sh commands, if this is ever enabled on Windows we would also want to run\n+\t\t\t\t// the pwsh equivalent\n+\t\t\t\tterminal.sendText('echo \"$A $B\"');\n \n \t\t\t\t// Poll for the echo results to show up\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~a1~'), '~a1~ should be printed');\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~b1~'), '~b1~ should be printed');\n+\t\t\t\ttry {\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~a1~'), '~a1~ should be printed');\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~b1~'), '~b1~ should be printed');\n+\t\t\t\t} catch (err) {\n+\t\t\t\t\tconsole.error('DATA UP UNTIL NOW:', data);\n+\t\t\t\t\tthrow err;\n+\t\t\t\t}\n \n \t\t\t\t// Wait for terminal to be disposed\n \t\t\t\tawait new Promise<void>(r => {\n@@ -896,16 +895,18 @@ import { assertNoRpc, poll } from '../utils';\n \t\t\t\t\tdata += sanitizeData(e.data);\n \t\t\t\t}));\n \n-\t\t\t\t// Run both PowerShell and sh commands, errors don't matter we're just looking for\n-\t\t\t\t// the correct output\n-\t\t\t\tterminal.sendText('$env:A');\n-\t\t\t\tterminal.sendText('echo $A');\n-\t\t\t\tterminal.sendText('$env:B');\n-\t\t\t\tterminal.sendText('echo $B');\n+\t\t\t\t// Run sh commands, if this is ever enabled on Windows we would also want to run\n+\t\t\t\t// the pwsh equivalent\n+\t\t\t\tterminal.sendText('echo \"$A $B\"');\n \n \t\t\t\t// Poll for the echo results to show up\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~a1~'), '~a1~ should be printed');\n-\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~b2~'), '~b2~ should be printed');\n+\t\t\t\ttry {\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~a1~'), '~a1~ should be printed');\n+\t\t\t\t\tawait poll<void>(() => Promise.resolve(), () => data.includes('~b2~'), '~b2~ should be printed');\n+\t\t\t\t} catch (err) {\n+\t\t\t\t\tconsole.error('DATA UP UNTIL NOW:', data);\n+\t\t\t\t\tthrow err;\n+\t\t\t\t}\n \n \t\t\t\t// Wait for terminal to be disposed\n \t\t\t\tawait new Promise<void>(r => {\n"}
{"id": 302, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## PR summary\r\n\r\nFor labelcolor={linecolor,markeredgecolor,markerfacecolor}, text will match the specified attribute if consistent, but fall back to black if they differ within a single labeled artist.\r\n\r\nThese tests use 10 random colours out of the ['r', 'g', 'b'] set, so 3 (all red, all green, all blue) out of 3**10 will result in the text _not_ being black. This is rare (0.0051%), but does happen once in a while (e.g., https://github.com/matplotlib/matplotlib/actions/runs/10967975776/job/30458622005).\r\n\r\nInstead, just hard-code some different colours in the test.\r\n\r\n## PR checklist\r\n\r\n- [n/a] \"closes #0000\" is in the body of the PR description to [link the related issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue)\r\n- [x] new and changed code is [tested](https://matplotlib.org/devdocs/devel/testing.html)\r\n- [n/a] *Plotting related* features are demonstrated in an [example](https://matplotlib.org/devdocs/devel/document.html#write-examples-and-tutorials)\r\n- [n/a] *New Features* and *API Changes* are noted with a [directive and release note](https://matplotlib.org/devdocs/devel/api_changes.html#announce-changes-deprecations-and-new-features)\r\n- [n/a] Documentation complies with [general](https://matplotlib.org/devdocs/devel/document.html#write-rest-pages) and [docstring](https://matplotlib.org/devdocs/devel/document.html#write-docstrings) guidelines\n\n### Code Patch:\n@@ -868,8 +868,8 @@ def test_legend_pathcollection_labelcolor_linecolor_iterable():\n     # test the labelcolor for labelcolor='linecolor' on PathCollection\n     # with iterable colors\n     fig, ax = plt.subplots()\n-    colors = np.random.default_rng().choice(['r', 'g', 'b'], 10)\n-    ax.scatter(np.arange(10), np.arange(10)*1, label='#1', c=colors)\n+    colors = np.array(['r', 'g', 'b', 'c', 'm'] * 2)\n+    ax.scatter(np.arange(10), np.arange(10), label='#1', c=colors)\n \n     leg = ax.legend(labelcolor='linecolor')\n     text, = leg.get_texts()\n@@ -915,8 +915,8 @@ def test_legend_pathcollection_labelcolor_markeredgecolor_iterable():\n     # test the labelcolor for labelcolor='markeredgecolor' on PathCollection\n     # with iterable colors\n     fig, ax = plt.subplots()\n-    colors = np.random.default_rng().choice(['r', 'g', 'b'], 10)\n-    ax.scatter(np.arange(10), np.arange(10)*1, label='#1', edgecolor=colors)\n+    colors = np.array(['r', 'g', 'b', 'c', 'm'] * 2)\n+    ax.scatter(np.arange(10), np.arange(10), label='#1', edgecolor=colors)\n \n     leg = ax.legend(labelcolor='markeredgecolor')\n     for text, color in zip(leg.get_texts(), ['k']):\n@@ -970,8 +970,8 @@ def test_legend_pathcollection_labelcolor_markerfacecolor_iterable():\n     # test the labelcolor for labelcolor='markerfacecolor' on PathCollection\n     # with iterable colors\n     fig, ax = plt.subplots()\n-    colors = np.random.default_rng().choice(['r', 'g', 'b'], 10)\n-    ax.scatter(np.arange(10), np.arange(10)*1, label='#1', facecolor=colors)\n+    colors = np.array(['r', 'g', 'b', 'c', 'm'] * 2)\n+    ax.scatter(np.arange(10), np.arange(10), label='#1', facecolor=colors)\n \n     leg = ax.legend(labelcolor='markerfacecolor')\n     for text, color in zip(leg.get_texts(), ['k']):\n"}
{"id": 294, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Rationale for this change\nAdditional checks for `io.EOF` to avoid flaky tests\n\n\n### Code Patch:\n@@ -21,6 +21,7 @@ package example\n \n import (\n \t\"database/sql\"\n+\t\"io\"\n \t\"reflect\"\n \t\"strconv\"\n \t\"strings\"\n@@ -264,7 +265,7 @@ func (r *SqlBatchReader) Next() bool {\n \n \trows := 0\n \tfor rows < maxBatchSize && r.rows.Next() {\n-\t\tif err := r.rows.Scan(r.rowdest...); err != nil {\n+\t\tif err := r.rows.Scan(r.rowdest...); err != nil && err.Error() != io.EOF.Error() {\n \t\t\t// Not really useful except for testing Flight SQL clients\n \t\t\tdetail := wrapperspb.StringValue{Value: r.schema.String()}\n \t\t\tif st, sterr := status.New(codes.Unknown, err.Error()).WithDetails(&detail); sterr != nil {\n\n@@ -22,6 +22,8 @@ package example\n import (\n \t\"context\"\n \t\"database/sql\"\n+\t\"errors\"\n+\t\"io\"\n \t\"strings\"\n \t\"sync/atomic\"\n \n@@ -173,7 +175,9 @@ func (s *SqliteTablesSchemaBatchReader) Next() bool {\n \t\tfor rows.Next() {\n \t\t\tif err := rows.Scan(&tableName, &name, &typ, &nn); err != nil {\n \t\t\t\trows.Close()\n-\t\t\t\ts.err = err\n+\t\t\t\tif err.Error() != io.EOF.Error() {\n+\t\t\t\t\ts.err = err\n+\t\t\t\t}\n \t\t\t\treturn false\n \t\t\t}\n \n@@ -186,7 +190,7 @@ func (s *SqliteTablesSchemaBatchReader) Next() bool {\n \t\t}\n \n \t\trows.Close()\n-\t\tif rows.Err() != nil {\n+\t\tif rows.Err() != nil && !errors.Is(rows.Err(), io.EOF) {\n \t\t\ts.err = rows.Err()\n \t\t\treturn false\n \t\t}\n\n@@ -18,7 +18,9 @@ package flight\n \n import (\n \t\"bytes\"\n+\t\"errors\"\n \t\"fmt\"\n+\t\"io\"\n \t\"sync/atomic\"\n \n \t\"github.com/apache/arrow-go/v18/arrow\"\n@@ -256,7 +258,7 @@ func ConcatenateReaders(rdrs []array.RecordReader, ch chan<- StreamChunk) {\n \t\t\tch <- StreamChunk{Data: rec}\n \t\t}\n \t\tif e, ok := r.(haserr); ok {\n-\t\t\tif e.Err() != nil {\n+\t\t\tif e.Err() != nil && !errors.Is(e.Err(), io.EOF) {\n \t\t\t\tch <- StreamChunk{Err: e.Err()}\n \t\t\t\treturn\n \t\t\t}\n"}
{"id": 319, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis issue can be caused by a non-existing path but also a misunderstanding from the config file. A short example will help the user.\n\n\n### Code Patch:\n@@ -101,7 +101,9 @@ log.cleanup.interval.mins=1\n # This is a comma separated host:port pairs, each corresponding to a zk\n # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n # You can also append an optional chroot string to the urls to specify the\n-# root directory for all kafka znodes.\n+# root directory for all kafka znodes, e.g.\n+# \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/kafka\". If you use a chroot\n+# string, be sure to manualy create this namespace in zookeeper.\n zk.connect=localhost:2181\n \n # Timeout in ms for connecting to zookeeper\n"}
{"id": 66, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nxref #14626\n\n### Code Patch:\n@@ -915,23 +915,12 @@ def test_format(self):\n         self._check_method_works(Index.format)\n \n         # GH 14626\n-        # our formatting is different by definition when we have\n-        # ms vs us precision (e.g. trailing zeros);\n-        # so don't compare this case\n-        def datetime_now_without_trailing_zeros():\n-            now = datetime.now()\n-\n-            while str(now).endswith(\"000\"):\n-                now = datetime.now()\n-\n-            return now\n-\n-        index = Index([datetime_now_without_trailing_zeros()])\n-\n         # windows has different precision on datetime.datetime.now (it doesn't\n         # include us since the default for Timestamp shows these but Index\n-        # formating does not we are skipping\n-        if not is_platform_windows():\n+        # formating does not we are skipping)\n+        now = datetime.now()\n+        if not str(now).endswith(\"000\"):\n+            index = Index([now])\n             formatted = index.format()\n             expected = [str(index[0])]\n             self.assertEqual(formatted, expected)\n"}
{"id": 35, "ground_truth": "Logic", "predicted": "Floating point operations", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nPlatforms: linux\n\n  This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_grad_with_manual_interleaved_ScheduleClass0_use_new_runtime_True&suite=ScheduleTest&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/42919034022).\n\n  Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes.\n\n  **Debugging instructions (after clicking on the recent samples link):**\n  DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\n  To find relevant log snippets:\n  1. Click on the workflow logs linked above\n  2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n  3. Grep for `test_grad_with_manual_interleaved_ScheduleClass0_use_new_runtime_True`\n  4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n  \n  \n\n<details><summary>Sample error message</summary>\n\n```\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 1721, in wrapper\n    raise rv\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 158 / 512 (30.9%)\nGreatest absolute difference: 0.0002696812152862549 at index (331,) (up to 4e-05 allowed)\nGreatest relative difference: 0.09954891353845596 at index (176,) (up to 1e-05 allowed)\n\nTo execute this test, run the following from the base repo dir:\n    python test/distributed/pipelining/test_schedule_multiproc.py ScheduleTest.test_grad_with_manual_interleaved_ScheduleClass0_use_new_runtime_True\n\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n```\n\n</details>\n\n\n  Test file path: `distributed/pipelining/test_schedule_multiproc.py`\n\n  For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.\n\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @clee2000\n\n### Code Patch:\n@@ -513,7 +513,7 @@ def test_grad_with_manual_interleaved(self, ScheduleClass, use_new_runtime):\n             for name, p in stage_module.named_parameters():\n                 ref_p = ref_submod.get_parameter(name)\n                 try:\n-                    torch.testing.assert_close(p.grad, ref_p.grad, rtol=1e-5, atol=4e-5)\n+                    torch.testing.assert_close(p.grad, ref_p.grad, rtol=1e-5, atol=1e-3)\n                 except AssertionError:\n                     print(f\"Gradient test failed for {name}: {p.grad} vs {ref_p.grad}\")\n                     raise\n"}
{"id": 89, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nhttps://scans.gradle.com/s/bfgjiqwioneey/tests/task/:profiler:test/details/com.splunk.opentelemetry.profiler.snapshot.LongRunningBackgroundTaskTest/traceBackgroundThreadProfilingContinuesAfterEntrySpanEnds()?top-execution=1\n\n### Code Patch:\n@@ -24,6 +24,8 @@\n import com.splunk.opentelemetry.profiler.snapshot.simulation.Server;\n import io.opentelemetry.sdk.autoconfigure.OpenTelemetrySdkExtension;\n import java.time.Duration;\n+import java.util.concurrent.CountDownLatch;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.RegisterExtension;\n \n@@ -40,20 +42,28 @@ class LongRunningBackgroundTaskTest {\n           .with(new SnapshotVolumePropagator((c) -> true))\n           .build();\n \n+  private CountDownLatch slowTaskLatch = new CountDownLatch(1);\n+\n   @RegisterExtension\n   public final Server server =\n       Server.builder(sdk).named(\"server\").performing(Background.task(slowTask())).build();\n \n   private Runnable slowTask() {\n     return () -> {\n       try {\n-        Thread.sleep(250);\n+        slowTaskLatch.await();\n       } catch (InterruptedException e) {\n-        e.printStackTrace();\n+        Thread.currentThread().interrupt();\n       }\n     };\n   }\n \n+  @AfterEach\n+  void reset() {\n+    slowTaskLatch.countDown();\n+    slowTaskLatch = new CountDownLatch(1);\n+  }\n+\n   @Test\n   void traceBackgroundThreadProfilingContinuesAfterEntrySpanEnds() {\n     server.send(new Message());\n\n@@ -18,6 +18,7 @@\n \n import io.opentelemetry.context.Context;\n import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n import java.util.function.UnaryOperator;\n@@ -44,10 +45,22 @@ public static <T> UnaryOperator<Message> task(Callable<T> task) {\n \n   /** Perform background task within the context of the same trace. */\n   public static UnaryOperator<Message> task(Runnable task) {\n+    CountDownLatch latch = new CountDownLatch(1);\n     return message -> {\n       var executor = Context.current().wrap(Executors.newSingleThreadExecutor());\n       try {\n-        executor.submit(task);\n+        Runnable runnable =\n+            () -> {\n+              latch.countDown();\n+              task.run();\n+            };\n+        executor.submit(runnable);\n+        // wait until the task has started\n+        try {\n+          latch.await();\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+        }\n         return message;\n       } finally {\n         executor.shutdown();\n"}
{"id": 223, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThanks for Diana Carroll to report this issue (https://spark-project.atlassian.net/browse/SPARK-1100)\n\nthe current saveAsTextFile/SequenceFile will overwrite the output directory silently if the directory already exists, this behaviour is not desirable because\n\noverwriting the data silently is not user-friendly\n\nif the partition number of two writing operation changed, then the output directory will contain the results generated by two runnings\n\nMy fix includes:\n\nadd some new APIs with a flag for users to define whether he/she wants to overwrite the directory:\nif the flag is set to true, then the output directory is deleted first and then written into the new data to prevent the output directory contains results from multiple rounds of running;\n\nif the flag is set to false, Spark will throw an exception if the output directory already exists\n\nchanged JavaAPI part\n\ndefault behaviour is overwriting\n\nTwo questions\n\nshould we deprecate the old APIs without such a flag?\n\nI noticed that Spark Streaming also called these APIs, I thought we don't need to change the related part in streaming? @tdas\n\n\n### Code Patch:\n@@ -30,18 +30,15 @@ import scala.reflect.ClassTag\n \n import com.clearspring.analytics.stream.cardinality.HyperLogLog\n import org.apache.hadoop.conf.{Configurable, Configuration}\n-import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.{FileSystem, Path}\n import org.apache.hadoop.io.SequenceFile.CompressionType\n import org.apache.hadoop.io.compress.CompressionCodec\n import org.apache.hadoop.mapred.{FileOutputCommitter, FileOutputFormat, JobConf, OutputFormat}\n-import org.apache.hadoop.mapreduce.{OutputFormat => NewOutputFormat}\n-import org.apache.hadoop.mapreduce.{Job => NewAPIHadoopJob}\n-import org.apache.hadoop.mapreduce.{RecordWriter => NewRecordWriter}\n+import org.apache.hadoop.mapreduce.{OutputFormat => NewOutputFormat, Job => NewAPIHadoopJob, RecordWriter => NewRecordWriter, JobContext, SparkHadoopMapReduceUtil}\n import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat => NewFileOutputFormat}\n \n // SparkHadoopWriter and SparkHadoopMapReduceUtil are actually source files defined in Spark.\n import org.apache.hadoop.mapred.SparkHadoopWriter\n-import org.apache.hadoop.mapreduce.SparkHadoopMapReduceUtil\n \n import org.apache.spark._\n import org.apache.spark.Partitioner.defaultPartitioner\n@@ -604,8 +601,12 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])\n     val job = new NewAPIHadoopJob(conf)\n     job.setOutputKeyClass(keyClass)\n     job.setOutputValueClass(valueClass)\n+\n     val wrappedConf = new SerializableWritable(job.getConfiguration)\n-    NewFileOutputFormat.setOutputPath(job, new Path(path))\n+    val outpath = new Path(path)\n+    NewFileOutputFormat.setOutputPath(job, outpath)\n+    val jobFormat = outputFormatClass.newInstance\n+    jobFormat.checkOutputSpecs(job)\n     val formatter = new SimpleDateFormat(\"yyyyMMddHHmm\")\n     val jobtrackerID = formatter.format(new Date())\n     val stageId = self.id\n@@ -633,7 +634,7 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])\n       committer.commitTask(hadoopContext)\n       return 1\n     }\n-    val jobFormat = outputFormatClass.newInstance\n+\n     /* apparently we need a TaskAttemptID to construct an OutputCommitter;\n      * however we're only going to use this local OutputCommitter for\n      * setupJob/commitJob, so we just use a dummy \"map\" task.\n@@ -642,7 +643,7 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])\n     val jobTaskContext = newTaskAttemptContext(wrappedConf.value, jobAttemptId)\n     val jobCommitter = jobFormat.getOutputCommitter(jobTaskContext)\n     jobCommitter.setupJob(jobTaskContext)\n-    val count = self.context.runJob(self, writeShard _).sum\n+    self.context.runJob(self, writeShard _)\n     jobCommitter.commitJob(jobTaskContext)\n   }\n \n@@ -696,10 +697,10 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])\n    * MapReduce job.\n    */\n   def saveAsHadoopDataset(conf: JobConf) {\n-    val outputFormatClass = conf.getOutputFormat\n+    val outputFormatInstance = conf.getOutputFormat\n     val keyClass = conf.getOutputKeyClass\n     val valueClass = conf.getOutputValueClass\n-    if (outputFormatClass == null) {\n+    if (outputFormatInstance == null) {\n       throw new SparkException(\"Output format class not set\")\n     }\n     if (keyClass == null) {\n@@ -712,6 +713,12 @@ class PairRDDFunctions[K: ClassTag, V: ClassTag](self: RDD[(K, V)])\n     logDebug(\"Saving as hadoop file of type (\" + keyClass.getSimpleName + \", \" +\n       valueClass.getSimpleName + \")\")\n \n+    if (outputFormatInstance.isInstanceOf[FileOutputFormat[_, _]]) {\n+      // FileOutputFormat ignores the filesystem parameter\n+      val ignoredFs = FileSystem.get(conf)\n+      conf.getOutputFormat.checkOutputSpecs(ignoredFs, conf)\n+    }\n+\n     val writer = new SparkHadoopWriter(conf)\n     writer.preSetup()\n \n\n@@ -24,9 +24,11 @@ import scala.io.Source\n import com.google.common.io.Files\n import org.apache.hadoop.io._\n import org.apache.hadoop.io.compress.DefaultCodec\n+import org.apache.hadoop.mapred.FileAlreadyExistsException\n import org.scalatest.FunSuite\n \n import org.apache.spark.SparkContext._\n+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\n \n class FileSuite extends FunSuite with LocalSparkContext {\n \n@@ -208,4 +210,44 @@ class FileSuite extends FunSuite with LocalSparkContext {\n     assert(rdd.count() === 3)\n     assert(rdd.count() === 3)\n   }\n+\n+  test (\"prevent user from overwriting the empty directory (old Hadoop API)\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val tempdir = Files.createTempDir()\n+    val randomRDD = sc.parallelize(Array((1, \"a\"), (1, \"a\"), (2, \"b\"), (3, \"c\")), 1)\n+    intercept[FileAlreadyExistsException] {\n+      randomRDD.saveAsTextFile(tempdir.getPath)\n+    }\n+  }\n+\n+  test (\"prevent user from overwriting the non-empty directory (old Hadoop API)\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val tempdir = Files.createTempDir()\n+    val randomRDD = sc.parallelize(Array((1, \"a\"), (1, \"a\"), (2, \"b\"), (3, \"c\")), 1)\n+    randomRDD.saveAsTextFile(tempdir.getPath + \"/output\")\n+    assert(new File(tempdir.getPath + \"/output/part-00000\").exists() === true)\n+    intercept[FileAlreadyExistsException] {\n+      randomRDD.saveAsTextFile(tempdir.getPath + \"/output\")\n+    }\n+  }\n+\n+  test (\"prevent user from overwriting the empty directory (new Hadoop API)\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val tempdir = Files.createTempDir()\n+    val randomRDD = sc.parallelize(Array((\"key1\", \"a\"), (\"key2\", \"a\"), (\"key3\", \"b\"), (\"key4\", \"c\")), 1)\n+    intercept[FileAlreadyExistsException] {\n+      randomRDD.saveAsNewAPIHadoopFile[TextOutputFormat[String, String]](tempdir.getPath)\n+    }\n+  }\n+\n+  test (\"prevent user from overwriting the non-empty directory (new Hadoop API)\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val tempdir = Files.createTempDir()\n+    val randomRDD = sc.parallelize(Array((\"key1\", \"a\"), (\"key2\", \"a\"), (\"key3\", \"b\"), (\"key4\", \"c\")), 1)\n+    randomRDD.saveAsTextFile(tempdir.getPath + \"/output\")\n+    assert(new File(tempdir.getPath + \"/output/part-00000\").exists() === true)\n+    intercept[FileAlreadyExistsException] {\n+      randomRDD.saveAsNewAPIHadoopFile[TextOutputFormat[String, String]](tempdir.getPath)\n+    }\n+  }\n }\n"}
{"id": 157, "ground_truth": "Unordered Data", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nhttps://github.com/emqx/emqx/actions/runs/17472259934/job/49624282534?pr=15846#step:4:381\r\n\n\n### Code Patch:\n@@ -496,7 +496,7 @@ t_device_id(TCConfig) ->\n         200,\n         10,\n         ?assertMatch(\n-            {200, #{<<\"values\">> := [[33]]}},\n+            {200, #{<<\"values\">> := [_ | _]}},\n             scan_table(DeviceId, TCConfig)\n         )\n     ),\n"}
{"id": 347, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nEven with Kahan summation, we were occasionally getting floating point differences at the 14th decimal point, well beyond anything a GIS use case would care about.\r\n\r\nFixes #112461\r\nFixes #112463\n\n### Code Patch:\n@@ -94,12 +94,6 @@ tests:\n - class: org.elasticsearch.smoketest.DocsClientYamlTestSuiteIT\n   method: test {yaml=reference/rest-api/watcher/put-watch/line_120}\n   issue: https://github.com/elastic/elasticsearch/issues/99517\n-- class: org.elasticsearch.xpack.esql.expression.function.aggregate.SpatialCentroidTests\n-  method: \"testAggregateIntermediate {TestCase=<geo_point> #2}\"\n-  issue: https://github.com/elastic/elasticsearch/issues/112461\n-- class: org.elasticsearch.xpack.esql.expression.function.aggregate.SpatialCentroidTests\n-  method: testAggregateIntermediate {TestCase=<geo_point>}\n-  issue: https://github.com/elastic/elasticsearch/issues/112463\n - class: org.elasticsearch.xpack.esql.action.ManyShardsIT\n   method: testRejection\n   issue: https://github.com/elastic/elasticsearch/issues/112406\n\n@@ -22,13 +22,15 @@\n import org.elasticsearch.xpack.esql.expression.function.FunctionName;\n import org.elasticsearch.xpack.esql.expression.function.MultiRowTestCaseSupplier;\n import org.elasticsearch.xpack.esql.expression.function.TestCaseSupplier;\n+import org.hamcrest.BaseMatcher;\n+import org.hamcrest.Description;\n+import org.hamcrest.Matcher;\n \n-import java.nio.ByteOrder;\n import java.util.List;\n import java.util.function.Supplier;\n import java.util.stream.Stream;\n \n-import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.closeTo;\n \n @FunctionName(\"st_centroid_agg\")\n public class SpatialCentroidTests extends AbstractAggregationTestCase {\n@@ -74,16 +76,58 @@ private static TestCaseSupplier makeSupplier(TestCaseSupplier.TypedDataSupplier\n                 count++;\n             }\n \n-            var expected = new BytesRef(\n-                WellKnownBinary.toWKB(new Point(xSum.value() / count, ySum.value() / count), ByteOrder.LITTLE_ENDIAN)\n-            );\n+            var expectedX = xSum.value() / count;\n+            var expectedY = ySum.value() / count;\n \n             return new TestCaseSupplier.TestCase(\n                 List.of(fieldTypedData),\n                 \"SpatialCentroid[field=Attribute[channel=0]]\",\n                 fieldTypedData.type(),\n-                equalTo(expected)\n+                centroidMatches(expectedX, expectedY, 1e-14)\n             );\n         });\n     }\n+\n+    @SuppressWarnings(\"SameParameterValue\")\n+    private static Matcher<BytesRef> centroidMatches(double x, double y, double error) {\n+        return new TestCentroidMatcher(x, y, error);\n+    }\n+\n+    private static class TestCentroidMatcher extends BaseMatcher<BytesRef> {\n+        private final double x;\n+        private final double y;\n+        private final Matcher<Double> mx;\n+        private final Matcher<Double> my;\n+\n+        private TestCentroidMatcher(double x, double y, double error) {\n+            this.x = x;\n+            this.y = y;\n+            this.mx = closeTo(x, error);\n+            this.my = closeTo(y, error);\n+        }\n+\n+        @Override\n+        public boolean matches(Object item) {\n+            if (item instanceof BytesRef wkb) {\n+                var point = (Point) WellKnownBinary.fromWKB(GeometryValidator.NOOP, false, wkb.bytes, wkb.offset, wkb.length);\n+                return mx.matches(point.getX()) && my.matches(point.getY());\n+            }\n+            return false;\n+        }\n+\n+        @Override\n+        public void describeMismatch(Object item, Description description) {\n+            if (item instanceof BytesRef wkb) {\n+                var point = (Point) WellKnownBinary.fromWKB(GeometryValidator.NOOP, false, wkb.bytes, wkb.offset, wkb.length);\n+                description.appendText(\"was \").appendValue(point);\n+            } else {\n+                description.appendText(\"was \").appendValue(item);\n+            }\n+        }\n+\n+        @Override\n+        public void describeTo(Description description) {\n+            description.appendValue(\"    POINT (\" + x + \" \" + y + \")\");\n+        }\n+    }\n }\n"}
{"id": 132, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR removes an assertion from `SparkConnectServiceSuite.scala` to eliminate flakiness which affects the following tests in the test suite.\r\n\r\n* `SPARK-44776: LocalTableScanExec`\r\n* `SPARK-41224: collect data using arrow`\r\n\r\nIn those tests, [VerifyEvents#onNext](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L913) is called and checked `assert(executeHolder.eventsManager.status == ExecuteStatus.Analyzed)`, which can fail and this PR proposes to remove this assertion.\r\nIf this assertion fails, an exception will be thrown and [VerifyEvents#onError](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L921) is called but the reason this `onError` is called is not due to a query/command fails. So `executeHolder.eventsManager.hasError.isDefined` never changes to `true`.\r\n\r\n`VerifyEvents#onNext` is indirectly called from [ExecuteGrpcResponseSender#sendResponse](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/execution/ExecuteGrpcResponseSender.scala#L398), which runs on a sender thread.\r\nOn the other hand, an operation status is changed on another thread. Especially, transition to `Analyzed` status is done [here](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/execution/SparkConnectPlanExecution.scala#L75). But the status can transition to `ReadyForExecution` or `Finished` before `VerifyEvents#onNext` is called.\r\nSo, those tests can occasionally fail.\r\n\r\nYou can easily reproduce this issue by inserting sleep like as follows and then run those tests.\r\n```\r\n   def onNext(v: proto.ExecutePlanResponse): Unit = {\r\n     if (v.hasSchema) {\r\n+      Thread.sleep(5000)\r\n       assert(executeHolder.eventsManager.status == ExecuteStatus.Analyzed)\r\n     }\r\n```\r\n\r\nThe solution this PR proposes is just removing the assertion.\r\nIn `VerifyEvents#onNext`, there is another assertion `assert(executeHolder.eventsManager.status == ExecuteStatus.Finished)` and I think having only this assertion is enough because the status changes to `Finished` through `Analyzed`.\r\n\r\n\r\n### Why are the changes needed?\r\nFor test stability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nInserting sleep into `onNext` like explained above and run the problematic tests.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\r\n\n\n### Code Patch:\n@@ -911,9 +911,6 @@ class SparkConnectServiceSuite\n       }\n     }\n     def onNext(v: proto.ExecutePlanResponse): Unit = {\n-      if (v.hasSchema) {\n-        assert(executeHolder.eventsManager.status == ExecuteStatus.Analyzed)\n-      }\n       if (v.hasMetrics) {\n         assert(executeHolder.eventsManager.status == ExecuteStatus.Finished)\n       }\n"}
{"id": 386, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Body\n\nThe test_listener_logs_failed_serialization is flaky - likely due to a race condition. This should be improve.\r\n\r\nExample failure: \r\n\r\nhttps://github.com/apache/airflow/actions/runs/10714765649/job/29709338209?pr=41331#step:7:5256\r\n\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n___________________ test_listener_logs_failed_serialization ____________________\r\n\r\nself = <MagicMock name='mock.warning' id='139678763968448'>\r\n\r\n    def assert_called_once(self):\r\n        \"\"\"assert that the mock was called only once.\r\n        \"\"\"\r\n        if not self.call_count == 1:\r\n            msg = (\"Expected '%s' to have been called once. Called %s times.%s\"\r\n                   % (self._mock_name or 'mock',\r\n                      self.call_count,\r\n                      self._calls_repr()))\r\n>           raise AssertionError(msg)\r\nE           AssertionError: Expected 'warning' to have been called once. Called 0 times.\r\n\r\n/usr/local/lib/python3.8/unittest/mock.py:892: AssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test_listener_logs_failed_serialization():\r\n        listener = OpenLineageListener()\r\n        listener.log = MagicMock()\r\n        listener.adapter = OpenLineageAdapter(\r\n            client=OpenLineageClient(transport=ConsoleTransport(config=ConsoleConfig()))\r\n        )\r\n        event_time = dt.datetime.now()\r\n    \r\n        fut = listener.submit_callable(\r\n            listener.adapter.dag_failed,\r\n            dag_id=\"\",\r\n            run_id=\"\",\r\n            end_date=event_time,\r\n            execution_date=threading.Thread(),\r\n            dag_run_state=DagRunState.FAILED,\r\n            task_ids=[\"task_id\"],\r\n            msg=\"\",\r\n        )\r\n        assert fut.exception(10)\r\n>       listener.log.warning.assert_called_once()\r\nE       AssertionError: Expected 'warning' to have been called once. Called 0 times.\r\n\r\ntests/providers/openlineage/plugins/test_listener.py:628: AssertionError\r\n----------------------------- Captured stdout call -----------------------------\r\n```\n\n### Committer\n\n- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n\n### Code Patch:\n@@ -17,8 +17,8 @@\n from __future__ import annotations\n \n import datetime as dt\n-import threading\n import uuid\n+from concurrent.futures import Future\n from contextlib import suppress\n from typing import Callable\n from unittest import mock\n@@ -606,26 +606,33 @@ def test_listener_on_dag_run_state_changes_configure_process_pool_size(mock_exec\n     mock_executor.return_value.submit.assert_called_once()\n \n \n-@pytest.mark.flaky(reruns=5)\n def test_listener_logs_failed_serialization():\n     listener = OpenLineageListener()\n+    callback_future = Future()\n+\n+    def set_result(*args, **kwargs):\n+        callback_future.set_result(True)\n+\n     listener.log = MagicMock()\n+    listener.log.warning = MagicMock(side_effect=set_result)\n     listener.adapter = OpenLineageAdapter(\n         client=OpenLineageClient(transport=ConsoleTransport(config=ConsoleConfig()))\n     )\n     event_time = dt.datetime.now()\n-\n     fut = listener.submit_callable(\n         listener.adapter.dag_failed,\n         dag_id=\"\",\n         run_id=\"\",\n         end_date=event_time,\n-        execution_date=threading.Thread(),\n+        execution_date=callback_future,\n         dag_run_state=DagRunState.FAILED,\n         task_ids=[\"task_id\"],\n         msg=\"\",\n     )\n     assert fut.exception(10)\n+    callback_future.result(10)\n+    assert callback_future.done()\n+    listener.log.debug.assert_not_called()\n     listener.log.warning.assert_called_once()\n \n \n"}
{"id": 340, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nI am aware of a case where the same set of plugins would lead to one controller starting successfully and one controller failing to start, depending on the order in which the plugins were listed. We currently use file system iteration order, which is undefined. This PR sorts the list by filename before passing the result to the topological sorting function, so every controller with the same set of plugins will now load them in the same order. This should make it easier to debug such problems, because they will either always occur, or they will never occur ‚Äî but at least the behavior will be deterministic.\r\n\r\n### Testing done\r\n\r\nI added a log statement to print the list of active plugins as consumed by the `UberClassLoader` and verified that at each level of the topological sort the entries were in lexicographical order.\r\n\r\n### Proposed changelog entries\r\n\r\nList plugins in deterministic order to improve diagnosability of plugin linkage errors.\r\n\r\n### Proposed upgrade guidelines\r\n\r\nN/A\r\n\r\n```[tasklist]\r\n### Submitter checklist\r\n- [ ] The Jira issue, if it exists, is well-described.\r\n- [ ] The changelog entries and upgrade guidelines are appropriate for the audience affected by the change (users or developers, depending on the change) and are in the imperative mood (see [examples](https://github.com/jenkins-infra/jenkins.io/blob/master/content/_data/changelogs/weekly.yml)). Fill in the **Proposed upgrade guidelines** section only if there are breaking changes or changes that may require extra steps from users during upgrade.\r\n- [ ] There is automated testing or an explanation as to why this change has no tests.\r\n- [ ] New public classes, fields, and methods are annotated with `@Restricted` or have `@since TODO` Javadocs, as appropriate.\r\n- [ ] New deprecations are annotated with `@Deprecated(since = \"TODO\")` or `@Deprecated(forRemoval = true, since = \"TODO\")`, if applicable.\r\n- [ ] New or substantially changed JavaScript is not defined inline and does not call `eval` to ease future introduction of Content Security Policy (CSP) directives (see [documentation](https://www.jenkins.io/doc/developer/security/csp/)).\r\n- [ ] For dependency updates, there are links to external changelogs and, if possible, full differentials.\r\n- [ ] For new APIs and extension points, there is a link to at least one consumer.\r\n```\r\n\r\n### Desired reviewers\r\n\r\n@mention\r\n\r\n<!-- Comment:\r\nIf you need an accelerated review process by the community (e.g., for critical bugs), mention @jenkinsci/core-pr-reviewers.\r\n-->\r\n\r\nBefore the changes are marked as `ready-for-merge`:\r\n\r\n```[tasklist]\r\n### Maintainer checklist\r\n- [ ] There are at least two (2) approvals for the pull request and no outstanding requests for change.\r\n- [ ] Conversations in the pull request are over, or it is explicit that a reviewer is not blocking the change.\r\n- [ ] Changelog entries in the pull request title and/or **Proposed changelog entries** are accurate, human-readable, and in the imperative mood.\r\n- [ ] Proper changelog labels are set so that the changelog can be generated automatically.\r\n- [ ] If the change needs additional upgrade steps from users, the `upgrade-guide-needed` label is set and there is a **Proposed upgrade guidelines** section in the pull request title (see [example](https://github.com/jenkinsci/jenkins/pull/4387)).\r\n- [ ] If it would make sense to backport the change to LTS, a Jira issue must exist, be a _Bug_ or _Improvement_, and be labeled as `lts-candidate` to be considered (see [query](https://issues.jenkins.io/issues/?filter=12146)).\r\n```\r\n\n\n<a href=\"https://gitpod.io/#https://github.com/jenkinsci/jenkins/pull/8453\"><img src=\"https://gitpod.io/button/open-in-gitpod.svg\"/></a>\n\n\n\n### Code Patch:\n@@ -9,6 +9,7 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Iterator;\n import java.util.List;\n import java.util.ServiceLoader;\n@@ -64,7 +65,11 @@ private void listPluginFiles(PluginManager pm, String extension, Collection<File\n         if (files == null)\n             throw new IOException(\"Jenkins is unable to create \" + pm.rootDir + \"\\nPerhaps its security privilege is insufficient\");\n \n-        all.addAll(Arrays.asList(files));\n+        List<File> pluginFiles = new ArrayList<>();\n+        pluginFiles.addAll(List.of(files));\n+        pluginFiles.sort(Comparator.comparing(File::getName));\n+\n+        all.addAll(pluginFiles);\n     }\n \n     /**\n@@ -76,15 +81,16 @@ private void listPluginFiles(PluginManager pm, String extension, Collection<File\n     protected void getBundledPluginsFromProperty(final List<File> r) {\n         String hplProperty = SystemProperties.getString(\"hudson.bundled.plugins\");\n         if (hplProperty != null) {\n+            List<File> pluginFiles = new ArrayList<>();\n             for (String hplLocation : hplProperty.split(\",\")) {\n                 File hpl = new File(hplLocation.trim());\n                 if (hpl.exists()) {\n-                    r.add(hpl);\n+                    pluginFiles.add(hpl);\n                 } else if (hpl.getName().contains(\"*\")) {\n                     try {\n                         new DirScanner.Glob(hpl.getName(), null).scan(hpl.getParentFile(), new FileVisitor() {\n                             @Override public void visit(File f, String relativePath) throws IOException {\n-                                r.add(f);\n+                                pluginFiles.add(f);\n                             }\n                         });\n                     } catch (IOException x) {\n@@ -94,6 +100,8 @@ protected void getBundledPluginsFromProperty(final List<File> r) {\n                     LOGGER.warning(\"bundled plugin \" + hplLocation + \" does not exist\");\n                 }\n             }\n+            pluginFiles.sort(Comparator.comparing(File::getName));\n+            r.addAll(pluginFiles);\n         }\n     }\n \n"}
{"id": 239, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #14294\r\n\r\nI ran the test with 1 thread and saw it consistently yielded 5 files, so this should be the proper lower bound.\n\n### Code Patch:\n@@ -135,7 +135,6 @@ SELECT count(*) > 1 FROM glob('__TEST_DIR__/file_size_bytes_csv5/*.csv')\n true\n \n # each thread sees ~240kb if it's balanced, what about a 190kb limit\n-# even in the case of extreme thread imbalance, this always yield around 8 files\n statement ok\n COPY (FROM bigdata) TO '__TEST_DIR__/file_size_bytes_csv6' (FORMAT CSV, FILE_SIZE_BYTES '190kb', PER_THREAD_OUTPUT TRUE);\n \n@@ -144,9 +143,9 @@ SELECT COUNT(*) FROM read_csv_auto('__TEST_DIR__/file_size_bytes_csv6/*.csv')\n ----\n 100000\n \n-# ~2 files per thread, around 8 in total\n+# ~2 files per thread, around 8 in total (5 output files in case of extreme thread imbalance and only 1 thread runs)\n query I\n-SELECT count(*) BETWEEN 6 AND 10 FROM glob('__TEST_DIR__/file_size_bytes_csv6/*.csv')\n+SELECT count(*) BETWEEN 5 AND 10 FROM glob('__TEST_DIR__/file_size_bytes_csv6/*.csv')\n ----\n 1\n \n"}
{"id": 27, "ground_truth": "Async wait", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n@StephanEwen, @aljoscha: Can someone please confirm that this is the place to disable the POJO types?\n\nThere are various issues when working with POJOs in the Java API, for example [1].\n\n[1] https://mail-archives.apache.org/mod_mbox/incubator-flink-dev/201407.mbox/%3C53D96049.1060509%40cse.uta.edu%3E\n\n\n### Code Patch:\n@@ -19,61 +19,59 @@\n \n package org.apache.flink.api.java.io;\n \n-import org.junit.Assert;\n-import org.junit.Test;\n import org.apache.flink.api.common.io.InputFormat;\n import org.apache.flink.api.java.DataSet;\n import org.apache.flink.api.java.ExecutionEnvironment;\n-import org.apache.flink.api.java.io.AvroInputFormat;\n-import org.apache.flink.api.java.typeutils.PojoTypeInfo;\n+import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n import org.apache.flink.api.java.typeutils.TypeExtractor;\n import org.apache.flink.core.fs.Path;\n import org.apache.flink.types.TypeInformation;\n+import org.junit.Assert;\n+import org.junit.Test;\n \n public class AvroInputFormatTypeExtractionTest {\n \n \t@Test\n \tpublic void testTypeExtraction() {\n \t\ttry {\n \t\t\tInputFormat<MyAvroType, ?> format = new AvroInputFormat<MyAvroType>(new Path(\"file:///ignore/this/file\"), MyAvroType.class);\n-\t\t\t\n+\n \t\t\tTypeInformation<?> typeInfoDirect = TypeExtractor.getInputFormatTypes(format);\n-\t\t\t\n+\n \t\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n \t\t\tDataSet<MyAvroType> input = env.createInput(format);\n \t\t\tTypeInformation<?> typeInfoDataSet = input.getType();\n-\t\t\t\n-\t\t\t\n-\t\t\tAssert.assertTrue(typeInfoDirect instanceof PojoTypeInfo);\n-\t\t\tAssert.assertTrue(typeInfoDataSet instanceof PojoTypeInfo);\n-\t\t\t\n+\n+\n+\t\t\tAssert.assertTrue(typeInfoDirect instanceof GenericTypeInfo);\n+\t\t\tAssert.assertTrue(typeInfoDataSet instanceof GenericTypeInfo);\n+\n \t\t\tAssert.assertEquals(MyAvroType.class, typeInfoDirect.getTypeClass());\n \t\t\tAssert.assertEquals(MyAvroType.class, typeInfoDataSet.getTypeClass());\n-\t\t}\n-\t\tcatch (Exception e) {\n+\t\t} catch (Exception e) {\n \t\t\te.printStackTrace();\n \t\t\tAssert.fail(e.getMessage());\n \t\t}\n \t}\n-\t\n+\n \tpublic static final class MyAvroType {\n-\t\t\n+\n \t\tpublic String theString;\n-\t\t\n+\n \t\tprivate double aDouble;\n-\t\t\n+\n \t\tpublic double getaDouble() {\n \t\t\treturn aDouble;\n \t\t}\n-\t\t\n+\n \t\tpublic void setaDouble(double aDouble) {\n \t\t\tthis.aDouble = aDouble;\n \t\t}\n-\t\t\n+\n \t\tpublic void setTheString(String theString) {\n \t\t\tthis.theString = theString;\n \t\t}\n-\t\t\n+\n \t\tpublic String getTheString() {\n \t\t\treturn theString;\n \t\t}\n\n@@ -291,7 +291,10 @@ under the License.\n \t\t\t\t\t\t</configuration>\n \t\t\t\t\t</execution>\n \n-\t\t\t\t\t<!-- WordCountPOJO -->\n+\t\t\t\t\t<!-- WordCountPOJO\n+\n+\t\t\t\t\tNote: disabled for now, because of problems with POJO types\n+\n \t\t\t\t\t<execution>\n \t\t\t\t\t\t<id>WordCountPOJO</id>\n \t\t\t\t\t\t<phase>package</phase>\n@@ -313,8 +316,7 @@ under the License.\n \t\t\t\t\t\t\t\t<include>**/java/wordcount/util/WordCountData.class</include>\n \t\t\t\t\t\t\t</includes>\n \t\t\t\t\t\t</configuration>\n-\t\t\t\t\t</execution>\n-\t\t\t\t\t\n+\t\t\t\t\t</execution> -->\n \t\t\t\t</executions>\n \t\t\t</plugin>\n \t\t</plugins>\n\n@@ -53,6 +53,15 @@ public class WordCountPOJO {\n \t// *************************************************************************\n \n \tpublic static void main(String[] args) throws Exception {\n+\t\t// ====================================================================\n+\t\t// IMPORTANT\n+\t\t//\n+\t\t// Note: this example is currently not working, because support for\n+\t\t// POJO types has been disabled. As soon as all known issues (see [1])\n+\t\t// are fixed, we will enable POJO support again.\n+\t\t//\n+\t\t// [1] https://mail-archives.apache.org/mod_mbox/incubator-flink-dev/201407.mbox/%3C53D96049.1060509%40cse.uta.edu%3E\n+\t\t// ====================================================================\n \n \t\tparseParameters(args);\n \n\n@@ -792,11 +792,13 @@ private <X> TypeInformation<X> privateGetForClass(Class<X> clazz) {\n \t\t\t// special case handling for Class, this should not be handled by the POJO logic\n \t\t\treturn new GenericTypeInfo<X>(clazz);\n \t\t}\n-\t\tTypeInformation<X> pojoType =  analyzePojo(clazz);\n-\t\tif (pojoType != null) {\n-\t\t\treturn pojoType;\n-\t\t}\n \n+//\t\tDisable POJO types for now (see https://mail-archives.apache.org/mod_mbox/incubator-flink-dev/201407.mbox/%3C53D96049.1060509%40cse.uta.edu%3E)\n+//\n+//\t\tTypeInformation<X> pojoType =  analyzePojo(clazz);\n+//\t\tif (pojoType != null) {\n+//\t\t\treturn pojoType;\n+//\t\t}\n \n \t\t// return a generic type\n \t\treturn new GenericTypeInfo<X>(clazz);\n\n@@ -30,6 +30,7 @@\n import org.apache.flink.api.java.typeutils.BasicTypeInfo;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n import org.junit.BeforeClass;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import org.apache.flink.api.java.DataSet;\n@@ -128,6 +129,7 @@ public void testCoGroupKeyFields6() {\n \t\tds1.coGroup(ds2).where(5).equalTo(0);\n \t}\n \n+\t@Ignore\n \t@Test\n \tpublic void testCoGroupKeyExpressions1() {\n \n@@ -143,6 +145,7 @@ public void testCoGroupKeyExpressions1() {\n \t\t}\n \t}\n \n+\t@Ignore\n \t@Test(expected = InvalidProgramException.class)\n \tpublic void testCoGroupKeyExpressions2() {\n \n@@ -154,6 +157,7 @@ public void testCoGroupKeyExpressions2() {\n \t\tds1.coGroup(ds2).where(\"myInt\").equalTo(\"myString\");\n \t}\n \n+\t@Ignore\n \t@Test(expected = InvalidProgramException.class)\n \tpublic void testCoGroupKeyExpressions3() {\n \n@@ -165,6 +169,7 @@ public void testCoGroupKeyExpressions3() {\n \t\tds1.coGroup(ds2).where(\"myInt\", \"myString\").equalTo(\"myString\");\n \t}\n \n+\t@Ignore\n \t@Test(expected = IllegalArgumentException.class)\n \tpublic void testCoGroupKeyExpressions4() {\n \n@@ -175,7 +180,7 @@ public void testCoGroupKeyExpressions4() {\n \t\t// should not work, cogroup key non-existent\n \t\tds1.coGroup(ds2).where(\"myNonExistent\").equalTo(\"myInt\");\n \t}\n-\t\n+\n \t@Test\n \tpublic void testCoGroupKeySelectors1() {\n \t\t\n\n@@ -30,6 +30,7 @@\n import org.apache.flink.api.java.tuple.Tuple5;\n import org.apache.flink.api.java.typeutils.BasicTypeInfo;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import org.apache.flink.api.java.DataSet;\n@@ -112,6 +113,7 @@ public void testGroupByKeyFields5() {\n \t\ttupleDs.groupBy(-1);\n \t}\n \n+\t@Ignore\n \t@Test\n \tpublic void testGroupByKeyExpressions1() {\n \n@@ -129,6 +131,7 @@ public void testGroupByKeyExpressions1() {\n \t\t}\n \t}\n \n+\t@Ignore\n \t@Test(expected = UnsupportedOperationException.class)\n \tpublic void testGroupByKeyExpressions2() {\n \n@@ -139,6 +142,7 @@ public void testGroupByKeyExpressions2() {\n \t\tlongDs.groupBy(\"myInt\");\n \t}\n \n+\t@Ignore\n \t@Test(expected = InvalidProgramException.class)\n \tpublic void testGroupByKeyExpressions3() {\n \n@@ -152,6 +156,7 @@ public void testGroupByKeyExpressions3() {\n \n \t}\n \n+\t@Ignore\n \t@Test(expected = IllegalArgumentException.class)\n \tpublic void testGroupByKeyExpressions4() {\n \n\n@@ -30,6 +30,7 @@\n import org.apache.flink.api.java.typeutils.BasicTypeInfo;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n import org.junit.BeforeClass;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import org.apache.flink.api.java.DataSet;\n@@ -128,6 +129,7 @@ public void testJoinKeyFields6() {\n \t\tds1.join(ds2).where(5).equalTo(0);\n \t}\n \n+\t@Ignore\n \t@Test\n \tpublic void testJoinKeyExpressions1() {\n \n@@ -143,6 +145,7 @@ public void testJoinKeyExpressions1() {\n \t\t}\n \t}\n \n+\t@Ignore\n \t@Test(expected = InvalidProgramException.class)\n \tpublic void testJoinKeyExpressions2() {\n \n@@ -154,6 +157,7 @@ public void testJoinKeyExpressions2() {\n \t\tds1.join(ds2).where(\"myInt\").equalTo(\"myString\");\n \t}\n \n+\t@Ignore\n \t@Test(expected = InvalidProgramException.class)\n \tpublic void testJoinKeyExpressions3() {\n \n@@ -165,6 +169,7 @@ public void testJoinKeyExpressions3() {\n \t\tds1.join(ds2).where(\"myInt\", \"myString\").equalTo(\"myString\");\n \t}\n \n+\t@Ignore\n \t@Test(expected = IllegalArgumentException.class)\n \tpublic void testJoinKeyExpressions4() {\n \n\n@@ -24,6 +24,7 @@\n import org.apache.flink.api.java.typeutils.PojoTypeInfo;\n import org.apache.flink.api.java.typeutils.TypeExtractor;\n import org.apache.flink.types.TypeInformation;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n @SuppressWarnings(\"unused\")\n@@ -51,6 +52,7 @@ static class SimplePojo {\n \t\tInteger[] intWrapperArray;\n \t}\n \n+\t@Ignore\n \t@Test\n \tpublic void testSimplePojoTypeExtraction() {\n \t\tTypeInformation<SimplePojo> type = TypeExtractor.getForClass(SimplePojo.class);\n@@ -66,6 +68,7 @@ static class NestedPojoOuter {\n \t\tNestedPojoInner inner;\n \t}\n \n+\t@Ignore\n \t@Test\n \tpublic void testNestedPojoTypeExtraction() {\n \t\tTypeInformation<NestedPojoOuter> type = TypeExtractor.getForClass(NestedPojoOuter.class);\n@@ -82,14 +85,16 @@ static class Recursive2Pojo {\n \t\tRecursive1Pojo rec;\n \t}\n \n+\t@Ignore\n \t@Test\n \tpublic void testRecursivePojoTypeExtraction() {\n \t\t// This one tests whether a recursive pojo is detected using the set of visited\n \t\t// types in the type extractor. The recursive field will be handled using the generic serializer.\n \t\tTypeInformation<Recursive1Pojo> type = TypeExtractor.getForClass(Recursive1Pojo.class);\n \t\tassertTrue(\"Extracted type is not a Pojo type but should be.\", type instanceof PojoTypeInfo);\n \t}\n-\t\n+\n+\t@Ignore\n \t@Test\n \tpublic void testRecursivePojoObjectTypeExtraction() {\n \t\tTypeInformation<Recursive1Pojo> type = TypeExtractor.getForObject(new Recursive1Pojo());\n\n@@ -41,7 +41,6 @@\n import org.apache.flink.api.java.typeutils.BasicTypeInfo;\n import org.apache.flink.api.java.typeutils.GenericTypeInfo;\n import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n-import org.apache.flink.api.java.typeutils.PojoTypeInfo;\n import org.apache.flink.api.java.typeutils.PrimitiveArrayTypeInfo;\n import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n@@ -62,7 +61,6 @@\n \n public class TypeExtractorTest {\n \n-\t\n \t@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n \t@Test\n \tpublic void testBasicType() {\n@@ -307,11 +305,11 @@ public CustomType cross(CustomType first, Integer second) throws Exception {\n \n \t\tAssert.assertFalse(ti.isBasicType());\n \t\tAssert.assertFalse(ti.isTupleType());\n-\t\tAssert.assertTrue(ti instanceof PojoTypeInfo);\n+\t\tAssert.assertTrue(ti instanceof GenericTypeInfo);\n \t\tAssert.assertEquals(ti.getTypeClass(), CustomType.class);\n \n \t\t// use getForClass()\n-\t\tAssert.assertTrue(TypeExtractor.getForClass(CustomType.class) instanceof PojoTypeInfo);\n+\t\tAssert.assertTrue(TypeExtractor.getForClass(CustomType.class) instanceof GenericTypeInfo);\n \t\tAssert.assertEquals(TypeExtractor.getForClass(CustomType.class).getTypeClass(), ti.getTypeClass());\n \n \t\t// use getForObject()\n@@ -320,7 +318,7 @@ public CustomType cross(CustomType first, Integer second) throws Exception {\n \n \t\tAssert.assertFalse(ti2.isBasicType());\n \t\tAssert.assertFalse(ti2.isTupleType());\n-\t\tAssert.assertTrue(ti2 instanceof PojoTypeInfo);\n+\t\tAssert.assertTrue(ti2 instanceof GenericTypeInfo);\n \t\tAssert.assertEquals(ti2.getTypeClass(), CustomType.class);\n \t}\n \n@@ -360,7 +358,7 @@ public Tuple2<Long, CustomType> map(Tuple2<Long, CustomType> value) throws Excep\n \t\tAssert.assertEquals(Tuple2.class, tti.getTypeClass());\n \t\t\n \t\tAssert.assertEquals(Long.class, tti.getTypeAt(0).getTypeClass());\n-\t\tAssert.assertTrue(tti.getTypeAt(1) instanceof PojoTypeInfo);\n+\t\tAssert.assertTrue(tti.getTypeAt(1) instanceof GenericTypeInfo);\n \t\tAssert.assertEquals(CustomType.class, tti.getTypeAt(1).getTypeClass());\n \n \t\t// use getForObject()\n@@ -373,7 +371,7 @@ public Tuple2<Long, CustomType> map(Tuple2<Long, CustomType> value) throws Excep\n \t\t\n \t\tAssert.assertEquals(Tuple2.class, tti2.getTypeClass());\n \t\tAssert.assertEquals(Long.class, tti2.getTypeAt(0).getTypeClass());\n-\t\tAssert.assertTrue(tti2.getTypeAt(1) instanceof PojoTypeInfo);\n+\t\tAssert.assertTrue(tti2.getTypeAt(1) instanceof GenericTypeInfo);\n \t\tAssert.assertEquals(CustomType.class, tti2.getTypeAt(1).getTypeClass());\n \t}\n \n\n@@ -18,48 +18,48 @@\n \n package org.apache.flink.api.java.typeutils.runtime;\n \n-import java.util.Random;\n-\n+import com.google.common.base.Objects;\n import org.apache.flink.api.common.typeutils.SerializerTestBase;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.api.java.typeutils.TypeExtractor;\n-import org.apache.flink.api.java.typeutils.runtime.PojoSerializer;\n import org.apache.flink.types.TypeInformation;\n+import org.junit.Ignore;\n \n-import com.google.common.base.Objects;\n+import java.util.Random;\n \n /**\n  * A test for the {@link org.apache.flink.api.java.typeutils.runtime.PojoSerializer}.\n  */\n+@Ignore\n public class PojoSerializerTest extends SerializerTestBase<PojoSerializerTest.TestUserClass> {\n \tprivate TypeInformation<TestUserClass> type = TypeExtractor.getForClass(TestUserClass.class);\n \n \t@Override\n \tprotected TypeSerializer<TestUserClass> createSerializer() {\n \t\tTypeSerializer<TestUserClass> serializer = type.createSerializer();\n-\t\tassert(serializer instanceof PojoSerializer);\n+\t\tassert (serializer instanceof PojoSerializer);\n \t\treturn serializer;\n \t}\n-\t\n+\n \t@Override\n \tprotected int getLength() {\n \t\treturn -1;\n \t}\n-\t\n+\n \t@Override\n \tprotected Class<TestUserClass> getTypeClass() {\n \t\treturn TestUserClass.class;\n \t}\n-\t\n+\n \t@Override\n \tprotected TestUserClass[] getTestData() {\n \t\tRandom rnd = new Random(874597969123412341L);\n \n-\t\treturn new TestUserClass[] {\n-\t\t\tnew TestUserClass(rnd.nextInt(), \"foo\", rnd.nextDouble(), new int[] {1,2,3},\n-\t\t\t\t\tnew NestedTestUserClass(rnd.nextInt(), \"foo@boo\", rnd.nextDouble(), new int[] {10, 11, 12})),\n-\t\t\tnew TestUserClass(rnd.nextInt(), \"bar\", rnd.nextDouble(), new int[] {4,5,6},\n-\t\t\t\t\tnew NestedTestUserClass(rnd.nextInt(), \"bar@bas\", rnd.nextDouble(), new int[] {20, 21, 22}))\n+\t\treturn new TestUserClass[]{\n+\t\t\t\tnew TestUserClass(rnd.nextInt(), \"foo\", rnd.nextDouble(), new int[]{1, 2, 3},\n+\t\t\t\t\t\tnew NestedTestUserClass(rnd.nextInt(), \"foo@boo\", rnd.nextDouble(), new int[]{10, 11, 12})),\n+\t\t\t\tnew TestUserClass(rnd.nextInt(), \"bar\", rnd.nextDouble(), new int[]{4, 5, 6},\n+\t\t\t\t\t\tnew NestedTestUserClass(rnd.nextInt(), \"bar@bas\", rnd.nextDouble(), new int[]{20, 21, 22}))\n \t\t};\n \n \t}\n@@ -73,7 +73,8 @@ public static class TestUserClass {\n \n \t\tprivate NestedTestUserClass nestedClass;\n \n-\t\tpublic TestUserClass() {}\n+\t\tpublic TestUserClass() {\n+\t\t}\n \n \t\tpublic TestUserClass(int dumm1, String dumm2, double dumm3, int[] dumm4, NestedTestUserClass nestedClass) {\n \t\t\tthis.dumm1 = dumm1;\n@@ -124,7 +125,8 @@ public static class NestedTestUserClass {\n \t\tpublic double dumm3;\n \t\tprivate int[] dumm4;\n \n-\t\tpublic NestedTestUserClass() {}\n+\t\tpublic NestedTestUserClass() {\n+\t\t}\n \n \t\tpublic NestedTestUserClass(int dumm1, String dumm2, double dumm3, int[] dumm4) {\n \t\t\tthis.dumm1 = dumm1;\n\n@@ -21,8 +21,9 @@\n import org.apache.flink.example.java.wordcount.WordCountPOJO;\n import org.apache.flink.test.testdata.WordCountData;\n import org.apache.flink.test.util.JavaProgramTestBase;\n+import org.junit.Ignore;\n \n-\n+@Ignore\n public class WordCountPOJOITCase extends JavaProgramTestBase {\n \n \tprotected String textPath;\n\n@@ -64,7 +64,6 @@ public void testEverything() {\n \t\t\tfwClusters.close();\n \n \t\t\tString jarPath = \"target/maven-test-jar.jar\";\n-//\t\t\tString jarPath = \"/home/aljoscha/maven-test-jar.jar\";\n \n \t\t\t// run KMeans\n \t\t\tcluster.setNumTaskTracker(2);\n\n@@ -24,6 +24,7 @@\n \n import org.apache.flink.api.common.Plan;\n import org.apache.flink.api.common.Program;\n+import org.apache.flink.api.java.functions.KeySelector;\n import org.apache.flink.api.java.functions.RichMapFunction;\n import org.apache.flink.api.java.functions.RichReduceFunction;\n import org.apache.flink.api.java.tuple.Tuple2;\n@@ -79,7 +80,12 @@ public Plan getPlan(String... args) {\n \t\t\t.map(new SelectNearestCenter()).withBroadcastSet(loop, \"centroids\")\n \t\t\t// count and sum point coordinates for each centroid\n \t\t\t.map(new CountAppender())\n-\t\t\t.groupBy(\"f0\").reduce(new CentroidAccumulator())\n+\t\t\t.groupBy(new KeySelector<DummyTuple3IntPointLong, Integer>() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic Integer getKey(DummyTuple3IntPointLong value) throws Exception {\n+\t\t\t\t\treturn value.f0;\n+\t\t\t\t}\n+\t\t\t}).reduce(new CentroidAccumulator())\n \t\t\t// compute new centroids from point counts and coordinate sums\n \t\t\t.map(new CentroidAverager());\n \n"}
{"id": 337, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nRemoved white supremacist language.\n\n### Code Patch:\n@@ -131,6 +131,3 @@ Sometimes good PRs get abandoned by the author for one reason or another. If the\n \n To make this easier, we do appreciate it if folks enable the \"Allow edits from maintainers\" flag on their pull requests.\n \n-## On ideologically motivated changes\n-\n-This is a purely technical project. As such, it is not an appropriate arena to advertise your personal politics or religious beliefs. Any changes that appear ideologically motivated will be rejected.\n"}
{"id": 206, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFixes #145 -  the issue was caused by the slightly variable results returned by the GitHub API.\r\n\r\nIn all the tests in `FunctionalSpec`, we create several test fixtures with the GitHub API, each created to be used once and thrown away:\r\n\r\n* A brand-new GitHub repository, populated with whatever Git history we require\r\n* A brand new PR in the repository, which we also merge\r\n\r\nBefore continuing on to the next step, which would be having Prout scan the repo for recently-merged PRs, we perform several checks to ensure that the GitHub API confirms the status of the merged PR:\r\n\r\nhttps://github.com/guardian/prout/blob/a710e347afb94ca719e11b9c5b6b3312f1245e98/test/lib/Helpers.scala#L180-L195\r\n\r\nHowever, looking a the differing logs produced by the failure & success cases in #145, it became apparent that even though we've checked the GitHub API.\n\n### Code Patch:\n@@ -14,7 +14,7 @@ jobs:\n     runs-on: ubuntu-latest\n     steps:\n       - name: Checkout\n-        uses: actions/checkout@v3\n+        uses: actions/checkout@v5\n       - uses: guardian/setup-scala@v1\n       - name: Build and Test\n         env:\n\n@@ -16,12 +16,16 @@\n \n package controllers\n \n+import cats._\n+import cats.data._\n+import cats.syntax.all._\n+import cats.effect.IO\n import com.github.blemale.scaffeine.{LoadingCache, Scaffeine}\n import com.madgag.scalagithub.model.RepoId\n-import lib._\n+import lib.*\n import lib.actions.Parsers.parseGitHubHookJson\n import play.api.libs.json.{JsArray, JsNumber}\n-import play.api.mvc._\n+import play.api.mvc.*\n \n import scala.concurrent.Future\n \n@@ -50,7 +54,7 @@ class Api(\n \n   def xRequestId(implicit request: RequestHeader): Option[String] = request.headers.get(\"X-Request-ID\")\n \n-  def updateForRepo(repoId: RepoId): Future[Result] = {\n+  def updateForRepo(repoId: RepoId): IO[Result] = {\n     logger.debug(s\"update requested for $repoId\")\n     for {\n       acceptList <- repoAcceptListService.acceptList()\n@@ -63,8 +67,8 @@ class Api(\n     .maximumSize(500)\n     .build(scanSchedulerFactory.createFor)\n \n-  def updateFor(repoId: RepoId, acceptList: RepoAcceptList): Future[Result] = {\n-    val scanGuardF = Future { // wrapped in a future to avoid timing attacks\n+  def updateFor(repoId: RepoId, acceptList: RepoAcceptList): IO[Result] = {\n+    val scanGuardF: IO[Seq[PullRequestCheckpointsStateChangeSummary]] = IO.delay { // wrapped in a future to avoid timing attacks\n       val knownRepo = acceptList.allKnownRepos(repoId)\n       logger.info(s\"$repoId known=$knownRepo\")\n       require(knownRepo, s\"${repoId.fullName} not on known-repo whitelist\")\n@@ -88,12 +92,11 @@ class Api(\n     val mightBePrivate = !acceptList.publicRepos(repoId)\n     if (mightBePrivate) {\n       // Response must be immediate, with no private information (e.g. even acknowledging that repo exists)\n-      Future.successful(NoContent)\n+      IO.pure(NoContent)\n     } else {\n       // we can delay the response to return information about the repo config, and the updates generated\n       for {\n-        scanGuard <- scanGuardF\n-        scan <- scanGuard\n+        scan <- scanGuardF\n       } yield Ok(JsArray(scan.map(summary => JsNumber(summary.prCheckpointDetails.pr.number))))\n     }\n   }\n\n@@ -20,8 +20,10 @@ import org.apache.pekko.actor.ActorSystem\n import com.madgag.scalagithub.model.RepoId\n import lib.{Bot, RepoSnapshot}\n import play.api.Logging\n+import play.api.mvc.{Action, AnyContent}\n \n import scala.concurrent.ExecutionContext\n+import cats.effect.unsafe.implicits.global\n \n class Application(\n   repoAcceptListService: RepoAcceptListService,\n@@ -33,19 +35,20 @@ class Application(\n   bot: Bot\n ) extends AbstractAppController(cc) with Logging {\n \n-  def index = Action { implicit req =>\n+  def index: Action[AnyContent] = Action { implicit req =>\n     Ok(views.html.userPages.index())\n   }\n \n   def configDiagnostic(repoId: RepoId) = repoAuthenticated(repoId).async { implicit req =>\n-    for {\n+    (for {\n       repoFetchedByProut <- bot.github.getRepo(repoId)\n-      proutPresenceQuickCheck <- repoAcceptListService.hasProutConfigFile(repoFetchedByProut)\n-      repoSnapshot <- repoSnapshotFactory.snapshot(repoFetchedByProut.repoId)\n+      repo = repoFetchedByProut.result\n+      proutPresenceQuickCheck <- repoAcceptListService.hasProutConfigFile(repo)\n+      repoSnapshot <- repoSnapshotFactory.snapshot(repo)\n       diagnostic <- repoSnapshot.diagnostic()\n     } yield {\n       Ok(views.html.userPages.repo(proutPresenceQuickCheck, repoSnapshot, diagnostic, sentryApiClientOpt))\n-    }\n+    }).unsafeToFuture()\n   }\n \n }\n\n@@ -1,16 +1,21 @@\n package controllers\n \n+import cats._\n+import cats.data._\n+import cats.syntax.all._\n+import cats.effect.IO\n import org.apache.pekko.actor.ActorSystem\n-import com.madgag.github.Implicits._\n+import com.madgag.github.Implicits.*\n import com.madgag.scalagithub.GitHub\n import com.madgag.scalagithub.model.{Repo, RepoId}\n import com.typesafe.scalalogging.LazyLogging\n+import fs2.Chunk\n import lib.ConfigFinder.ProutConfigFileName\n \n import java.util.concurrent.atomic.AtomicReference\n import scala.concurrent.ExecutionContext.Implicits.global\n import scala.concurrent.Future\n-import scala.concurrent.duration._\n+import scala.concurrent.duration.*\n \n case class RepoAcceptList(allKnownRepos: Set[RepoId], publicRepos: Set[RepoId])\n \n@@ -19,19 +24,19 @@ class RepoAcceptListService()(implicit\n   actorSystem: ActorSystem\n ) extends LazyLogging {\n \n-  lazy val repoAcceptList = new AtomicReference[Future[RepoAcceptList]](getAllKnownRepos)\n+  lazy val repoAcceptList = new AtomicReference[IO[RepoAcceptList]](getAllKnownRepos)\n \n-  def acceptList(): Future[RepoAcceptList] = repoAcceptList.get()\n+  def acceptList(): IO[RepoAcceptList] = repoAcceptList.get()\n \n-  def hasProutConfigFile(repo: Repo): Future[Boolean] = for {\n+  def hasProutConfigFile(repo: Repo): IO[Boolean] = for {\n     treeT <- repo.trees2.getRecursively(s\"heads/${repo.default_branch}\").trying\n   } yield treeT.map(_.tree.exists(_.path.endsWith(ProutConfigFileName))).getOrElse(false)\n \n-  private def getAllKnownRepos: Future[RepoAcceptList] = for { // check this to see if it always expends quota...\n-    allRepos <- github.listReposAccessibleToTheApp.allItems()\n-    proutRepos <- Future.traverse(allRepos.flatMap(repos => repos.repositories)){ repo =>\n-      hasProutConfigFile(repo).map(hasConfig => Option.when(hasConfig)(repo))\n-    }.map(_.flatten.toSet)\n+  private def getAllKnownRepos: IO[RepoAcceptList] = for { // check this to see if it always expends quota...\n+    proutRepos <- github.listReposAccessibleToTheApp()\n+      .mapChunks(_.flatMap(repos => Chunk.from(repos.repositories)))\n+      .evalFilterAsync(4)(hasProutConfigFile)\n+      .compile.to(Set)\n   } yield RepoAcceptList(proutRepos.map(_.repoId), proutRepos.filterNot(_.`private`).map(_.repoId))\n \n \n\n@@ -2,7 +2,7 @@ package lib\n \n import com.madgag.github.apps.GitHubAppAuth\n import com.madgag.scalagithub.model.Account\n-import com.madgag.scalagithub.{GitHub, GitHubCredentials}\n+import com.madgag.scalagithub.{AccountAccess, GitHub, GitHubCredentials}\n import play.api.Logging\n \n import java.nio.file.Path\n@@ -14,10 +14,10 @@ case class Identity(login: String, html_url: String) {\n \n case class Bot(\n   workingDir: Path,\n-  gitHubCredsProvider: GitHubCredentials.Provider,\n+  accountAccess: AccountAccess,\n   identity: Identity\n ) {\n-  val github = new GitHub(gitHubCredsProvider)\n+  val github = accountAccess.gitHub\n }\n \n object Bot extends Logging {\n@@ -32,7 +32,7 @@ object Bot extends Logging {\n       installationAccess <- githubAppAuth.accessSoleInstallation()\n     } yield Bot(\n       workingDir,\n-      installationAccess.credentials,\n+      installationAccess.accountAccess(),\n       Identity(app.slug, app.html_url)\n     )\n     ).recover { case ex =>\n\n@@ -32,7 +32,7 @@ import scala.util.Try\n import scala.util.matching.Regex\n \n trait CheckpointSnapshoter {\n-  def snapshot(checkpoint: Checkpoint): Future[Iterator[AbbreviatedObjectId]]\n+  def snapshot(checkpoint: Checkpoint): IO[Iterator[AbbreviatedObjectId]]\n }\n \n object CheckpointSnapshoter extends CheckpointSnapshoter {\n@@ -44,7 +44,7 @@ object CheckpointSnapshoter extends CheckpointSnapshoter {\n \n   val hexRegex: Regex = \"\"\"\\b\\p{XDigit}{40}\\b\"\"\".r\n \n-  def snapshot(checkpoint: Checkpoint): Future[Iterator[AbbreviatedObjectId]] = {\n+  def snapshot(checkpoint: Checkpoint): IO[Iterator[AbbreviatedObjectId]] = {\n \n     val clientForCheckpoint = if (checkpoint.sslVerification) client else insecureClient\n \n\n@@ -1,11 +1,15 @@\n package lib\n \n-import org.apache.pekko.stream.Materializer\n-import com.madgag.git._\n+import cats.*\n+import cats.data.*\n+import cats.effect.IO\n+import cats.syntax.all.*\n+import com.madgag.git.*\n import com.madgag.scalagithub.GitHub\n import com.madgag.scalagithub.model.RepoId\n import lib.sentry.SentryApiClient\n import org.apache.pekko.actor.ActorSystem\n+import org.apache.pekko.stream.Materializer\n import play.api.Logger\n \n import scala.concurrent.ExecutionContext.Implicits.global\n@@ -36,9 +40,10 @@ class Droid(\n     }\n   }\n \n-  def processMergedPullRequestsOn(repoSnapshot: RepoSnapshot): Future[Seq[PullRequestCheckpointsStateChangeSummary]] = for {\n+  def processMergedPullRequestsOn(repoSnapshot: RepoSnapshot): IO[Seq[PullRequestCheckpointsStateChangeSummary]] = for {\n     _ <- repoUpdater.attemptToCreateMissingLabels(repoSnapshot.repoLevelDetails)\n-    summaryOpts <- Future.traverse(repoSnapshot.mergedPullRequestSnapshots)(prSnapshot => prUpdater.process(prSnapshot, repoSnapshot))\n+    summaryOpts <-\n+      repoSnapshot.mergedPullRequestSnapshots.parUnorderedTraverse(prSnapshot => prUpdater.process(prSnapshot, repoSnapshot))\n   } yield summaryOpts.flatten\n \n \n\n@@ -12,7 +12,8 @@ object EverythingYouWantToKnowAboutACheckpoint extends Logging {\n     val timeBetweenMergeAndSnapshot = java.time.Duration.between(pr.merged_at.get.toInstant, snapshot.time)\n \n     def prCommitsSeenAndNotSeen(siteCommitId: ObjectId): PRCommitVisibility = {\n-      implicit val repoThreadLocal = gitRepo.getObjectDatabase.threadLocalResources\n+      implicit val repoThreadLocal: ThreadLocalObjectDatabaseResources =\n+        gitRepo.getObjectDatabase.threadLocalResources\n       implicit val w: RevWalk = new RevWalk(repoThreadLocal.reader())\n       val siteCommit = siteCommitId.asRevCommit\n \n\n@@ -1,15 +1,13 @@\n package lib\n \n-import org.apache.pekko.stream.Materializer\n import com.madgag.scalagithub.GitHub\n import com.madgag.scalagithub.commands.CreateComment\n-import com.madgag.scalagithub.model.{PullRequest, Repo}\n+import com.madgag.scalagithub.model.PullRequest\n import com.madgag.time.Implicits._\n import com.typesafe.scalalogging.LazyLogging\n import lib.Config.CheckpointMessages\n import lib.RepoSnapshot.WorthyOfCommentWindow\n import lib.Responsibility.responsibilityAndRecencyFor\n-import lib.gitgithub.LabelMapping\n import lib.labels.{Overdue, PullRequestCheckpointStatus, Seen}\n import lib.sentry.{PRSentryRelease, SentryApiClient}\n \n@@ -21,19 +19,18 @@ class PRUpdater(delayer: Delayer) extends LazyLogging {\n \n   def process(prSnapshot: PRSnapshot, repoSnapshot: RepoSnapshot)(implicit\n     g: GitHub,\n-    m: Materializer,\n     sentryApiClientOpt: Option[SentryApiClient]\n-  ): Future[Option[PullRequestCheckpointsStateChangeSummary]] = {\n+  ): IO[Option[PullRequestCheckpointsStateChangeSummary]] = {\n     logger.trace(s\"handling ${prSnapshot.pr.prId.slug}\")\n     for {\n       snapshot <- getSummaryOfCheckpointChangesGiven(prSnapshot, repoSnapshot)\n     } yield snapshot\n   }\n \n-  private def getSummaryOfCheckpointChangesGiven(prSnapshot: PRSnapshot, repoSnapshot: RepoSnapshot)(implicit\n+  private def getSummaryOfCheckpointChangesGiven(prSnapshot: PRSnapshot, repoSnapshot: RepoSnapshot)(using\n     gitHub: GitHub,\n     sentryApiClientOpt: Option[SentryApiClient]\n-  ): Future[Option[PullRequestCheckpointsStateChangeSummary]] = {\n+  ): IO[Option[PullRequestCheckpointsStateChangeSummary]] = {\n     val pr = prSnapshot.pr\n     val (oldStateLabelsSeq, userLabels) = prSnapshot.labels.map(_.name).partition(repoSnapshot.allPossibleCheckpointPRLabels)\n     val oldLabels = oldStateLabelsSeq.toSet\n\n@@ -63,7 +63,7 @@ case class PRCheckpointDetails(\n     everythingByCheckpoint.values.groupBy(_.checkpointStatus).mapV(_.map(_.snapshot.checkpoint).toSet)\n \n   val soonestPendingCheckpointOverdueTime: Option[java.time.Instant] = {\n-    implicit val periodOrdering = Ordering.by[Period, Duration](_.toStandardDuration)\n+    implicit val periodOrdering: Ordering[Period] = Ordering.by[Period, Duration](_.toStandardDuration)\n \n     checkpointsByState.get(Pending).map(_.flatMap(_.details.overdueInstantFor(pr)).min)\n   }\n\n@@ -16,26 +16,30 @@\n \n package lib\n \n-import com.madgag.git._\n-import com.madgag.github.Implicits._\n-import com.madgag.scala.collection.decorators._\n+import cats.*\n+import cats.data.*\n+import cats.effect.IO\n+import cats.syntax.all.*\n+import com.madgag.git.*\n+import com.madgag.github.Implicits.*\n+import com.madgag.scala.collection.decorators.*\n import com.madgag.scalagithub.GitHub\n-import com.madgag.scalagithub.GitHub._\n+import com.madgag.scalagithub.GitHub.*\n import com.madgag.scalagithub.model.{PullRequest, Repo, RepoId}\n-import com.madgag.time.Implicits._\n+import com.madgag.time.Implicits.*\n import io.lemonlabs.uri.Url\n import lib.Config.Checkpoint\n import lib.gitgithub.LabelMapping\n-import lib.labels._\n+import lib.labels.*\n import org.apache.pekko.actor.ActorSystem\n import org.eclipse.jgit.lib.{ObjectId, Repository}\n import org.eclipse.jgit.revwalk.{RevCommit, RevWalk}\n import play.api.Logging\n \n import java.time.ZonedDateTime\n+import scala.concurrent.*\n import scala.concurrent.ExecutionContext.Implicits.global\n-import scala.concurrent._\n-import scala.concurrent.duration._\n+import scala.concurrent.duration.*\n import scala.util.Success\n \n object RepoSnapshot {\n@@ -67,14 +71,14 @@ object RepoSnapshot {\n     }\n \n     def isMergedToMain(pr: PullRequest)(implicit repo: Repo): Boolean =\n-      pr.merged_at.isDefined && pr.base.ref == repo.default_branch\n+      pr.merged_at.isDefined\n \n     def snapshot(repoId: RepoId): Future[RepoSnapshot] = for {\n       githubRepo <- github.getRepo(repoId)\n       repoSnapshot <- snapshot(githubRepo)\n     } yield repoSnapshot\n \n-    def snapshot(implicit githubRepo: Repo): Future[RepoSnapshot] = {\n+    def snapshot(implicit githubRepo: Repo): IO[RepoSnapshot] = {\n       val mergedPullRequestsF = logAround(\"fetch PRs\")(fetchMergedPullRequests())\n       val hooksF = logAround(\"fetch repo hooks\")(fetchRepoHooks())\n       val gitRepoF = logAround(\"fetch git repo\")(fetchLatestCopyOfGitRepo())\n@@ -89,31 +93,38 @@ object RepoSnapshot {\n       )\n     }\n \n-    def prSnapshot(prNumber: Int)(implicit repo: Repo): Future[PRSnapshot] = for {\n+    def prSnapshot(prNumber: Int)(implicit repo: Repo): IO[PRSnapshot] = for {\n       prResponse <- repo.pullRequests.get(prNumber)\n       pr = prResponse.result\n-      labelsResponse <- pr.labels.list().all()\n+      labelsResponse <- pr.labels.list().compile.toList\n     } yield PRSnapshot(pr, labelsResponse)\n \n     def fetchMergedPullRequests()(implicit repo: Repo): Future[Seq[PRSnapshot]] = {\n       val now = ZonedDateTime.now()\n       val timeThresholdForScan = now.minus(WorthyOfScanWindow)\n+      val criteriaForClosedPrsBasedOnTheDefaultBranch: Map[String, String] =\n+        ClosedPRsMostlyRecentlyUpdated + (\"base\" -> repo.default_branch)\n \n-      def isNewEnoughToBeWorthScanning(pr: PullRequest) = pr.merged_at.exists(_.isAfter(timeThresholdForScan))\n+      def isMergedRecentlyEnoughToBeWorthScanning(pr: PullRequest) =\n+        pr.merged_at.exists(_.isAfter(timeThresholdForScan))\n \n       (for {\n-        litePullRequests: Seq[PullRequest] <-\n-          repo.pullRequests.list(ClosedPRsMostlyRecentlyUpdated).take(2).all(): Future[Seq[PullRequest]]\n-        pullRequests <-\n-          Future.traverse(litePullRequests.filter(isMergedToMain).filter(isNewEnoughToBeWorthScanning).take(MaxPRsToScanPerRepo))(pr => prSnapshot(pr.number))\n+        pullRequests: Seq[PullRequest] <-\n+          repo.pullRequests.list(criteriaForClosedPrsBasedOnTheDefaultBranch)\n+            .takeWhile(_.updated_at >= timeThresholdForScan)\n+            .filter(isMergedRecentlyEnoughToBeWorthScanning)\n+            .take(MaxPRsToScanPerRepo)\n+            .parEvalMapUnordered(4) {\n+              pr => prSnapshot(pr.number)\n+            }.compile.toList\n       } yield {\n         log(s\"PRs merged to master size=${pullRequests.size}\")\n         pullRequests\n       }) andThen { case cprs => log(s\"Merged Pull Requests fetched: ${cprs.map(_.map(_.pr.number).sorted.reverse)}\") }\n     }\n \n     private def fetchLatestCopyOfGitRepo()(implicit githubRepo: Repo): Future[Repository] = (for {\n-      creds <- bot.gitHubCredsProvider()\n+      creds <- bot.accountAccess.credentials()\n     } yield {\n         val repoId = githubRepo.repoId\n         RepoUtil.getGitRepo(\n@@ -122,10 +133,11 @@ object RepoSnapshot {\n           Some(creds.git))\n     }) andThen { case r => log(s\"Git Repo ref count: ${r.map(_.getRefDatabase.getRefs.size)}\") }\n \n-    private def fetchRepoHooks()(implicit githubRepo: Repo) = if (githubRepo.permissions.exists(_.admin)) githubRepo.hooks.list().map(_.flatMap(_.config.get(\"url\").map(Url.parse))).all() else {\n-      log(s\"No admin rights to check hooks\")\n-      Future.successful(Seq.empty)\n-    }\n+    private def fetchRepoHooks()(using githubRepo: Repo) = \n+      if (githubRepo.permissions.exists(_.admin)) githubRepo.hooks.list().flatMap(_.config.get(\"url\").map(Url.parse)).compile.toList else {\n+        log(s\"No admin rights to check hooks\")\n+        IO.pure(Seq.empty)\n+      }\n   }\n }\n \n@@ -197,21 +209,21 @@ case class RepoSnapshot(\n     checkpoint <- allAvailableCheckpoints\n   } yield prLabel.labelFor(checkpoint.name)\n \n-  def diagnostic(): Future[Diagnostic] = for {\n+  def diagnostic(): IO[Diagnostic] = for {\n     snapshots <- snapshotOfAllAvailableCheckpoints()\n   } yield Diagnostic(snapshots, mergedPRs.map { pr =>\n     PRCheckpointDetails(pr, snapshots.filter(s => activeCheckpointsByPullRequest(pr).contains(s.checkpoint)), repoLevelDetails.gitRepo)\n   })\n \n-  def snapshotOfAllAvailableCheckpoints(): Future[Set[CheckpointSnapshot]] =\n-    Future.sequence(allAvailableCheckpoints.map(takeCheckpointSnapshot))\n+  def snapshotOfAllAvailableCheckpoints(): IO[Set[CheckpointSnapshot]] =\n+    allAvailableCheckpoints.parUnorderedTraverse(takeCheckpointSnapshot)\n \n   val activeCheckpoints: Set[Checkpoint] = activeCheckpointsByPullRequest.values.flatten.toSet\n \n-  lazy val snapshotsOfActiveCheckpointsF: Map[Checkpoint, Future[CheckpointSnapshot]] =\n+  lazy val snapshotsOfActiveCheckpointsF: Map[Checkpoint, IO[CheckpointSnapshot]] =\n     activeCheckpoints.map { c => c -> takeCheckpointSnapshot(c) }.toMap\n \n-  def takeCheckpointSnapshot(checkpoint: Checkpoint): Future[CheckpointSnapshot] = for (\n+  def takeCheckpointSnapshot(checkpoint: Checkpoint): IO[CheckpointSnapshot] = for (\n     possibleIdsTry <- checkpointSnapshoter.snapshot(checkpoint).trying\n   ) yield {\n     val objectIdTry = for (possibleIds <- possibleIdsTry) yield {\n@@ -222,7 +234,8 @@ case class RepoSnapshot(\n     CheckpointSnapshot(checkpoint, objectIdTry)\n   }\n \n-  lazy val activeSnapshotsF: Future[Set[CheckpointSnapshot]] =\n+  lazy val activeSnapshotsF: IO[Set[CheckpointSnapshot]] =\n+    \n     Future.sequence(activeCheckpoints.map(snapshotsOfActiveCheckpointsF))\n \n   def checkpointSnapshotsFor(pr: PullRequest, oldState: PRCheckpointState): Future[Set[CheckpointSnapshot]] =\n\n@@ -1,6 +1,7 @@\n package lib\n \n-import com.madgag.github.Implicits._\n+import cats.effect.IO\n+import com.madgag.github.Implicits.*\n import com.madgag.scalagithub.GitHub\n import com.madgag.scalagithub.commands.CreateLabel\n import lib.labels.{CheckpointTestStatus, PullRequestCheckpointStatus}\n@@ -14,10 +15,10 @@ class RepoUpdater(implicit\n   ec: ExecutionContext\n ) {\n \n-  def attemptToCreateMissingLabels(repoLevelDetails: RepoLevelDetails): Future[_] = {\n+  def attemptToCreateMissingLabels(repoLevelDetails: RepoLevelDetails): IO[_] = {\n     for {\n-      existingLabels <- repoLevelDetails.repo.labels.list().all()\n-      createdLabels <- Future.traverse(missingLabelsGiven(repoLevelDetails, existingLabels.map(_.name).toSet)) {\n+      existingLabels <- repoLevelDetails.repo.labels.list().compile.toList\n+      createdLabels <- IO.parTraverse(missingLabelsGiven(repoLevelDetails, existingLabels.map(_.name).toSet).toList) {\n         missingLabel => repoLevelDetails.repo.labels.create(missingLabel)\n       }\n     } yield createdLabels\n\n@@ -3,10 +3,11 @@ package lib.slack\n import cats.data.NonEmptySeq\n import com.madgag.scalagithub.model.{PullRequest, User}\n import io.lemonlabs.uri.Url\n+import lib.*\n import lib.labels.Seen\n-import lib._\n import play.api.Logging\n import play.api.libs.json.{JsValue, Json}\n+import play.api.libs.ws.WSBodyWritables.*\n import play.api.libs.ws.WSClient\n \n import scala.concurrent.{ExecutionContext, Future}\n\n@@ -2,7 +2,7 @@ name := \"prout\"\n \n version := \"1.0-SNAPSHOT\"\n \n-scalaVersion := \"2.13.16\"\n+scalaVersion := \"3.3.6\"\n \n updateOptions := updateOptions.value.withCachedResolution(true)\n \n@@ -18,7 +18,7 @@ lazy val root = (project in file(\".\")).enablePlugins(PlayScala, BuildInfoPlugin)\n Test / testOptions +=\n   Tests.Argument(TestFrameworks.ScalaTest, \"-u\", s\"test-results/scala-${scalaVersion.value}\")\n \n-val playGitHubVersion = \"9.0.0\"\n+val playGitHubVersion = \"9.0.2-SNAPSHOT\"\n \n val jacksonVersion         = \"2.19.2\"\n val jacksonDatabindVersion = \"2.19.2\"\n@@ -45,7 +45,7 @@ libraryDependencies ++= jacksonDatabindOverrides ++ jacksonOverrides ++ akkaSeri\n libraryDependencies ++= Seq(\n   filters,\n   ws,\n-  \"com.softwaremill.macwire\" %% \"macros\" % \"2.6.6\" % Provided, // slight finesse: 'provided' as only used for compile\n+  \"com.softwaremill.macwire\" %% \"macros\" % \"2.6.7\" % Provided, // slight finesse: 'provided' as only used for compile\n   \"com.madgag\" %% \"scala-collection-plus\" % \"1.0.0\",\n   \"org.typelevel\" %% \"cats-core\" % \"2.13.0\",\n   \"com.github.blemale\" %% \"scaffeine\" % \"5.3.0\",\n@@ -55,10 +55,10 @@ libraryDependencies ++= Seq(\n   \"ch.qos.logback\" % \"logback-classic\" % \"1.5.18\",\n   \"com.github.nscala-time\" %% \"nscala-time\" % \"3.0.0\",\n   \"com.indoorvivants\" %% \"scala-uri\" % \"4.2.0\",\n-  \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.9.5\",\n+  \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.9.6\",\n   \"com.madgag.play-git-hub\" %% \"core\" % playGitHubVersion,\n   \"com.madgag.play-git-hub\" %% \"testkit\" % playGitHubVersion % Test,\n-  \"com.madgag.scala-git\" %% \"scala-git-test\" % \"6.0.0\" % Test,\n+  \"com.madgag.scala-git\" %% \"scala-git-test\" % \"7.0.3\" % Test,\n   \"org.scalatestplus.play\" %% \"scalatestplus-play\" % \"7.0.2\" % Test\n )\n \n\n@@ -61,7 +61,7 @@ class FunctionalSpec extends Helpers with Inside with BeforeAndAfterAll {\n     }\n \n     \"not act on a pull request if it does not touch a .prout.json configured folder\" in {\n-      implicit val repoPR = mergePullRequestIn(createTestRepo(\"/multi-project.master-updated-before-feature-merged.git.zip\"), \"bard-feature\")\n+      implicit val repoPR: RepoPR = mergePullRequestIn(createTestRepo(\"/multi-project.master-updated-before-feature-merged.git.zip\"), \"bard-feature\")\n \n       repoPR setCheckpointTo zeroId\n \n@@ -73,7 +73,7 @@ class FunctionalSpec extends Helpers with Inside with BeforeAndAfterAll {\n     }\n \n     \"act on a pull request if it touches a .prout.json configured folder\" in {\n-      implicit val repoPR = mergePullRequestIn(createTestRepo(\"/multi-project.master-updated-before-feature-merged.git.zip\"), \"food-feature\")\n+      implicit val repoPR: RepoPR = mergePullRequestIn(createTestRepo(\"/multi-project.master-updated-before-feature-merged.git.zip\"), \"food-feature\")\n \n       repoPR setCheckpointTo zeroId\n \n@@ -91,7 +91,7 @@ class FunctionalSpec extends Helpers with Inside with BeforeAndAfterAll {\n     }\n \n     \"report an overdue merge without being called\" in {\n-      implicit val repoPR = mergePullRequestIn(createTestRepo(\"/impatient-top-level-config.git.zip\"), \"feature-1\")\n+      implicit val repoPR: RepoPR = mergePullRequestIn(createTestRepo(\"/impatient-top-level-config.git.zip\"), \"feature-1\")\n \n       repoPR setCheckpointTo zeroId\n \n@@ -114,7 +114,7 @@ class FunctionalSpec extends Helpers with Inside with BeforeAndAfterAll {\n     }\n \n     \"report a broken site as overdue\" in {\n-      implicit val repoPR = mergePullRequestIn(createTestRepo(\"/impatient-top-level-config.git.zip\"), \"feature-1\")\n+      implicit val repoPR: RepoPR = mergePullRequestIn(createTestRepo(\"/impatient-top-level-config.git.zip\"), \"feature-1\")\n \n       repoPR setCheckpointFailureTo new Exception(\"This website went Boom!\")\n \n@@ -136,7 +136,7 @@ class FunctionalSpec extends Helpers with Inside with BeforeAndAfterAll {\n     }\n \n     \"use custom messages in comments when set in the config\" in {\n-      implicit val repoPR = mergePullRequestIn(createTestRepo(\"/simple-with-messages.git.zip\"), \"feature-elephant\")\n+      implicit val repoPR: RepoPR = mergePullRequestIn(createTestRepo(\"/simple-with-messages.git.zip\"), \"feature-elephant\")\n \n       repoPR setCheckpointTo zeroId\n \n\n@@ -11,7 +11,8 @@ class ConfigFinderSpec extends PlaySpec {\n   def configFilesIn(repoPath: String): Set[String] = {\n     val localGitRepo: Repository = test.unpackRepo(repoPath)\n \n-    implicit val repoThreadLocal = localGitRepo.getObjectDatabase.threadLocalResources\n+    implicit val repoThreadLocal: ThreadLocalObjectDatabaseResources = \n+      localGitRepo.getObjectDatabase.threadLocalResources\n \n     val master = localGitRepo.resolve(\"master\").asRevCommit(new RevWalk(repoThreadLocal.reader()))\n \n\n@@ -10,42 +10,42 @@ import play.api.libs.json._\n \n class ConfigSpec extends PlaySpec with OptionValues with Inside {\n \n-   \"Config json parsing\" must {\n-     \"parse normal Checkpoint config\" in {\n-       val details = checkpointDetailsFrom(\"/sample.checkpoint.json\")\n-\n-       details mustEqual JsSuccess(CheckpointDetails(Uri.parse(\"https://membership.theguardian.com/\"), minutes(14)))\n-       details.get.sslVerification mustBe true\n-      }\n-\n-     \"parse Checkpoint config with custom messages\" in {\n-       val details = checkpointDetailsFrom(\"/sample.messages.checkpoint.json\")\n-\n-       details mustEqual JsSuccess(\n-         CheckpointDetails(\n-           url = Uri.parse(\"https://www.theguardian.com\"),\n-           overdue = minutes(20),\n-           messages = Some(CheckpointMessages(Seen -> \"prout/seen.md\", Overdue -> \"prout/overdue.md\"))\n-         )\n-       )\n-     }\n-\n-     \"parse Checkpoint config with one custom message\" in {\n-       val details = checkpointDetailsFrom(\"/sample.one.message.checkpoint.json\")\n+  \"Config json parsing\" must {\n+    \"parse normal Checkpoint config\" in {\n+      val details = checkpointDetailsFrom(\"/sample.checkpoint.json\")\n \n-       details mustEqual JsSuccess(\n-         CheckpointDetails(\n-           url = Uri.parse(\"https://www.theguardian.com\"),\n-           overdue = minutes(15),\n-           messages = Some(CheckpointMessages(Seen -> \"prout/seen.md\"))\n-         )\n-       )\n+      details mustEqual JsSuccess(CheckpointDetails(Uri.parse(\"https://membership.theguardian.com/\"), minutes(14)))\n+      details.get.sslVerification mustBe true\n      }\n \n-     \"parse insecure config\" in {\n-       checkpointDetailsFrom(\"/sample.insecure.checkpoint.json\").get.sslVerification mustBe false\n-     }\n-   }\n+    \"parse Checkpoint config with custom messages\" in {\n+      val details = checkpointDetailsFrom(\"/sample.messages.checkpoint.json\")\n+\n+      details mustEqual JsSuccess(\n+        CheckpointDetails(\n+          url = Uri.parse(\"https://www.theguardian.com\"),\n+          overdue = minutes(20),\n+          messages = Some(CheckpointMessages(Seen -> \"prout/seen.md\", Overdue -> \"prout/overdue.md\"))\n+        )\n+      )\n+    }\n+\n+    \"parse Checkpoint config with one custom message\" in {\n+      val details = checkpointDetailsFrom(\"/sample.one.message.checkpoint.json\")\n+\n+      details mustEqual JsSuccess(\n+        CheckpointDetails(\n+          url = Uri.parse(\"https://www.theguardian.com\"),\n+          overdue = minutes(15),\n+          messages = Some(CheckpointMessages(Seen -> \"prout/seen.md\"))\n+        )\n+      )\n+    }\n+\n+    \"parse insecure config\" in {\n+      checkpointDetailsFrom(\"/sample.insecure.checkpoint.json\").get.sslVerification mustBe false\n+    }\n+  }\n \n   def checkpointDetailsFrom(resourcePath: String): JsResult[CheckpointDetails] = {\n     Json.parse(getClass.getResourceAsStream(resourcePath)).validate[CheckpointDetails]\n\n@@ -1,18 +1,18 @@\n package lib\n \n-import com.madgag.git._\n+import com.madgag.git.*\n import lib.GitChanges.affectedFolders\n-import org.eclipse.jgit.lib.Repository\n-import org.eclipse.jgit.revwalk.RevCommit\n-import org.scalatestplus.play._\n+import org.eclipse.jgit.lib.{ObjectReader, Repository}\n+import org.eclipse.jgit.revwalk.{RevCommit, RevWalk}\n+import org.scalatestplus.play.*\n \n class GitChangesSpec extends PlaySpec {\n \n   \"Multi folder config\" must {\n \n     \"detect changes in top-level folder\" in {\n       implicit val localGitRepo: Repository = test.unpackRepo(\"/simple.git.zip\")\n-      implicit val (revWalk, reader) = localGitRepo.singleThreadedReaderTuple\n+      implicit val (revWalk: RevWalk, reader: ObjectReader) = localGitRepo.singleThreadedReaderTuple\n \n       def commitAt(revstr: String) = localGitRepo.resolve(revstr).asRevCommit\n \n@@ -25,7 +25,7 @@ class GitChangesSpec extends PlaySpec {\n \n     \"detect changes in affected folders\" in {\n       implicit val localGitRepo: Repository = test.unpackRepo(\"/multi-folder.git.zip\")\n-      implicit val (revWalk, reader) = localGitRepo.singleThreadedReaderTuple\n+      implicit val (revWalk: RevWalk, reader: ObjectReader) = localGitRepo.singleThreadedReaderTuple\n \n       def commitAt(revstr: String) = localGitRepo.resolve(revstr).asRevCommit\n \n@@ -41,7 +41,7 @@ class GitChangesSpec extends PlaySpec {\n \n     \"not confuse changes on master with changes on the feature branch\" in {\n       implicit val localGitRepo: Repository = test.unpackRepo(\"/multi-project.master-updated-before-feature-merged.git.zip\")\n-      implicit val (revWalk, reader) = localGitRepo.singleThreadedReaderTuple\n+      implicit val (revWalk: RevWalk, reader: ObjectReader) = localGitRepo.singleThreadedReaderTuple\n \n       def commitAt(revstr: String) = localGitRepo.resolve(revstr).asRevCommit\n \n\n@@ -3,7 +3,7 @@ package lib\n import com.madgag.github.Implicits._\n import com.madgag.github.apps.GitHubAppAuth\n import com.madgag.playgithub.testkit.TestRepoCreation\n-import com.madgag.scalagithub.GitHubCredentials\n+import com.madgag.scalagithub.{AccountCredentials, GitHubCredentials}\n import com.madgag.scalagithub.commands.{CreatePullRequest, MergePullRequest}\n import com.madgag.scalagithub.model._\n import lib.sentry.SentryApiClient\n@@ -41,8 +41,8 @@ trait Helpers extends PlaySpec with OneAppPerSuiteWithComponents with Inspectors\n   val testFixturesInstallationAccess: com.madgag.github.apps.InstallationAccess =\n     gitHubAppAuth.accessSoleInstallation().futureValue\n \n-  val testFixturesAccount: Account = testFixturesInstallationAccess.installedOnAccount\n-  val testFixturesCredentials: GitHubCredentials.Provider = testFixturesInstallationAccess.credentials\n+  val testFixtureAccountCredentials: AccountCredentials =\n+    AccountCredentials(testFixturesInstallationAccess.installedOnAccount, testFixturesInstallationAccess.credentials)\n \n   def labelsOnPR()(implicit repoPR: RepoPR): Set[String] = labelsOn(repoPR.pr)\n \n@@ -61,16 +61,7 @@ trait Helpers extends PlaySpec with OneAppPerSuiteWithComponents with Inspectors\n   case class RepoPR(pr: PullRequest) {\n     val githubRepo = pr.baseRepo\n \n-    def currentPR(): PullRequest = {\n-      def fetchPR(): PullRequest = githubRepo.pullRequests.get(pr.number).futureValue\n-\n-      eventually {\n-        val pr = fetchPR()\n-        Thread.sleep(1000)\n-        fetchPR() mustEqual pr\n-        pr\n-      }\n-    }\n+    def currentPR(): PullRequest = eventuallyConsistent { githubRepo.pullRequests.get(pr.number) }\n \n     def listComments(): Seq[Comment] = pr.comments2.list().all().futureValue\n \n@@ -116,19 +107,28 @@ trait Helpers extends PlaySpec with OneAppPerSuiteWithComponents with Inspectors\n     override val toString: String = pr.html_url\n   }\n \n+\n   def scan[T](shouldAddComment: Boolean)(issueFun: PullRequest => T)(implicit repoPR: RepoPR): Unit = {\n     val commentsBeforeScan = repoPR.listComments()\n-    whenReady(repoPR.scheduler.scan()) { s =>\n-      eventually {\n-        inside(repoPR) { case _ =>\n-          val commentsAfterScan = repoPR.listComments()\n-          commentsAfterScan must have size (commentsBeforeScan.size+(if (shouldAddComment) 1 else 0))\n-          issueFun(repoPR.currentPR())\n-        }\n+    scanOnce(requireScanFindsAPr = commentsBeforeScan.isEmpty) // if we're starting out, scan enough times that we actually find the PR\n+\n+    eventually {\n+      inside(repoPR) { case _ =>\n+        val commentsAfterScan = repoPR.listComments()\n+        commentsAfterScan must have size (commentsBeforeScan.size+(if (shouldAddComment) 1 else 0))\n+        issueFun(repoPR.currentPR())\n       }\n     }\n   }\n \n+  private def scanOnce(requireScanFindsAPr: Boolean)(implicit repoPR: RepoPR): Unit = if (requireScanFindsAPr) {\n+    eventually {\n+      whenReady(repoPR.scheduler.scan()) { _ must not be empty } // ensure that scan found a PR\n+    }\n+  } else {\n+    repoPR.scheduler.scan().futureValue // just ensure that a scan has taken place - note that once fully deployed, scan will not return a PR\n+  }\n+\n   def waitUntil[T](shouldAddComment: Boolean)(issueFun: PullRequest => T)(implicit repoPR: RepoPR): Unit = {\n     val commentsBeforeScan = repoPR.listComments()\n     eventually {\n@@ -157,7 +157,7 @@ trait Helpers extends PlaySpec with OneAppPerSuiteWithComponents with Inspectors\n     merging: String,\n     prText: PRText = PRText(\"title\", \"desc\"),\n     userLabels: Set[String] = Set.empty\n-  ) = {\n+  ): RepoPR = {\n     eventually {\n       whenReady(repo.refs.get(s\"heads/$merging\")) { _.ref must endWith(merging) }\n     }\n"}
{"id": 351, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nCorrected Spelling errors.\n\n\n### Code Patch:\n\n\n\n\n@@ -0,0 +1,3 @@\n+0\n+1\n+test-topic 1 0\n\n\n\n\n\n\n\n\n\n@@ -0,0 +1,3 @@\n+0\n+1\n+test-topic 1 0\n\n\n\n\n\n@@ -390,8 +390,8 @@ object ZkUtils extends Logging {\n   }\n \n   /**\n-   * Update the value of a persistent node with the given path and data.\n-   * create parrent directory if necessary. Never throw NodeExistException.\n+   * Update the value of a ephemeral node with the given path and data.\n+   * create parent directory if necessary. Never throw NodeExistException.\n    */\n   def updateEphemeralPath(client: ZkClient, path: String, data: String): Unit = {\n     try {\n\n\n\n@@ -0,0 +1,6 @@\n+#Mon May 11 13:18:44 PDT 2015\n+distributionBase=GRADLE_USER_HOME\n+distributionPath=wrapper/dists\n+zipStoreBase=GRADLE_USER_HOME\n+zipStorePath=wrapper/dists\n+distributionUrl=https\\://services.gradle.org/distributions/gradle-2.0-bin.zip\n"}
{"id": 249, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--\r\n  Thanks for sending a pull request!\r\n    1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n    2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n-->\r\n\r\n### Description of PR\r\n\r\nJIRA: YARN-11642. Fix Flaky Test TestTimelineAuthFilterForV2#testPutTimelineEntities.\r\n\r\nWe can find the following unit test failure report:\r\n\r\n[Report1\r\n](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6410/1/testReport/org.apache.hadoop.yarn.server.timelineservice.security/TestTimelineAuthFilterForV2/testPutTimelineEntities_boolean__boolean__3_/)\r\n\r\n```\r\norg.opentest4j.AssertionFailedError: expected: <2> but was: <1>\r\n...\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.publishAndVerifyEntity(TestTimelineAuthFilterForV2.java:324)\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.publishWithRetries(TestTimelineAuthFilterForV2.java:337)\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.testPutTimelineEntities(TestTimelineAuthFilterForV2.java:383)\r\n```\r\n\r\n[Report2\r\n](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6396/2/testReport/org.apache.hadoop.yarn.server.timelineservice.security/TestTimelineAuthFilterForV2/testPutTimelineEntities_boolean__boolean__3_/)\r\n\r\n```\r\nexpected: <true> but was: <false>\r\n...\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.verifyEntity(TestTimelineAuthFilterForV2.java:294)\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.testPutTimelineEntities(TestTimelineAuthFilterForV2.java:441)\r\n```\r\n\r\nThe reason for this error is that `PerNodeTimelineCollectorsAuxService` was not initialized normally.\r\n\r\n```\r\n2024-01-05 09:34:21,511 WARN  [main] collector.PerNodeTimelineCollectorsAuxService (StringUtils.java:startupShutdownMessage(755)) - failed to register any UNIX signal loggers: \r\njava.lang.IllegalStateException: Can't re-install the signal handlers.\r\nat org.apache.hadoop.util.SignalLogger.register(SignalLogger.java:73)\r\nat org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:753)\r\n... \r\n```\r\n\r\n`PerNodeTimelineCollectorsAuxService` is initialized using a static method and is initialized in parallel by multiple threads(because our unit tests are parallel.), resulting in an error. \r\n\r\n```\r\n/usr/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-6396/yetus-m2/hadoop-trunk-patch-0 -Dsurefire.rerunFailingTestsCount=2 -Pparallel-tests -Pshelltest -Pnative -Drequire.fuse -Drequire.openssl -Drequire.snappy -Drequire.valgrind -Drequire.zstd -Drequire.test.libhadoop -Pyarn-ui clean test -fae ...\r\n```\r\n\r\nWe can change the static `initialization` to `new PerNodeTimelineCollectorsAuxService()` to avoid this problem, which has been verified in the local environment.\r\n\r\n### How was this patch tested?\r\n\r\nJunit Test.\r\n\r\n### For code changes:\r\n\r\n- [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n- [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n- [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n- [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n\r\n\n\n### Code Patch:\n@@ -211,7 +211,8 @@ public void initialize() throws Exception {\n     }\n     UserGroupInformation.setConfiguration(conf);\n     collectorManager = new DummyNodeTimelineCollectorManager();\n-    auxService = PerNodeTimelineCollectorsAuxService.launchServer(\n+    PerNodeTimelineCollectorsAuxService as = new PerNodeTimelineCollectorsAuxService();\n+    auxService = as.launchServer(\n         new String[0], collectorManager, conf);\n     if (withKerberosLogin) {\n       SecurityUtil.login(conf, YarnConfiguration.TIMELINE_SERVICE_KEYTAB,\n"}
{"id": 166, "ground_truth": "OS", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThere's a test in `test_utils.py` that flakes very often, but only on Python 3.8 and only in CI (locally it's all fine). I've tried a couple of ways to fix it but at this point it's not worth the effort, so just skipping it on 3.8.\n\n### Code Patch:\n@@ -7,6 +7,7 @@\n import pytest\n \n import sentry_sdk\n+from sentry_sdk._compat import PY38\n from sentry_sdk.integrations import Integration\n from sentry_sdk._queue import Queue\n from sentry_sdk.utils import (\n@@ -901,6 +902,7 @@ def target():\n     assert (main_thread.ident, main_thread.name) == results.get(timeout=1)\n \n \n+@pytest.mark.skipif(PY38, reason=\"Flakes a lot on 3.8 in CI.\")\n def test_get_current_thread_meta_failed_to_get_main_thread():\n     results = Queue(maxsize=1)\n \n"}
{"id": 417, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFix attendees emails test which was trying to match two strings together when order can change and cause flakiness.  Resolved by expecting string to include emails so order shouldn't matter\n\n### Code Patch:\n@@ -16,10 +16,10 @@\n \n   it '#attendees_emails' do\n     attendees = Fabricate.times(4, :attending_meeting_invitation, meeting: meeting)\n-\n-    expect(event.attendees_emails).to eq(\n-      attendees.map(&:member).map(&:email).sort.join(', ')\n-    )\n+    emails = attendees.map(&:member).map(&:email)\n+    emails.each do |email|\n+      expect(event.attendees_emails).to include(email)\n+    end\n   end\n \n   it '#time' do\n"}
{"id": 15, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nAdd address and/or port to errors where applicable for better reporting.\n\nSee joyent/node#7005 and #16 \n\n\n### Code Patch:\n@@ -63,6 +63,25 @@ function isPipeName(s) {\n   return util.isString(s) && toNumber(s) === false;\n }\n \n+// format exceptions\n+function detailedException(err, syscall, address, port, additional) {\n+  var details;\n+  if (port && port > 0) {\n+    details = address + ':' + port;\n+  } else {\n+    details = address;\n+  }\n+\n+  if (additional) {\n+    details += ' - Local (' + additional + ')';\n+  }\n+  var ex = errnoException(err, syscall, details);\n+  ex.address = address;\n+  if (port) {\n+    ex.port = port;\n+  }\n+  return ex;\n+}\n \n exports.createServer = function() {\n   return new Server(arguments[0], arguments[1]);\n@@ -755,7 +774,7 @@ function afterWrite(status, handle, req, err) {\n   }\n \n   if (status < 0) {\n-    var ex = errnoException(status, 'write', err);\n+    var ex = detailedException(status, 'write', req.address, req.port);\n     debug('write failure', ex);\n     self._destroy(ex, req.cb);\n     return;\n@@ -817,28 +836,46 @@ function connect(self, address, port, addressType, localAddress, localPort) {\n     err = bind(localAddress, localPort);\n \n     if (err) {\n-      self._destroy(errnoException(err, 'bind'));\n+      var ex = detailedException(err, 'bind', localAddress, localPort);\n+      self._destroy(ex);\n       return;\n     }\n   }\n \n-  var req = { oncomplete: afterConnect };\n+  var req = {\n+    oncomplete: afterConnect,\n+    port: undefined,\n+    address: undefined,\n+    localAddress: undefined,\n+    localPort: undefined\n+  };\n   if (addressType === 6 || addressType === 4) {\n     port = port | 0;\n     if (port <= 0 || port > 65535)\n       throw new RangeError('Port should be > 0 and < 65536');\n \n+    req.port = port;\n+    req.address = address;\n     if (addressType === 6) {\n       err = self._handle.connect6(req, address, port);\n     } else if (addressType === 4) {\n       err = self._handle.connect(req, address, port);\n     }\n   } else {\n+    req.address = address;\n     err = self._handle.connect(req, address, afterConnect);\n   }\n \n   if (err) {\n-    self._destroy(errnoException(err, 'connect'));\n+    self._getsockname();\n+    var details;\n+    if (self._sockname) {\n+      ex.localAddress = self._sockname.address;\n+      ex.localPort = self._sockname.port;\n+      details = ex.localAddress + ':' + ex.localPort;\n+    }\n+    var ex = detailedException(err, 'connect', address, port, details);\n+    self._destroy(ex);\n   }\n }\n \n@@ -921,6 +958,9 @@ Socket.prototype.connect = function(options, cb) {\n         // There are no event listeners registered yet so defer the\n         // error event to the next tick.\n         process.nextTick(function() {\n+          err.host = options.host;\n+          err.port = options.port;\n+          err.message = err.message + ' ' + options.host + ':' + options.port;\n           self.emit('error', err);\n           self._destroy();\n         });\n@@ -988,7 +1028,18 @@ function afterConnect(status, handle, req, readable, writable) {\n \n   } else {\n     self._connecting = false;\n-    self._destroy(errnoException(status, 'connect'));\n+    var details;\n+    if (req.localAddress && req.localPort) {\n+      ex.localAddress = req.localAddress;\n+      ex.localPort = req.localPort;\n+      details = ex.localAddress + ':' + ex.localPort;\n+    }\n+    var ex = detailedException(status,\n+                               'connect',\n+                               req.address,\n+                               req.port,\n+                               details);\n+    self._destroy(ex);\n   }\n }\n \n@@ -1117,7 +1168,7 @@ Server.prototype._listen2 = function(address, port, addressType, backlog, fd) {\n     debug('_listen2: create a handle');\n     var rval = createServerHandle(address, port, addressType, fd);\n     if (util.isNumber(rval)) {\n-      var error = errnoException(rval, 'listen');\n+      var error = detailedException(rval, 'listen', address, port);\n       process.nextTick(function() {\n         self.emit('error', error);\n       });\n@@ -1134,7 +1185,7 @@ Server.prototype._listen2 = function(address, port, addressType, backlog, fd) {\n   var err = _listen(self._handle, backlog);\n \n   if (err) {\n-    var ex = errnoException(err, 'listen');\n+    var ex = detailedException(err, 'listen', address, port);\n     self._handle.close();\n     self._handle = null;\n     process.nextTick(function() {\n@@ -1182,8 +1233,10 @@ function listen(self, address, port, addressType, backlog, fd, exclusive) {\n         err = uv.UV_EADDRINUSE;\n     }\n \n-    if (err)\n-      return self.emit('error', errnoException(err, 'bind'));\n+    if (err) {\n+      var ex = detailedException(err, 'bind', address, port);\n+      return self.emit('error', ex);\n+    }\n \n     self._handle = handle;\n     self._listen2(address, port, addressType, backlog, fd);\n\n@@ -0,0 +1,9 @@\n+var common = require('../common');\n+var assert = require('assert');\n+var net = require('net');\n+var fp = '/blah/fadfa';\n+var server = net.createServer(assert.fail);\n+server.listen(fp, assert.fail);\n+server.on('error', common.mustCall(function(e) {\n+  assert.equal(e.address, fp)\n+}));\n\n@@ -0,0 +1,11 @@\n+var common = require('../common');\n+var assert = require('assert');\n+var net = require('net');\n+\n+var server = net.createServer(assert.fail);\n+server.listen(1, '1.1.1.1', assert.fail);\n+server.on('error', common.mustCall(function(e) {\n+  assert.equal(e.address, '1.1.1.1');\n+  assert.equal(e.port, 1);\n+  assert.equal(e.syscall, 'listen');\n+}));\n\n@@ -0,0 +1,12 @@\n+var common = require('../common');\n+var net = require('net');\n+var assert = require('assert');\n+var fp = '/tmp/fadagagsdfgsdf';\n+var c = net.connect(fp);\n+\n+c.on('connect', assert.fail);\n+\n+c.on('error', common.mustCall(function(e) {\n+  assert.equal(e.code, 'ENOENT');\n+  assert.equal(e.message, 'connect ENOENT ' + fp);\n+}));\n\n@@ -0,0 +1,13 @@\n+var common = require('../common');\n+var net = require('net');\n+var assert = require('assert');\n+\n+var c = net.createConnection(common.PORT, 'blah.blah');\n+\n+c.on('connect', assert.fail);\n+\n+c.on('error', common.mustCall(function(e) {\n+  assert.equal(e.code, 'ENOTFOUND');\n+  assert.equal(e.port, common.PORT);\n+  assert.equal(e.hostname, 'blah.blah');\n+}));\n\n@@ -0,0 +1,13 @@\n+var common = require('../common');\n+var net = require('net');\n+var assert = require('assert');\n+\n+var c = net.createConnection(common.PORT);\n+\n+c.on('connect', assert.fail);\n+\n+c.on('error', common.mustCall(function(e) {\n+  assert.equal(e.code, 'ECONNREFUSED');\n+  assert.equal(e.port, common.PORT);\n+  assert.equal(e.address, '127.0.0.1');\n+}));\n"}
{"id": 374, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test was flaking because before the suite runs a bunch of settings are added which are critical to making the test reliable. Inside the settings part it opens the editor, edits it and saves the file via ctrl/cmd+s. This is all fine, but it doesn't verify anything so the editor may end up closing before ctrl/cmd+s actually gets handled. We disable the modal in smoke tests since it needs to run headlessly, so it's difficult to see that the file never actually saves and a dirty file is closed and discarded.\r\n\r\nThe fix is to verify settings.json actually does save by changing the shared Editors.saveOpenedFile mechanism to ensure the dirty indicator isn't present on the active tab.\r\n\r\nFixes #254893\r\nPart of #246731\r\n\r\nFYI @aiday-mar \r\n\r\n<!-- Thank you for submitting a Pull Request. Please:\r\n* Read our Pull Request guidelines:\r\n  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests\r\n* Associate an issue with the Pull Request.\r\n* Ensure that the code is up-to-date with the `main` branch.\r\n* Include a description of the proposed changes and how to test them.\r\n-->\r\n\n\n### Code Patch:\n@@ -15,6 +15,7 @@ export class Editors {\n \t\t} else {\n \t\t\tawait this.code.sendKeybinding('ctrl+s');\n \t\t}\n+\t\tawait this.code.waitForElements('.tab.active.dirty', false, results => results.length === 0);\n \t}\n \n \tasync selectTab(fileName: string): Promise<void> {\n"}
{"id": 344, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nAs documented in #33323, we have frequent failures of the flaky triggerer job tests.\r\n\r\nThe flaky failures are about some errors when we close all the sessions in teardown of the test. It turns out that the tests had side-effect - they have not waited for the TriggererJob thread to complete, they merely marked them to be stopped, but they have not waited for those to complete - this is quite plausible explanation of the flaky test failures - since those threads have 1 second sleep, it's more than likely that the session has been created and used by the thread while the teardown has been attempting to close all the sessions.\r\n\r\nThis side effect could also have an effect for other tests that were run after - because in a busy test run machine, the side effects could propagate further than just to the teardown, so it could also explain why sometimes (very rarely) other job tests failed with similar errors.\r\n\r\nThe fix is to join the runner after marking it to be stopped.\r\n\r\nFixes: #33323 (Hopefully)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n\n### Code Patch:\n@@ -143,6 +143,8 @@ def __init__(self, password, **kwargs):\n     finally:\n         # We always have to stop the runner\n         triggerer_job_runner.trigger_runner.stop = True\n+        triggerer_job_runner.trigger_runner.join(30)\n+\n     # Since we have now an in-memory process of forwarding the logs to stdout,\n     # give it more time for the trigger event to write the log.\n     time.sleep(0.5)\n@@ -257,6 +259,7 @@ def test_trigger_lifecycle(session):\n     finally:\n         # We always have to stop the runner\n         job_runner.trigger_runner.stop = True\n+        job_runner.trigger_runner.join(30)\n \n \n class TestTriggerRunner:\n@@ -408,7 +411,7 @@ def handle_events(self):\n             pytest.fail(\"did not observe 2 loops in the runner thread\")\n     finally:\n         job_runner.trigger_runner.stop = True\n-        job_runner.trigger_runner.join()\n+        job_runner.trigger_runner.join(30)\n         thread.join()\n     instances = path.read_text().splitlines()\n     assert len(instances) == 1\n@@ -514,7 +517,7 @@ async def create_triggers(self):\n     finally:\n         job_runner.trigger_runner.stop = True\n         # with suppress(MockTriggerException):\n-        job_runner.trigger_runner.join()\n+        job_runner.trigger_runner.join(30)\n         thread.join()\n \n \n@@ -545,6 +548,7 @@ def test_trigger_firing(session):\n     finally:\n         # We always have to stop the runner\n         job_runner.trigger_runner.stop = True\n+        job_runner.trigger_runner.join(30)\n \n \n def test_trigger_failing(session):\n@@ -578,6 +582,7 @@ def test_trigger_failing(session):\n     finally:\n         # We always have to stop the runner\n         job_runner.trigger_runner.stop = True\n+        job_runner.trigger_runner.join(30)\n \n \n def test_trigger_cleanup(session):\n"}
{"id": 39, "ground_truth": "Async Wait", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n```\r\n[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-801)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-802)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-803)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-804)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-805)[2022-03-24T11:45:03.974Z] tests/python/test_data_iterator.py:111: AssertionError\r\n\r\n[2022-03-24T11:45:03.974Z] ---------------------------------- Hypothesis ----------------------------------\r\n\r\n[2022-03-24T11:45:03.974Z] Falsifying example: test_data_iterator(\r\n\r\n[2022-03-24T11:45:03.974Z]     n_samples_per_batch=4, n_features=2, n_batches=1, subsample=True,\r\n\r\n[2022-03-24T11:45:03.974Z] )\r\n```\r\n\r\nhttps://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline\n\n### Code Patch:\n@@ -22,7 +22,7 @@ def test_gpu_single_batch() -> None:\n     strategies.integers(0, 13),\n     strategies.booleans(),\n )\n-@settings(deadline=None)\n+@settings(deadline=None, print_blob=True)\n def test_gpu_data_iterator(\n     n_samples_per_batch: int, n_features: int, n_batches: int, subsample: bool\n ) -> None:\n\n@@ -30,7 +30,7 @@ def train_result(param, dmat, num_rounds):\n class TestGPULinear:\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_coordinate(self, param, num_rounds, dataset):\n         assume(len(dataset.y) > 0)\n         param['updater'] = 'gpu_coord_descent'\n@@ -45,7 +45,7 @@ def test_gpu_coordinate(self, param, num_rounds, dataset):\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, strategies.floats(1e-5, 1.0),\n            strategies.floats(1e-5, 1.0))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_coordinate_regularised(self, param, num_rounds, dataset, alpha, lambd):\n         assume(len(dataset.y) > 0)\n         param['updater'] = 'gpu_coord_descent'\n\n@@ -247,7 +247,7 @@ def predict_df(x):\n \n     @given(strategies.integers(1, 10),\n            tm.dataset_strategy, shap_parameter_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_shap(self, num_rounds, dataset, param):\n         param.update({\"predictor\": \"gpu_predictor\", \"gpu_id\": 0})\n         param = dataset.set_params(param)\n@@ -261,7 +261,7 @@ def test_shap(self, num_rounds, dataset, param):\n \n     @given(strategies.integers(1, 10),\n            tm.dataset_strategy, shap_parameter_strategy)\n-    @settings(deadline=None, max_examples=20)\n+    @settings(deadline=None, max_examples=20, print_blob=True)\n     def test_shap_interactions(self, num_rounds, dataset, param):\n         param.update({\"predictor\": \"gpu_predictor\", \"gpu_id\": 0})\n         param = dataset.set_params(param)\n@@ -312,14 +312,14 @@ def run_predict_leaf_booster(self, param, num_rounds, dataset):\n         np.testing.assert_equal(cpu_leaf, gpu_leaf)\n \n     @given(predict_parameter_strategy, tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_predict_leaf_gbtree(self, param, dataset):\n         param['booster'] = 'gbtree'\n         param['tree_method'] = 'gpu_hist'\n         self.run_predict_leaf_booster(param, 10, dataset)\n \n     @given(predict_parameter_strategy, tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_predict_leaf_dart(self, param, dataset):\n         param['booster'] = 'dart'\n         param['tree_method'] = 'gpu_hist'\n@@ -330,7 +330,7 @@ def test_predict_leaf_dart(self, param, dataset):\n     @given(df=data_frames([column('x0', elements=strategies.integers(min_value=0, max_value=3)),\n                            column('x1', elements=strategies.integers(min_value=0, max_value=5))],\n                           index=range_indexes(min_size=20, max_size=50)))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_predict_categorical_split(self, df):\n         from sklearn.metrics import mean_squared_error\n \n\n@@ -46,7 +46,7 @@ class TestGPUUpdaters:\n     cputest = test_up.TestTreeMethod()\n \n     @given(parameter_strategy, strategies.integers(1, 20), tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_hist(self, param, num_rounds, dataset):\n         param[\"tree_method\"] = \"gpu_hist\"\n         param = dataset.set_params(param)\n@@ -56,7 +56,7 @@ def test_gpu_hist(self, param, num_rounds, dataset):\n \n     @given(strategies.integers(10, 400), strategies.integers(3, 8),\n            strategies.integers(1, 2), strategies.integers(4, 7))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     @pytest.mark.skipif(**tm.no_pandas())\n     def test_categorical(self, rows, cols, rounds, cats):\n         self.cputest.run_categorical_basic(rows, cols, rounds, cats, \"gpu_hist\")\n@@ -76,7 +76,7 @@ def test_invalid_category(self):\n     @pytest.mark.skipif(**tm.no_cupy())\n     @given(parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_gpu_hist_device_dmatrix(self, param, num_rounds, dataset):\n         # We cannot handle empty dataset yet\n         assume(len(dataset.y) > 0)\n@@ -88,7 +88,7 @@ def test_gpu_hist_device_dmatrix(self, param, num_rounds, dataset):\n \n     @given(parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_external_memory(self, param, num_rounds, dataset):\n         # We cannot handle empty dataset yet\n         assume(len(dataset.y) > 0)\n@@ -127,7 +127,7 @@ def test_empty_dmatrix_prediction(self):\n \n     @pytest.mark.mgpu\n     @given(tm.dataset_strategy, strategies.integers(0, 10))\n-    @settings(deadline=None, max_examples=10)\n+    @settings(deadline=None, max_examples=10, print_blob=True)\n     def test_specified_gpu_id_gpu_update(self, dataset, gpu_id):\n         param = {'tree_method': 'gpu_hist', 'gpu_id': gpu_id}\n         param = dataset.set_params(param)\n\n@@ -27,7 +27,7 @@\n from test_with_dask import run_empty_dmatrix_auc      # noqa\n from test_with_dask import run_auc                    # noqa\n from test_with_dask import run_boost_from_prediction  # noqa\n-from test_with_dask import run_boost_from_prediction_multi_clasas  # noqa\n+from test_with_dask import run_boost_from_prediction_multi_class  # noqa\n from test_with_dask import run_dask_classifier        # noqa\n from test_with_dask import run_empty_dmatrix_cls      # noqa\n from test_with_dask import _get_client_workers        # noqa\n@@ -216,7 +216,7 @@ def test_boost_from_prediction(local_cuda_cluster: LocalCUDACluster) -> None:\n         X_, y_ = load_digits(return_X_y=True)\n         X = dd.from_array(X_, chunksize=100).map_partitions(cudf.from_pandas)\n         y = dd.from_array(y_, chunksize=100).map_partitions(cudf.from_pandas)\n-        run_boost_from_prediction_multi_clasas(X, y, \"gpu_hist\", client)\n+        run_boost_from_prediction_multi_class(X, y, \"gpu_hist\", client)\n \n \n class TestDistributedGPU:\n@@ -231,7 +231,7 @@ def test_dask_dataframe(self, local_cuda_cluster: LocalCUDACluster) -> None:\n         num_rounds=strategies.integers(1, 20),\n         dataset=tm.dataset_strategy,\n     )\n-    @settings(deadline=duration(seconds=120), suppress_health_check=suppress)\n+    @settings(deadline=duration(seconds=120), suppress_health_check=suppress, print_blob=True)\n     @pytest.mark.skipif(**tm.no_cupy())\n     @pytest.mark.parametrize(\n         \"local_cuda_cluster\", [{\"n_workers\": 2}], indirect=[\"local_cuda_cluster\"]\n\n@@ -108,7 +108,8 @@ def run_data_iterator(\n         evals_result=results_from_it,\n         verbose_eval=False,\n     )\n-    assert non_increasing(results_from_it[\"Train\"][\"rmse\"])\n+    if not subsample:\n+        assert non_increasing(results_from_it[\"Train\"][\"rmse\"])\n \n     X, y = it.as_arrays()\n     Xy = xgb.DMatrix(X, y)\n@@ -125,7 +126,8 @@ def run_data_iterator(\n         verbose_eval=False,\n     )\n     arr_predt = from_arrays.predict(Xy)\n-    assert non_increasing(results_from_arrays[\"Train\"][\"rmse\"])\n+    if not subsample:\n+        assert non_increasing(results_from_arrays[\"Train\"][\"rmse\"])\n \n     rtol = 1e-2\n     # CPU sketching is more memory efficient but less consistent due to small chunks\n@@ -146,7 +148,7 @@ def run_data_iterator(\n     strategies.integers(0, 13),\n     strategies.booleans(),\n )\n-@settings(deadline=None)\n+@settings(deadline=None, print_blob=True)\n def test_data_iterator(\n     n_samples_per_batch: int,\n     n_features: int,\n\n@@ -26,7 +26,7 @@ def train_result(param, dmat, num_rounds):\n class TestLinear:\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, coord_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_coordinate(self, param, num_rounds, dataset, coord_param):\n         param['updater'] = 'coord_descent'\n         param.update(coord_param)\n@@ -41,7 +41,7 @@ def test_coordinate(self, param, num_rounds, dataset, coord_param):\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, coord_strategy, strategies.floats(1e-5, 1.0),\n            strategies.floats(1e-5, 1.0))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_coordinate_regularised(self, param, num_rounds, dataset, coord_param, alpha, lambd):\n         param['updater'] = 'coord_descent'\n         param['alpha'] = alpha\n@@ -54,7 +54,7 @@ def test_coordinate_regularised(self, param, num_rounds, dataset, coord_param, a\n \n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_shotgun(self, param, num_rounds, dataset):\n         param['updater'] = 'shotgun'\n         param = dataset.set_params(param)\n@@ -71,7 +71,7 @@ def test_shotgun(self, param, num_rounds, dataset):\n     @given(parameter_strategy, strategies.integers(10, 50),\n            tm.dataset_strategy, strategies.floats(1e-5, 1.0),\n            strategies.floats(1e-5, 1.0))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_shotgun_regularised(self, param, num_rounds, dataset, alpha, lambd):\n         param['updater'] = 'shotgun'\n         param['alpha'] = alpha\n\n@@ -38,7 +38,7 @@ def train_result(param, dmat, num_rounds):\n class TestTreeMethod:\n     @given(exact_parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_exact(self, param, num_rounds, dataset):\n         param['tree_method'] = 'exact'\n         param = dataset.set_params(param)\n@@ -51,7 +51,7 @@ def test_exact(self, param, num_rounds, dataset):\n         strategies.integers(1, 20),\n         tm.dataset_strategy,\n     )\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_approx(self, param, hist_param, num_rounds, dataset):\n         param[\"tree_method\"] = \"approx\"\n         param = dataset.set_params(param)\n@@ -86,7 +86,7 @@ def test_pruner(self):\n \n     @given(exact_parameter_strategy, hist_parameter_strategy, strategies.integers(1, 20),\n            tm.dataset_strategy)\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     def test_hist(self, param, hist_param, num_rounds, dataset):\n         param['tree_method'] = 'hist'\n         param = dataset.set_params(param)\n@@ -241,7 +241,7 @@ def run_categorical_basic(self, rows, cols, rounds, cats, tree_method):\n \n     @given(strategies.integers(10, 400), strategies.integers(3, 8),\n            strategies.integers(1, 2), strategies.integers(4, 7))\n-    @settings(deadline=None)\n+    @settings(deadline=None, print_blob=True)\n     @pytest.mark.skipif(**tm.no_pandas())\n     def test_categorical(self, rows, cols, rounds, cats):\n         self.run_categorical_basic(rows, cols, rounds, cats, \"approx\")\n\n@@ -337,33 +337,33 @@ def test_dask_predict_shape_infer(client: \"Client\") -> None:\n     assert prediction.shape[1] == 3\n \n \n-def run_boost_from_prediction_multi_clasas(\n+def run_boost_from_prediction_multi_class(\n     X: xgb.dask._DaskCollection,\n     y: xgb.dask._DaskCollection,\n     tree_method: str,\n-    client: \"Client\"\n+    client: \"Client\",\n ) -> None:\n     model_0 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4, tree_method=tree_method\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=768\n     )\n     model_0.fit(X=X, y=y)\n     margin = xgb.dask.inplace_predict(\n         client, model_0.get_booster(), X, predict_type=\"margin\"\n     )\n \n     model_1 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4, tree_method=tree_method\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=768\n     )\n     model_1.fit(X=X, y=y, base_margin=margin)\n     predictions_1 = xgb.dask.predict(\n         client,\n         model_1.get_booster(),\n         xgb.dask.DaskDMatrix(client, X, base_margin=margin),\n-        output_margin=True\n+        output_margin=True,\n     )\n \n     model_2 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=8, tree_method=tree_method\n+        learning_rate=0.3, n_estimators=8, tree_method=tree_method, max_bin=768\n     )\n     model_2.fit(X=X, y=y)\n     predictions_2 = xgb.dask.inplace_predict(\n@@ -382,26 +382,29 @@ def run_boost_from_prediction_multi_clasas(\n \n \n def run_boost_from_prediction(\n-    X: xgb.dask._DaskCollection, y: xgb.dask._DaskCollection, tree_method: str, client: \"Client\"\n+    X: xgb.dask._DaskCollection,\n+    y: xgb.dask._DaskCollection,\n+    tree_method: str,\n+    client: \"Client\",\n ) -> None:\n     X = client.persist(X)\n     y = client.persist(y)\n \n     model_0 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4,\n-        tree_method=tree_method)\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=512\n+    )\n     model_0.fit(X=X, y=y)\n     margin = model_0.predict(X, output_margin=True)\n \n     model_1 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=4,\n-        tree_method=tree_method)\n+        learning_rate=0.3, n_estimators=4, tree_method=tree_method, max_bin=512\n+    )\n     model_1.fit(X=X, y=y, base_margin=margin)\n     predictions_1 = model_1.predict(X, base_margin=margin)\n \n     cls_2 = xgb.dask.DaskXGBClassifier(\n-        learning_rate=0.3, random_state=0, n_estimators=8,\n-        tree_method=tree_method)\n+        learning_rate=0.3, n_estimators=8, tree_method=tree_method, max_bin=512\n+    )\n     cls_2.fit(X=X, y=y)\n     predictions_2 = cls_2.predict(X)\n \n@@ -415,8 +418,8 @@ def run_boost_from_prediction(\n     unmargined = xgb.dask.DaskXGBClassifier(n_estimators=4)\n     unmargined.fit(X=X, y=y, eval_set=[(X, y)], base_margin=margin)\n \n-    margined_res = margined.evals_result()['validation_0']['logloss']\n-    unmargined_res = unmargined.evals_result()['validation_0']['logloss']\n+    margined_res = margined.evals_result()[\"validation_0\"][\"logloss\"]\n+    unmargined_res = unmargined.evals_result()[\"validation_0\"][\"logloss\"]\n \n     assert len(margined_res) == len(unmargined_res)\n     for i in range(len(margined_res)):\n@@ -429,12 +432,11 @@ def test_boost_from_prediction(tree_method: str, client: \"Client\") -> None:\n     from sklearn.datasets import load_breast_cancer, load_digits\n     X_, y_ = load_breast_cancer(return_X_y=True)\n     X, y = dd.from_array(X_, chunksize=200), dd.from_array(y_, chunksize=200)\n-\n     run_boost_from_prediction(X, y, tree_method, client)\n \n     X_, y_ = load_digits(return_X_y=True)\n     X, y = dd.from_array(X_, chunksize=100), dd.from_array(y_, chunksize=100)\n-    run_boost_from_prediction_multi_clasas(X, y, tree_method, client)\n+    run_boost_from_prediction_multi_class(X, y, tree_method, client)\n \n \n def test_inplace_predict(client: \"Client\") -> None:\n@@ -1292,7 +1294,7 @@ def minimum_bin():\n \n     @given(params=hist_parameter_strategy,\n            dataset=tm.dataset_strategy)\n-    @settings(deadline=None, suppress_health_check=suppress)\n+    @settings(deadline=None, suppress_health_check=suppress, print_blob=True)\n     def test_hist(\n             self, params: Dict, dataset: tm.TestDataset, client: \"Client\"\n     ) -> None:\n@@ -1301,7 +1303,7 @@ def test_hist(\n \n     @given(params=exact_parameter_strategy,\n            dataset=tm.dataset_strategy)\n-    @settings(deadline=None, suppress_health_check=suppress)\n+    @settings(deadline=None, suppress_health_check=suppress, print_blob=True)\n     def test_approx(\n             self, client: \"Client\", params: Dict, dataset: tm.TestDataset\n     ) -> None:\n"}
{"id": 339, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nSometimes the execution last exactly SLEEP milliseconds so it becomes a failure.\r\n\r\nSee [JENKINS-54459](https://issues.jenkins-ci.org/browse/JENKINS-54459).\r\n\r\n<!-- Comment: \r\nIf the issue is not fully described in the ticket, add more information here (justification, pull request links, etc.).\r\n\r\n * We do not require JIRA issues for minor improvements.\r\n * Bugfixes should have a JIRA issue (backporting process).\r\n * Major new features should have a JIRA issue reference.\r\n-->\r\n\r\n### Proposed changelog entries\r\n\r\nN/A\r\n\r\n<!-- Comment: \r\nThe changelogs will be integrated by the core maintainers after the merge.  See the changelog examples here: https://jenkins.io/changelog/ -->\r\n\r\n### Submitter checklist\r\n\r\n- [X] JIRA issue is well described\r\n- [X] Changelog entry appropriate for the audience affected by the change (users or developer, depending on the change). [Examples](https://github.com/jenkins-infra/jenkins.io/blob/master/content/_data/changelogs/weekly.yml)\r\n      * Use the `Internal: ` prefix if the change has no user-visible impact (API, test frameworks, etc.)\r\n- [X] Appropriate autotests or explanation to why this change has no tests\r\n- [ ] N/A For dependency updates: links to external changelogs and, if possible, full diffs\r\n\r\n<!-- For new API and extension points: Link to the reference implementation in open-source (or example in Javadoc) -->\r\n\r\n### Desired reviewers\r\n\r\n@reviewbybees \r\n\r\n<!-- Comment:\r\nIf you need an accelerated review process by the community (e.g., for critical bugs), mention @jenkinsci/code-reviewers\r\n-->\r\n\n\n### Code Patch:\n@@ -97,7 +97,7 @@ public void sleepWorksTest() throws Exception {\n         long timeElapsed = Duration.between(start, finish).toMillis();\n \n         // Check delay works\n-        Assert.assertTrue(timeElapsed > SLEEP);\n+        Assert.assertTrue(timeElapsed >= SLEEP);\n \n         // Check result is true\n         Assert.assertTrue(finalResult == null ? false : finalResult);\n"}
{"id": 73, "ground_truth": "Logic", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nBuild: https://dev.azure.com/monacotools/a6d41577-0fa3-498e-af22-257312ff0545/_build/results?buildId=317618\nChanges: https://github.com/Microsoft/vscode/compare/a58d48f...30dbfbb\n\nThe RandomInt, \"should include min and max\" unit test failed, which, if it were truly random and I if understand probability well enough, has a 1 in 20 trillion chance of happening (0.8 ^ 100).\n\n```\nrandomInt positive numbers max: 5, min: 0 should include min and max (reason: Expected 2,1,1,3,2,2,1,4,4,5,4,2,4,5,3,2,3,2,3,1,5,1,2,4,3,1,4,3,3,2,1,1,1,4,2,3,5,4,3,2,1,1,1,1,2,3,1,1,3,3,2,3,3,5,1,3,3,2,4,2,2,2,4,4,1,4,1,1,5,4,5,3,3,2,3,4,3,1,3,2,2,4,2,4,2,2,3,5,3,2,4,3,4,1,1,2,5,2,5,4 to include 0.\n```\n\n### Code Patch:\n@@ -102,7 +102,7 @@ export function isPointWithinTriangle(\n /**\n  * Function to get a (pseudo)random integer from a provided `max`...[`min`] range.\n  * Both `min` and `max` values are inclusive. The `min` value is optional and defaults\n- * to `0` if not explicitely specified.\n+ * to `0` if not explicitly specified.\n  *\n  * @throws in the next cases:\n  * \t- if provided `min` or `max` is not a number\n\n@@ -54,8 +54,8 @@ suite('randomInt', () => {\n \t\t\t\t});\n \t\t\t}\n \n-\t\t\ttest(`should include min and max`, async () => {\n-\t\t\t\tlet iterations = 100;\n+\t\t\ttest('should include min and max', async () => {\n+\t\t\t\tlet iterations = 125;\n \t\t\t\tconst results = [];\n \t\t\t\twhile (iterations-- > 0) {\n \t\t\t\t\tresults.push(randomInt(max, min));\n@@ -74,9 +74,9 @@ suite('randomInt', () => {\n \t};\n \n \tsuite('positive numbers', () => {\n-\t\ttestRandomIntUtil(5, 2, 'max: 5, min: 2');\n-\t\ttestRandomIntUtil(5, 0, 'max: 5, min: 0');\n-\t\ttestRandomIntUtil(5, undefined, 'max: 5, min: undefined');\n+\t\ttestRandomIntUtil(4, 2, 'max: 4, min: 2');\n+\t\ttestRandomIntUtil(4, 0, 'max: 4, min: 0');\n+\t\ttestRandomIntUtil(4, undefined, 'max: 4, min: undefined');\n \t\ttestRandomIntUtil(1, 0, 'max: 0, min: 0');\n \t});\n \n@@ -137,14 +137,14 @@ suite('randomInt', () => {\n \n \t\ttest('should throw if \"min\" is > \"max\" #5', () => {\n \t\t\tassert.throws(() => {\n-\t\t\t\trandomInt(-5, 0);\n-\t\t\t}, `\"max\"(-5) param should be greater than \"min\"(0).\"`);\n+\t\t\t\trandomInt(-4, 0);\n+\t\t\t}, `\"max\"(-4) param should be greater than \"min\"(0).\"`);\n \t\t});\n \n \t\ttest('should throw if \"min\" is > \"max\" #6', () => {\n \t\t\tassert.throws(() => {\n-\t\t\t\trandomInt(-5);\n-\t\t\t}, `\"max\"(-5) param should be greater than \"min\"(0).\"`);\n+\t\t\t\trandomInt(-4);\n+\t\t\t}, `\"max\"(-4) param should be greater than \"min\"(0).\"`);\n \t\t});\n \n \t\ttest('should throw if \"max\" is `NaN`', () => {\n@@ -155,7 +155,7 @@ suite('randomInt', () => {\n \n \t\ttest('should throw if \"min\" is `NaN`', () => {\n \t\t\tassert.throws(() => {\n-\t\t\t\trandomInt(5, NaN);\n+\t\t\t\trandomInt(4, NaN);\n \t\t\t}, `\"min\" param is not a number.\"`);\n \t\t});\n \n"}
{"id": 300, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test checks if loading an app in the background will catch and re-raise an exception. It occasionally fails on pypy3. Possibly, the background loading thread does not finish before the assert line, and so there is no exception yet. Try to address this by waiting for the lock before asserting. This could still fail if the thread starts *very* slowly.\r\n\r\ncloses #4292 \n\n### Code Patch:\n@@ -334,7 +334,9 @@ def bad_load():\n     lazy = DispatchingApp(bad_load, use_eager_loading=False)\n \n     with pytest.raises(BadExc):\n-        lazy._flush_bg_loading_exception()\n+        # reduce flakiness by waiting for the internal loading lock\n+        with lazy._lock:\n+            lazy._flush_bg_loading_exception()\n \n \n def test_with_appcontext(runner):\n"}
{"id": 393, "ground_truth": "OS", "predicted": "OS", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\n`test-blob-slice-with-large-size`\n\n### Platform\n\nSmartOS\n\n### Console output\n\n```console\n---\nduration_ms: 290.507\nexitcode: 1\nseverity: fail\nstack: |-\n  /home/iojs/build/workspace/node-test-commit-smartos/nodes/smartos23-x64/test/pummel/test-blob-slice-with-large-size.js:18\n      throw e;\n      ^\n\n  RangeError: Array buffer allocation failed\n      at new ArrayBuffer (<anonymous>)\n      at new Uint8Array (<anonymous>)\n      at new FastBuffer (node:internal/buffer:961:5)\n      at createUnsafeBuffer (node:internal/buffer:1097:12)\n      at allocate (node:buffer:445:10)\n      at Function.allocUnsafe (node:buffer:410:10)\n      at Object.<anonymous> (/home/iojs/build/workspace/node-test-commit-smartos/nodes/smartos23-x64/test/pummel/test-blob-slice-with-large-size.js:12:22)\n      at Module._compile (node:internal/modules/cjs/loader:1734:14)\n      at Object..js (node:internal/modules/cjs/loader:1899:10)\n      at Module.load (node:internal/modules/cjs/loader:1469:32)\n\n  Node.js v24.0.0-pre\n```\n\n### Build links\n\n- https://ci.nodejs.org/job/node-test-commit-smartos/59405/nodes=smartos23-x64/testReport/junit/(root)/pummel/test_blob_slice_with_large_size/\n\n### Additional information\n\nIt seems to reproduce quite rarely\n\n### Code Patch:\n@@ -1,4 +1,7 @@\n 'use strict';\n+\n+// This tests that Blob.prototype.slice() works correctly when the size of the\n+// Blob is outside the range of 32-bit signed integers.\n const common = require('../common');\n \n // Buffer with size > INT32_MAX\n@@ -14,8 +17,11 @@ try {\n   const slicedBlob = blob.slice(size - 1, size);\n   assert.strictEqual(slicedBlob.size, 1);\n } catch (e) {\n-  if (e.code !== 'ERR_MEMORY_ALLOCATION_FAILED') {\n-    throw e;\n+  if (e.code === 'ERR_MEMORY_ALLOCATION_FAILED') {\n+    common.skip('insufficient space for Buffer.allocUnsafe');\n+  }\n+  if (/Array buffer allocation failed/.test(e.message)) {\n+    common.skip('insufficient space for Blob.prototype.slice()');\n   }\n-  common.skip('insufficient space for Buffer.allocUnsafe');\n+  throw e;\n }\n"}
{"id": 287, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis enables golangci-lint via build tags for integration tests (this\r\nshould have been done long ago!), and fixes the linting errors.\r\n\r\nTwo tests were updated to reduce flakiness:\r\n\r\n* apply config: wait for nodes to issue \"boot done\" sequence event\r\nbefore proceeding\r\n* recover: kill pods even if they appear after the initial set gets\r\nkilled (potential race condition with previous test).\r\n\r\nSigned-off-by: Andrey Smirnov <smirnov.andrey@gmail.com>\r\n\r\n\r\n\n\n### Code Patch:\n@@ -22,6 +22,14 @@ run:\n   skip-files:\n     - .*\\\\.pb\\\\.go$\n \n+  # list of build tags, all linters use it. Default is empty list.\n+  build-tags:\n+    - integration\n+    - integration_api\n+    - integration_cli\n+    - integration_k8s\n+    - integration_provision\n+\n # output configuration options\n output:\n   # colored-line-number|line-number|json|tab|checkstyle, default is \"colored-line-number\"\n\n@@ -16,6 +16,8 @@ import (\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/talos-systems/go-retry/retry\"\n+\n \t\"github.com/talos-systems/talos/internal/integration/base\"\n \tmachineapi \"github.com/talos-systems/talos/pkg/machinery/api/machine\"\n \t\"github.com/talos-systems/talos/pkg/machinery/client\"\n@@ -40,6 +42,7 @@ const applyConfigTestSysctlVal = \"1\"\n \n const assertRebootedRebootTimeout = 10 * time.Minute\n \n+// ApplyConfigSuite ...\n type ApplyConfigSuite struct {\n \tbase.K8sSuite\n \n@@ -69,15 +72,17 @@ func (suite *ApplyConfigSuite) TearDownTest() {\n \t}\n }\n \n-// TestRecoverControlPlane removes the control plane components and attempts to recover them with the recover API.\n-func (suite *ApplyConfigSuite) TestRecoverControlPlane() {\n+// TestApply verifies the apply config API.\n+func (suite *ApplyConfigSuite) TestApply() {\n \tif !suite.Capabilities().SupportsReboot {\n \t\tsuite.T().Skip(\"cluster doesn't support reboot\")\n \t}\n \n \tnodes := suite.DiscoverNodes().NodesByType(machine.TypeJoin)\n \tsuite.Require().NotEmpty(nodes)\n \n+\tsuite.WaitForBootDone(suite.ctx)\n+\n \tsort.Strings(nodes)\n \n \tnode := nodes[0]\n@@ -93,7 +98,7 @@ func (suite *ApplyConfigSuite) TestRecoverControlPlane() {\n \tsuite.Assert().Nilf(err, \"failed to marshal updated machine config data (node %q): %w\", node, err)\n \n \tsuite.AssertRebooted(suite.ctx, node, func(nodeCtx context.Context) error {\n-\t\t_, err := suite.Client.ApplyConfiguration(nodeCtx, &machineapi.ApplyConfigurationRequest{\n+\t\t_, err = suite.Client.ApplyConfiguration(nodeCtx, &machineapi.ApplyConfigurationRequest{\n \t\t\tData: cfgDataOut,\n \t\t})\n \t\tif err != nil {\n@@ -105,8 +110,16 @@ func (suite *ApplyConfigSuite) TestRecoverControlPlane() {\n \t}, assertRebootedRebootTimeout)\n \n \t// Verify configuration change\n-\tnewProvider, err := suite.readConfigFromNode(nodeCtx)\n-\tsuite.Assert().Nilf(err, \"failed to read updated configuration from node %q: %w\", node, err)\n+\tvar newProvider config.Provider\n+\n+\tsuite.Require().Nilf(retry.Constant(time.Minute, retry.WithUnits(time.Second)).Retry(func() error {\n+\t\tnewProvider, err = suite.readConfigFromNode(nodeCtx)\n+\t\tif err != nil {\n+\t\t\treturn retry.ExpectedError(err)\n+\t\t}\n+\n+\t\treturn nil\n+\t}), \"failed to read updated configuration from node %q: %w\", node, err)\n \n \tsuite.Assert().Equal(\n \t\tnewProvider.Machine().Sysctls()[applyConfigTestSysctl],\n@@ -122,9 +135,9 @@ func (suite *ApplyConfigSuite) readConfigFromNode(nodeCtx context.Context) (conf\n \tif err != nil {\n \t\treturn nil, fmt.Errorf(\"error creating reader: %w\", err)\n \t}\n-\tdefer reader.Close()\n+\tdefer reader.Close() //nolint: errcheck\n \n-\tif err := copyFromReaderWithErrChan(cfgData, reader, errCh); err != nil {\n+\tif err = copyFromReaderWithErrChan(cfgData, reader, errCh); err != nil {\n \t\treturn nil, fmt.Errorf(\"error reading: %w\", err)\n \t}\n \n@@ -142,6 +155,7 @@ func copyFromReaderWithErrChan(out io.Writer, in io.Reader, errCh <-chan error)\n \tvar chanErr error\n \n \twg.Add(1)\n+\n \tgo func() {\n \t\tdefer wg.Done()\n \n\n@@ -63,27 +63,27 @@ func (suite *DiskUsageSuite) TestDiskUsageRequests() {\n \t}\n \n \tcases := []*testParams{\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 0,\n \t\t\tall:            false,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 1,\n \t\t\tall:            false,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 0,\n \t\t\tall:            true,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 1,\n \t\t\tall:            true,\n \t\t\tpaths:          defaultPaths,\n \t\t},\n-\t\t&testParams{\n+\t\t{\n \t\t\trecursionDepth: 0,\n \t\t\tall:            true,\n \t\t\tpaths:          append([]string{\"/this/is/going/to/fail\"}, defaultPaths...),\n@@ -93,7 +93,6 @@ func (suite *DiskUsageSuite) TestDiskUsageRequests() {\n \tsizes := map[string]int64{}\n \n \tfor _, params := range cases {\n-\n \t\tlookupPaths := map[string]bool{}\n \t\tfor _, path := range params.paths {\n \t\t\tlookupPaths[path] = true\n@@ -110,16 +109,19 @@ func (suite *DiskUsageSuite) TestDiskUsageRequests() {\n \t\tsuite.Require().NoError(err)\n \n \t\tresponseCount := 0\n+\n \t\tfor {\n \t\t\tinfo, err := stream.Recv()\n \t\t\tresponseCount++\n+\n \t\t\tif err != nil {\n \t\t\t\tif err == io.EOF || status.Code(err) == codes.Canceled {\n \t\t\t\t\tbreak\n \t\t\t\t}\n \n \t\t\t\tsuite.Require().NoError(err)\n \t\t\t}\n+\n \t\t\tif size, ok := sizes[info.Name]; ok {\n \t\t\t\tsuite.Require().EqualValues(size, info.Size)\n \t\t\t}\n\n@@ -139,6 +139,7 @@ func (suite *DmesgSuite) TestClusterHasDmesg() {\n \t\t\tif err == io.EOF {\n \t\t\t\tbreak\n \t\t\t}\n+\n \t\t\tsuite.Require().NoError(err)\n \t\t}\n \n\n@@ -23,6 +23,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/constants\"\n )\n \n+// EtcdSuite ...\n type EtcdSuite struct {\n \tbase.APISuite\n \n@@ -115,7 +116,9 @@ func (suite *EtcdSuite) TestEtcdLeaveCluster() {\n \tsuite.Require().NoError(err)\n \n \tfor {\n-\t\tinfo, err := stream.Recv()\n+\t\tvar info *machineapi.FileInfo\n+\n+\t\tinfo, err = stream.Recv()\n \t\tif err != nil {\n \t\t\tif err == io.EOF || status.Code(err) == codes.Canceled {\n \t\t\t\tbreak\n\n@@ -160,6 +160,7 @@ func (suite *LogsSuite) TestTailStreaming0() {\n \tsuite.testStreaming(0)\n }\n \n+//nolint: gocyclo\n func (suite *LogsSuite) testStreaming(tailLines int32) {\n \tif tailLines >= 0 {\n \t\t// invoke machined enough times to generate\n@@ -189,7 +190,9 @@ func (suite *LogsSuite) testStreaming(tailLines int32) {\n \t\tdefer close(respCh)\n \n \t\tfor {\n-\t\t\tmsg, err := logsStream.Recv()\n+\t\t\tvar msg *common.Data\n+\n+\t\t\tmsg, err = logsStream.Recv()\n \t\t\tif err != nil {\n \t\t\t\terrCh <- err\n \t\t\t\treturn\n\n@@ -19,6 +19,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/client\"\n )\n \n+// RebootSuite ...\n type RebootSuite struct {\n \tbase.APISuite\n \n@@ -67,6 +68,8 @@ func (suite *RebootSuite) TestRebootNodeByNode() {\n }\n \n // TestRebootAllNodes reboots all cluster nodes at the same time.\n+//\n+//nolint: gocyclo\n func (suite *RebootSuite) TestRebootAllNodes() {\n \tif !suite.Capabilities().SupportsReboot {\n \t\tsuite.T().Skip(\"cluster doesn't support reboots\")\n@@ -128,7 +131,7 @@ func (suite *RebootSuite) TestRebootAllNodes() {\n \n \t\t\t\t\tif bootIDAfter == bootIDBefore {\n \t\t\t\t\t\t// bootID should be different after reboot\n-\t\t\t\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s + %s, after %s\", node, bootIDBefore, bootIDAfter))\n+\t\t\t\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s, after %s\", node, bootIDBefore, bootIDAfter))\n \t\t\t\t\t}\n \n \t\t\t\t\treturn nil\n\n@@ -21,6 +21,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/config/types/v1alpha1/machine\"\n )\n \n+// RecoverSuite ...\n type RecoverSuite struct {\n \tbase.K8sSuite\n \n@@ -73,6 +74,8 @@ func (suite *RecoverSuite) TestRecoverControlPlane() {\n \n \t\t\tsuite.Assert().NoError(err)\n \n+\t\t\tdeletedPods := make(map[string]struct{})\n+\n \t\t\tvar eg errgroup.Group\n \n \t\t\tfor _, pod := range pods.Items {\n@@ -81,12 +84,9 @@ func (suite *RecoverSuite) TestRecoverControlPlane() {\n \t\t\t\teg.Go(func() error {\n \t\t\t\t\tsuite.T().Logf(\"Deleting %s\", pod.GetName())\n \n-\t\t\t\t\terr := suite.Clientset.CoreV1().Pods(pod.GetNamespace()).Delete(suite.ctx, pod.GetName(), metav1.DeleteOptions{})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n+\t\t\t\t\tdeletedPods[pod.GetName()] = struct{}{}\n \n-\t\t\t\t\treturn err\n+\t\t\t\t\treturn suite.Clientset.CoreV1().Pods(pod.GetNamespace()).Delete(suite.ctx, pod.GetName(), metav1.DeleteOptions{})\n \t\t\t\t})\n \t\t\t}\n \n@@ -100,6 +100,16 @@ func (suite *RecoverSuite) TestRecoverControlPlane() {\n \t\t\t\t})\n \n \t\t\t\tsuite.Assert().NoError(err)\n+\n+\t\t\t\tfor _, pod := range pods.Items {\n+\t\t\t\t\tif _, ok := deletedPods[pod.GetName()]; !ok {\n+\t\t\t\t\t\tsuite.T().Logf(\"Deleting %s\", pod.GetName())\n+\n+\t\t\t\t\t\tdeletedPods[pod.GetName()] = struct{}{}\n+\n+\t\t\t\t\t\tsuite.Require().NoError(suite.Clientset.CoreV1().Pods(pod.GetNamespace()).Delete(suite.ctx, pod.GetName(), metav1.DeleteOptions{}))\n+\t\t\t\t\t}\n+\t\t\t\t}\n \t\t\t}\n \n \t\t\tnodes := suite.DiscoverNodes().NodesByType(machine.TypeControlPlane)\n\n@@ -16,6 +16,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/config/types/v1alpha1/machine\"\n )\n \n+// ResetSuite ...\n type ResetSuite struct {\n \tbase.APISuite\n \n@@ -56,6 +57,7 @@ func (suite *ResetSuite) TestResetNodeByNode() {\n \t}\n \n \tinitNodeAddress := \"\"\n+\n \tfor _, node := range suite.Cluster.Info().Nodes {\n \t\tif node.Type == machine.TypeInit {\n \t\t\tinitNodeAddress = node.PrivateIP.String()\n@@ -78,13 +80,13 @@ func (suite *ResetSuite) TestResetNodeByNode() {\n \n \t\tsuite.T().Log(\"Resetting node\", node)\n \n+\t\t// TODO: there is no good way to assert that node was reset and disk contents were really wiped\n+\n \t\t// uptime should go down after Reset, as it reboots the node\n \t\tsuite.AssertRebooted(suite.ctx, node, func(nodeCtx context.Context) error {\n \t\t\t// force reboot after reset, as this is the only mode we can test\n \t\t\treturn suite.Client.Reset(nodeCtx, true, true)\n \t\t}, 10*time.Minute)\n-\n-\t\t// TODO: there is no good way to assert that node was reset and disk contents were really wiped\n \t}\n }\n \n\n@@ -74,6 +74,7 @@ func (suite *VersionSuite) TestSameVersionCluster() {\n \tsuite.Require().Len(v.Messages, len(nodes))\n \n \texpectedVersion := v.Messages[0].Version.Tag\n+\n \tfor _, version := range v.Messages {\n \t\tsuite.Assert().Equal(expectedVersion, version.Version.Tag)\n \t}\n\n@@ -20,6 +20,7 @@ import (\n \t\"github.com/talos-systems/talos/internal/app/machined/pkg/runtime\"\n \t\"github.com/talos-systems/talos/pkg/cluster\"\n \t\"github.com/talos-systems/talos/pkg/cluster/check\"\n+\tmachineapi \"github.com/talos-systems/talos/pkg/machinery/api/machine\"\n \t\"github.com/talos-systems/talos/pkg/machinery/client\"\n \t\"github.com/talos-systems/talos/pkg/machinery/client/config\"\n \t\"github.com/talos-systems/talos/pkg/machinery/config/types/v1alpha1/machine\"\n@@ -50,6 +51,20 @@ func (apiSuite *APISuite) SetupSuite() {\n \n \tapiSuite.Client, err = client.New(context.TODO(), opts...)\n \tapiSuite.Require().NoError(err)\n+\n+\t// clear any connection refused errors left after the previous tests\n+\tnodes := apiSuite.DiscoverNodes().Nodes()\n+\n+\tif len(nodes) > 0 {\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n+\t\tdefer cancel()\n+\n+\t\tfor i := 0; i < len(nodes); i++ {\n+\t\t\t_, err = apiSuite.Client.Version(client.WithNodes(ctx, nodes...))\n+\t\t}\n+\n+\t\tapiSuite.Require().NoError(err)\n+\t}\n }\n \n // DiscoverNodes provides list of Talos nodes in the cluster.\n@@ -76,7 +91,7 @@ func (apiSuite *APISuite) DiscoverNodes() cluster.Info {\n \treturn apiSuite.discoveredNodes\n }\n \n-// RandomNode returns a random node of the specified type (or any type if no types are specified).\n+// RandomDiscoveredNode returns a random node of the specified type (or any type if no types are specified).\n func (apiSuite *APISuite) RandomDiscoveredNode(types ...machine.Type) string {\n \tnodeInfo := apiSuite.DiscoverNodes()\n \n@@ -204,7 +219,7 @@ func (apiSuite *APISuite) AssertRebooted(ctx context.Context, node string, reboo\n \n \t\tif bootIDAfter == bootIDBefore {\n \t\t\t// bootID should be different after reboot\n-\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s + %s, after %s\", node, bootIDBefore, bootIDAfter))\n+\t\t\treturn retry.ExpectedError(fmt.Errorf(\"bootID didn't change for node %q: before %s, after %s\", node, bootIDBefore, bootIDAfter))\n \t\t}\n \n \t\treturn nil\n@@ -217,6 +232,38 @@ func (apiSuite *APISuite) AssertRebooted(ctx context.Context, node string, reboo\n \t}\n }\n \n+// WaitForBootDone waits for boot phase done event.\n+func (apiSuite *APISuite) WaitForBootDone(ctx context.Context) {\n+\tnodes := apiSuite.DiscoverNodes().Nodes()\n+\n+\tnodesNotDoneBooting := make(map[string]struct{})\n+\n+\tfor _, node := range nodes {\n+\t\tnodesNotDoneBooting[node] = struct{}{}\n+\t}\n+\n+\tctx, cancel := context.WithTimeout(client.WithNodes(ctx, nodes...), 3*time.Minute)\n+\tdefer cancel()\n+\n+\tapiSuite.Require().NoError(apiSuite.Client.EventsWatch(ctx, func(ch <-chan client.Event) {\n+\t\tdefer cancel()\n+\n+\t\tfor event := range ch {\n+\t\t\tif msg, ok := event.Payload.(*machineapi.SequenceEvent); ok {\n+\t\t\t\tif msg.GetAction() == machineapi.SequenceEvent_STOP && msg.GetSequence() == runtime.SequenceBoot.String() {\n+\t\t\t\t\tdelete(nodesNotDoneBooting, event.Node)\n+\n+\t\t\t\t\tif len(nodesNotDoneBooting) == 0 {\n+\t\t\t\t\t\treturn\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}, client.WithTailEvents(-1)))\n+\n+\tapiSuite.Require().Empty(nodesNotDoneBooting)\n+}\n+\n // TearDownSuite closes Talos API client.\n func (apiSuite *APISuite) TearDownSuite() {\n \tif apiSuite.Client != nil {\n\n@@ -52,8 +52,8 @@ type ConfiguredSuite interface {\n }\n \n // SetConfig implements ConfiguredSuite.\n-func (suite *TalosSuite) SetConfig(config TalosSuite) {\n-\t*suite = config\n+func (talosSuite *TalosSuite) SetConfig(config TalosSuite) {\n+\t*talosSuite = config\n }\n \n // NamedSuite interface provides names for test suites.\n\n@@ -52,7 +52,7 @@ func (cliSuite *CLISuite) DiscoverNodes() cluster.Info {\n \treturn nil\n }\n \n-// RandomNode returns a random node of the specified type (or any type if no types are specified).\n+// RandomDiscoveredNode returns a random node of the specified type (or any type if no types are specified).\n func (cliSuite *CLISuite) RandomDiscoveredNode(types ...machine.Type) string {\n \tnodeInfo := cliSuite.DiscoverNodes()\n \n@@ -97,7 +97,6 @@ func (cliSuite *CLISuite) discoverKubectl() cluster.Info {\n \n func (cliSuite *CLISuite) buildCLICmd(args []string) *exec.Cmd {\n \t// TODO: add support for calling `talosctl config endpoint` before running talosctl\n-\n \targs = append([]string{\"--talosconfig\", cliSuite.TalosConfig}, args...)\n \n \treturn exec.Command(cliSuite.TalosctlPath, args...)\n@@ -108,6 +107,7 @@ func (cliSuite *CLISuite) RunCLI(args []string, options ...RunOption) {\n \tRun(&cliSuite.Suite, cliSuite.buildCLICmd(args), options...)\n }\n \n+// RunAndWaitForMatch retries command until output matches.\n func (cliSuite *CLISuite) RunAndWaitForMatch(args []string, regex *regexp.Regexp, duration time.Duration, options ...retry.Option) {\n \tcliSuite.Assert().NoError(retry.Constant(duration, options...).Retry(func() error {\n \t\tstdout, _, err := RunAndWait(&cliSuite.Suite, cliSuite.buildCLICmd(args))\n\n@@ -25,6 +25,8 @@ func (wrapper *infoWrapper) NodesByType(t machine.Type) []string {\n \t\treturn append([]string(nil), wrapper.masterNodes...)\n \tcase machine.TypeJoin:\n \t\treturn append([]string(nil), wrapper.workerNodes...)\n+\tcase machine.TypeUnknown:\n+\t\tfallthrough\n \tdefault:\n \t\tpanic(\"unreachable\")\n \t}\n\n@@ -21,6 +21,7 @@ import (\n \t\"github.com/talos-systems/talos/pkg/machinery/constants\"\n )\n \n+//nolint: gocyclo\n func discoverNodesK8s(client *client.Client, suite *TalosSuite) (cluster.Info, error) {\n \tctx, ctxCancel := context.WithTimeout(context.Background(), time.Minute)\n \tdefer ctxCancel()\n\n@@ -100,7 +100,7 @@ func StdoutMatchFunc(f MatchFunc) RunOption {\n \t}\n }\n \n-// StderrtMatchFunc appends to the list of MatchFuncs to run against stderr.\n+// StderrMatchFunc appends to the list of MatchFuncs to run against stderr.\n func StderrMatchFunc(f MatchFunc) RunOption {\n \treturn func(opts *runOptions) {\n \t\topts.stderrMatchers = append(opts.stderrMatchers, f)\n@@ -124,6 +124,7 @@ func RunAndWait(suite *suite.Suite, cmd *exec.Cmd) (stdoutBuf, stderrBuf *bytes.\n \t\tif index < 0 {\n \t\t\tcontinue\n \t\t}\n+\n \t\tswitch strings.ToUpper(keyvalue[:index]) {\n \t\tcase \"PATH\":\n \t\t\tfallthrough\n@@ -142,6 +143,8 @@ func RunAndWait(suite *suite.Suite, cmd *exec.Cmd) (stdoutBuf, stderrBuf *bytes.\n }\n \n // Run executes command and asserts on its exit status/output.\n+//\n+//nolint: gocyclo\n func Run(suite *suite.Suite, cmd *exec.Cmd, options ...RunOption) {\n \tvar opts runOptions\n \n\n@@ -30,6 +30,7 @@ func (suite *CrashdumpSuite) TestRun() {\n \t}\n \n \targs := []string{}\n+\n \tfor _, node := range suite.Cluster.Info().Nodes {\n \t\tswitch node.Type {\n \t\tcase machine.TypeInit:\n@@ -38,6 +39,8 @@ func (suite *CrashdumpSuite) TestRun() {\n \t\t\targs = append(args, \"--control-plane-nodes\", node.PrivateIP.String())\n \t\tcase machine.TypeJoin:\n \t\t\targs = append(args, \"--worker-nodes\", node.PrivateIP.String())\n+\t\tcase machine.TypeUnknown:\n+\t\t\tpanic(\"unexpected\")\n \t\t}\n \t}\n \n\n@@ -39,6 +39,7 @@ func splitLine(line string) []string {\n \t\t\tcolumns = append(columns, strings.TrimSpace(part))\n \t\t}\n \t}\n+\n \treturn columns\n }\n \n@@ -54,7 +55,7 @@ func parseLine(line string) (*duInfo, error) {\n \n \tif len(columns) == 3 {\n \t\tres.node = columns[0]\n-\t\toffset += 1\n+\t\toffset++\n \t}\n \n \tsize, err := strconv.ParseInt(columns[offset], 10, 64)\n@@ -74,6 +75,7 @@ func (suite *DiskUsageSuite) TestSuccess() {\n \tnode := suite.RandomDiscoveredNode()\n \n \tvar folderSize int64 = 4096\n+\n \tsuite.RunCLI([]string{\"list\", \"--nodes\", node, folder, \"-l\"},\n \t\tbase.StdoutMatchFunc(func(stdout string) error {\n \t\t\tlines := strings.Split(strings.TrimSpace(stdout), \"\\n\")\n\n@@ -26,6 +26,7 @@ func (suite *GenSuite) SuiteName() string {\n \treturn \"cli.GenSuite\"\n }\n \n+// SetupTest ...\n func (suite *GenSuite) SetupTest() {\n \tvar err error\n \tsuite.tmpDir, err = ioutil.TempDir(\"\", \"talos\")\n@@ -37,6 +38,7 @@ func (suite *GenSuite) SetupTest() {\n \tsuite.Require().NoError(os.Chdir(suite.tmpDir))\n }\n \n+// TearDownTest ...\n func (suite *GenSuite) TearDownTest() {\n \tif suite.savedCwd != \"\" {\n \t\tsuite.Require().NoError(os.Chdir(suite.savedCwd))\n\n@@ -25,6 +25,8 @@ func (suite *HealthSuite) SuiteName() string {\n }\n \n // TestClientSide does successful health check run from client-side.\n+//\n+//nolint: gocyclo\n func (suite *HealthSuite) TestClientSide() {\n \tif suite.Cluster == nil {\n \t\tsuite.T().Skip(\"Cluster is not available, skipping test\")\n@@ -47,6 +49,8 @@ func (suite *HealthSuite) TestClientSide() {\n \t\t\t\targs = append(args, \"--control-plane-nodes\", node.PrivateIP.String())\n \t\t\tcase machine.TypeJoin:\n \t\t\t\targs = append(args, \"--worker-nodes\", node.PrivateIP.String())\n+\t\t\tcase machine.TypeInit, machine.TypeUnknown:\n+\t\t\t\tpanic(\"unexpected\")\n \t\t\t}\n \t\t}\n \t} else {\n@@ -58,6 +62,8 @@ func (suite *HealthSuite) TestClientSide() {\n \t\t\t\targs = append(args, \"--control-plane-nodes\", node.PrivateIP.String())\n \t\t\tcase machine.TypeJoin:\n \t\t\t\targs = append(args, \"--worker-nodes\", node.PrivateIP.String())\n+\t\t\tcase machine.TypeUnknown:\n+\t\t\t\tpanic(\"unexpected\")\n \t\t\t}\n \t\t}\n \t}\n\n@@ -42,7 +42,7 @@ func (suite *RestartSuite) TestSystem() {\n \tsuite.RunAndWaitForMatch([]string{\"service\", \"-n\", node, \"trustd\"}, regexp.MustCompile(`EVENTS\\s+\\[Running\\]: Health check successful`), 30*time.Second)\n }\n \n-// TestKubernetes restarts K8s container.\n+// TestK8s restarts K8s container.\n func (suite *RestartSuite) TestK8s() {\n \tif testing.Short() {\n \t\tsuite.T().Skip(\"skipping in short mode\")\n\n@@ -27,6 +27,7 @@ func (suite *ValidateSuite) SuiteName() string {\n \treturn \"cli.ValidateSuite\"\n }\n \n+// SetupTest ...\n func (suite *ValidateSuite) SetupTest() {\n \tvar err error\n \tsuite.tmpDir, err = ioutil.TempDir(\"\", \"talos\")\n@@ -38,6 +39,7 @@ func (suite *ValidateSuite) SetupTest() {\n \tsuite.Require().NoError(os.Chdir(suite.tmpDir))\n }\n \n+// TearDownTest ...\n func (suite *ValidateSuite) TearDownTest() {\n \tif suite.savedCwd != \"\" {\n \t\tsuite.Require().NoError(os.Chdir(suite.savedCwd))\n@@ -53,7 +55,11 @@ func (suite *ValidateSuite) TestValidate() {\n \tsuite.RunCLI([]string{\"gen\", \"config\", \"foobar\", \"https://10.0.0.1\"})\n \n \tfor _, configFile := range []string{\"init.yaml\", \"controlplane.yaml\", \"join.yaml\"} {\n+\t\tconfigFile := configFile\n+\n \t\tfor _, mode := range []string{\"cloud\", \"container\"} {\n+\t\t\tmode := mode\n+\n \t\t\tsuite.Run(fmt.Sprintf(\"%s-%s\", configFile, mode), func() {\n \t\t\t\tsuite.RunCLI([]string{\"validate\", \"-m\", mode, \"-c\", configFile})\n \t\t\t})\n\n@@ -37,7 +37,7 @@ func (suite *VersionSuite) TestShortVersion() {\n \t)\n }\n \n-// TestClientVersion verifies only client version output.\n+// TestClient verifies only client version output.\n func (suite *VersionSuite) TestClient() {\n \tsuite.RunCLI([]string{\"version\", \"--client\"},\n \t\tbase.StdoutShouldMatch(regexp.MustCompile(`Client:\\n\\s*Tag:\\s*`+regexp.QuoteMeta(suite.Version))),\n\n@@ -45,6 +45,9 @@ var (\n \tstateDir         string\n )\n \n+// TestIntegration ...\n+//\n+//nolint: gocyclo\n func TestIntegration(t *testing.T) {\n \tif talosConfig == \"\" {\n \t\tt.Error(\"--talos.config is not provided\")\n@@ -139,7 +142,8 @@ func init() {\n \tflag.Int64Var(&provision_test.DefaultSettings.DiskGB, \"talos.provision.disk\", provision_test.DefaultSettings.DiskGB, \"disk size (in GiB) for each VM (provision tests only)\")\n \tflag.IntVar(&provision_test.DefaultSettings.MasterNodes, \"talos.provision.masters\", provision_test.DefaultSettings.MasterNodes, \"master node count (provision tests only)\")\n \tflag.IntVar(&provision_test.DefaultSettings.WorkerNodes, \"talos.provision.workers\", provision_test.DefaultSettings.WorkerNodes, \"worker node count (provision tests only)\")\n-\tflag.StringVar(&provision_test.DefaultSettings.TargetInstallImageRegistry, \"talos.provision.target-installer-registry\", provision_test.DefaultSettings.TargetInstallImageRegistry, \"image registry for target installer image (provision tests only)\")\n+\tflag.StringVar(&provision_test.DefaultSettings.TargetInstallImageRegistry, \"talos.provision.target-installer-registry\",\n+\t\tprovision_test.DefaultSettings.TargetInstallImageRegistry, \"image registry for target installer image (provision tests only)\")\n \tflag.StringVar(&provision_test.DefaultSettings.CustomCNIURL, \"talos.provision.custom-cni-url\", provision_test.DefaultSettings.CustomCNIURL, \"custom CNI URL for the cluster (provision tests only)\")\n \n \tallSuites = append(allSuites, api.GetAllSuites()...)\n\n@@ -32,8 +32,8 @@ func (suite *VersionSuite) TestExpectedVersion() {\n \tapiServerVersion, err := suite.DiscoveryClient.ServerVersion()\n \tsuite.Require().NoError(err)\n \n-\texpectedApiServerVersion := fmt.Sprintf(\"v%s\", constants.DefaultKubernetesVersion)\n-\tsuite.Assert().Equal(expectedApiServerVersion, apiServerVersion.GitVersion)\n+\texpectedAPIServerVersion := fmt.Sprintf(\"v%s\", constants.DefaultKubernetesVersion)\n+\tsuite.Assert().Equal(expectedAPIServerVersion, apiServerVersion.GitVersion)\n \n \tcheckKernelVersion := suite.Capabilities().RunsTalosKernel\n \n@@ -51,6 +51,7 @@ func (suite *VersionSuite) TestExpectedVersion() {\n \t\tsuite.Assert().Equal(\"linux\", node.Status.NodeInfo.OperatingSystem)\n \t\tsuite.Assert().Equal(expectedContainerRuntimeVersion, node.Status.NodeInfo.ContainerRuntimeVersion)\n \t\tsuite.Assert().Equal(expectedKubeletVersion, node.Status.NodeInfo.KubeletVersion)\n+\n \t\tif checkKernelVersion {\n \t\t\tsuite.Assert().Equal(expectedKernelVersion, node.Status.NodeInfo.KernelVersion)\n \t\t}\n\n@@ -131,6 +131,8 @@ func upgradeLastReleaseToCurrent() upgradeSpec {\n }\n \n // upgradeSingeNodePreserve upgrade last release of Talos to the current version of Talos for single-node cluster with preserve.\n+//\n+//nolint: deadcode,unused\n func upgradeSingeNodePreserve() upgradeSpec {\n \treturn upgradeSpec{\n \t\tShortName: fmt.Sprintf(\"preserve-%s-%s\", nextVersion, DefaultSettings.CurrentVersion),\n@@ -151,6 +153,7 @@ func upgradeSingeNodePreserve() upgradeSpec {\n \t}\n }\n \n+// UpgradeSuite ...\n type UpgradeSuite struct {\n \tsuite.Suite\n \tbase.TalosSuite\n@@ -242,7 +245,7 @@ func (suite *UpgradeSuite) setupCluster() {\n \tsuite.stateDir, err = ioutil.TempDir(\"\", \"talos-integration\")\n \tsuite.Require().NoError(err)\n \n-\tsuite.T().Logf(\"initalizing provisioner with cluster name %q, state directory %q\", clusterName, suite.stateDir)\n+\tsuite.T().Logf(\"initializing provisioner with cluster name %q, state directory %q\", clusterName, suite.stateDir)\n \n \trequest := provision.ClusterRequest{\n \t\tName: clusterName,\n@@ -399,7 +402,7 @@ func (suite *UpgradeSuite) assertSameVersionCluster(client *talosclient.Client,\n \t}\n }\n \n-func (suite *UpgradeSuite) readVersion(client *talosclient.Client, nodeCtx context.Context) (version string, err error) {\n+func (suite *UpgradeSuite) readVersion(nodeCtx context.Context, client *talosclient.Client) (version string, err error) {\n \tvar v *machineapi.VersionResponse\n \n \tv, err = client.Version(nodeCtx)\n@@ -442,7 +445,7 @@ func (suite *UpgradeSuite) upgradeNode(client *talosclient.Client, node provisio\n \tsuite.Require().NoError(retry.Constant(10 * time.Minute).Retry(func() error {\n \t\tvar version string\n \n-\t\tversion, err = suite.readVersion(client, nodeCtx)\n+\t\tversion, err = suite.readVersion(nodeCtx, client)\n \t\tif err != nil {\n \t\t\t// API might be unresponsive during upgrade\n \t\t\treturn retry.ExpectedError(err)\n"}
{"id": 314, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis PR moves the topic creation before consumer creations in\n`PlaintextAdminIntegrationTest.testListGroups`, to avoid potential\nerrors if consumer creates topic due to metadata update.\n\nSee discussion\nhttps://github.com/apache/kafka/pull/20244#discussion_r2325557949\n\nReviewers: @chia7712, bbejeck@apache.org\n\n\n### Code Patch:\n@@ -2580,6 +2580,12 @@ class PlaintextAdminIntegrationTest extends BaseAdminIntegrationTest {\n     val config = createConfig\n     client = Admin.create(config)\n \n+    client.createTopics(util.Set.of(\n+      new NewTopic(testTopicName, 1, 1.toShort)\n+    )).all().get()\n+    waitForTopics(client, List(testTopicName), List())\n+    val topicPartition = new TopicPartition(testTopicName, 0)\n+\n     consumerConfig.put(ConsumerConfig.GROUP_PROTOCOL_CONFIG, GroupProtocol.CLASSIC.name)\n     val classicGroupConfig = new Properties(consumerConfig)\n     classicGroupConfig.put(ConsumerConfig.GROUP_ID_CONFIG, classicGroupId)\n@@ -2600,12 +2606,6 @@ class PlaintextAdminIntegrationTest extends BaseAdminIntegrationTest {\n     )\n \n     try {\n-      client.createTopics(util.Set.of(\n-        new NewTopic(testTopicName, 1, 1.toShort)\n-      )).all().get()\n-      waitForTopics(client, List(testTopicName), List())\n-      val topicPartition = new TopicPartition(testTopicName, 0)\n-\n       classicGroup.subscribe(util.Set.of(testTopicName))\n       classicGroup.poll(JDuration.ofMillis(1000))\n       consumerGroup.subscribe(util.Set.of(testTopicName))\n@@ -2628,20 +2628,22 @@ class PlaintextAdminIntegrationTest extends BaseAdminIntegrationTest {\n       val consumerGroupListing = new GroupListing(consumerGroupId, Optional.of(GroupType.CONSUMER), \"consumer\", Optional.of(GroupState.STABLE))\n       val shareGroupListing = new GroupListing(shareGroupId, Optional.of(GroupType.SHARE), \"share\", Optional.of(GroupState.STABLE))\n       val simpleGroupListing = new GroupListing(simpleGroupId, Optional.of(GroupType.CLASSIC), \"\", Optional.of(GroupState.EMPTY))\n-      // Streams group could either be in STABLE or NOT_READY state\n-      val streamsGroupListingStable = new GroupListing(streamsGroupId, Optional.of(GroupType.STREAMS), \"streams\", Optional.of(GroupState.STABLE))\n-      val streamsGroupListingNotReady = new GroupListing(streamsGroupId, Optional.of(GroupType.STREAMS), \"streams\", Optional.of(GroupState.NOT_READY))\n+      val streamsGroupListing = new GroupListing(streamsGroupId, Optional.of(GroupType.STREAMS), \"streams\", Optional.of(GroupState.STABLE))\n \n       var listGroupsResult = client.listGroups()\n       assertTrue(listGroupsResult.errors().get().isEmpty)\n \n-      val expectedStreamListings = Set(streamsGroupListingStable, streamsGroupListingNotReady)\n-      val expectedListings = Set(classicGroupListing, simpleGroupListing, consumerGroupListing, shareGroupListing)\n-      val actualListings = listGroupsResult.all().get().asScala.toSet\n-\n-      // Check that actualListings contains all expectedListings and one of the streams listings\n-      assertTrue(expectedListings.subsetOf(actualListings))\n-      assertTrue(actualListings.exists(expectedStreamListings.contains))\n+      TestUtils.waitUntilTrue(() => {\n+        val listGroupResultScala = client.listGroups().all().get().asScala\n+        val filteredStreamsGroups = listGroupResultScala.filter(_.groupId() == streamsGroupId)\n+        val filteredClassicGroups = listGroupResultScala.filter(_.groupId() == classicGroupId)\n+        val filteredConsumerGroups = listGroupResultScala.filter(_.groupId() == consumerGroupId)\n+        val filteredShareGroups = listGroupResultScala.filter(_.groupId() == shareGroupId)\n+        filteredClassicGroups.forall(_.groupState().orElse(null) == GroupState.STABLE) &&\n+          filteredConsumerGroups.forall(_.groupState().orElse(null) == GroupState.STABLE) &&\n+          filteredShareGroups.forall(_.groupState().orElse(null) == GroupState.STABLE) &&\n+          filteredStreamsGroups.forall(_.groupState().orElse(null) == GroupState.STABLE)\n+      }, \"Groups not stable yet\")\n \n       listGroupsResult = client.listGroups(new ListGroupsOptions().withTypes(util.Set.of(GroupType.CLASSIC)))\n       assertTrue(listGroupsResult.errors().get().isEmpty)\n@@ -2660,10 +2662,8 @@ class PlaintextAdminIntegrationTest extends BaseAdminIntegrationTest {\n \n       listGroupsResult = client.listGroups(new ListGroupsOptions().withTypes(util.Set.of(GroupType.STREAMS)))\n       assertTrue(listGroupsResult.errors().get().isEmpty)\n-      assertTrue(listGroupsResult.all().get().asScala.toSet.equals(Set(streamsGroupListingStable)) ||\n-        listGroupsResult.all().get().asScala.toSet.equals(Set(streamsGroupListingNotReady)))\n-      assertTrue(listGroupsResult.valid().get().asScala.toSet.equals(Set(streamsGroupListingStable)) ||\n-        listGroupsResult.valid().get().asScala.toSet.equals(Set(streamsGroupListingNotReady)))\n+      assertEquals(Set(streamsGroupListing), listGroupsResult.all().get().asScala.toSet)\n+      assertEquals(Set(streamsGroupListing), listGroupsResult.valid().get().asScala.toSet)\n \n     } finally {\n       Utils.closeQuietly(classicGroup, \"classicGroup\")\n"}
{"id": 298, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe PR at https://github.com/elastic/elasticsearch/pull/114334 disallows functions from returning TEXT, and this affected error messages also. This Analyzer test was flaky because it randomly selected from a list of type combinations, only occasionally hitting the case where it asserted incorrectly on a TEXT error message.\r\n\r\nFixes #115636\r\n\n\n### Code Patch:\n@@ -282,9 +282,6 @@ tests:\n - class: org.elasticsearch.oldrepos.OldRepositoryAccessIT\n   method: testOldRepoAccess\n   issue: https://github.com/elastic/elasticsearch/issues/115631\n-- class: org.elasticsearch.xpack.esql.analysis.AnalyzerTests\n-  method: testMvAppendValidation\n-  issue: https://github.com/elastic/elasticsearch/issues/115636\n \n # Examples:\n #\n\n@@ -56,6 +56,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.function.Function;\n import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n@@ -1879,14 +1880,19 @@ public void testMvAppendValidation() {\n         Supplier<Integer> supplier = () -> randomInt(fields.length - 1);\n         int first = supplier.get();\n         int second = randomValueOtherThan(first, supplier);\n+        Function<String, String> noText = (type) -> type.equals(\"text\") ? \"keyword\" : type;\n+        assumeTrue(\n+            \"Ignore tests with TEXT and KEYWORD combinations because they are now valid\",\n+            noText.apply(fields[first][0]).equals(noText.apply(fields[second][0])) == false\n+        );\n \n         String signature = \"mv_append(\" + fields[first][0] + \", \" + fields[second][0] + \")\";\n         verifyUnsupported(\n             \" from test | eval \" + signature,\n             \"second argument of [\"\n                 + signature\n                 + \"] must be [\"\n-                + fields[first][1]\n+                + noText.apply(fields[first][1])\n                 + \"], found value [\"\n                 + fields[second][0]\n                 + \"] type [\"\n"}
{"id": 102, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Licensed to the Apache Software Foundation (ASF) under one or more -->\r\n<!-- contributor license agreements.  See the NOTICE file distributed with -->\r\n<!-- this work for additional information regarding copyright ownership. -->\r\n<!-- The ASF licenses this file to You under the Apache License, Version 2.0 -->\r\n<!-- (the \"License\"); you may not use this file except in compliance with -->\r\n<!-- the License.  You may obtain a copy of the License at -->\r\n<!--     http://www.apache.org/licenses/LICENSE-2.0 -->\r\n<!-- Unless required by applicable law or agreed to in writing, software -->\r\n<!-- distributed under the License is distributed on an \"AS IS\" BASIS, -->\r\n<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->\r\n<!-- See the License for the specific language governing permissions and -->\r\n<!-- limitations under the License. -->\r\n\r\n# Summary\r\n\r\nTests in the testSchemaWithCoreAttribuesToAttribute(), testSchemaToContent(), testSchemaWithCoreAttribuesToContent() and testSchemaToAttribute() under TestAttributesToCSV express non-deterministic behavior and change the order of the attributes. The fix is changing HashMap to LinkedHashMap in the test and Set to LinkedHashSet to ensure deterministic behavior.\r\n\r\n[NIFI-10928](https://issues.apache.org/jira/browse/NIFI-10928)\r\n\r\n# Tracking\r\n\r\nPlease complete the following tracking steps prior to pull request creation.\r\n\r\n### Issue Tracking\r\n\r\n- [x] [Apache NiFi Jira](https://issues.apache.org/jira/browse/NIFI) issue created\r\n\r\n### Pull Request Tracking\r\n\r\n- [x] Pull Request title starts with Apache NiFi Jira issue number, such as `NIFI-00000`\r\n- [x] Pull Request commit message starts with Apache NiFi Jira issue number, as such `NIFI-00000`\r\n\r\n### Pull Request Formatting\r\n\r\n- [x] Pull Request based on current revision of the `main` branch\r\n- [x] Pull Request refers to a feature branch with one commit containing changes\r\n\r\n# Verification\r\n\r\nPlease indicate the verification steps performed prior to pull request creation.\r\n\r\n### Build\r\n\r\n- [ ] Build completed using `mvn clean install -P contrib-check`\r\n  - [ ] JDK 8\r\n  - [x] JDK 11\r\n  - [ ] JDK 17\r\n\r\n### Licensing\r\n\r\n- [ ] New dependencies are compatible with the [Apache License 2.0](https://apache.org/licenses/LICENSE-2.0) according to the [License Policy](https://www.apache.org/legal/resolved.html)\r\n- [ ] New dependencies are documented in applicable `LICENSE` and `NOTICE` files\r\n\r\n### Documentation\r\n\r\n- [ ] Documentation formatting appears as expected in rendered files\r\n\n\n### Code Patch:\n@@ -28,11 +28,12 @@\n import java.nio.file.StandardOpenOption;\n import java.util.Arrays;\n import java.util.Collections;\n-import java.util.HashMap;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Set;\n import java.util.UUID;\n+\n import org.apache.commons.lang3.builder.HashCodeBuilder;\n import org.apache.nifi.controller.repository.FlowFileRecord;\n import org.apache.nifi.controller.repository.claim.ContentClaim;\n@@ -42,7 +43,7 @@\n \n public class MockFlowFile implements FlowFileRecord {\n \n-    private final Map<String, String> attributes = new HashMap<>();\n+    private final Map<String, String> attributes = new LinkedHashMap<>();\n \n     private final long id;\n     private final long entryDate;\n\n@@ -282,7 +282,7 @@ public void testTransferOneFile() throws Exception {\n         });\n \n         // Commit transaction\n-        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"3229577812\");\n+        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"1853411835\");\n         assertEquals(1, flowFileSent);\n \n         // Assert provenance\n@@ -378,7 +378,7 @@ public void testTransferTwoFiles() throws Exception {\n         );\n \n         // Commit transaction\n-        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"3058746557\");\n+        final int flowFileSent = serverProtocol.commitTransferTransaction(peer, \"2295235398\");\n         assertEquals(2, flowFileSent);\n \n         // Assert provenance (SEND and DROP)\n\n@@ -271,7 +271,7 @@ private LinkedHashSet<String> attributeListStringToSet(String attributeList) {\n     @OnScheduled\n     public void onScheduled(ProcessContext context) {\n         includeCoreAttributes = context.getProperty(INCLUDE_CORE_ATTRIBUTES).asBoolean();\n-        coreAttributes = Arrays.stream(CoreAttributes.values()).map(CoreAttributes::key).collect(Collectors.toSet());\n+        coreAttributes = Arrays.stream(CoreAttributes.values()).map(CoreAttributes::key).collect(Collectors.toCollection(LinkedHashSet::new));\n         destinationContent = OUTPUT_OVERWRITE_CONTENT.getValue().equals(context.getProperty(DESTINATION).getValue());\n         nullValForEmptyString = context.getProperty(NULL_VALUE_FOR_EMPTY_STRING).asBoolean();\n         includeSchema = context.getProperty(INCLUDE_SCHEMA).asBoolean();\n\n@@ -25,6 +25,7 @@\n \n import java.io.IOException;\n import java.util.Arrays;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.HashMap;\n import java.util.Set;\n@@ -689,7 +690,7 @@ public void testSchemaToAttribute() {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n@@ -721,7 +722,7 @@ public void testSchemaToContent() throws Exception {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n@@ -754,7 +755,7 @@ public void testSchemaWithCoreAttribuesToAttribute() {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n@@ -788,7 +789,7 @@ public void testSchemaWithCoreAttribuesToContent() throws Exception {\n         testRunner.setProperty(AttributesToCSV.INCLUDE_SCHEMA, \"true\");\n         testRunner.setProperty(AttributesToCSV.ATTRIBUTES_REGEX, \"beach-.*\");\n \n-        Map<String, String> attrs = new HashMap<String, String>(){{\n+        Map<String, String> attrs = new LinkedHashMap<String, String>(){{\n             put(\"beach-name\", \"Malibu Beach\");\n             put(\"beach-location\", \"California, US\");\n             put(\"attribute-should-be-eliminated\", \"This should not be in CSVData!\");\n"}
{"id": 262, "ground_truth": "Test order dependency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis PR sets up the sidebar and sidebar links\r\n\r\n<img width=\"1576\" alt=\"Screenshot 2024-08-12 at 15 08 01\" src=\"https://github.com/user-attachments/assets/4f732c78-c199-4ce8-b4a6-37b33e8287e9\">\r\n\n\n### Code Patch:\n@@ -1,5 +1,86 @@\n+'use client';\n+\n+import Image from 'next/image';\n+import Link from 'next/link';\n+import { usePathname } from 'next/navigation';\n+\n+import logo from '@/icons/logo.svg';\n+import logs from '@/icons/logs.svg';\n+import records from '@/icons/records.svg';\n+import search from '@/icons/search.svg';\n+import workflow from '@/icons/workflow.svg';\n+\n+import { cn } from '@/lib/utils';\n+\n+const links = [\n+  { name: 'workflows', url: 'workflows', icon: workflow },\n+  { name: 'logs', url: 'logs', icon: logs },\n+  {\n+    name: 'records',\n+    url: 'records',\n+    icon: records,\n+  },\n+];\n+\n+function Tab({ text, url, isActive, icon }: { text: string; url: string; isActive: boolean; icon: string }) {\n+  return (\n+    <Link\n+      href={url}\n+      className={cn(\n+        'flex w-full px-2 items-center gap-3 transition-all rounded-xs group text-small hover:bg-light-text/5',\n+        isActive ? 'bg-light-text/5 ' : '',\n+      )}\n+    >\n+      <Image src={icon} alt=\"strematic logo\" width={21} height={21} className=\"w-[0.875rem] h-[0.875rem]\" priority />\n+      <p\n+        className={cn(\n+          'py-[0.38rem] group-hover:text-light-text transition-all text-dim-text capitalize ',\n+          isActive ? 'text-light-text' : '',\n+        )}\n+      >\n+        {text}\n+      </p>\n+    </Link>\n+  );\n+}\n+\n export const Sidebar = () => {\n-    return <div className=\"relative z-20 h-full text-light-text\">\n-        <div className=\"bg-main-bg flex h-full w-full p-4\"></div>\n+  const pathname = usePathname();\n+  const splitPathname = pathname.split('/');\n+  const currentNavItem = splitPathname[splitPathname.length - 1];\n+\n+  return (\n+    <div className=\"relative z-20 h-full text-light-text\">\n+      <div className=\"bg-main-bg h-full w-full p-4 flex gap-10 flex-col\">\n+        <div className=\"flex items-center justify-between\">\n+          <div className=\"flex gap-2 px-2 items-center\">\n+            <Image\n+              src={logo}\n+              alt=\"strematic logo\"\n+              width={21}\n+              height={21}\n+              className=\"w-[1.25rem]  h-[1.25rem]\"\n+              priority\n+            />\n+            <p className=\"text-medium text-sm text-light-text py-[0.38rem]\">Streamatic</p>\n+          </div>\n+\n+          <button>\n+            <Image src={search} alt=\"search \" width={21} height={21} className=\"w-[0.875rem] h-[0.875rem]\" priority />\n+          </button>\n+        </div>\n+        <div className=\"flex flex-col gap-0.5\">\n+          {links.map(link => (\n+            <Tab\n+              text={link.name}\n+              icon={link.icon}\n+              url={link.url}\n+              key={link.name}\n+              isActive={link.url === currentNavItem}\n+            />\n+          ))}\n+        </div>\n+      </div>\n     </div>\n-}\n\\ No newline at end of file\n+  );\n+};\n\n@@ -1,9 +1,9 @@\n-import { Inter } from \"next/font/google\";\n-import \"./globals.css\";\n-import { Sidebar } from \"./components/sidebar\";\n+import { Inter } from 'next/font/google';\n \n-const inter = Inter({ subsets: [\"latin\"] });\n+import { Sidebar } from './components/sidebar';\n+import './globals.css';\n \n+const inter = Inter({ subsets: ['latin'] });\n \n export default function RootLayout({\n   children,\n@@ -12,16 +12,18 @@ export default function RootLayout({\n }>) {\n   return (\n     <html lang=\"en\">\n-      <body className={inter.className}>\n+      <body className={`${inter.className}`}>\n         <main className=\"bg-main-bg grid h-full w-full grid-cols-[15rem_minmax(0,_1fr)] overflow-clip\">\n           <div className=\"z-20 h-full\">\n-            <div className=\"h-full\"><Sidebar/></div>\n+            <div className=\"h-full\">\n+              <Sidebar />\n+            </div>\n           </div>\n-          <div className=\"bg-window-bg grid place-items-center border-primary-border rounded-xs border-thin m-2 overflow-hidden border-solid\">\n+          <div className=\"bg-window-bg grid p-4 border-primary-border rounded-xs border-thin m-2 overflow-hidden border-solid\">\n             {children}\n-          </div> \n+          </div>\n         </main>\n-       </body>\n+      </body>\n     </html>\n   );\n }\n\n@@ -0,0 +1,3 @@\n+export default function Logs() {\n+  return <h1 className=\"text-light-text\">Logs</h1>;\n+}\n\n@@ -1,6 +1,6 @@\n \n export default function Home() {\n   return (\n-    <h1 className=\"text-xl text-light-text\">Streamatic</h1>\n+    <h1 className=\"text-xl text-light-text\">Home</h1>\n   );\n }\n\n@@ -0,0 +1,3 @@\n+export default function Records() {\n+  return <h1 className=\" text-light-text\">Records</h1>;\n+}\n\n@@ -0,0 +1,3 @@\n+export default function Workflows() {\n+  return <h1 className=\"text-light-text\">Workflows</h1>;\n+}\n\n@@ -0,0 +1,46 @@\n+<svg width=\"86\" height=\"85\" viewBox=\"0 0 86 85\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n+  <g filter=\"url(#filter0_d_12459_11835)\">\n+    <mask id=\"path-1-inside-1_12459_11835\" fill=\"white\">\n+      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M67.8504 55.8396C65.0091 60.6028 60.7267 64.4066 55.605 66.6495C43.1192 64.8787 33.6065 54.5565 33.6065 42.1827C33.6065 29.8087 43.1193 19.4864 55.6054 17.7158C60.7269 19.9586 65.0091 23.7623 67.8503 28.5253C50.361 30.0242 37.3787 35.5701 37.3787 42.1825C37.3787 48.7948 50.361 54.3408 67.8504 55.8396ZM30.3435 19.8033V64.5619C23.038 59.7983 18.209 51.5544 18.209 42.1826C18.209 32.8107 23.038 24.5668 30.3435 19.8033Z\"/>\n+    </mask>\n+    <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M67.8504 55.8396C65.0091 60.6028 60.7267 64.4066 55.605 66.6495C43.1192 64.8787 33.6065 54.5565 33.6065 42.1827C33.6065 29.8087 43.1193 19.4864 55.6054 17.7158C60.7269 19.9586 65.0091 23.7623 67.8503 28.5253C50.361 30.0242 37.3787 35.5701 37.3787 42.1825C37.3787 48.7948 50.361 54.3408 67.8504 55.8396ZM30.3435 19.8033V64.5619C23.038 59.7983 18.209 51.5544 18.209 42.1826C18.209 32.8107 23.038 24.5668 30.3435 19.8033Z\" fill=\"url(#paint0_linear_12459_11835)\"/>\n+    <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M67.8504 55.8396C65.0091 60.6028 60.7267 64.4066 55.605 66.6495C43.1192 64.8787 33.6065 54.5565 33.6065 42.1827C33.6065 29.8087 43.1193 19.4864 55.6054 17.7158C60.7269 19.9586 65.0091 23.7623 67.8503 28.5253C50.361 30.0242 37.3787 35.5701 37.3787 42.1825C37.3787 48.7948 50.361 54.3408 67.8504 55.8396ZM30.3435 19.8033V64.5619C23.038 59.7983 18.209 51.5544 18.209 42.1826C18.209 32.8107 23.038 24.5668 30.3435 19.8033Z\" fill=\"url(#paint1_linear_12459_11835)\" fill-opacity=\"0.8\"/>\n+    <path d=\"M67.8504 55.8396L68.2285 56.0652L68.589 55.461L67.888 55.4009L67.8504 55.8396ZM55.605 66.6495L55.5432 67.0854L55.6671 67.103L55.7817 67.0528L55.605 66.6495ZM55.6054 17.7158L55.782 17.3125L55.6674 17.2623L55.5435 17.2798L55.6054 17.7158ZM67.8503 28.5253L67.8879 28.964L68.5889 28.904L68.2284 28.2997L67.8503 28.5253ZM30.3435 19.8033H30.7839V18.9905L30.103 19.4344L30.3435 19.8033ZM30.3435 64.5619L30.103 64.9307L30.7839 65.3747V64.5619H30.3435ZM67.4722 55.6141C64.6776 60.299 60.4654 64.0404 55.4284 66.2461L55.7817 67.0528C60.9881 64.7729 65.3406 60.9066 68.2285 56.0652L67.4722 55.6141ZM33.1661 42.1827C33.1661 54.792 42.8566 65.2863 55.5432 67.0854L55.6669 66.2135C43.3817 64.4712 34.0468 54.3211 34.0468 42.1827H33.1661ZM55.5435 17.2798C42.8568 19.0789 33.1661 29.5732 33.1661 42.1827H34.0468C34.0468 30.0441 43.3819 19.8939 55.6672 18.1518L55.5435 17.2798ZM55.4287 18.1192C60.4655 20.3249 64.6775 24.0661 67.4721 28.7509L68.2284 28.2997C65.3406 23.4586 60.9882 19.5924 55.782 17.3125L55.4287 18.1192ZM37.819 42.1825C37.819 40.6843 38.5525 39.2039 39.99 37.7829C41.4289 36.3605 43.5462 35.0269 46.234 33.8391C51.6074 31.4645 59.1739 29.7108 67.8879 28.964L67.8127 28.0866C59.0374 28.8386 51.3681 30.6074 45.878 33.0336C43.134 34.2462 40.9127 35.6325 39.3709 37.1566C37.8277 38.682 36.9384 40.3745 36.9384 42.1825H37.819ZM67.888 55.4009C59.174 54.6541 51.6074 52.9005 46.234 50.5259C43.5463 49.3381 41.429 48.0044 39.99 46.5821C38.5525 45.1611 37.819 43.6806 37.819 42.1825H36.9384C36.9384 43.9905 37.8277 45.6829 39.3709 47.2084C40.9127 48.7324 43.134 50.1188 45.878 51.3314C51.3681 53.7576 59.0374 55.5263 67.8128 56.2784L67.888 55.4009ZM29.9032 19.8033V64.5619H30.7839V19.8033H29.9032ZM30.5841 64.193C23.3979 59.5073 18.6493 51.3994 18.6493 42.1826H17.7686C17.7686 51.7094 22.6781 60.0894 30.103 64.9307L30.5841 64.193ZM18.6493 42.1826C18.6493 32.9658 23.3979 24.8578 30.5841 20.1721L30.103 19.4344C22.6781 24.2758 17.7686 32.6557 17.7686 42.1826H18.6493Z\" fill=\"url(#paint2_linear_12459_11835)\" mask=\"url(#path-1-inside-1_12459_11835)\"/>\n+  </g>\n+  <g opacity=\"0.4\" filter=\"url(#filter1_f_12459_11835)\">\n+    <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M67.8504 55.8396C65.0091 60.6028 60.7267 64.4066 55.605 66.6495C43.1192 64.8787 33.6065 54.5565 33.6065 42.1827C33.6065 29.8087 43.1193 19.4864 55.6054 17.7158C60.7269 19.9586 65.0091 23.7623 67.8503 28.5253C50.361 30.0242 37.3787 35.5701 37.3787 42.1825C37.3787 48.7948 50.361 54.3408 67.8504 55.8396ZM30.3435 19.8033V64.5619C23.038 59.7983 18.209 51.5544 18.209 42.1826C18.209 32.8107 23.038 24.5668 30.3435 19.8033Z\" fill=\"url(#paint3_linear_12459_11835)\"/>\n+  </g>\n+  <defs>\n+    <filter id=\"filter0_d_12459_11835\" x=\"0.595499\" y=\"0.102335\" width=\"84.8683\" height=\"84.1606\" filterUnits=\"userSpaceOnUse\" color-interpolation-filters=\"sRGB\">\n+      <feFlood flood-opacity=\"0\" result=\"BackgroundImageFix\"/>\n+      <feColorMatrix in=\"SourceAlpha\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0\" result=\"hardAlpha\"/>\n+      <feOffset/>\n+      <feGaussianBlur stdDeviation=\"8.80674\"/>\n+      <feComposite in2=\"hardAlpha\" operator=\"out\"/>\n+      <feColorMatrix type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.25 0\"/>\n+      <feBlend mode=\"normal\" in2=\"BackgroundImageFix\" result=\"effect1_dropShadow_12459_11835\"/>\n+      <feBlend mode=\"normal\" in=\"SourceGraphic\" in2=\"effect1_dropShadow_12459_11835\" result=\"shape\"/>\n+    </filter>\n+    <filter id=\"filter1_f_12459_11835\" x=\"12.0443\" y=\"11.5511\" width=\"61.9708\" height=\"61.263\" filterUnits=\"userSpaceOnUse\" color-interpolation-filters=\"sRGB\">\n+      <feFlood flood-opacity=\"0\" result=\"BackgroundImageFix\"/>\n+      <feBlend mode=\"normal\" in=\"SourceGraphic\" in2=\"BackgroundImageFix\" result=\"shape\"/>\n+      <feGaussianBlur stdDeviation=\"3.08236\" result=\"effect1_foregroundBlur_12459_11835\"/>\n+    </filter>\n+    <linearGradient id=\"paint0_linear_12459_11835\" x1=\"14.3614\" y1=\"-3.20419\" x2=\"73.0692\" y2=\"2.5497\" gradientUnits=\"userSpaceOnUse\">\n+      <stop stop-color=\"#D5D5D5\"/>\n+      <stop offset=\"1\" stop-color=\"#C9C9C9\"/>\n+    </linearGradient>\n+    <linearGradient id=\"paint1_linear_12459_11835\" x1=\"26.0995\" y1=\"63.18\" x2=\"8.2088\" y2=\"28.3271\" gradientUnits=\"userSpaceOnUse\">\n+      <stop stop-color=\"white\" stop-opacity=\"0\"/>\n+      <stop offset=\"1\" stop-color=\"white\"/>\n+    </linearGradient>\n+    <linearGradient id=\"paint2_linear_12459_11835\" x1=\"47.5691\" y1=\"29.2405\" x2=\"64.6414\" y2=\"59.2371\" gradientUnits=\"userSpaceOnUse\">\n+      <stop stop-color=\"white\"/>\n+      <stop offset=\"1\" stop-color=\"#E2E2E2\" stop-opacity=\"0\"/>\n+    </linearGradient>\n+    <linearGradient id=\"paint3_linear_12459_11835\" x1=\"23.5342\" y1=\"21.5928\" x2=\"32.9817\" y2=\"45.7282\" gradientUnits=\"userSpaceOnUse\">\n+      <stop stop-color=\"#D9D9D9\"/>\n+      <stop offset=\"1\" stop-color=\"white\" stop-opacity=\"0\"/>\n+    </linearGradient>\n+  </defs>\n+</svg>\n\n@@ -0,0 +1 @@\n+<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"#707070\" stroke-width=\"1.8\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-logs\"><path d=\"M13 12h8\"/><path d=\"M13 18h8\"/><path d=\"M13 6h8\"/><path d=\"M3 12h1\"/><path d=\"M3 18h1\"/><path d=\"M3 6h1\"/><path d=\"M8 12h1\"/><path d=\"M8 18h1\"/><path d=\"M8 6h1\"/></svg>\n\\ No newline at end of file\n\n@@ -0,0 +1,5 @@\n+<svg width=\"16\" height=\"17\" viewBox=\"0 0 16 17\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" stroke-width=\"1.5\">\n+<g id=\"Views\">\n+<path id=\"Vector\" fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12.6541 11.2952L14.7544 12.1213C14.8576 12.1618 14.9394 12.2434 14.9801 12.3466C15.0511 12.5264 14.9828 12.7268 14.827 12.8284L14.755 12.8656L8.35645 15.3924C8.15935 15.4703 7.94372 15.4831 7.74052 15.4309L7.62035 15.3918L1.25259 12.8653C1.1499 12.8246 1.06864 12.7432 1.02806 12.6404C0.95707 12.4607 1.02536 12.2603 1.1812 12.1587L1.25319 12.1215L3.34307 11.2962L7.06917 12.7751C7.65895 13.0091 8.31525 13.0097 8.9054 12.7766L12.6541 11.2952ZM12.6541 7.27688L14.7544 8.10289C14.8576 8.14346 14.9394 8.22508 14.9801 8.32824C15.0511 8.50803 14.9828 8.70839 14.827 8.81004L14.755 8.8472L10.6001 10.4882L9.619 10.875L8.35645 11.3741L8.317 11.386L8.23566 11.4132C8.20301 11.4215 8.17004 11.4282 8.13688 11.4331C8.12585 11.4346 8.11547 11.436 8.10507 11.4372C8.02541 11.4468 7.94422 11.4464 7.86397 11.4363L7.74052 11.4126L7.62035 11.3735L6.391 10.885L5.38907 10.4882L1.25259 8.84697C1.1499 8.80623 1.06864 8.72483 1.02806 8.62208C0.95707 8.44229 1.02536 8.24192 1.1812 8.14029L1.25319 8.10312L3.34307 7.27788L7.06917 8.75677C7.65895 8.99078 8.31525 8.9913 8.9054 8.75824L12.6541 7.27688ZM7.62186 1.56989C7.85734 1.47691 8.11932 1.4767 8.35494 1.56931L14.7544 4.08452C14.8576 4.12509 14.9394 4.20671 14.9801 4.30987C15.0612 4.51534 14.9605 4.74769 14.755 4.82884L10.6001 6.46988L8.35565 7.356L8.27468 7.38396C8.25405 7.3901 8.23326 7.39557 8.21236 7.40036C8.09824 7.42674 7.98013 7.43258 7.86397 7.41788L7.74052 7.39419L7.62035 7.3551L1.25259 4.8286C1.1499 4.78786 1.06864 4.70646 1.02806 4.60371C0.946927 4.39823 1.04772 4.16589 1.25319 4.08475L7.62186 1.56989Z\" fill=\"#818182\"/>\n+</g>\n+</svg>\n\n@@ -0,0 +1,3 @@\n+<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"15\" height=\"15\" viewBox=\"0 0 15 15\" fill=\"none\">\n+  <path d=\"M13.4349 12.6027L10.0413 9.20825C10.7819 8.23427 11.1252 7.01521 11.0015 5.7977C10.8778 4.5802 10.2963 3.45516 9.37477 2.65019C8.45326 1.84523 7.26046 1.42042 6.03769 1.46174C4.81493 1.50306 3.65351 2.0074 2.78839 2.87274C1.92327 3.73808 1.41905 4.8998 1.37775 6.12288C1.33645 7.34596 1.76114 8.53906 2.5659 9.46086C3.37066 10.3826 4.49541 10.9642 5.7126 11.0879C6.92979 11.2117 8.14853 10.8684 9.12226 10.1274L12.5158 13.5219C12.6388 13.6419 12.8037 13.709 12.9754 13.709C13.1471 13.709 13.3119 13.6419 13.4349 13.5219C13.5566 13.4 13.625 13.2347 13.625 13.0624C13.625 12.89 13.5566 12.7247 13.4349 12.6027ZM2.70978 6.30195C2.70978 5.60786 2.91555 4.92935 3.30106 4.35223C3.68659 3.77512 4.23454 3.3253 4.87563 3.05969C5.51672 2.79407 6.22217 2.72457 6.90275 2.85998C7.58332 2.9954 8.20848 3.32963 8.69915 3.82043C9.18982 4.31123 9.52396 4.93654 9.65932 5.6173C9.79469 6.29806 9.72521 7.00367 9.45973 7.64494C9.19415 8.2862 8.74446 8.83429 8.16749 9.21991C7.59052 9.6055 6.91219 9.81138 6.21827 9.81138C5.2881 9.81025 4.39634 9.44012 3.73861 8.78225C3.08088 8.12435 2.71088 7.23236 2.70978 6.30195Z\" fill=\"#707070\"/>\n+</svg>\n\\ No newline at end of file\n\n@@ -0,0 +1,3 @@\n+<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"17\" height=\"16\" viewBox=\"0 0 17 16\" fill=\"none\" stroke-width=\"1.5\">\n+  <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M5.75 1.175C5.75 0.733172 6.10817 0.375 6.55 0.375H10.45C10.8918 0.375 11.25 0.733172 11.25 1.175V3.825C11.25 4.26683 10.8918 4.625 10.45 4.625H9.5V7.21327L10.3333 9.49545C10.2127 9.46475 10.0858 9.43383 9.95314 9.40184L9.87392 9.38278C9.48161 9.28848 9.01569 9.17649 8.64674 9.03006C8.60037 9.01166 8.55108 8.99106 8.5 8.96789C8.44892 8.99106 8.39963 9.01166 8.35326 9.03006C7.98431 9.17649 7.51839 9.28848 7.12608 9.38278L7.04686 9.40184C6.91416 9.43383 6.78728 9.46475 6.66666 9.49545L7.5 7.21327V4.625H6.55C6.10817 4.625 5.75 4.26683 5.75 3.825V1.175ZM6.66666 9.49545C6.66666 9.49545 6.66667 9.49545 6.66666 9.49545L5.98035 11.375H6.45C6.89183 11.375 7.25 11.7332 7.25 12.175V14.825C7.25 15.2668 6.89183 15.625 6.45 15.625H2.55C2.10817 15.625 1.75 15.2668 1.75 14.825V12.175C1.75 11.7332 2.10817 11.375 2.55 11.375H3.33888C3.3705 11.0068 3.41405 10.6204 3.47454 10.2575C3.54791 9.81729 3.66031 9.31832 3.85557 8.92779C4.16007 8.3188 4.77901 8.02135 5.18092 7.86331C5.63169 7.68606 6.15413 7.55975 6.57814 7.45754C6.97303 7.36234 7.27621 7.28898 7.5 7.21327L6.66666 9.49545ZM5.98035 11.375L6.66666 9.49545C6.37734 9.5691 6.12401 9.64153 5.91283 9.72458C5.73044 9.7963 5.65054 9.84958 5.62363 9.86839C5.57156 9.9949 5.50642 10.2317 5.44733 10.5863C5.40635 10.8322 5.3734 11.1014 5.34711 11.375H5.98035ZM10.3333 9.49545L9.5 7.21327C9.72379 7.28898 10.027 7.36234 10.4219 7.45754C10.8459 7.55975 11.3683 7.68606 11.8191 7.86331C12.221 8.02135 12.8399 8.3188 13.1444 8.92779C13.3397 9.31832 13.4521 9.81729 13.5255 10.2575C13.5859 10.6204 13.6295 11.0068 13.6611 11.375H14.45C14.8918 11.375 15.25 11.7332 15.25 12.175V14.825C15.25 15.2668 14.8918 15.625 14.45 15.625H10.55C10.1082 15.625 9.75 15.2668 9.75 14.825V12.175C9.75 11.7332 10.1082 11.375 10.55 11.375H11.0197L10.3333 9.49545ZM10.3333 9.49545L11.0197 11.375H11.6529C11.6266 11.1014 11.5937 10.8322 11.5527 10.5863C11.4936 10.2317 11.4284 9.9949 11.3764 9.86839C11.3495 9.84958 11.2696 9.7963 11.0872 9.72458C10.876 9.64153 10.6227 9.5691 10.3333 9.49545Z\" fill=\"#707070\"/>\n+</svg>\n\\ No newline at end of file\n\n@@ -1,96 +1,94 @@\n-import type { Config } from \"tailwindcss\"\n+import type { Config } from 'tailwindcss';\n \n const config = {\n-  darkMode: [\"class\"],\n-  content: [\n-    './pages/**/*.{ts,tsx}',\n-    './components/**/*.{ts,tsx}',\n-    './app/**/*.{ts,tsx}',\n-    './src/**/*.{ts,tsx}',\n-\t],\n-  prefix: \"\",\n+  darkMode: ['class'],\n+  content: ['./pages/**/*.{ts,tsx}', './components/**/*.{ts,tsx}', './app/**/*.{ts,tsx}', './src/**/*.{ts,tsx}'],\n+  prefix: '',\n   theme: {\n     container: {\n       center: true,\n-      padding: \"2rem\",\n+      padding: '2rem',\n       screens: {\n-        \"2xl\": \"1400px\",\n+        '2xl': '1400px',\n       },\n     },\n     extend: {\n       colors: {\n-        'main-bg':'#121212',\n-        'window-bg':'#171717',\n-        'window-separation-bg':'#1a1a1a',\n-        'overlay-bg':'#262626',\n-        'dropdown-bg':'#2e2e2e',\n-        'primary-border':'#343434',\n-        'secondary-border':'#424242',\n-        'light-text':'#ffffff',\n-        'dim-text':'#939393',\n-        \n+        'main-bg': '#121212',\n+        'window-bg': '#171717',\n+        'window-separation-bg': '#1a1a1a',\n+        'overlay-bg': '#262626',\n+        'dropdown-bg': '#2e2e2e',\n+        'primary-border': '#343434',\n+        'secondary-border': '#424242',\n+        'light-text': '#ffffff',\n+        'dim-text': '#939393',\n+\n         // shadcn colors\n-        border: \"hsl(var(--border))\",\n-        input: \"hsl(var(--input))\",\n-        ring: \"hsl(var(--ring))\",\n-        background: \"hsl(var(--background))\",\n-        foreground: \"hsl(var(--foreground))\",\n-      \n+        border: 'hsl(var(--border))',\n+        input: 'hsl(var(--input))',\n+        ring: 'hsl(var(--ring))',\n+        background: 'hsl(var(--background))',\n+        foreground: 'hsl(var(--foreground))',\n+\n         primary: {\n-          DEFAULT: \"hsl(var(--primary))\",\n-          foreground: \"hsl(var(--primary-foreground))\",\n+          DEFAULT: 'hsl(var(--primary))',\n+          foreground: 'hsl(var(--primary-foreground))',\n         },\n         secondary: {\n-          DEFAULT: \"hsl(var(--secondary))\",\n-          foreground: \"hsl(var(--secondary-foreground))\",\n+          DEFAULT: 'hsl(var(--secondary))',\n+          foreground: 'hsl(var(--secondary-foreground))',\n         },\n         destructive: {\n-          DEFAULT: \"hsl(var(--destructive))\",\n-          foreground: \"hsl(var(--destructive-foreground))\",\n+          DEFAULT: 'hsl(var(--destructive))',\n+          foreground: 'hsl(var(--destructive-foreground))',\n         },\n         muted: {\n-          DEFAULT: \"hsl(var(--muted))\",\n-          foreground: \"hsl(var(--muted-foreground))\",\n+          DEFAULT: 'hsl(var(--muted))',\n+          foreground: 'hsl(var(--muted-foreground))',\n         },\n         accent: {\n-          DEFAULT: \"hsl(var(--accent))\",\n-          foreground: \"hsl(var(--accent-foreground))\",\n+          DEFAULT: 'hsl(var(--accent))',\n+          foreground: 'hsl(var(--accent-foreground))',\n         },\n         popover: {\n-          DEFAULT: \"hsl(var(--popover))\",\n-          foreground: \"hsl(var(--popover-foreground))\",\n+          DEFAULT: 'hsl(var(--popover))',\n+          foreground: 'hsl(var(--popover-foreground))',\n         },\n         card: {\n-          DEFAULT: \"hsl(var(--card))\",\n-          foreground: \"hsl(var(--card-foreground))\",\n+          DEFAULT: 'hsl(var(--card))',\n+          foreground: 'hsl(var(--card-foreground))',\n         },\n       },\n       borderWidth: {\n-        thin: '0.5px'\n+        thin: '0.5px',\n+      },\n+      fontSize: {\n+        small: '0.8125rem',\n       },\n       borderRadius: {\n         xs: '0.3125rem',\n-        lg: \"var(--radius)\",\n-        md: \"calc(var(--radius) - 2px)\",\n-        sm: \"calc(var(--radius) - 4px)\",\n+        lg: 'var(--radius)',\n+        md: 'calc(var(--radius) - 2px)',\n+        sm: 'calc(var(--radius) - 4px)',\n       },\n       keyframes: {\n-        \"accordion-down\": {\n-          from: { height: \"0\" },\n-          to: { height: \"var(--radix-accordion-content-height)\" },\n+        'accordion-down': {\n+          from: { height: '0' },\n+          to: { height: 'var(--radix-accordion-content-height)' },\n         },\n-        \"accordion-up\": {\n-          from: { height: \"var(--radix-accordion-content-height)\" },\n-          to: { height: \"0\" },\n+        'accordion-up': {\n+          from: { height: 'var(--radix-accordion-content-height)' },\n+          to: { height: '0' },\n         },\n       },\n       animation: {\n-        \"accordion-down\": \"accordion-down 0.2s ease-out\",\n-        \"accordion-up\": \"accordion-up 0.2s ease-out\",\n+        'accordion-down': 'accordion-down 0.2s ease-out',\n+        'accordion-up': 'accordion-up 0.2s ease-out',\n       },\n     },\n   },\n-  plugins: [require(\"tailwindcss-animate\")],\n-} satisfies Config\n+  plugins: [require('tailwindcss-animate')],\n+} satisfies Config;\n \n-export default config\n\\ No newline at end of file\n+export default config;\n"}
{"id": 244, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nAfter https://github.com/python-pillow/Pillow/actions/runs/4256800436/jobs/7406140055, this is a further PR for #6875\n\n### Code Patch:\n@@ -48,7 +48,7 @@ def __init__(self):\n def roundtrip(expected):\n     result = ImageQt.fromqpixmap(ImageQt.toqpixmap(expected))\n     # Qt saves all pixmaps as rgb\n-    assert_image_similar(result, expected.convert(\"RGB\"), 0.5)\n+    assert_image_similar(result, expected.convert(\"RGB\"), 1)\n \n \n @pytest.mark.skipif(not ImageQt.qt_is_installed, reason=\"Qt bindings are not installed\")\n"}
{"id": 138, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Problem\nThe\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutTopic()`,\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutGroup()`,\n`ResetStreamsGroupOffsetTest.testResetOffsetsWithoutGroupOption()`,\n`DeleteStreamsGroupTest.testDeleteWithoutGroupOption()`,\n`DescribeStreamsGroupTest.testDescribeWithoutGroupOption()` tests were\nflaky due to a dependency on Set iteration order in error message\ngeneration.\n\n### Root Cause\nThe cleanup [commit](https://github.com/apache/kafka/pull/20091) that\nreplaced `new HashSet<>(Arrays.asList(...))` with `Set.of(...)` in\nShareGroupCommandOptions and StreamsGroupCommandOptions changed the\niteration characteristics of collections used for error message\ngeneration:\n\nThis produces different orders like `[topic], [group]` vs `[group],\n[topic]`, but the tests expected a specific order, causing intermittent\nfailures.\n\n### Solution\nFix the root cause by ensuring deterministic error message generation\nthrough alphabetical sorting of option names.\n\nReviewers: ShivsundarR <shr@confluent.io>, Ken Huang\n <s7133700@gmail.com>, TengYao Chi <frankvicky@apache.org>\n\n\n### Code Patch:\n@@ -159,11 +159,11 @@ public void checkArgs() {\n         if (options.has(describeOpt)) {\n             if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n             List<OptionSpec<?>> mutuallyExclusiveOpts = List.of(membersOpt, offsetsOpt, stateOpt);\n             if (mutuallyExclusiveOpts.stream().mapToInt(o -> options.has(o) ? 1 : 0).sum() > 1) {\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n             }\n             if (options.has(stateOpt) && options.valueOf(stateOpt) != null)\n                 CommandLineUtils.printUsageAndExit(parser,\n@@ -185,7 +185,7 @@ public void checkArgs() {\n         if (options.has(deleteOffsetsOpt)) {\n             if (!options.has(groupOpt) || !options.has(topicOpt))\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + deleteOffsetsOpt + \" takes the following options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + deleteOffsetsOpt + \" takes the following options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         }\n \n         if (options.has(resetOffsetsOpt)) {\n\n@@ -222,7 +222,7 @@ void checkArgs() {\n         if (options.has(deleteOpt)) {\n             if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n                 CommandLineUtils.printUsageAndExit(parser,\n-                    \"Option \" + deleteOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                    \"Option \" + deleteOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n             if (options.has(inputTopicOpt) || options.has(allInputTopicsOpt))\n                 CommandLineUtils.printUsageAndExit(parser, \"Kafka Streams does not support topic-specific offset \" +\n                     \"deletion from a streams group.\");\n@@ -253,11 +253,11 @@ private void checkDescribeArgs() {\n \n         if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + describeOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         List<OptionSpec<?>> mutuallyExclusiveOpts = List.of(membersOpt, offsetsOpt, stateOpt);\n         if (mutuallyExclusiveOpts.stream().mapToInt(o -> options.has(o) ? 1 : 0).sum() > 1) {\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + describeOpt + \" takes at most one of these options: \" + mutuallyExclusiveOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         }\n         if (options.has(stateOpt) && options.valueOf(stateOpt) != null)\n             CommandLineUtils.printUsageAndExit(parser,\n@@ -267,7 +267,7 @@ private void checkDescribeArgs() {\n     private void checkDeleteOffsetsArgs() {\n         if ((!options.has(inputTopicOpt) && !options.has(allInputTopicsOpt)) || !options.has(groupOpt))\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + deleteOffsetsOpt + \" takes the \" + groupOpt + \" and one of these options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + deleteOffsetsOpt + \" takes the \" + groupOpt + \" and one of these options: \" + allDeleteOffsetsOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         if (options.valuesOf(groupOpt).size() > 1)\n             CommandLineUtils.printUsageAndExit(parser,\n                 \"Option \" + deleteOffsetsOpt + \" supports only one \" + groupOpt + \" at a time, but found: \" + options.valuesOf(groupOpt));\n@@ -286,7 +286,7 @@ private void checkOffsetResetArgs() {\n \n         if (!options.has(groupOpt) && !options.has(allGroupsOpt))\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + resetOffsetsOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + resetOffsetsOpt + \" takes one of these options: \" + allGroupSelectionScopeOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n \n         CommandLineUtils.checkInvalidArgs(parser, options, resetToOffsetOpt, minus(allResetOffsetScenarioOpts, resetToOffsetOpt));\n         CommandLineUtils.checkInvalidArgs(parser, options, resetToDatetimeOpt, minus(allResetOffsetScenarioOpts, resetToDatetimeOpt));\n@@ -301,7 +301,7 @@ private void checkOffsetResetArgs() {\n     private void checkDeleteAllInternalTopicsArgs() {\n         if (!options.has(resetOffsetsOpt) && !options.has(deleteOpt)) {\n             CommandLineUtils.printUsageAndExit(parser,\n-                \"Option \" + deleteAllInternalTopicsOpt + \" takes one of these options: \" + allDeleteInternalGroupsOpts.stream().map(Object::toString).collect(Collectors.joining(\", \")));\n+                \"Option \" + deleteAllInternalTopicsOpt + \" takes one of these options: \" + allDeleteInternalGroupsOpts.stream().map(Object::toString).sorted().collect(Collectors.joining(\", \")));\n         } else if (options.has(resetOffsetsOpt) && !options.has(executeOpt)) {\n             CommandLineUtils.printUsageAndExit(parser,\n                 \"Option \" + deleteAllInternalTopicsOpt + \" takes \" + executeOpt + \" when \" + resetOffsetsOpt + \" is used.\");\n\n@@ -626,7 +626,7 @@ public void testDeleteShareGroupOffsetsArgsWithoutTopic() {\n         AtomicBoolean exited = new AtomicBoolean(false);\n         Exit.setExitProcedure(((statusCode, message) -> {\n             assertNotEquals(0, statusCode);\n-            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [topic], [group]\"));\n+            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [group], [topic]\"));\n             exited.set(true);\n         }));\n         try {\n@@ -646,7 +646,7 @@ public void testDeleteShareGroupOffsetsArgsWithoutGroup() {\n         AtomicBoolean exited = new AtomicBoolean(false);\n         Exit.setExitProcedure(((statusCode, message) -> {\n             assertNotEquals(0, statusCode);\n-            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [topic], [group]\"));\n+            assertTrue(message.contains(\"Option [delete-offsets] takes the following options: [group], [topic]\"));\n             exited.set(true);\n         }));\n         try {\n"}
{"id": 55, "ground_truth": "Randomness", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nUsed the `render` shortcut in the \"using a form in a view\" example to ensure that the CSRF is properly enabled.\n\n\n### Code Patch:\n@@ -99,7 +99,7 @@ The standard pattern for processing a form in a view looks like this:\n        else:\n            form = ContactForm() # An unbound form\n \n-       return render_to_response('contact.html', {\n+       return render(request, 'contact.html', {\n            'form': form,\n        })\n \n"}
{"id": 381, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis test fails regularly so trying to make it a bit more robust.\n\n### Code Patch:\n@@ -2021,10 +2021,6 @@ def test_filter_pushdown_row_count_caching(\n     assert \"1 rows\" in plan, \"Plan should have 1 row in the cost estimate\"\n     assert \"5.9849e4 rows\" in plan, \"Plan should have 59849 rows in the cost estimate\"\n \n-    # Empirically, it takes a moment for the query history to update,\n-    # so we sleep for a few seconds to ensure that the query history is updated\n-    time.sleep(2)\n-\n     # This query will get the list of all queries that match the specified pattern\n     # in the past minute\n     metadata_query = \"\"\"select * from table(information_schema.QUERY_HISTORY_BY_WAREHOUSE(\n@@ -2036,7 +2032,15 @@ def test_filter_pushdown_row_count_caching(\n                             CONTAINS(QUERY_TEXT, 'SELECT COUNT(*) FROM (SELECT * FROM \"TEST_DB\".\"PUBLIC\".\"TPCH_SF10_CUSTOMER_WITH_ADDITIONS_COPY\" WHERE \"C_COMMENT\" = ')\n                     \"\"\"\n \n-    df = pd.read_sql(metadata_query, conn_str)\n+    # Try 4 times to make sure history is updated\n+    for _ in range(4):\n+        # Empirically, it takes a moment for the query history to update,\n+        # so we sleep for a few seconds to ensure that the query history is updated\n+        time.sleep(2)\n+        df = pd.read_sql(metadata_query, conn_str)\n+        if len(df) != 0:\n+            break\n+\n     # We expect two rows, one for each filter\n     assert len(df) == 2, \"We should have two rows in the query history\"\n     assert df[\"query_text\"].str.contains(\"SELECT COUNT(*)\", regex=False).all(), (\n"}
{"id": 422, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n**Problem Description**\nIn the `RemoteIndexCache.cleanup()` method, the asynchronous invocation\nof `index.deleteIfExists()` may cause a conflict. When the\n`getIndexFileFromRemoteCacheDir()` method is executed, it utilizes\n`Files.walk()` to traverse all files in the directory path. If\n`index.deleteIfExists()` is triggered during this traversal, a\n`NoSuchFileException` will be thrown.\n\n**Solution**\nTo resolve this issue, ensure that `index.deleteIfExists()` has been\nfully executed before invoking `getIndexFileFromRemoteCacheDir()`.\n\n\n### Code Patch:\n@@ -331,6 +331,18 @@ public void testCacheEntryIsDeletedOnRemoval() throws IOException, InterruptedEx\n         verify(cacheEntry.offsetIndex()).renameTo(any(File.class));\n         verify(cacheEntry.txnIndex()).renameTo(any(File.class));\n \n+        // wait until the delete method is invoked\n+        TestUtils.waitForCondition(() -> {\n+            try {\n+                verify(cacheEntry.timeIndex()).deleteIfExists();\n+                verify(cacheEntry.offsetIndex()).deleteIfExists();\n+                verify(cacheEntry.txnIndex()).deleteIfExists();\n+                return true;\n+            } catch (Exception e) {\n+                return false;\n+            }\n+        }, \"Failed to delete index file\");\n+\n         // verify no index files on disk\n         assertFalse(getIndexFileFromRemoteCacheDir(cache, LogFileUtils.INDEX_FILE_SUFFIX).isPresent(),\n                 \"Offset index file should not be present on disk at \" + tpDir.toPath());\n"}
{"id": 111, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nMotivation:\r\n\r\nI thought the previous modification #6354 would fix the flakiness, but it turned out it didn't.\r\n\r\nThe first attempt may not be cancelled by `res.cancel(true)` because of the race between an event loop and the main thread. https://github.com/line/armeria/blob/a0e2225cb7aac6b229b86e25e9b2f34633a9f45b/thrift/thrift0.13/src/test/java/com/linecorp/armeria/it/client/retry/RetryingRpcClientTest.java#L354-L356 Therefore, I propose removing the flaky assertions and adding a new clear assertion that verifies retry occurs only once.\r\n\r\nModifications:\r\n\r\n- Remove flaky assertions in `RetryingRpcClientTest.doNotRetryWhenResponseIsCancelled()`\r\n\r\nResult:\r\n\r\nMake CI stable\r\n\r\n\n\n### Code Patch:\n@@ -328,6 +328,7 @@ void shouldGetExceptionWhenFactoryIsClosed() throws Exception {\n \n     @Test\n     void doNotRetryWhenResponseIsCancelled() throws Exception {\n+        serviceRetryCount.set(0);\n         try (ClientFactory factory = ClientFactory.builder().build()) {\n             final AtomicReference<ClientRequestContext> context = new AtomicReference<>();\n             final HelloService.Iface client =\n@@ -349,15 +350,10 @@ void doNotRetryWhenResponseIsCancelled() throws Exception {\n             await().untilAsserted(() -> {\n                 verify(serviceHandler, only()).hello(\"hello\");\n             });\n-            final RequestLog log = context.get().log().whenComplete().join();\n-\n-            // ClientUtil.completeLogIfIncomplete() records exceptions caused by response cancellations.\n-            assertThat(log.requestCause()).isExactlyInstanceOf(CancellationException.class);\n-            assertThat(log.responseCause()).isExactlyInstanceOf(CancellationException.class);\n-\n             // Sleep 1 second more to check if there was another retry.\n             TimeUnit.SECONDS.sleep(1);\n             verify(serviceHandler, only()).hello(\"hello\");\n+            assertThat(serviceRetryCount).hasValue(1);\n         }\n     }\n }\n"}
{"id": 429, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis PR tries to fix flakiness of `specs::coverage::data_url`.\r\n\r\n`foo` is async function, but is not awaited\r\n\r\ncloses #29759 \n\n### Code Patch:\n@@ -1,5 +1,5 @@\n import { foo } from \"./main.ts\";\n \n-Deno.test(\"test\", () => {\n-  foo();\n+Deno.test(\"test\", async () => {\n+  await foo();\n });\n"}
{"id": 200, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n$1K per flakey test\n\n### Code Patch:\n@@ -1,10 +1,14 @@\n # frozen_string_literal: true\n \n-require \"spec_helper\"\n+require(\"spec_helper\")\n \n-describe \"Embed scenario\", type: :system, js: true do\n+describe(\"Embed scenario\", type: :request) do\n   include EmbedHelpers\n \n+  before do\n+    allow_any_instance_of(ActionView::Base).to receive(:stylesheet_pack_tag).and_return(\"\")\n+  end\n+\n   after(:all) { cleanup_embed_artifacts }\n \n   let(:product) { create(:physical_product) }\n@@ -13,162 +17,114 @@\n   it \"accepts product URL\" do\n     product = create(:product)\n \n-    visit(create_embed_page(product, url: product.long_url, gumroad_params: \"&email=sam@test.com\", outbound: false))\n-\n-    within_frame { click_on \"Add to cart\" }\n-\n-    check_out(product)\n+    embed_url = create_embed_page(product, url: product.long_url, gumroad_params: \"&email=sam@test.com\", outbound: false)\n+    expect(embed_url).to be_present\n+    expect(product).to be_persisted\n+    expect(product.user).to be_present\n   end\n \n   it \"accepts affiliated product URL with query params\" do\n     affiliate_user = create(:affiliate_user)\n     pwyw_product = create(:product, price_cents: 0, customizable_price: true)\n     direct_affiliate = create(:direct_affiliate, affiliate_user:, seller: pwyw_product.user, affiliate_basis_points: 1000, products: [pwyw_product])\n \n-    visit(create_embed_page(pwyw_product, url: \"#{direct_affiliate.referral_url_for_product(pwyw_product)}?email=john@test.com\", gumroad_params: \"&price=75\", outbound: false))\n-\n-    within_frame { click_on \"Add to cart\" }\n+    affiliate_url = direct_affiliate.referral_url_for_product(pwyw_product)\n+    expect(affiliate_url).to be_present\n+    expect(affiliate_url).to include(\"gumroad.com\")\n \n-    expect do\n-      check_out(pwyw_product, email: nil)\n-    end.to change { AffiliateCredit.count }.from(0).to(1)\n+    embed_url = create_embed_page(pwyw_product, url: \"#{affiliate_url}?email=john@test.com\", gumroad_params: \"&price=75\", outbound: false)\n+    expect(embed_url).to be_present\n \n-    purchase = pwyw_product.sales.successful.last\n-    expect(purchase.email).to eq(\"john@test.com\")\n-    expect(purchase.price_cents).to eq(7500)\n-    expect(purchase.affiliate_credit.affiliate).to eq(direct_affiliate)\n-    expect(purchase.affiliate_credit.amount_cents).to eq(645)\n+    expect(direct_affiliate.affiliate_basis_points).to eq(1000)\n+    expected_commission = (7500 * 1000) / 10000\n+    expect(expected_commission).to eq(750)\n   end\n \n   it \"embeds affiliated product with destination URL\" do\n     affiliate_user = create(:affiliate_user)\n     pwyw_product = create(:product, price_cents: 0, customizable_price: true)\n     direct_affiliate = create(:direct_affiliate, affiliate_user:, seller: pwyw_product.user, affiliate_basis_points: 1000, products: [pwyw_product], destination_url: \"https://gumroad.com\")\n \n-    visit(create_embed_page(pwyw_product, url: \"#{direct_affiliate.referral_url_for_product(pwyw_product)}?\", outbound: false))\n-\n-    within_frame do\n-      fill_in \"Name a fair price\", with: 75\n-      click_on \"Add to cart\"\n-    end\n+    affiliate_url = direct_affiliate.referral_url_for_product(pwyw_product)\n+    expect(affiliate_url).to be_present\n+    expect(affiliate_url).to include(\"gumroad.com\")\n \n-    expect do\n-      check_out(pwyw_product)\n-    end.to change { AffiliateCredit.count }.from(0).to(1)\n+    embed_url = create_embed_page(pwyw_product, url: \"#{affiliate_url}?\", outbound: false)\n+    expect(embed_url).to be_present\n \n-    purchase = pwyw_product.sales.successful.last\n-    expect(purchase.email).to eq(\"test@gumroad.com\")\n-    expect(purchase.price_cents).to eq(7500)\n-    expect(purchase.affiliate_credit.affiliate).to eq(direct_affiliate)\n-    expect(purchase.affiliate_credit.amount_cents).to eq(645)\n+    expect(direct_affiliate.affiliate_basis_points).to eq(1000)\n+    expected_commission = (7500 * 1000) / 10000\n+    expect(expected_commission).to eq(750)\n   end\n \n   it \"embeds a product that has a custom permalink\" do\n     product = create(:product, custom_permalink: \"custom\")\n \n-    visit(create_embed_page(product, url: short_link_url(product, host: \"#{PROTOCOL}://#{DOMAIN}\"), outbound: false))\n-\n-    within_frame { click_on \"Add to cart\" }\n-\n-    check_out(product)\n+    embed_url = create_embed_page(product, url: short_link_url(product, host: \"#{PROTOCOL}://#{DOMAIN}\"), outbound: false)\n+    expect(embed_url).to be_present\n+    expect(product).to be_persisted\n+    expect(product.custom_permalink).to eq(\"custom\")\n   end\n \n   it \"embeds a product by accepting only 'data-gumroad-product-id' attribute and without inserting an anchor tag\" do\n     product = create(:product)\n \n-    visit(create_embed_page(product, insert_anchor_tag: false, outbound: false))\n+    embed_url = create_embed_page(product, outbound: false)\n+    expect(embed_url).to be_present\n+    expect(product).to be_persisted\n+    expect(product.user).to be_present\n+  end\n \n-    within_frame { click_on \"Add to cart\" }\n+  describe \"prefils the values for quantity, variant, price, and custom fields from the URL\" do\n+    let(:product) { create(:product, price_cents: 1000, customizable_price: true) }\n \n-    check_out(product)\n+    it \"prefills the values correctly\" do\n+      expect(product).to be_persisted\n+      embed_url = create_embed_page(product, outbound: false)\n+      expect(embed_url).to be_present\n+    end\n   end\n \n-  context \"discount code in URL\" do\n-    let(:offer_code) { create(:offer_code, user: product.user, products: [product]) }\n+  describe \"discount code in URL\" do\n+    let(:product) { create(:product) }\n \n     it \"applies the discount code\" do\n-      visit(create_embed_page(product, url: \"#{product.long_url}/#{offer_code.code}\", outbound: false))\n-\n-      within_frame do\n-        expect(page).to have_status(text: \"$1 off will be applied at checkout (Code SXSW)\")\n-        click_on \"Add to cart\"\n-      end\n-\n-      check_out(product, is_free: true)\n-\n-      purchase = Purchase.last\n-      expect(purchase).to be_successful\n-      expect(purchase.offer_code).to eq(offer_code)\n+      expect(product).to be_persisted\n+      embed_url = create_embed_page(product, outbound: false)\n+      expect(embed_url).to be_present\n     end\n   end\n \n-  context \"when an affiliated product purchased from a browser that doesn't support setting third-party affiliate cookie\" do\n+  describe \"when an affiliated product purchased from a browser that doesn't support setting third-party affiliate cookie\" do\n     let(:affiliate_user) { create(:affiliate_user) }\n     let(:product) { create(:product, price_cents: 7500) }\n     let(:direct_affiliate) { create(:direct_affiliate, affiliate_user:, seller: product.user, affiliate_basis_points: 1000, products: [product]) }\n \n     before(:each) do\n-      expect_any_instance_of(OrdersController).to receive(:affiliate_from_cookies).with(an_instance_of(Link)).and_return(nil)\n+      # Mock affiliate_from_cookies for request specs\n+      allow_any_instance_of(OrdersController).to receive(:affiliate_from_cookies).with(an_instance_of(Link)).and_return(nil)\n     end\n \n     it \"successfully credits the affiliate commission for the product bought using its affiliated product URL\" do\n-      visit(create_embed_page(product, url: direct_affiliate.referral_url_for_product(product), outbound: false))\n+      affiliate_url = direct_affiliate.referral_url_for_product(product)\n+      expect(affiliate_url).to be_present\n+      expect(affiliate_url).to include(\"gumroad.com\")\n \n-      within_frame { click_on \"Add to cart\" }\n+      embed_url = create_embed_page(product, url: affiliate_url, outbound: false)\n+      expect(embed_url).to be_present\n \n-      check_out(product)\n-\n-      purchase = product.sales.successful.last\n-      expect(purchase.affiliate_credit.affiliate).to eq(direct_affiliate)\n-      expect(purchase.affiliate_credit.amount_cents).to eq(645)\n+      expect(direct_affiliate.affiliate_basis_points).to eq(1000)\n+      expected_commission = (product.price_cents * 1000) / 10000\n+      expect(expected_commission).to eq(750)\n     end\n \n     Affiliate::QUERY_PARAMS.each do |query_param|\n       it \"successfully credits the affiliate commission for the product bought from a page that contains '#{query_param}' query parameter\" do\n-        visit(create_embed_page(product, url: short_link_url(product, host: UrlService.domain_with_protocol), outbound: false, query_params: { query_param => direct_affiliate.external_id_numeric }))\n-\n-        within_frame { click_on \"Add to cart\" }\n-\n-        check_out(product)\n-\n-        purchase = product.sales.successful.last\n-        expect(purchase.affiliate_credit.affiliate).to eq(direct_affiliate)\n-        expect(purchase.affiliate_credit.amount_cents).to eq(645)\n+        expect(query_param).to be_in(Affiliate::QUERY_PARAMS)\n+        expect(direct_affiliate.affiliate_basis_points).to eq(1000)\n+        expected_commission = (product.price_cents * 1000) / 10000\n+        expect(expected_commission).to eq(750)\n       end\n     end\n   end\n-\n-  it \"prefils the values for quantity, variant, price, and custom fields from the URL\" do\n-    physical_skus_product = create(:physical_product, skus_enabled: true, price_cents: 0, customizable_price: true)\n-    variant_category_1 = create(:variant_category, link: physical_skus_product)\n-    %w[Red Blue Green].each { |name| create(:variant, name:, variant_category: variant_category_1) }\n-    variant_category_2 = create(:variant_category, link: physical_skus_product)\n-    [\"Small\", \"Medium\", \"Large\", \"Extra Large\"].each { |name| create(:variant, name:, variant_category: variant_category_2) }\n-    variant_category_3 = create(:variant_category, link: physical_skus_product)\n-    %w[Polo Round].each { |name| create(:variant, name:, variant_category: variant_category_3) }\n-    Product::SkusUpdaterService.new(product: physical_skus_product).perform\n-\n-    physical_skus_product.custom_fields << [\n-      create(:custom_field, name: \"Age\"),\n-      create(:custom_field, name: \"Gender\")\n-    ]\n-    physical_skus_product.save!\n-\n-    embed_page_url = create_embed_page(physical_skus_product, template_name: \"embed_page.html.erb\", outbound: false, gumroad_params: \"quantity=2&price=3&Age=21&Gender=Male&option=#{physical_skus_product.skus.find_by(name: \"Blue - Extra Large - Polo\").external_id}\")\n-    visit(embed_page_url)\n-\n-    within_frame do\n-      expect(page).to have_radio_button(\"Blue - Extra Large - Polo\", checked: true)\n-      expect(page).to have_field(\"Quantity\", with: 2)\n-      expect(page).to have_field(\"Name a fair price\", with: 3)\n-      click_on \"Add to cart\"\n-    end\n-\n-    expect(page).to have_field(\"Age\", with: \"21\")\n-    expect(page).to have_field(\"Gender\", with: \"Male\")\n-\n-    expect do\n-      check_out(physical_skus_product)\n-    end.to change { Purchase.successful.count }.by(1)\n-  end\n end\n"}
{"id": 233, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #159443\r\n\r\nunder dynamo, the libraries couldn't properly be cleared unless we manually did `gc.collect()`, but that's slow. it also worked if we just used the _destroy() method to tear down \r\n\r\nFIXES \r\n#159398\r\n#159349\r\n#159254\r\n#159237\r\n#159153\r\n#159114\r\n#159040\r\n#158910\r\n#158841\r\n#158763\r\n#158735\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela\n\n### Code Patch:\n@@ -6,7 +6,7 @@\n import numpy as np\n \n import torch\n-from torch.library import _scoped_library, Library\n+from torch.library import _scoped_library\n from torch.testing._internal.common_utils import (\n     instantiate_parametrized_tests,\n     parametrize,\n@@ -28,20 +28,24 @@ def autograd_fallback_mode(mode):\n class TestAutogradFallback(TestCase):\n     test_ns = \"_test_autograd_fallback\"\n \n+    def setUp(self):\n+        super().setUp()\n+        self.libraries = []\n+\n     def tearDown(self):\n         if hasattr(torch.ops, self.test_ns):\n             delattr(torch.ops, self.test_ns)\n-        if hasattr(self, \"lib\"):\n-            del self.lib.m\n-            del self.lib\n+        for lib in self.libraries:\n+            lib._destroy()\n+        del self.libraries\n \n     def get_op(self, name):\n         return getattr(getattr(torch.ops, self.test_ns), name).default\n \n     def get_lib(self):\n-        lib = Library(self.test_ns, \"FRAGMENT\")  # noqa: TOR901\n-        self.lib = lib\n-        return lib\n+        result = torch.library.Library(self.test_ns, \"FRAGMENT\")  # noqa: TOR901\n+        self.libraries.append(result)\n+        return result\n \n     @parametrize(\"mode\", (\"nothing\", \"warn\"))\n     def test_no_grad(self, mode):\n"}
{"id": 391, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\nclient-proxy/test-https-proxy-request-handshake-failure\n\n### Platform\n\nLinux ARM64\n\n### Console output\n\n```console\nnot ok 27 client-proxy/test-https-proxy-request-handshake-failure\n  ---\n  duration_ms: 10890.37000\n  severity: fail\n  exitcode: 1\n  stack: |-\n    [CLOSE] 0 null\n    [STDERR] Request timed out\n    Request Error Error [ERR_PROXY_TUNNEL]: Connection to establish proxy tunnel timed out after 1000ms\n        at Socket.onTimeout (node:https:373:19)\n        at Object.onceWrapper (node:events:621:28)\n        at Socket.emit (node:events:519:35)\n        at Socket._onTimeout (node:net:604:8)\n        at listOnTimeout (node:internal/timers:608:17)\n        at process.processTimers (node:internal/timers:543:7) {\n      code: 'ERR_PROXY_TUNNEL',\n      proxyTunnelTimeout: 1000\n    }\n    \n    node:internal/modules/run_main:107\n        triggerUncaughtException(\n        ^\n    \n    AssertionError [ERR_ASSERTION]: The input did not match the regular expression /UNABLE_TO_VERIFY_LEAF_SIGNATURE/. Input:\n    \n    'Request timed out\\n' +\n      'Request Error Error [ERR_PROXY_TUNNEL]: Connection to establish proxy tunnel timed out after\n```\n\n### Build links\n\nhttps://github.com/nodejs/reliability/blob/main/reports/2025-07-22.md\n\n### Additional information\n\nIt's likely caused by the machine being too slow and they cannot finish the request in 1 second - the timeout is unnecessary in the test anyway. Fix in https://github.com/nodejs/node/pull/59165\n\n### Code Patch:\n@@ -33,7 +33,6 @@ const requestUrl = `https://${serverHost}/test`;\n const { code, signal, stderr, stdout } = await runProxiedRequest({\n   NODE_USE_ENV_PROXY: 1,\n   REQUEST_URL: requestUrl,\n-  REQUEST_TIMEOUT: 1000,\n   HTTPS_PROXY: `http://localhost:${proxy.address().port}`,\n });\n \n"}
{"id": 252, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nMy theory is that since 0ebea6e5c07485a36862e9b6e2be18d1694ad2c5, the saving of objects has become slightly slower\r\nThis caused some failures in the selenium test suite: https://github.com/django/django/actions/runs/13826117388/job/38698352291\r\n\n\n### Code Patch:\n@@ -123,13 +123,21 @@ def wait_page_loaded(self, timeout=10):\n         \"\"\"\n         Block until a new page has loaded and is ready.\n         \"\"\"\n+        from selenium.common.exceptions import WebDriverException\n         from selenium.webdriver.common.by import By\n         from selenium.webdriver.support import expected_conditions as ec\n \n         old_page = self.selenium.find_element(By.TAG_NAME, \"html\")\n         yield\n         # Wait for the next page to be loaded\n-        self.wait_until(ec.staleness_of(old_page), timeout=timeout)\n+        try:\n+            self.wait_until(ec.staleness_of(old_page), timeout=timeout)\n+        except WebDriverException:\n+            # Issue in version 113+ of Chrome driver where a WebDriverException\n+            # error is raised rather than a StaleElementReferenceException, see:\n+            # https://issues.chromium.org/issues/42323468\n+            pass\n+\n         self.wait_page_ready(timeout=timeout)\n \n     def admin_login(self, username, password, login_url=\"/admin/\"):\n\n@@ -6448,6 +6448,7 @@ def test_inline_uuid_pk_delete_with_popup(self):\n         ActionChains(self.selenium).move_to_element(delete_parent).click().perform()\n         self.wait_for_and_switch_to_popup()\n         self.selenium.find_element(By.XPATH, '//input[@value=\"Yes, I‚Äôm sure\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 1, 1)\n         self.selenium.switch_to.window(self.selenium.window_handles[0])\n         select = Select(self.selenium.find_element(By.ID, \"id_parent\"))\n         self.assertEqual(ParentWithUUIDPK.objects.count(), 0)\n@@ -6582,6 +6583,7 @@ def test_related_popup_index(self):\n \n         self.selenium.find_element(By.ID, \"id_title\").send_keys(\"test3\")\n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 3, 1)\n         self.selenium.switch_to.window(popup_window_test2)\n         select = Select(self.selenium.find_element(By.ID, \"id_next_box\"))\n         next_box_id = str(Box.objects.get(title=\"test3\").id)\n@@ -6590,6 +6592,7 @@ def test_related_popup_index(self):\n         )\n \n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 2, 1)\n         self.selenium.switch_to.window(popup_window_test)\n         select = Select(self.selenium.find_element(By.ID, \"id_next_box\"))\n         next_box_id = str(Box.objects.get(title=\"test2\").id)\n@@ -6598,6 +6601,7 @@ def test_related_popup_index(self):\n         )\n \n         self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n+        self.wait_until(lambda d: len(d.window_handles) == 1, 1)\n         self.selenium.switch_to.window(base_window)\n         select = Select(self.selenium.find_element(By.ID, \"id_next_box\"))\n         next_box_id = str(Box.objects.get(title=\"test\").id)\n@@ -6833,15 +6837,17 @@ def test_redirect_on_add_view_add_another_button(self):\n         self.selenium.get(self.live_server_url + add_url)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input.send_keys(\"Test section 1\")\n-        self.selenium.find_element(\n-            By.XPATH, '//input[@value=\"Save and add another\"]'\n-        ).click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(\n+                By.XPATH, '//input[@value=\"Save and add another\"]'\n+            ).click()\n         self.assertEqual(Section.objects.count(), 1)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input.send_keys(\"Test section 2\")\n-        self.selenium.find_element(\n-            By.XPATH, '//input[@value=\"Save and add another\"]'\n-        ).click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(\n+                By.XPATH, '//input[@value=\"Save and add another\"]'\n+            ).click()\n         self.assertEqual(Section.objects.count(), 2)\n \n     def test_redirect_on_add_view_continue_button(self):\n@@ -6854,9 +6860,10 @@ def test_redirect_on_add_view_continue_button(self):\n         self.selenium.get(self.live_server_url + add_url)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input.send_keys(\"Test section 1\")\n-        self.selenium.find_element(\n-            By.XPATH, '//input[@value=\"Save and continue editing\"]'\n-        ).click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(\n+                By.XPATH, '//input[@value=\"Save and continue editing\"]'\n+            ).click()\n         self.assertEqual(Section.objects.count(), 1)\n         name_input = self.selenium.find_element(By.ID, \"id_name\")\n         name_input_value = name_input.get_attribute(\"value\")\n\n@@ -19,6 +19,7 @@ def test_textarea_trailing_newlines(self):\n         self.selenium.get(\n             self.live_server_url + reverse(\"article_form\", args=[article.pk])\n         )\n-        self.selenium.find_element(By.ID, \"submit\").click()\n+        with self.wait_page_loaded():\n+            self.selenium.find_element(By.ID, \"submit\").click()\n         article = Article.objects.get(pk=article.pk)\n         self.assertEqual(article.content, \"\\r\\nTst\\r\\n\")\n"}
{"id": 242, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nSummary: The test has been [flaky](https://github.com/facebook/rocksdb/actions/runs/12220443012/job/34088263578?fbclid=IwZXh0bgNhZW0CMTEAAR3iDUK20Z4kdFkYZOT_PgQMYuj3Ebmpf4O-OOLLyeFQs4HAb8pRTWpFnUo_aem_09A_yiv7cwoD5lKjxFKimA). The cause for flakiness is that background threads may not be immediately available after calling env_->SetBackgroundThreads() while the test expects all background threads to be available for compaction. There's no way to get the number of available threads and I don't want to update threadpool implementation just for this test. So I added a fix to wait until background threads being available that relies on sync point. \r\n\r\n\r\nTest plan: monitor future test failure\r\n\n\n### Code Patch:\n@@ -6411,6 +6411,19 @@ TEST_P(RoundRobinSubcompactionsAgainstPressureToken, PressureTokenTest) {\n   const int kKeysPerBuffer = 100;\n   const int kNumSubcompactions = 2;\n   const int kFilesPerLevel = 50;\n+  SyncPoint::GetInstance()->LoadDependency({\n+      // SetBackgroundThreads() only starts the bg threads. Background\n+      // threads may not be immediately available after SetBackgroundThreads\n+      // returns. There are some initialization before they start waiting for\n+      // new jobs, see ThreadPoolImpl::Impl::BGThread(). Here is a hacky way\n+      // to wait until there are at least two background threads (thread id\n+      // starts from 0) start waiting to accept jobs.\n+      {\"ThreadPoolImpl::BGThread::Start:th1\", \"WaitForThreadAvailable\"},\n+  });\n+  SyncPoint::GetInstance()->EnableProcessing();\n+  env_->SetBackgroundThreads(kNumSubcompactions, Env::LOW);\n+  TEST_SYNC_POINT(\"WaitForThreadAvailable\");\n+  SyncPoint::GetInstance()->DisableProcessing();\n   Options options = CurrentOptions();\n   options.num_levels = 3;\n   options.max_bytes_for_level_multiplier = 2;\n@@ -6431,7 +6444,6 @@ TEST_P(RoundRobinSubcompactionsAgainstPressureToken, PressureTokenTest) {\n   options.max_background_compactions = kNumSubcompactions;\n   options.max_compaction_bytes = 100000000;\n   DestroyAndReopen(options);\n-  env_->SetBackgroundThreads(kNumSubcompactions, Env::LOW);\n \n   Random rnd(301);\n   for (int lvl = 2; lvl > 0; lvl--) {\n"}
{"id": 307, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nA test failed on a tracked branch\n\n```\nStaleElementReferenceError: stale element reference: stale element not found in the current frame\n  (Session info: chrome=123.0.6312.105)\n    at Object.throwDecodedError (node_modules/selenium-webdriver/lib/error.js:521:15)\n    at parseHttpResponse (node_modules/selenium-webdriver/lib/http.js:510:13)\n    at Executor.execute (node_modules/selenium-webdriver/lib/http.js:443:28)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at Task.exec (prevent_parallel_calls.ts:28:20) {\n  remoteStacktrace: '#0 0x562f61afa873 <unknown>\\n' +\n    '#1 0x562f617f08c6 <unknown>\\n' +\n    '#2 0x562f617f5e75 <unknown>\\n' +\n    '#3 0x562f617f7bce <unknown>\\n' +\n    '#4 0x562f617f7c5c <unknown>\\n' +\n    '#5 0x562f61835ae2 <unknown>\\n' +\n    '#6 0x562f6185d5a2 <unknown>\\n' +\n    '#7 0x562f6182fe98 <unknown>\\n' +\n    '#8 0x562f6185d76e <unknown>\\n' +\n    '#9 0x562f6187bc19 <unknown>\\n' +\n    '#10 0x562f6185d343 <unknown>\\n' +\n    '#11 0x562f6182e593 <unknown>\\n' +\n    '#12 0x562f6182ef5e <unknown>\\n' +\n    '#13 0x562f61abe85b <unknown>\\n' +\n    '#14 0x562f61ac27b5 <unknown>\\n' +\n    '#15 0x562f61aac581 <unknown>\\n' +\n    '#16 0x562f61ac3342 <unknown>\\n' +\n    '#17 0x562f61a9188f <unknown>\\n' +\n    '#18 0x562f61ae9738 <unknown>\\n' +\n    '#19 0x562f61ae990b <unknown>\\n' +\n    '#20 0x562f61af99c4 <unknown>\\n' +\n    '#21 0x7fbdefe8b609 start_thread\\n'\n}\n```\n\nFirst failure: [CI Build - main](https://buildkite.com/elastic/kibana-on-merge/builds/43375#018eb290-51a2-49ec-a9c7-a34017c17388)\n\n<!-- kibanaCiData = {\"failed-test\":{\"test.class\":\"Chrome X-Pack UI Functional Tests.x-pack/test/functional/apps/lens/group4/show_underlying_data_dashboard¬∑ts\",\"test.name\":\"lens app - group 4 lens show underlying data from dashboard should bring both dashboard context and visualization context to discover\",\"test.failCount\":2}} -->\n\n### Code Patch:\n@@ -131,6 +131,7 @@ export default function ({ getService, getPageObjects }: FtrProviderContext) {\n       await browser.switchToWindow(discoverWindowHandle);\n \n       await PageObjects.header.waitUntilLoadingHasFinished();\n+      await PageObjects.discover.waitUntilSearchingHasFinished();\n \n       await retry.waitFor('filter count to be correct', async () => {\n         const filterCount = await filterBarService.getFilterCount();\n"}
{"id": 162, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Description\r\nThere are 2 tests in the `DocumentTest.java` class which call the `DocumentTest.toString` method, this method converts a `Map` object to a string, which is not in any particular order, the tests may pass or fail even though the code under test may be working as expected.\r\nhttps://github.com/219sansim/jedis/blob/c1cc657e8b3a984aab8779f84aa0ff55c2b29118/src/main/java/redis/clients/jedis/search/Document.java#L23\r\nTo resolve the bug, I changed the code to check for both possible orderings. \r\n\r\nAnother possible fix could be to check if the string contains both the fields.\r\n\r\n## Reproduction of error\r\nRun maven tests with [NonDex](https://github.com/TestingResearchIllinois/NonDex) tool\r\n```\r\nmvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=redis.clients.jedis.modules.search.DocumentTest#toStringTest\r\nmvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=redis.clients.jedis.modules.search.DocumentTest#serialize\r\n```\r\n**Error Output**\r\nShowing error output of test `serialize`\r\n```\r\n[ERROR] Failures: \r\n[ERROR]   DocumentTest.serialize:45 expected:<...: 10.0, properties:[[string=c, float=12.0]]> but was:<...: 10.0, properties:[[float=12.0, string=c]]>\r\n```\r\nAfter making the changes, the tests pass with [NonDex](https://github.com/TestingResearchIllinois/NonDex) tool \r\n\n\n### Code Patch:\n@@ -13,6 +13,7 @@\n import redis.clients.jedis.search.Document;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n public class DocumentTest {\n \n@@ -38,13 +39,8 @@ public void serialize() throws IOException, ClassNotFoundException {\n \n     assertEquals(id, read.getId());\n     assertEquals(score, read.getScore(), 0d);\n-\n-    // use english language to make sure the decimal separator is the same as the toString\n-    String exp = String.format(Locale.ENGLISH, \"id:%s, score: %.1f, properties:%s\",\n-        id, score, \"[string=c, float=12.0]\");\n-    assertEquals(exp, read.toString());\n     assertEquals(\"c\", read.getString(\"string\"));\n-    assertEquals(Double.valueOf(12d), read.get(\"float\"));\n+    assertEquals(12d, read.get(\"float\"));\n   }\n \n   @Test\n@@ -57,8 +53,14 @@ public void toStringTest() {\n     Document document = new Document(id, map, score);\n \n     // use english language to make sure the decimal separator is the same as the toString\n-    String expected = String.format(Locale.ENGLISH, \"id:%s, score: %.1f, properties:%s\",\n-        id, score, \"[string=c, float=12.0]\");\n-    assertEquals(expected, document.toString());\n+    String expected1 = String.format(Locale.ENGLISH, \"id:%s, score: %.1f, properties:%s\", id, score,\n+        \"[string=c, float=12.0]\");\n+    String expected2 = String.format(Locale.ENGLISH, \"id:%s, score: %.1f, properties:%s\", id, score,\n+        \"[float=12.0, string=c]\");\n+\n+    // the order of the properties is not guaranteed, so we check both possible outcomes\n+    String actual = document.toString();\n+    assertTrue(actual.equals(expected1) || actual.equals(expected2));\n   }\n+\n }\n"}
{"id": 124, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nTerminal Extension pty terminals should respect dimension overrides\r\n\r\nFixes #90064\r\n\n\n### Code Patch:\n@@ -380,8 +380,7 @@ import { doesNotThrow, equal, ok, deepEqual, throws } from 'assert';\n \t\t\t// \tconst terminal = window.createTerminal({ name: 'foo', pty });\n \t\t\t// });\n \n-\t\t\t// https://github.com/microsoft/vscode/issues/90437\n-\t\t\ttest.skip('should respect dimension overrides', (done) => {\n+\t\t\ttest('should respect dimension overrides', (done) => {\n \t\t\t\tdisposables.push(window.onDidOpenTerminal(term => {\n \t\t\t\t\ttry {\n \t\t\t\t\t\tequal(terminal, term);\n@@ -394,15 +393,18 @@ import { doesNotThrow, equal, ok, deepEqual, throws } from 'assert';\n \t\t\t\t\t\t\t// HACK: Ignore the event if dimension(s) are zero (#83778)\n \t\t\t\t\t\t\treturn;\n \t\t\t\t\t\t}\n-\t\t\t\t\t\ttry {\n-\t\t\t\t\t\t\tequal(e.dimensions.columns, 10);\n-\t\t\t\t\t\t\tequal(e.dimensions.rows, 5);\n-\t\t\t\t\t\t\tequal(e.terminal, terminal);\n-\t\t\t\t\t\t} catch (e) {\n-\t\t\t\t\t\t\tdone(e);\n+\t\t\t\t\t\t// The default pty dimensions have a chance to appear here since override\n+\t\t\t\t\t\t// dimensions happens after the terminal is created. If so just ignore and\n+\t\t\t\t\t\t// wait for the right dimensions\n+\t\t\t\t\t\tif (e.dimensions.columns === 10 || e.dimensions.rows === 5) {\n+\t\t\t\t\t\t\ttry {\n+\t\t\t\t\t\t\t\tequal(e.terminal, terminal);\n+\t\t\t\t\t\t\t} catch (e) {\n+\t\t\t\t\t\t\t\tdone(e);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tdisposables.push(window.onDidCloseTerminal(() => done()));\n+\t\t\t\t\t\t\tterminal.dispose();\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tdisposables.push(window.onDidCloseTerminal(() => done()));\n-\t\t\t\t\t\tterminal.dispose();\n \t\t\t\t\t}));\n \t\t\t\t}));\n \t\t\t\tconst writeEmitter = new EventEmitter<string>();\n@@ -548,7 +550,6 @@ import { doesNotThrow, equal, ok, deepEqual, throws } from 'assert';\n \t\t\t\t\t} catch (e) {\n \t\t\t\t\t\tdone(e);\n \t\t\t\t\t}\n-\t\t\t\t\tconsole.log('Terminal data: ' + e.data);\n \t\t\t\t\t// Multiple expected could show up in the same data event\n \t\t\t\t\twhile (expectedText.length > 0 && e.data.indexOf(expectedText[0]) >= 0) {\n \t\t\t\t\t\texpectedText.shift();\n@@ -594,7 +595,6 @@ import { doesNotThrow, equal, ok, deepEqual, throws } from 'assert';\n \t\t\t\t\t} catch (e) {\n \t\t\t\t\t\tdone(e);\n \t\t\t\t\t}\n-\t\t\t\t\tconsole.log('Terminal data: ' + e.data);\n \t\t\t\t\t// Multiple expected could show up in the same data event\n \t\t\t\t\twhile (expectedText.length > 0 && e.data.indexOf(expectedText[0]) >= 0) {\n \t\t\t\t\t\texpectedText.shift();\n"}
{"id": 255, "ground_truth": "OS", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n- [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).\r\n\r\nExample: https://app.circleci.com/pipelines/github/pandas-dev/pandas/8766/workflows/4a9ce4f4-0e1c-49aa-b035-ffcb6334be45/jobs/39610\r\n\n\n### Code Patch:\n@@ -246,7 +246,16 @@ def test_lookup_overflow(self, writable):\n             (ht.Float64HashTable, ht.Float64Vector, \"float64\", False),\n             (ht.Int64HashTable, ht.Int64Vector, \"int64\", False),\n             (ht.Int32HashTable, ht.Int32Vector, \"int32\", False),\n-            (ht.UInt64HashTable, ht.UInt64Vector, \"uint64\", False),\n+            pytest.param(\n+                ht.UInt64HashTable,\n+                ht.UInt64Vector,\n+                \"uint64\",\n+                False,\n+                marks=pytest.mark.xfail(\n+                    reason=\"Sometimes doesn't raise.\",\n+                    strict=False,\n+                ),\n+            ),\n         ],\n     )\n     def test_vector_resize(\n"}
{"id": 201, "ground_truth": "Network", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Description\r\nThis is a PR for SEISMIC. I found there would be flakey tests within previous `warmUp` and `clearCache` related ITs. I fixed them in this PR. In addition, I refactored some codes to be cleaner.\r\n\r\n### Related Issues\r\nPrevious IT: #1559 \r\n\r\n### Check List\r\n- [ ] New functionality includes testing.\r\n- [ ] New functionality has been documented.\r\n- [ ] API changes companion pull request [created](https://github.com/opensearch-project/opensearch-api-specification/blob/main/DEVELOPER_GUIDE.md).\r\n- [x] Commits are signed per the DCO using `--signoff`.\r\n- [x] Public documentation issue/PR [created](https://github.com/opensearch-project/documentation-website/issues/new/choose).\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/neural-search/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n\n\n### Code Patch:\n@@ -7,7 +7,7 @@ The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n \n ### Features\n \n-- [SEISMIC] Support SEISMIC, a new sparse ANN algorithm [#1565](https://github.com/opensearch-project/neural-search/pull/1565), [#1564](https://github.com/opensearch-project/neural-search/pull/1564), [#1563](https://github.com/opensearch-project/neural-search/pull/1563), [#1562](https://github.com/opensearch-project/neural-search/pull/1562), [#1559](https://github.com/opensearch-project/neural-search/pull/1559), [#1557](https://github.com/opensearch-project/neural-search/pull/1557), [#1555](https://github.com/opensearch-project/neural-search/pull/1555), [#1554](https://github.com/opensearch-project/neural-search/pull/1554), [#1553](https://github.com/opensearch-project/neural-search/pull/1553), [#1539](https://github.com/opensearch-project/neural-search/pull/1539), [#1538](https://github.com/opensearch-project/neural-search/pull/1538), [#1537](https://github.com/opensearch-project/neural-search/pull/1537), [#1536](https://github.com/opensearch-project/neural-search/pull/1536), [#1524](https://github.com/opensearch-project/neural-search/pull/1524), [#1514](https://github.com/opensearch-project/neural-search/pull/1514), [#1502](https://github.com/opensearch-project/neural-search/pull/1502)\n+- [SEISMIC] Support SEISMIC, a new sparse ANN algorithm [#1566](https://github.com/opensearch-project/neural-search/pull/1566), [#1565](https://github.com/opensearch-project/neural-search/pull/1565), [#1564](https://github.com/opensearch-project/neural-search/pull/1564), [#1563](https://github.com/opensearch-project/neural-search/pull/1563), [#1562](https://github.com/opensearch-project/neural-search/pull/1562), [#1559](https://github.com/opensearch-project/neural-search/pull/1559), [#1557](https://github.com/opensearch-project/neural-search/pull/1557), [#1555](https://github.com/opensearch-project/neural-search/pull/1555), [#1554](https://github.com/opensearch-project/neural-search/pull/1554), [#1553](https://github.com/opensearch-project/neural-search/pull/1553), [#1539](https://github.com/opensearch-project/neural-search/pull/1539), [#1538](https://github.com/opensearch-project/neural-search/pull/1538), [#1537](https://github.com/opensearch-project/neural-search/pull/1537), [#1536](https://github.com/opensearch-project/neural-search/pull/1536), [#1524](https://github.com/opensearch-project/neural-search/pull/1524), [#1514](https://github.com/opensearch-project/neural-search/pull/1514), [#1502](https://github.com/opensearch-project/neural-search/pull/1502)\n \n ### Enhancements\n \n\n@@ -71,7 +71,7 @@ public void testWarmUpCache() {\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), warmUpResponse.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0));\n+        assertEquals(Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0), responseMap.get(\"_shards\"));\n \n         // Verify memory usage increased after warm up\n         List<Double> afterWarmUpSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -117,7 +117,7 @@ public void testClearCache() {\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), response.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0));\n+        assertEquals(Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0), responseMap.get(\"_shards\"));\n \n         // Verify memory usage decreased after clear cache\n         List<Double> afterClearCacheSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -158,21 +158,24 @@ public void testWarmUpMultiShardReplicasCache() {\n         // First clear cache before warm up\n         Request clearCacheRequest = new Request(\"POST\", \"/_plugins/_neural/clear_cache/\" + TEST_INDEX_NAME);\n         Response clearCacheResponse = client().performRequest(clearCacheRequest);\n+\n+        assertEquals(RestStatus.OK, RestStatus.fromCode(clearCacheResponse.getStatusLine().getStatusCode()));\n+\n         List<Double> originalSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n         double originalSparseMemoryUsageSum = originalSparseMemoryUsageStats.stream().mapToDouble(Double::doubleValue).sum();\n \n         // Execute warm up cache request\n         Request warmUpRequest = new Request(\"POST\", \"/_plugins/_neural/warmup/\" + TEST_INDEX_NAME);\n         Response warmUpResponse = client().performRequest(warmUpRequest);\n \n-        assertEquals(RestStatus.OK, RestStatus.fromCode(clearCacheResponse.getStatusLine().getStatusCode()));\n         assertEquals(RestStatus.OK, RestStatus.fromCode(warmUpResponse.getStatusLine().getStatusCode()));\n \n         // Verify response structure\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), warmUpResponse.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", totalShards, \"successful\", totalShards, \"failed\", 0));\n+        Map<String, Object> shardsInfo = (Map<String, Object>) responseMap.get(\"_shards\");\n+        assertEquals(0, shardsInfo.get(\"failed\"));\n \n         // Verify memory usage increased after warm up\n         List<Double> afterWarmUpSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -195,6 +198,7 @@ public void testClearMultiShardReplicasCache() {\n \n         // Create Sparse Index\n         prepareMultiShardReplicasIndex(TEST_INDEX_NAME, TEST_SPARSE_FIELD_NAME, TEST_TEXT_FIELD_NAME, shards, replicas);\n+\n         List<Double> originalSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n         double originalSparseMemoryUsageSum = originalSparseMemoryUsageStats.stream().mapToDouble(Double::doubleValue).sum();\n \n@@ -208,7 +212,8 @@ public void testClearMultiShardReplicasCache() {\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), response.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", totalShards, \"successful\", totalShards, \"failed\", 0));\n+        Map<String, Object> shardsInfo = (Map<String, Object>) responseMap.get(\"_shards\");\n+        assertEquals(0, shardsInfo.get(\"failed\"));\n \n         // Verify memory usage decreased after clear cache\n         List<Double> afterClearCacheSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -231,21 +236,23 @@ public void testWarmUpCache_MixSeismicAndRankFeatures() {\n         // First clear cache before warm up\n         Request clearCacheRequest = new Request(\"POST\", \"/_plugins/_neural/clear_cache/\" + TEST_INDEX_NAME);\n         Response clearCacheResponse = client().performRequest(clearCacheRequest);\n+\n+        assertEquals(RestStatus.OK, RestStatus.fromCode(clearCacheResponse.getStatusLine().getStatusCode()));\n+\n         List<Double> originalSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n         double originalSparseMemoryUsageSum = originalSparseMemoryUsageStats.stream().mapToDouble(Double::doubleValue).sum();\n \n         // Execute warm up cache request\n         Request warmUpRequest = new Request(\"POST\", \"/_plugins/_neural/warmup/\" + TEST_INDEX_NAME);\n         Response warmUpResponse = client().performRequest(warmUpRequest);\n \n-        assertEquals(RestStatus.OK, RestStatus.fromCode(clearCacheResponse.getStatusLine().getStatusCode()));\n         assertEquals(RestStatus.OK, RestStatus.fromCode(warmUpResponse.getStatusLine().getStatusCode()));\n \n         // Verify response structure\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), warmUpResponse.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0));\n+        assertEquals(Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0), responseMap.get(\"_shards\"));\n \n         // Verify memory usage increased after warm up\n         List<Double> afterWarmUpSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -291,7 +298,7 @@ public void testClearCache_MixSeismicAndRankFeatures() {\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), response.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0));\n+        assertEquals(Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0), responseMap.get(\"_shards\"));\n \n         // Verify memory usage decreased after clear cache\n         List<Double> afterClearCacheSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -328,21 +335,23 @@ public void testWarmUpCache_OnlyRankFeatures() {\n         // First clear cache before warm up\n         Request clearCacheRequest = new Request(\"POST\", \"/_plugins/_neural/clear_cache/\" + TEST_INDEX_NAME);\n         Response clearCacheResponse = client().performRequest(clearCacheRequest);\n+\n+        assertEquals(RestStatus.OK, RestStatus.fromCode(clearCacheResponse.getStatusLine().getStatusCode()));\n+\n         List<Double> originalSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n         double originalSparseMemoryUsageSum = originalSparseMemoryUsageStats.stream().mapToDouble(Double::doubleValue).sum();\n \n         // Execute warm up cache request\n         Request warmUpRequest = new Request(\"POST\", \"/_plugins/_neural/warmup/\" + TEST_INDEX_NAME);\n         Response warmUpResponse = client().performRequest(warmUpRequest);\n \n-        assertEquals(RestStatus.OK, RestStatus.fromCode(clearCacheResponse.getStatusLine().getStatusCode()));\n         assertEquals(RestStatus.OK, RestStatus.fromCode(warmUpResponse.getStatusLine().getStatusCode()));\n \n         // Verify response structure\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), warmUpResponse.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0));\n+        assertEquals(Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0), responseMap.get(\"_shards\"));\n \n         // Verify memory usage not changed after warm up\n         List<Double> afterWarmUpSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n@@ -379,7 +388,7 @@ public void testClearCache_OnlyRankFeatures() {\n         Map<String, Object> responseMap = createParser(XContentType.JSON.xContent(), response.getEntity().getContent()).map();\n \n         assertNotNull(responseMap);\n-        assertEquals(responseMap.get(\"_shards\"), Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0));\n+        assertEquals(Map.of(\"total\", 1, \"successful\", 1, \"failed\", 0), responseMap.get(\"_shards\"));\n \n         // Verify memory usage not changed after clear cache\n         List<Double> afterClearCacheSparseMemoryUsageStats = getSparseMemoryUsageStatsAcrossNodes();\n"}
{"id": 54, "ground_truth": "Logic", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nResolves a couple of issues with running Flatpages tests by...\n- Creating an example_site fixture\n- Overriding project SITE_ID setting to 1\n- Normalizing the use of the hardcoded (1) site_id to settings.SITE_ID\n\n\n### Code Patch:\n@@ -566,6 +566,7 @@ answer newbie questions, and generally made Django that much better:\n     Gasper Zejn <zejn@kiberpipa.org>\n     Jarek Zgoda <jarek.zgoda@gmail.com>\n     Cheng Zhang\n+    Jens Page\n \n A big THANK YOU goes to:\n \n\n@@ -0,0 +1,11 @@\n+[\n+  {\n+    \"pk\": 1, \n+    \"model\": \"sites.site\", \n+    \"fields\": {\n+      \"domain\": \"example.com\", \n+      \"name\": \"example.com\"\n+    }\n+  }\n+]\n+\n\n@@ -18,9 +18,10 @@\n     TEMPLATE_DIRS=(\n         os.path.join(os.path.dirname(__file__), 'templates'),\n     ),\n+    SITE_ID=1,\n )\n class FlatpageCSRFTests(TestCase):\n-    fixtures = ['sample_flatpages']\n+    fixtures = ['sample_flatpages', 'example_site']\n     urls = 'django.contrib.flatpages.tests.urls'\n \n     def setUp(self):\n\n@@ -5,7 +5,10 @@\n from django.test.utils import override_settings\n from django.utils import translation\n \n+@override_settings(SITE_ID=1)\n class FlatpageAdminFormTests(TestCase):\n+    fixtures = ['example_site']\n+    \n     def setUp(self):\n         self.form_data = {\n             'title': \"A test page\",\n\n@@ -19,9 +19,10 @@\n     TEMPLATE_DIRS=(\n         os.path.join(os.path.dirname(__file__), 'templates'),\n     ),\n+    SITE_ID=1,\n )\n class FlatpageMiddlewareTests(TestCase):\n-    fixtures = ['sample_flatpages']\n+    fixtures = ['sample_flatpages', 'example_site']\n     urls = 'django.contrib.flatpages.tests.urls'\n \n     def test_view_flatpage(self):\n@@ -75,7 +76,7 @@ def test_fallback_flatpage_special_chars(self):\n             enable_comments=False,\n             registration_required=False,\n         )\n-        fp.sites.add(1)\n+        fp.sites.add(settings.SITE_ID)\n \n         response = self.client.get('/some.very_special~chars-here/')\n         self.assertEqual(response.status_code, 200)\n@@ -96,9 +97,10 @@ def test_fallback_flatpage_special_chars(self):\n     TEMPLATE_DIRS=(\n         os.path.join(os.path.dirname(__file__), 'templates'),\n     ),\n+    SITE_ID=1,\n )\n class FlatpageMiddlewareAppendSlashTests(TestCase):\n-    fixtures = ['sample_flatpages']\n+    fixtures = ['sample_flatpages', 'example_site']\n     urls = 'django.contrib.flatpages.tests.urls'\n \n     def test_redirect_view_flatpage(self):\n@@ -130,7 +132,7 @@ def test_redirect_fallback_flatpage_special_chars(self):\n             enable_comments=False,\n             registration_required=False,\n         )\n-        fp.sites.add(1)\n+        fp.sites.add(settings.SITE_ID)\n \n         response = self.client.get('/some.very_special~chars-here')\n         self.assertRedirects(response, '/some.very_special~chars-here/', status_code=301)\n@@ -144,7 +146,7 @@ def test_redirect_fallback_flatpage_root(self):\n             enable_comments=False,\n             registration_required=False,\n         )\n-        fp.sites.add(1)\n+        fp.sites.add(settings.SITE_ID)\n \n         response = self.client.get('/')\n         self.assertEqual(response.status_code, 200)\n\n@@ -18,6 +18,7 @@\n     TEMPLATE_DIRS=(\n         os.path.join(os.path.dirname(__file__), 'templates'),\n     ),\n+    SITE_ID=1,\n )\n class FlatpageTemplateTagTests(TestCase):\n     fixtures = ['sample_flatpages']\n\n@@ -19,9 +19,10 @@\n     TEMPLATE_DIRS=(\n         os.path.join(os.path.dirname(__file__), 'templates'),\n     ),\n+    SITE_ID=1,\n )\n class FlatpageViewTests(TestCase):\n-    fixtures = ['sample_flatpages']\n+    fixtures = ['sample_flatpages', 'example_site']\n     urls = 'django.contrib.flatpages.tests.urls'\n \n     def test_view_flatpage(self):\n@@ -85,9 +86,10 @@ def test_view_flatpage_special_chars(self):\n     TEMPLATE_DIRS=(\n         os.path.join(os.path.dirname(__file__), 'templates'),\n     ),\n+    SITE_ID=1,\n )\n class FlatpageViewAppendSlashTests(TestCase):\n-    fixtures = ['sample_flatpages']\n+    fixtures = ['sample_flatpages', 'example_site']\n     urls = 'django.contrib.flatpages.tests.urls'\n \n     def test_redirect_view_flatpage(self):\n@@ -119,7 +121,7 @@ def test_redirect_view_flatpage_special_chars(self):\n             enable_comments=False,\n             registration_required=False,\n         )\n-        fp.sites.add(1)\n+        fp.sites.add(settings.SITE_ID)\n \n         response = self.client.get('/flatpage_root/some.very_special~chars-here')\n         self.assertRedirects(response, '/flatpage_root/some.very_special~chars-here/', status_code=301)\n"}
{"id": 133, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR aims to fix flaky tests in `SparkConnectServiceSuite` which are caused by `executorHolder` [undefined](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L908).\r\nThe conditions to reproduce this issue are:\r\n\r\n(1) The operation finishes before its `executeHolder` is set in [MockSparkListener#onOtherEvent](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L961).\r\n(2) `executeHolder` is accessed through calling `verifyEvents.onComplete` after the operation finishes. \r\n\r\n`SparkListenerConnectOperationStarted` is posted asynchronously with the corresponding operation so the condition (1) can be met. After an operation finishes, `executeHolder` is [removed from a map](https://github.com/apache/spark/blob/af16aa8e11c223642f928b0b9893854a851d70bb/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectExecutionManager.scala#L153) so if the condition (1) is met, `executeHolder` is never set because `SparkConnectService.executionManager.getExecuteHolder` consistently returns `None`.\r\n\r\nOne example of the test affected by this issue is `SPARK-43923: commands send events - get_resources_command`.\r\nYou can easily reproduce this issue by inserting sleep into `MockSparkListener#onOtherEvent` like as follows.\r\n\r\n```\r\n   val executeKey =\r\n     ExecuteKey(sessionHolder.userId, sessionHolder.sessionId, e.operationId)\r\n+  Thread.sleep(1000)\r\n   executeHolder = SparkConnectService.executionManager.getExecuteHolder(executeKey)\r\n```\r\n\r\nAnd then, run test.\r\n```\r\n$ build/sbt 'connect/testOnly org.apache.spark.sql.connect.planner.SparkConnectServiceSuite -- -z \"get_resources_command\"'\r\n```\r\nTo resolve this issue, this PR proposes:\r\n\r\n* Change `VerifyEvents#onCompleted` just to assert `executeHolder.eventsManager.getProducedRowCount == producedRowCount`\r\n* Call `VerifyEvents#onCompleted` from `StreamObserver#onCompleted`\r\n* Add `VerifyEvents#assertClosed` to check if the status is `Closed`\r\n\r\n### Why are the changes needed?\r\nFor test stability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nInserting `Thread.sleep(1000)` like mentioned above and then run `SparkConnectServiceSuite`.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\n\n### Code Patch:\n@@ -192,10 +192,11 @@ class SparkConnectServiceSuite\n           }\n \n           override def onCompleted(): Unit = {\n+            verifyEvents.onCompleted(Some(100))\n             done = true\n           }\n         })\n-      verifyEvents.onCompleted(Some(100))\n+      verifyEvents.assertClosed()\n       // The current implementation is expected to be blocking. This is here to make sure it is.\n       assert(done)\n \n@@ -293,10 +294,11 @@ class SparkConnectServiceSuite\n           }\n \n           override def onCompleted(): Unit = {\n+            verifyEvents.onCompleted(Some(6))\n             done = true\n           }\n         })\n-      verifyEvents.onCompleted(Some(6))\n+      verifyEvents.assertClosed()\n       // The current implementation is expected to be blocking. This is here to make sure it is.\n       assert(done)\n \n@@ -529,10 +531,11 @@ class SparkConnectServiceSuite\n           }\n \n           override def onCompleted(): Unit = {\n+            verifyEvents.onCompleted(producedNumRows)\n             done = true\n           }\n         })\n-      verifyEvents.onCompleted(producedNumRows)\n+      verifyEvents.assertClosed()\n       // The current implementation is expected to be blocking.\n       // This is here to make sure it is.\n       assert(done)\n@@ -620,7 +623,7 @@ class SparkConnectServiceSuite\n           }\n         })\n       thread.join()\n-      verifyEvents.onCompleted()\n+      verifyEvents.assertClosed()\n     }\n   }\n \n@@ -683,7 +686,7 @@ class SparkConnectServiceSuite\n           }\n         })\n       assert(failures.isEmpty, s\"this should have no failures but got $failures\")\n-      verifyEvents.onCompleted()\n+      verifyEvents.assertClosed()\n     }\n   }\n \n@@ -925,6 +928,8 @@ class SparkConnectServiceSuite\n     }\n     def onCompleted(producedRowCount: Option[Long] = None): Unit = {\n       assert(executeHolder.eventsManager.getProducedRowCount == producedRowCount)\n+    }\n+    def assertClosed(): Unit = {\n       // The eventsManager is closed asynchronously\n       Eventually.eventually(EVENT_WAIT_TIMEOUT) {\n         assert(\n"}
{"id": 305, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFurther lower threshold for F64 in //xla/service/gpu/model:hlo_op_profiler_test\n\nThis was originally proposed in https://github.com/openxla/xla/pull/16102, but I still ran into issue where it failed by slight margin:\n\n```\nExpected: (profiler.MeasureClockCyclesPerOp(HloOpcode::kDivide, F64) .value() .clock_cycles()) > (300), actual: 296 vs 300\n```\n\nThat said, I ran 1000 tests and did not encounter this issue. Reducing the threshold to 280 since the bound seems very close and flaky test is no good either way.\n\n\n### Code Patch:\n@@ -45,7 +45,7 @@ TEST_F(HloOpProfilerTest, BasicMeasurementsAreCorrect) {\n   EXPECT_GT(profiler.MeasureClockCyclesPerOp(HloOpcode::kDivide, F64)\n                 .value()\n                 .clock_cycles(),\n-            300);\n+            280);\n   // c128 sqrt is slow.\n   EXPECT_GT(profiler.MeasureClockCyclesPerOp(HloOpcode::kSqrt, C128)\n                 .value()\n"}
{"id": 245, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Feature or Bugfix\r\n- Workaround\r\n\r\n### Purpose\r\n- Reduce distration due to failed continuous integration pipelines.\r\n\r\n### Detail\r\n- Increase the timeout duration in `linkcheck`-related test roots by a factor of 5x, to a quarter-second.\r\n\r\n### Relates\r\n- May resolve #12159.\n\n### Code Patch:\n@@ -1,3 +1,3 @@\n exclude_patterns = ['_build']\n linkcheck_anchors = True\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,3 +1,3 @@\n exclude_patterns = ['_build']\n linkcheck_anchors = True\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -3,4 +3,4 @@\n     '^broken_link$',\n     'br[0-9]ken_link',\n ]\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,3 +1,3 @@\n exclude_patterns = ['_build']\n linkcheck_anchors = True\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,2 +1,2 @@\n exclude_patterns = ['_build']\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,2 +1,2 @@\n exclude_patterns = ['_build']\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,2 +1,2 @@\n exclude_patterns = ['_build']\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,2 +1,2 @@\n exclude_patterns = ['_build']\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,3 +1,3 @@\n exclude_patterns = ['_build']\n linkcheck_anchors = True\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n\n@@ -1,4 +1,4 @@\n root_doc = 'links'\n exclude_patterns = ['_build']\n linkcheck_anchors = True\n-linkcheck_timeout = 0.05\n+linkcheck_timeout = 0.25\n"}
{"id": 338, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThis PR implements the Fullscreen API. \r\n\r\nThe final commit also adds some Qt UI (an \"exit fullscreen\" button that animates from the top down, sort of like how Chrome does it, and also Firefox). For Qt-backends, the escape key also exits out of fullscreen, fully.\r\n\r\nThe spec can be found [here](https://fullscreen.spec.whatwg.org/).\r\n\r\nNew web platform test results (fullscreen/api) with this patch series applied:\r\n\r\n```\r\nRan 57 tests finished in 13.7 seconds.\r\n  ‚Ä¢ 36 ran as expected. 0 tests skipped.\r\n  ‚Ä¢ 1 tests had errors unexpectedly\r\n  ‚Ä¢ 1 tests timed out unexpectedly\r\n  ‚Ä¢ 21 tests had unexpected subtest results\r\n```\r\n(up from 0 expected to 36. Another test succeeds when #4329 is applied)\r\n\r\nAdditional work that needs to happen:\r\n- https://fullscreen.spec.whatwg.org/#dom-document-fullscreenenabled needs to be implemented fully, where a document's \"allowed to use\" can be deterimined by the `allowFullscreen` attribute set on an iframe. This attribute can not be changed dynamically and once set for a document, is set for the rest of it's life time.\r\n\r\nAdditional work that's required can be determined by the WPT suite.\n\n### Code Patch:\n@@ -933,6 +933,32 @@ button, meter, progress, select {\n     animation-play-state: inherit;\n }\n \n+/* Fullscreen API defaults https://fullscreen.spec.whatwg.org/#user-agent-level-style-sheet-defaults */\n+*|*:not(:root):fullscreen {\n+  position:fixed !important;\n+  inset:0 !important;\n+  margin:0 !important;\n+  box-sizing:border-box !important;\n+  min-width:0 !important;\n+  max-width:none !important;\n+  min-height:0 !important;\n+  max-height:none !important;\n+  width:100% !important;\n+  height:100% !important;\n+  transform:none !important;\n+  /* intentionally not !important */\n+  object-fit:contain;\n+}\n+\n+iframe:fullscreen {\n+  border:none !important;\n+  padding:0 !important;\n+}\n+\n+*|*:not(:root):fullscreen::backdrop {\n+  background:black;\n+}\n+\n /* Default cross-fade transition */\n @keyframes -ua-view-transition-fade-out {\n     to { opacity: 0; }\n\n@@ -47,6 +47,9 @@\n   \"focus-within\": {\n     \"argument\": \"\"\n   },\n+  \"fullscreen\": {\n+    \"argument\": \"\"\n+  },\n   \"has\": {\n     \"argument\": \"<relative-selector-list>\"\n   },\n\n@@ -8,6 +8,7 @@\n #include <LibWeb/CSS/ComputedProperties.h>\n #include <LibWeb/CSS/Keyword.h>\n #include <LibWeb/CSS/Parser/Parser.h>\n+#include <LibWeb/CSS/PseudoClass.h>\n #include <LibWeb/CSS/SelectorEngine.h>\n #include <LibWeb/DOM/Attr.h>\n #include <LibWeb/DOM/Document.h>\n@@ -555,6 +556,9 @@ static inline bool matches_pseudo_class(CSS::Selector::SimpleSelector::PseudoCla\n         auto* focused_element = element.document().focused_element();\n         return focused_element && element.is_inclusive_ancestor_of(*focused_element);\n     }\n+    case CSS::PseudoClass::Fullscreen: {\n+        return element.is_fullscreen_element();\n+    }\n     case CSS::PseudoClass::FirstChild:\n         if (context.collect_per_element_selector_involvement_metadata) {\n             const_cast<DOM::Element&>(element).set_affected_by_sibling_position_or_count_pseudo_class(true);\n\n@@ -5,6 +5,7 @@\n  * Copyright (c) 2021-2024, Sam Atkins <sam@ladybird.org>\n  * Copyright (c) 2024, Matthew Olsson <mattco@serenityos.org>\n  * Copyright (c) 2025, Jelle Raaijmakers <jelle@ladybird.org>\n+ * Copyright (c) 2025, Simon Farre <simon.farre.cx@gmail.com>\n  *\n  * SPDX-License-Identifier: BSD-2-Clause\n  */\n@@ -135,6 +136,7 @@\n #include <LibWeb/HTML/Scripting/Agent.h>\n #include <LibWeb/HTML/Scripting/ClassicScript.h>\n #include <LibWeb/HTML/Scripting/ExceptionReporter.h>\n+#include <LibWeb/HTML/Scripting/TemporaryExecutionContext.h>\n #include <LibWeb/HTML/Scripting/WindowEnvironmentSettingsObject.h>\n #include <LibWeb/HTML/SharedResourceRequest.h>\n #include <LibWeb/HTML/Storage.h>\n@@ -156,6 +158,7 @@\n #include <LibWeb/Painting/DisplayList.h>\n #include <LibWeb/Painting/ViewportPaintable.h>\n #include <LibWeb/PermissionsPolicy/AutoplayAllowlist.h>\n+#include <LibWeb/Platform/EventLoopPlugin.h>\n #include <LibWeb/ResizeObserver/ResizeObserver.h>\n #include <LibWeb/ResizeObserver/ResizeObserverEntry.h>\n #include <LibWeb/SVG/SVGDecodedImageData.h>\n@@ -603,6 +606,10 @@ void Document::visit_edges(Cell::Visitor& visitor)\n         visitor.visit(event.target);\n     }\n \n+    for (auto& event : m_pending_fullscreen_events) {\n+        visitor.visit(event.element);\n+    }\n+\n     visitor.visit(m_adopted_style_sheets);\n     visitor.visit(m_script_blocking_style_sheet_set);\n \n@@ -4098,6 +4105,7 @@ void Document::run_unloading_cleanup_steps()\n     }\n \n     FileAPI::run_unloading_cleanup_steps(*this);\n+    fully_exit_fullscreen();\n }\n \n // https://html.spec.whatwg.org/multipage/document-lifecycle.html#destroy-a-document\n@@ -4452,6 +4460,9 @@ bool Document::is_allowed_to_use_feature(PolicyControlledFeature feature) const\n     case PolicyControlledFeature::FocusWithoutUserActivation:\n         // FIXME: Implement allowlist for this.\n         return true;\n+    case PolicyControlledFeature::Fullscreen:\n+        // FIXME: Implement the permissions policy specification\n+        return true;\n     }\n \n     // 4. Return false.\n@@ -6152,6 +6163,7 @@ void Document::remove_an_element_from_the_top_layer_immediately(GC::Ref<Element>\n     // FIXME: 3. Remove the UA !important overlay: auto rule targeting el, if it exists.\n     element->set_rendered_in_top_layer(false);\n     element->set_needs_style_update(true);\n+    invalidate_layout_tree(InvalidateLayoutTreeReason::DocumentRemoveAnElementFromTheTopLayer);\n }\n \n // https://drafts.csswg.org/css-position-4/#process-top-layer-removals\n@@ -6293,6 +6305,315 @@ void Document::remove_render_blocking_element(GC::Ref<Element> element)\n     m_render_blocking_elements.remove(element);\n }\n \n+// https://fullscreen.spec.whatwg.org/#run-the-fullscreen-steps\n+void Document::run_fullscreen_steps()\n+{\n+    // 1. Let pendingEvents be document‚Äôs list of pending fullscreen events.\n+    auto pending_events = GC::ConservativeVector<PendingFullscreenEvent> { vm().heap() };\n+    pending_events.extend(m_pending_fullscreen_events);\n+\n+    // 2. Empty document‚Äôs list of pending fullscreen events.\n+    m_pending_fullscreen_events.clear();\n+\n+    // 3. For each (type, element) in pendingEvents:\n+    for (auto const& [type, element] : pending_events) {\n+        // 1. Let target be element if element is connected and its node document is document, and otherwise let target be document.\n+        EventTarget* target = nullptr;\n+        if (element->is_connected() && &element->document() == this) {\n+            target = element;\n+        } else {\n+            target = this;\n+        }\n+        // 2. Fire an event named type, with its bubbles and composed attributes set to true, at target.\n+        switch (type) {\n+        case PendingFullscreenEvent::Type::Change:\n+            target->dispatch_event(Event::create(realm(), HTML::EventNames::fullscreenchange, EventInit { .bubbles = true, .composed = true }));\n+            break;\n+        case PendingFullscreenEvent::Type::Error:\n+            target->dispatch_event(Event::create(realm(), HTML::EventNames::fullscreenerror, EventInit { .bubbles = true, .composed = true }));\n+            break;\n+        }\n+    }\n+}\n+\n+void Document::append_pending_fullscreen_change(PendingFullscreenEvent::Type type, GC::Ref<Element> element)\n+{\n+    m_pending_fullscreen_events.append(PendingFullscreenEvent { type, element });\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#fullscreen-an-element\n+void Document::fullscreen_element_within_doc(GC::Ref<Element> element)\n+{\n+    auto const get_hide_until = [&](auto const& popover_list) {\n+        return HTML::HTMLElement::topmost_popover_ancestor(element, popover_list, nullptr, HTML::IsPopover::No);\n+    };\n+\n+    // 1. Let hideUntil be the result of running topmost popover ancestor given\n+    // element, null, and false.\n+    auto hide_until = get_hide_until(showing_hint_popover_list());\n+\n+    // Finding topmost popover ancestor algorithm takes different parameters than those\n+    // described by the fullscreen spec. Since the new algorithm takes 4 parameters, with the new \"popover list\"\n+    // we must also account for the auto popover list.\n+    // More can be read about this \"spec bug\" in https://github.com/whatwg/fullscreen/issues/245\n+    if (hide_until == nullptr)\n+        hide_until = get_hide_until(showing_auto_popover_list());\n+\n+    // Our hide_all_popovers_until takes a variant. topmost_popover_ancestor produces a Ptr<HTMLElement>\n+    Variant<GC::Ptr<HTML::HTMLElement>, GC::Ptr<Document>> hide_until_argument { hide_until };\n+\n+    // 2. If hideUntil is null, then set hideUntil to element‚Äôs node document.\n+    if (hide_until == nullptr)\n+        hide_until_argument = element->owner_document();\n+\n+    // 3. Run hide all popovers until given hideUntil, false, and true.\n+    HTML::HTMLElement::hide_all_popovers_until(hide_until_argument, HTML::FocusPreviousElement::No, HTML::FireEvents::Yes);\n+\n+    // 4. Set element‚Äôs fullscreen flag.\n+    element->set_fullscreen_flag(true);\n+    // 5. Remove from the top layer immediately given element.\n+    remove_an_element_from_the_top_layer_immediately(element);\n+    // 6. Add to the top layer given element.\n+    add_an_element_to_the_top_layer(element);\n+    element->invalidate_style(StyleInvalidationReason::Fullscreen);\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#fullscreen-element\n+GC::Ptr<Element> Document::fullscreen_element() const\n+{\n+    for (auto const& el : top_layer_elements().in_reverse()) {\n+        if (el->is_fullscreen_element())\n+            return el;\n+    }\n+    return nullptr;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#dom-document-fullscreenelement\n+GC::Ptr<Element> Document::fullscreen_element_for_bindings() const\n+{\n+    GC::Ptr<Element> fullscreen_element = this->fullscreen_element();\n+\n+    if (!fullscreen_element) {\n+        return nullptr;\n+    }\n+\n+    // 1. If this is a shadow root and its host is not connected, then return null.\n+    // Note: We're not a shadow root. See ShadowRoot::fullscreen_element_for_bindings() instead.\n+    // 2. Let candidate be the result of retargeting fullscreen element against this.\n+    auto* candidate = retarget(fullscreen_element.ptr(), const_cast<Document*>(this));\n+    if (!candidate) {\n+        return nullptr;\n+    }\n+    // 3. If candidate and this are in the same tree, then return candidate.\n+    if (auto* retargeted_element = as<Element>(candidate); retargeted_element && &retargeted_element->root() == &root()) {\n+        return retargeted_element;\n+    }\n+    // 4. Return null.\n+    return nullptr;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#dom-document-fullscreen\n+bool Document::fullscreen() const\n+{\n+    // The fullscreen getter steps are to return false if this's fullscreen element is null, and true otherwise.\n+    return fullscreen_element() != nullptr;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#dom-document-fullscreenenabled\n+bool Document::fullscreen_enabled() const\n+{\n+    // FIXME: Implement check policy check and \"is supported\" check.\n+    return is_allowed_to_use_feature(PolicyControlledFeature::Fullscreen);\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#fully-exit-fullscreen\n+void Document::fully_exit_fullscreen()\n+{\n+    // 1. If document‚Äôs fullscreen element is null, terminate these steps.\n+    GC::Ptr<Element> fullscreened_element = fullscreen_element();\n+    if (!fullscreened_element)\n+        return;\n+\n+    // 2. Unfullscreen elements whose fullscreen flag is set, within document‚Äôs top layer, except for document‚Äôs fullscreen element.\n+    GC::RootVector<GC::Ref<Element>, 8> fullscreen_elements { heap() };\n+    for (auto const& element : top_layer_elements()) {\n+        if (element->is_fullscreen_element() && element != fullscreened_element)\n+            fullscreen_elements.append(element);\n+    }\n+\n+    for (auto const& element : fullscreen_elements) {\n+        unfullscreen_element(element);\n+    }\n+\n+    // 3. Exit fullscreen document.\n+\n+    // Note/FIXME: Because Document::destroy() does not \"Assert: this is running as part of a task queued on document's relevant agent's event loop.\" and doesn't seem\n+    // to have any temporary execution context while it's running (because it's making exit_fullscreen crash), we check if there's an execution context\n+    // if not, it means this is running via the \"run_unloading_cleanup_steps\" and thus we must first add a context.\n+    HTML::TemporaryExecutionContext context(realm(), HTML::TemporaryExecutionContext::CallbacksEnabled::Yes);\n+    (void)exit_fullscreen();\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#exit-fullscreen\n+GC::Ref<WebIDL::Promise> Document::exit_fullscreen()\n+{\n+    auto& realm = this->realm();\n+\n+    auto* doc = this;\n+    // 1. Let promise be a new promise.\n+    auto promise = WebIDL::create_promise(realm);\n+\n+    // 2. If doc is not fully active or doc‚Äôs fullscreen element is null, then reject promise with a TypeError exception and return promise.\n+    if (!is_fully_active() || !fullscreen_element()) {\n+        WebIDL::reject_promise(realm, promise, JS::TypeError::create(realm, \"Document not fully active or no fullscreen element.\"sv));\n+        return promise;\n+    }\n+\n+    // 3. Let resize be false.\n+    bool resize = false;\n+\n+    // 4. Let docs be the result of collecting documents to unfullscreen given doc.\n+    GC::RootVector<GC::Ref<Document>> docs = doc->collect_documents_to_unfullscreen();\n+\n+    // 5. Let topLevelDoc be doc‚Äôs node navigable‚Äôs top-level traversable‚Äôs active document.\n+    auto top_level_doc = navigable()->top_level_traversable()->active_document();\n+\n+    // 6. If topLevelDoc is in docs, and it is a simple fullscreen document, then set doc to topLevelDoc and resize to true.\n+    if (top_level_doc->is_simple_fullscreen_document() && docs.contains_slow(GC::Ref { *top_level_doc })) {\n+        doc = top_level_doc;\n+        resize = true;\n+    }\n+\n+    // 7. If doc‚Äôs fullscreen element is not connected:\n+    if (auto fullscreen_element = doc->fullscreen_element(); !fullscreen_element->is_connected()) {\n+        // 1. Append (fullscreenchange, doc‚Äôs fullscreen element) to doc‚Äôs list of pending fullscreen events.\n+        doc->append_pending_fullscreen_change(PendingFullscreenEvent::Type::Change, *fullscreen_element);\n+        // 2. Unfullscreen doc‚Äôs fullscreen element.\n+        doc->unfullscreen_element(*fullscreen_element);\n+    }\n+\n+    // 8. Return promise, and run the remaining steps in parallel.\n+    Platform::EventLoopPlugin::the().deferred_invoke(GC::create_function(heap(), [&realm, doc = GC::Ref { *doc }, promise, resize]() {\n+        HTML::TemporaryExecutionContext context(realm, HTML::TemporaryExecutionContext::CallbacksEnabled::Yes);\n+        // FIXME: 9. Run the fully unlock the screen orientation steps with doc.\n+        // 10. If resize is true, resize doc‚Äôs viewport to its \"normal\" dimensions.\n+        // N.B: Fullscreen API is affected by site-isolation and will require additional work once site-isolation is implemented.\n+        if (resize)\n+            doc->page().client().page_did_request_exit_fullscreen();\n+\n+        // 11. If doc‚Äôs fullscreen element is null, then resolve promise with undefined and terminate these steps.\n+        if (!doc->fullscreen_element()) {\n+            WebIDL::resolve_promise(realm, promise, JS::js_undefined());\n+            return;\n+        }\n+\n+        // 12. Let exitDocs be the result of collecting documents to unfullscreen given doc.\n+        auto exit_docs = doc->collect_documents_to_unfullscreen();\n+\n+        // 13. Let descendantDocs be an ordered set consisting of doc‚Äôs descendant navigables' active documents whose fullscreen element is non-null, if any, in tree order.\n+        GC::RootVector<GC::Ref<Document>> descendant_docs(realm.heap());\n+        for (auto& descendant : doc->descendant_navigables()) {\n+            if (descendant->active_document()->fullscreen_element())\n+                descendant_docs.append(*descendant->active_document());\n+        }\n+\n+        // 14. For each exitDoc in exitDocs:\n+        for (auto const& exit_doc : exit_docs) {\n+            // 1. Append (fullscreenchange, exitDoc‚Äôs fullscreen element) to exitDoc‚Äôs list of pending fullscreen events.\n+            exit_doc->append_pending_fullscreen_change(PendingFullscreenEvent::Type::Change, *exit_doc->fullscreen_element());\n+            if (resize) {\n+                // 2. If resize is true, unfullscreen exitDoc.\n+                for (auto el : exit_doc->top_layer_elements()) {\n+                    if (el->is_fullscreen_element())\n+                        exit_doc->unfullscreen_element(el);\n+                }\n+            } else {\n+                // 3. Otherwise, unfullscreen exitDoc‚Äôs fullscreen element.\n+                exit_doc->unfullscreen_element(*exit_doc->fullscreen_element());\n+            }\n+        }\n+\n+        // 15. For each descendantDoc in descendantDocs:\n+        for (auto& descendant_doc : descendant_docs) {\n+            // 1. Append (fullscreenchange, descendantDoc‚Äôs fullscreen element) to descendantDoc‚Äôs list of pending fullscreen events.\n+            descendant_doc->append_pending_fullscreen_change(PendingFullscreenEvent::Type::Change, *descendant_doc->fullscreen_element());\n+            // 2. Unfullscreen descendantDoc.\n+            GC::RootVector<GC::Ref<Element>, 8> fullscreen_elements(realm.heap());\n+            for (auto const& element : descendant_doc->top_layer_elements()) {\n+                if (element->is_fullscreen_element())\n+                    fullscreen_elements.append(element);\n+            }\n+\n+            for (auto& el : fullscreen_elements) {\n+                descendant_doc->unfullscreen_element(el);\n+            }\n+        }\n+\n+        // NOTE: The order in which documents are unfullscreened is not observable, because run the fullscreen steps is invoked in tree order.\n+        // 16. Resolve promise with undefined.\n+        WebIDL::resolve_promise(realm, promise, JS::js_undefined());\n+    }));\n+    return promise;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#simple-fullscreen-document\n+bool Document::is_simple_fullscreen_document() const\n+{\n+    // A document is said to be a simple fullscreen document if there is exactly one element in its top layer that has its fullscreen flag set.\n+    u32 total = 0;\n+    for (auto const& element : top_layer_elements()) {\n+        if (element->is_fullscreen_element())\n+            ++total;\n+\n+        if (total > 1)\n+            return false;\n+    }\n+    return total == 1;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#collect-documents-to-unfullscreen\n+GC::RootVector<GC::Ref<Document>> Document::collect_documents_to_unfullscreen() const\n+{\n+    // 1. Let docs be an ordered set consisting of doc.\n+    GC::RootVector<GC::Ref<Document>> docs { heap() };\n+    docs.append(GC::Ref { *const_cast<Document*>(this) });\n+    // 2. While true:\n+    while (true) {\n+        //  1. Let lastDoc be docs‚Äôs last document.\n+        auto last_doc = docs.last();\n+        //  2. Assert: lastDoc‚Äôs fullscreen element is not null.\n+        VERIFY(last_doc->fullscreen_element());\n+        //  3. If lastDoc is not a simple fullscreen document, break.\n+        if (!last_doc->is_simple_fullscreen_document())\n+            break;\n+        //  4. Let container be lastDoc‚Äôs node navigable‚Äôs container.\n+        // Note on spec: It doesn't first check if `node navigable` is null.\n+        auto container = last_doc->navigable() ? last_doc->navigable()->container() : nullptr;\n+        //  5. If container is null, then break.\n+        if (!container)\n+            break;\n+        //  6. If container‚Äôs iframe fullscreen flag is set, break.\n+        if (auto* iframe_element = as<HTML::HTMLIFrameElement>(container.ptr()); iframe_element->iframe_fullscreen_flag())\n+            break;\n+\n+        //  7. Append container‚Äôs node document to docs.\n+        docs.append(container->document());\n+    }\n+    // 3. Return docs.\n+    return docs;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#unfullscreen-an-element\n+void Document::unfullscreen_element(GC::Ref<Element> element)\n+{\n+    // To unfullscreen an element, unset element‚Äôs fullscreen flag and iframe fullscreen flag (if any), and remove from the top layer immediately given element.\n+    element->set_fullscreen_flag(false);\n+    if (HTML::HTMLIFrameElement* iframe_element = as_if<HTML::HTMLIFrameElement>(element.ptr()))\n+        iframe_element->set_iframe_fullscreen_flag(false);\n+\n+    remove_an_element_from_the_top_layer_immediately(element);\n+}\n+\n // https://dom.spec.whatwg.org/#document-allow-declarative-shadow-roots\n void Document::set_allow_declarative_shadow_roots(bool allow)\n {\n@@ -6615,6 +6936,26 @@ void Document::set_onvisibilitychange(WebIDL::CallbackType* value)\n     set_event_handler_attribute(HTML::EventNames::visibilitychange, value);\n }\n \n+WebIDL::CallbackType* Document::onfullscreenchange()\n+{\n+    return event_handler_attribute(HTML::EventNames::fullscreenchange);\n+}\n+\n+void Document::set_onfullscreenchange(WebIDL::CallbackType* value)\n+{\n+    set_event_handler_attribute(HTML::EventNames::fullscreenchange, value);\n+}\n+\n+WebIDL::CallbackType* Document::onfullscreenerror()\n+{\n+    return event_handler_attribute(HTML::EventNames::fullscreenerror);\n+}\n+\n+void Document::set_onfullscreenerror(WebIDL::CallbackType* value)\n+{\n+    set_event_handler_attribute(HTML::EventNames::fullscreenerror, value);\n+}\n+\n ElementByIdMap& Document::element_by_id() const\n {\n     if (!m_element_by_id)\n\n@@ -56,6 +56,7 @@ enum class QuirksMode {\n \n #define ENUMERATE_INVALIDATE_LAYOUT_TREE_REASONS(X)       \\\n     X(DocumentAddAnElementToTheTopLayer)                  \\\n+    X(DocumentRemoveAnElementFromTheTopLayer)             \\\n     X(DocumentRequestAnElementToBeRemovedFromTheTopLayer) \\\n     X(ShadowRootSetInnerHTML)\n \n@@ -160,6 +161,15 @@ struct ElementCreationOptions {\n enum class PolicyControlledFeature : u8 {\n     Autoplay,\n     FocusWithoutUserActivation,\n+    Fullscreen,\n+};\n+\n+struct PendingFullscreenEvent {\n+    enum class Type {\n+        Change,\n+        Error,\n+    } type;\n+    GC::Ref<Element> element;\n };\n \n class WEB_API Document\n@@ -856,6 +866,12 @@ class WEB_API Document\n     [[nodiscard]] WebIDL::CallbackType* onvisibilitychange();\n     void set_onvisibilitychange(WebIDL::CallbackType*);\n \n+    // https://fullscreen.spec.whatwg.org/#api\n+    [[nodiscard]] WebIDL::CallbackType* onfullscreenchange();\n+    void set_onfullscreenchange(WebIDL::CallbackType*);\n+    [[nodiscard]] WebIDL::CallbackType* onfullscreenerror();\n+    void set_onfullscreenerror(WebIDL::CallbackType*);\n+\n     void reset_cursor_blink_cycle();\n \n     GC::Ref<EditingHostManager> editing_host_manager() const { return *m_editing_host_manager; }\n@@ -905,6 +921,22 @@ class WEB_API Document\n \n     ElementByIdMap& element_by_id() const;\n \n+    // https://fullscreen.spec.whatwg.org/#run-the-fullscreen-steps\n+    void run_fullscreen_steps();\n+    void append_pending_fullscreen_change(PendingFullscreenEvent::Type type, GC::Ref<Element> element);\n+\n+    void fullscreen_element_within_doc(GC::Ref<Element> element);\n+    GC::Ptr<Element> fullscreen_element() const;\n+    GC::Ptr<Element> fullscreen_element_for_bindings() const;\n+\n+    bool fullscreen() const;\n+    bool fullscreen_enabled() const;\n+\n+    void fully_exit_fullscreen();\n+    GC::Ref<WebIDL::Promise> exit_fullscreen();\n+\n+    void unfullscreen_element(GC::Ref<Element> element);\n+\n     auto& script_blocking_style_sheet_set() { return m_script_blocking_style_sheet_set; }\n     auto const& script_blocking_style_sheet_set() const { return m_script_blocking_style_sheet_set; }\n \n@@ -939,6 +971,9 @@ class WEB_API Document\n \n     void evaluate_media_rules();\n \n+    bool is_simple_fullscreen_document() const;\n+    GC::RootVector<GC::Ref<Document>> collect_documents_to_unfullscreen() const;\n+\n     enum class AddLineFeed {\n         Yes,\n         No,\n@@ -1287,6 +1322,9 @@ class WEB_API Document\n \n     // https://www.w3.org/TR/css-properties-values-api-1/#dom-window-registeredpropertyset-slot\n     HashMap<FlyString, GC::Ref<Web::CSS::CSSPropertyRule>> m_registered_custom_properties;\n+\n+    // https://fullscreen.spec.whatwg.org/#list-of-pending-fullscreen-events\n+    Vector<PendingFullscreenEvent> m_pending_fullscreen_events;\n };\n \n template<>\n\n@@ -159,6 +159,15 @@ interface Document : Node {\n \n     // https://www.w3.org/TR/SVG2/struct.html#InterfaceDocumentExtensions\n     readonly attribute SVGSVGElement? rootElement;\n+\n+    // https://fullscreen.spec.whatwg.org/#api\n+    attribute EventHandler onfullscreenchange;\n+    attribute EventHandler onfullscreenerror;\n+    // FIXME: [LegacyLenientSetter]\n+    readonly attribute boolean fullscreenEnabled;\n+    // FIXME: [LegacyLenientSetter, Unscopable]\n+    readonly attribute boolean fullscreen; // historical\n+    Promise<undefined> exitFullscreen();\n };\n \n dictionary ElementCreationOptions {\n\n@@ -8,4 +8,9 @@ interface mixin DocumentOrShadowRoot {\n \n     // https://www.w3.org/TR/web-animations-1/#extensions-to-the-documentorshadowroot-interface-mixin\n     sequence<Animation> getAnimations();\n+\n+    // https://fullscreen.spec.whatwg.org/#fullscreen-an-element\n+    // FIXME: [LegacyLenientSetter] not implemented\n+    [ImplementedAs=fullscreen_element_for_bindings]\n+    readonly attribute Element? fullscreenElement;\n };\n\n@@ -1,6 +1,7 @@\n /*\n  * Copyright (c) 2018-2024, Andreas Kling <andreas@ladybird.org>\n  * Copyright (c) 2022-2023, Sam Atkins <atkinssj@serenityos.org>\n+ * Copyright (c) 2025, Simon Farre <simon.farre.cx@gmail.com>\n  *\n  * SPDX-License-Identifier: BSD-2-Clause\n  */\n@@ -53,9 +54,11 @@\n #include <LibWeb/HTML/HTMLAreaElement.h>\n #include <LibWeb/HTML/HTMLBodyElement.h>\n #include <LibWeb/HTML/HTMLButtonElement.h>\n+#include <LibWeb/HTML/HTMLDialogElement.h>\n #include <LibWeb/HTML/HTMLFieldSetElement.h>\n #include <LibWeb/HTML/HTMLFrameSetElement.h>\n #include <LibWeb/HTML/HTMLHtmlElement.h>\n+#include <LibWeb/HTML/HTMLIFrameElement.h>\n #include <LibWeb/HTML/HTMLInputElement.h>\n #include <LibWeb/HTML/HTMLLIElement.h>\n #include <LibWeb/HTML/HTMLMenuElement.h>\n@@ -82,10 +85,12 @@\n #include <LibWeb/Layout/ListItemBox.h>\n #include <LibWeb/Layout/TreeBuilder.h>\n #include <LibWeb/Layout/Viewport.h>\n+#include <LibWeb/MathML/MathMLElement.h>\n #include <LibWeb/Namespace.h>\n #include <LibWeb/Page/Page.h>\n #include <LibWeb/Painting/PaintableBox.h>\n #include <LibWeb/Painting/ViewportPaintable.h>\n+#include <LibWeb/Platform/EventLoopPlugin.h>\n #include <LibWeb/SVG/SVGAElement.h>\n #include <LibWeb/Selection/Selection.h>\n #include <LibWeb/WebIDL/AbstractOperations.h>\n@@ -1388,6 +1393,7 @@ void Element::removed_from(Node* old_parent, Node& old_root)\n \n     play_or_cancel_animations_after_display_property_change();\n     remove_animations_from_timeline();\n+    removing_steps_fullscreen();\n }\n \n void Element::moved_from(GC::Ptr<Node> old_parent)\n@@ -2177,6 +2183,230 @@ WebIDL::ExceptionOr<void> Element::insert_adjacent_html(String const& position,\n     return {};\n }\n \n+// Used to signal what message should be shown when the promise for the algorithm rejects..\n+enum class RequestFullscreenError : u8 {\n+    False,\n+    ElementReadyCheckFailed,\n+    UnsupportedElement,\n+    NoTransientUserActivation,\n+    ElementNodeDocIsNotPendingDoc\n+};\n+\n+static constexpr String to_string(RequestFullscreenError error)\n+{\n+    switch (error) {\n+    // This should never be called with this value\n+    case RequestFullscreenError::False:\n+        return \"false\"_string;\n+    case RequestFullscreenError::ElementReadyCheckFailed:\n+        return \"Element ready check failed\"_string;\n+    case RequestFullscreenError::UnsupportedElement:\n+        return \"Not supported element\"_string;\n+    case RequestFullscreenError::NoTransientUserActivation:\n+        return \"No transient user activation available to consume\"_string;\n+    case RequestFullscreenError::ElementNodeDocIsNotPendingDoc:\n+        return \"Element's node document is not pending doc\"_string;\n+    }\n+    VERIFY_NOT_REACHED();\n+}\n+\n+// step 5 of requestFullscreen:\n+// 5. If any of conditions are false, set error to true\n+static RequestFullscreenError fullscreen_has_error_check(Element const& element)\n+{\n+    // This‚Äôs namespace is the HTML namespace or this is an SVG svg or MathML math element. [SVG] [MATHML]\n+    if (!(element.namespace_uri() == Namespace::HTML || element.is_svg_svg_element() || is<MathML::MathMLElement>(element)))\n+        return RequestFullscreenError::UnsupportedElement;\n+\n+    // This is not a dialog element\n+    if (is<HTML::HTMLDialogElement>(element))\n+        return RequestFullscreenError::UnsupportedElement;\n+\n+    // The fullscreen element ready check for this returns true.\n+    if (!element.element_ready_check())\n+        return RequestFullscreenError::ElementReadyCheckFailed;\n+    // FIXME: Implement 'Fullscreen is supported.' check\n+\n+    // This‚Äôs relevant global object has transient activation or\n+    // FIXME: the algorithm is triggered by a user generated orientation change.\n+    HTML::Window* window = as_if<HTML::Window>(&HTML::relevant_global_object(element));\n+\n+    // is_connected, in element_ready_check has returned true, there should be a window.\n+    ASSERT(window);\n+\n+    if (!window->has_transient_activation())\n+        return RequestFullscreenError::NoTransientUserActivation;\n+\n+    return RequestFullscreenError::False;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#fullscreen-element-ready-check\n+bool Element::element_ready_check() const\n+{\n+    // A fullscreen element ready check for an element element returns true if all of the following are true, and false otherwise:\n+\n+    // element is connected.\n+    if (!is_connected())\n+        return false;\n+\n+    // element‚Äôs node document is allowed to use the \"fullscreen\" feature.\n+    if (!m_document->is_allowed_to_use_feature(PolicyControlledFeature::Fullscreen))\n+        return false;\n+\n+    // element namespace is not the HTML namespace or element‚Äôs popover visibility state is hidden.\n+    if (namespace_uri() != Namespace::HTML)\n+        return true;\n+\n+    auto const* html_element = as_if<HTML::HTMLElement>(this);\n+    return html_element ? (html_element->popover_visibility_state() == HTML::HTMLElement::PopoverVisibilityState::Hidden) : false;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#dom-element-requestfullscreen\n+GC::Ref<WebIDL::Promise> Element::request_fullscreen()\n+{\n+    auto& realm = this->realm();\n+\n+    // 1. Let pendingDoc be this‚Äôs node document.\n+    auto pending_doc = m_document;\n+\n+    // 2. Let promise be a new promise.\n+    auto promise = WebIDL::create_promise(realm);\n+\n+    // 3. If pendingDoc is not fully active, then reject promise with a TypeError exception and return promise.\n+    if (!pending_doc->is_fully_active()) {\n+        WebIDL::reject_promise(realm, promise, JS::TypeError::create(realm, \"Document not fully active.\"_string));\n+        return promise;\n+    }\n+\n+    // 4. Let error be false.\n+    // 5. If any of conditions are false, set error to true\n+    RequestFullscreenError error = fullscreen_has_error_check(*this);\n+\n+    // 6. If error is false, then consume user activation given pendingDoc‚Äôs relevant global object.\n+    if (error == RequestFullscreenError::False) {\n+        auto& relevant_global = as<HTML::Window>(relevant_global_object(*pending_doc));\n+        relevant_global.consume_user_activation();\n+    }\n+\n+    // 7. Return promise, and run the remaining steps in parallel.\n+    Platform::EventLoopPlugin::the().deferred_invoke(GC::create_function(heap(), [&realm, error, pending_doc, requesting_element = GC::Ref<Element> { *this }, promise]() mutable {\n+        HTML::TemporaryExecutionContext context(realm, HTML::TemporaryExecutionContext::CallbacksEnabled::Yes);\n+        // N.B: Fullscreen API is affected by site-isolation and will require additional work once site-isolation is implemented.\n+\n+        // 8. If error is false, then resize pendingDoc‚Äôs node navigable‚Äôs top-level traversable‚Äôs\n+        // active document‚Äôs viewport‚Äôs dimensions FIXME: optionally taking into account options[\"navigationUI\"]:\n+        if (error == RequestFullscreenError::False)\n+            pending_doc->page().client().page_did_request_fullscreen_window();\n+\n+        // 9. If any of the following conditions are false, then set error to true:\n+        //      This‚Äôs node document is pendingDoc.\n+        //      The fullscreen element ready check for this returns true.\n+        if (pending_doc != requesting_element->owner_document())\n+            error = RequestFullscreenError::ElementNodeDocIsNotPendingDoc;\n+        if (!requesting_element->element_ready_check())\n+            error = RequestFullscreenError::ElementReadyCheckFailed;\n+\n+        // 10. If error is true:\n+        //      Append (fullscreenerror, this) to pendingDoc‚Äôs list of pending fullscreen events.\n+        //      Reject promise with a TypeError exception and terminate these steps.\n+        if (error != RequestFullscreenError::False) {\n+            pending_doc->append_pending_fullscreen_change(PendingFullscreenEvent::Type::Error, requesting_element);\n+            WebIDL::reject_promise(realm, promise, JS::TypeError::create(realm, to_string(error)));\n+            return;\n+        }\n+\n+        // 11. Let fullscreenElements be an ordered set initially consisting of this.\n+        Vector<GC::Ref<Element>> fullscreen_elements;\n+\n+        fullscreen_elements.append(requesting_element);\n+        // 12. While true:\n+        //      1. Let last be the last item of fullscreenElements.\n+        //      2. Let container be last‚Äôs node navigable‚Äôs container.\n+        //      3. If container is null, then break.\n+        //      4. Append container to fullscreenElements.\n+        while (true) {\n+            auto last = fullscreen_elements.last();\n+            auto container = last->navigable()->container();\n+            if (!container)\n+                break;\n+\n+            fullscreen_elements.append(*container);\n+        }\n+\n+        // 13. For each element in fullscreenElements:\n+        for (GC::Ref<Element>& element : fullscreen_elements) {\n+            // 1. Let doc be element‚Äôs node document.\n+            Document& doc = element->document();\n+            // 2. If element is doc‚Äôs fullscreen element, continue.\n+            if (doc.fullscreen_element() == element) {\n+                // Spec note: No need to notify observers when nothing has changed.\n+                continue;\n+            }\n+            // 4. If element is this and this is an iframe element, then set element‚Äôs iframe fullscreen flag.\n+            if (element == requesting_element && requesting_element->is_html_iframe_element()) {\n+                auto& iframe_element = static_cast<HTML::HTMLIFrameElement&>(*element);\n+                iframe_element.set_iframe_fullscreen_flag(true);\n+            }\n+            // 5. Fullscreen element within doc.\n+            doc.fullscreen_element_within_doc(element);\n+            // 6. Append (fullscreenchange, element) to doc‚Äôs list of pending fullscreen events.\n+            doc.append_pending_fullscreen_change(PendingFullscreenEvent::Type::Change, element);\n+        }\n+\n+        // 14. Resolve promise with undefined\n+        WebIDL::resolve_promise(realm, promise, JS::js_undefined());\n+    }));\n+\n+    return promise;\n+}\n+\n+// https://fullscreen.spec.whatwg.org/#removing-steps\n+void Element::removing_steps_fullscreen()\n+{\n+    // 1. Let document be removedNode‚Äôs node document.\n+    auto& document = this->document();\n+    // 2. Let nodes be removedNode‚Äôs shadow-including inclusive descendants that have their fullscreen flag set, in shadow-including tree order.\n+    // 3. For each node in nodes:\n+    for_each_shadow_including_inclusive_descendant([&](Node& node) {\n+        Element* element = as_if<Element>(node);\n+        if (!element)\n+            return TraversalDecision::Continue;\n+\n+        if (element->is_fullscreen_element()) {\n+            if (document.fullscreen_element() == element) {\n+                // 1. If node is document‚Äôs fullscreen element, exit fullscreen document.\n+                document.exit_fullscreen();\n+            } else {\n+                // 2. Otherwise, unfullscreen node.\n+                document.unfullscreen_element(*element);\n+            }\n+            // 3. If document‚Äôs top layer contains node, remove from the top layer immediately given node\n+            document.remove_an_element_from_the_top_layer_immediately(*element);\n+        }\n+        return TraversalDecision::Continue;\n+    });\n+}\n+\n+GC::Ptr<WebIDL::CallbackType> Element::onfullscreenchange()\n+{\n+    return event_handler_attribute(HTML::EventNames::fullscreenchange);\n+}\n+\n+void Element::set_onfullscreenchange(GC::Ptr<WebIDL::CallbackType> event_handler)\n+{\n+    set_event_handler_attribute(HTML::EventNames::fullscreenchange, event_handler);\n+}\n+\n+GC::Ptr<WebIDL::CallbackType> Element::onfullscreenerror()\n+{\n+    return event_handler_attribute(HTML::EventNames::fullscreenerror);\n+}\n+\n+void Element::set_onfullscreenerror(GC::Ptr<WebIDL::CallbackType> event_handler)\n+{\n+    set_event_handler_attribute(HTML::EventNames::fullscreenerror, event_handler);\n+}\n+\n // https://dom.spec.whatwg.org/#insert-adjacent\n WebIDL::ExceptionOr<GC::Ptr<Node>> Element::insert_adjacent(StringView where, GC::Ref<Node> node)\n {\n\n@@ -245,6 +245,19 @@ class WEB_API Element\n \n     WebIDL::ExceptionOr<void> insert_adjacent_html(String const& position, String const&);\n \n+    bool element_ready_check() const;\n+    GC::Ref<WebIDL::Promise> request_fullscreen();\n+    void removing_steps_fullscreen();\n+\n+    void set_fullscreen_flag(bool is_fullscreen) { m_fullscreen_flag = is_fullscreen; }\n+    bool is_fullscreen_element() const { return m_fullscreen_flag; }\n+\n+    GC::Ptr<WebIDL::CallbackType> onfullscreenchange();\n+    void set_onfullscreenchange(GC::Ptr<WebIDL::CallbackType>);\n+\n+    GC::Ptr<WebIDL::CallbackType> onfullscreenerror();\n+    void set_onfullscreenerror(GC::Ptr<WebIDL::CallbackType>);\n+\n     WebIDL::ExceptionOr<String> outer_html() const;\n     WebIDL::ExceptionOr<void> set_outer_html(String const&);\n \n@@ -629,6 +642,7 @@ class WEB_API Element\n     bool m_affected_by_sibling_position_or_count_pseudo_class : 1 { false };\n     bool m_affected_by_nth_child_pseudo_class : 1 { false };\n     bool m_affected_by_has_pseudo_class_with_relative_selector_that_has_sibling_combinator : 1 { false };\n+    bool m_fullscreen_flag : 1 { false };\n \n     size_t m_sibling_invalidation_distance { 0 };\n \n\n@@ -125,6 +125,11 @@ interface Element : Node {\n     undefined setPointerCapture(long pointerId);\n     undefined releasePointerCapture(long pointerId);\n     boolean hasPointerCapture(long pointerId);\n+\n+    // partial interface for https://fullscreen.spec.whatwg.org/#api\n+    Promise<undefined> requestFullscreen();\n+    attribute EventHandler onfullscreenchange;\n+    attribute EventHandler onfullscreenerror;\n };\n \n dictionary GetHTMLOptions {\n\n@@ -64,6 +64,7 @@ enum class ShouldComputeRole {\n     X(EditingInsertion)                             \\\n     X(ElementAttributeChange)                       \\\n     X(ElementSetShadowRoot)                         \\\n+    X(Fullscreen)                                   \\\n     X(HTMLDialogElementSetIsModal)                  \\\n     X(HTMLHyperlinkElementHrefChange)               \\\n     X(HTMLIFrameElementGeometryChange)              \\\n\n@@ -9,6 +9,7 @@\n #include <LibWeb/DOM/Document.h>\n #include <LibWeb/DOM/Event.h>\n #include <LibWeb/DOM/ShadowRoot.h>\n+#include <LibWeb/DOM/Utils.h>\n #include <LibWeb/HTML/HTMLTemplateElement.h>\n #include <LibWeb/HTML/Parser/HTMLParser.h>\n #include <LibWeb/Layout/BlockContainer.h>\n@@ -31,6 +32,30 @@ void ShadowRoot::finalize()\n     document().unregister_shadow_root({}, *this);\n }\n \n+// https://fullscreen.spec.whatwg.org/#dom-document-fullscreenelement\n+GC::Ptr<Element> ShadowRoot::fullscreen_element_for_bindings() const\n+{\n+    // 1. If this is a shadow root and its host is not connected, then return null.\n+    if (!host() || !host()->is_connected())\n+        return nullptr;\n+\n+    // 2. Let candidate be the result of retargeting fullscreen element against this.\n+    // Note: ShadowRoot does not have it's own top layer. But the algorithm says to get the fullscreen\n+    // element from the top layer, so it's grabbed from this' document.\n+\n+    auto* candidate = retarget(const_cast<ShadowRoot*>(this)->document().fullscreen_element().ptr(), const_cast<ShadowRoot*>(this));\n+\n+    if (!candidate)\n+        return nullptr;\n+\n+    // 3. If candidate and this are in the same tree, then return candidate.\n+    if (auto* retargeted_element = as<Element>(candidate); &retargeted_element->root() == &root())\n+        return retargeted_element;\n+\n+    // 4. Return null.\n+    return nullptr;\n+}\n+\n void ShadowRoot::initialize(JS::Realm& realm)\n {\n     WEB_SET_PROTOTYPE_FOR_INTERFACE(ShadowRoot);\n\n@@ -68,6 +68,8 @@ class WEB_API ShadowRoot final : public DocumentFragment {\n \n     virtual void finalize() override;\n \n+    GC::Ptr<Element> fullscreen_element_for_bindings() const;\n+\n protected:\n     virtual void visit_edges(Cell::Visitor&) override;\n \n\n@@ -383,7 +383,10 @@ void EventLoop::update_the_rendering()\n         document->update_animations_and_send_events(HighResolutionTime::relative_high_resolution_time(frame_timestamp, relevant_global_object(*document)));\n     };\n \n-    // FIXME: 12. For each doc of docs, run the fullscreen steps for doc. [FULLSCREEN]\n+    // 12. For each doc of docs, run the fullscreen steps for doc. [FULLSCREEN]\n+    for (auto& document : docs) {\n+        document->run_fullscreen_steps();\n+    }\n \n     // FIXME: 13. For each doc of docs, if the user agent detects that the backing storage associated with a CanvasRenderingContext2D or an OffscreenCanvasRenderingContext2D, context, has been lost, then it must run the context lost steps for each such context:\n \n\n@@ -65,6 +65,8 @@ namespace Web::HTML::EventNames {\n     __ENUMERATE_HTML_EVENT(focusin)                  \\\n     __ENUMERATE_HTML_EVENT(focusout)                 \\\n     __ENUMERATE_HTML_EVENT(formdata)                 \\\n+    __ENUMERATE_HTML_EVENT(fullscreenchange)         \\\n+    __ENUMERATE_HTML_EVENT(fullscreenerror)          \\\n     __ENUMERATE_HTML_EVENT(hashchange)               \\\n     __ENUMERATE_HTML_EVENT(input)                    \\\n     __ENUMERATE_HTML_EVENT(invalid)                  \\\n\n@@ -37,6 +37,9 @@ class HTMLIFrameElement final\n \n     virtual void visit_edges(Cell::Visitor&) override;\n \n+    void set_iframe_fullscreen_flag(bool iframe_fullscreen_flag) { m_iframe_fullscreen_flag = iframe_fullscreen_flag; }\n+    bool iframe_fullscreen_flag() const { return m_iframe_fullscreen_flag; }\n+\n private:\n     HTMLIFrameElement(DOM::Document&, DOM::QualifiedName);\n \n@@ -62,6 +65,9 @@ class HTMLIFrameElement final\n     // https://html.spec.whatwg.org/multipage/iframe-embed-object.html#current-navigation-was-lazy-loaded\n     bool m_current_navigation_was_lazy_loaded { false };\n \n+    // https://fullscreen.spec.whatwg.org/#iframe-fullscreen-flag\n+    bool m_iframe_fullscreen_flag { false };\n+\n     // https://html.spec.whatwg.org/multipage/iframe-embed-object.html#iframe-pending-resource-timing-start-time\n     Optional<HighResolutionTime::DOMHighResTimeStamp> m_pending_resource_start_time = {};\n \n\n@@ -20,6 +20,8 @@\n #include <LibWeb/HTML/HTMLImageElement.h>\n #include <LibWeb/HTML/HTMLMediaElement.h>\n #include <LibWeb/HTML/HTMLVideoElement.h>\n+#include <LibWeb/HTML/Scripting/TemporaryExecutionContext.h>\n+#include <LibWeb/HTML/TraversableNavigable.h>\n #include <LibWeb/Layout/Label.h>\n #include <LibWeb/Layout/Viewport.h>\n #include <LibWeb/Page/DragAndDropEventHandler.h>\n@@ -1258,7 +1260,16 @@ EventResult EventHandler::handle_keydown(UIEvents::KeyCode key, u32 modifiers, u\n     }\n \n     // https://html.spec.whatwg.org/multipage/interaction.html#close-requests\n+    // FIXME: Close requests should queue a global task on the user interaction task source, given `document`'s relevant global object.\n     if (key == UIEvents::KeyCode::Key_Escape) {\n+        // 1. If document's fullscreen element is not null, then:\n+        if (document->fullscreen()) {\n+            // 1. Fully exit fullscreen given document's node navigable's top-level traversable's active document.\n+            m_navigable->top_level_traversable()->active_document()->fully_exit_fullscreen();\n+            // 2. Return.\n+            return EventResult::Handled;\n+        }\n+\n         // 7. Let closedSomething be the result of processing close watchers on document's relevant global object.\n         auto closed_something = document->window()->close_watcher_manager()->process_close_watchers();\n \n\n@@ -329,6 +329,7 @@ class PageClient : public JS::Cell {\n     virtual void page_did_request_maximize_window() { }\n     virtual void page_did_request_minimize_window() { }\n     virtual void page_did_request_fullscreen_window() { }\n+    virtual void page_did_request_exit_fullscreen() { }\n     virtual void page_did_start_loading(URL::URL const&, bool is_redirect) { (void)is_redirect; }\n     virtual void page_did_create_new_document(Web::DOM::Document&) { }\n     virtual void page_did_change_active_document_in_top_level_browsing_context(Web::DOM::Document&) { }\n\n@@ -43,6 +43,7 @@ HeadlessWebView::HeadlessWebView(Core::AnonymousBuffer theme, Web::DevicePixelSi\n     };\n \n     on_reposition_window = [this](auto position) {\n+        m_previous_dimensions.set_location(position.template to_type<Web::DevicePixels>());\n         client().async_set_window_position(m_client_state.page_index, position.template to_type<Web::DevicePixels>());\n \n         client().async_did_update_window_rect(m_client_state.page_index);\n@@ -67,6 +68,7 @@ HeadlessWebView::HeadlessWebView(Core::AnonymousBuffer theme, Web::DevicePixelSi\n \n     on_maximize_window = [this]() {\n         m_viewport_size = screen_rect.size();\n+        m_previous_dimensions = screen_rect;\n \n         client().async_set_window_position(m_client_state.page_index, screen_rect.location());\n         client().async_set_window_size(m_client_state.page_index, screen_rect.size());\n@@ -76,6 +78,7 @@ HeadlessWebView::HeadlessWebView(Core::AnonymousBuffer theme, Web::DevicePixelSi\n     };\n \n     on_fullscreen_window = [this]() {\n+        m_previous_dimensions.set_size(m_viewport_size);\n         m_viewport_size = screen_rect.size();\n \n         client().async_set_window_position(m_client_state.page_index, screen_rect.location());\n@@ -85,6 +88,15 @@ HeadlessWebView::HeadlessWebView(Core::AnonymousBuffer theme, Web::DevicePixelSi\n         client().async_did_update_window_rect(m_client_state.page_index);\n     };\n \n+    on_exit_fullscreen_window = [this]() {\n+        m_viewport_size = m_previous_dimensions.size();\n+        client().async_set_window_position(m_client_state.page_index, m_previous_dimensions.location());\n+        client().async_set_window_size(m_client_state.page_index, m_previous_dimensions.size());\n+        client().async_set_viewport_size(m_client_state.page_index, m_previous_dimensions.size());\n+\n+        client().async_did_update_window_rect(m_client_state.page_index);\n+    };\n+\n     on_request_alert = [this](auto const&) {\n         m_pending_dialog = Web::Page::PendingDialog::Alert;\n     };\n\n@@ -36,6 +36,9 @@ class WEBVIEW_API HeadlessWebView : public WebView::ViewImplementation {\n     Web::Page::PendingDialog m_pending_dialog { Web::Page::PendingDialog::None };\n     Optional<String> m_pending_prompt_text;\n \n+    // When restoring from fullscreen, we need to know to what dimension.\n+    Web::DevicePixelRect m_previous_dimensions;\n+\n     // FIXME: We should implement UI-agnostic platform APIs to interact with the system clipboard.\n     Optional<Web::Clipboard::SystemClipboardItem> m_clipboard;\n \n\n@@ -426,6 +426,11 @@ void ViewImplementation::js_console_request_messages(i32 start_index)\n     client().async_js_console_request_messages(page_id(), start_index);\n }\n \n+void ViewImplementation::exit_fullscreen()\n+{\n+    client().async_exit_fullscreen(page_id());\n+}\n+\n void ViewImplementation::alert_closed()\n {\n     client().async_alert_closed(page_id());\n\n@@ -124,6 +124,7 @@ class WEBVIEW_API ViewImplementation : public SettingsObserver {\n     void run_javascript(String const&);\n     void js_console_input(String const&);\n     void js_console_request_messages(i32 start_index);\n+    void exit_fullscreen();\n \n     void alert_closed();\n     void confirm_closed(bool accepted);\n@@ -219,6 +220,7 @@ class WEBVIEW_API ViewImplementation : public SettingsObserver {\n     Function<void()> on_maximize_window;\n     Function<void()> on_minimize_window;\n     Function<void()> on_fullscreen_window;\n+    Function<void()> on_exit_fullscreen_window;\n     Function<void(Color current_color)> on_request_color_picker;\n     Function<void(Web::HTML::FileFilter const& accepted_file_types, Web::HTML::AllowMultipleFiles)> on_request_file_picker;\n     Function<void(Gfx::IntPoint content_position, i32 minimum_width, Vector<Web::HTML::SelectItem> items)> on_request_select_dropdown;\n\n@@ -619,12 +619,21 @@ void WebContentClient::did_request_minimize_window(u64 page_id)\n     }\n }\n \n-void WebContentClient::did_request_fullscreen_window(u64 page_id)\n+Messages::WebContentClient::DidRequestFullscreenWindowResponse WebContentClient::did_request_fullscreen_window(u64 page_id)\n {\n     if (auto view = view_for_page_id(page_id); view.has_value()) {\n         if (view->on_fullscreen_window)\n             view->on_fullscreen_window();\n     }\n+    return true;\n+}\n+\n+void WebContentClient::did_request_exit_fullscreen(u64 page_id)\n+{\n+    if (auto view = view_for_page_id(page_id); view.has_value()) {\n+        if (view->on_fullscreen_window)\n+            view->on_exit_fullscreen_window();\n+    }\n }\n \n void WebContentClient::did_request_file(u64 page_id, ByteString path, i32 request_id)\n\n@@ -122,7 +122,8 @@ class WEBVIEW_API WebContentClient final\n     virtual void did_request_resize_window(u64 page_id, Gfx::IntSize) override;\n     virtual void did_request_maximize_window(u64 page_id) override;\n     virtual void did_request_minimize_window(u64 page_id) override;\n-    virtual void did_request_fullscreen_window(u64 page_id) override;\n+    virtual Messages::WebContentClient::DidRequestFullscreenWindowResponse did_request_fullscreen_window(u64 page_id) override;\n+    virtual void did_request_exit_fullscreen(u64 page_id) override;\n     virtual void did_request_file(u64 page_id, ByteString path, i32) override;\n     virtual void did_request_color_picker(u64 page_id, Color current_color) override;\n     virtual void did_request_file_picker(u64 page_id, Web::HTML::FileFilter accepted_file_types, Web::HTML::AllowMultipleFiles) override;\n\n@@ -35,6 +35,7 @@\n #include <LibWeb/Dump.h>\n #include <LibWeb/HTML/BrowsingContext.h>\n #include <LibWeb/HTML/HTMLInputElement.h>\n+#include <LibWeb/HTML/Scripting/TemporaryExecutionContext.h>\n #include <LibWeb/HTML/SelectedFile.h>\n #include <LibWeb/HTML/Storage.h>\n #include <LibWeb/HTML/TraversableNavigable.h>\n@@ -1331,4 +1332,12 @@ void ConnectionFromClient::cookies_changed(Vector<Web::Cookie::Cookie> cookies)\n     }\n }\n \n+void ConnectionFromClient::exit_fullscreen(u64 page_id)\n+{\n+    if (auto page = this->page(page_id); page.has_value()) {\n+        Web::HTML::TemporaryExecutionContext context(page->page().top_level_browsing_context().active_document()->realm(), Web::HTML::TemporaryExecutionContext::CallbacksEnabled::Yes);\n+        page->page().top_level_browsing_context().active_document()->fully_exit_fullscreen();\n+    }\n+}\n+\n }\n\n@@ -160,6 +160,8 @@ class ConnectionFromClient final\n     virtual void system_time_zone_changed() override;\n     virtual void cookies_changed(Vector<Web::Cookie::Cookie>) override;\n \n+    virtual void exit_fullscreen(u64 page_id) override;\n+\n     NonnullOwnPtr<PageHost> m_page_host;\n \n     HashMap<int, Web::FileRequest> m_requested_files {};\n\n@@ -267,6 +267,11 @@ void PageClient::page_did_request_fullscreen_window()\n     client().async_did_request_fullscreen_window(m_id);\n }\n \n+void PageClient::page_did_request_exit_fullscreen()\n+{\n+    client().async_did_request_exit_fullscreen(m_id);\n+}\n+\n void PageClient::page_did_request_tooltip_override(Web::CSSPixelPoint position, ByteString const& title)\n {\n     auto device_position = page().css_to_device_point(position);\n\n@@ -120,6 +120,7 @@ class PageClient final : public Web::PageClient {\n     virtual void page_did_request_maximize_window() override;\n     virtual void page_did_request_minimize_window() override;\n     virtual void page_did_request_fullscreen_window() override;\n+    virtual void page_did_request_exit_fullscreen() override;\n     virtual void page_did_request_tooltip_override(Web::CSSPixelPoint, ByteString const&) override;\n     virtual void page_did_stop_tooltip_override() override;\n     virtual void page_did_enter_tooltip_area(ByteString const&) override;\n\n@@ -93,7 +93,8 @@ endpoint WebContentClient\n     did_request_resize_window(u64 page_id, Gfx::IntSize size) =|\n     did_request_maximize_window(u64 page_id) =|\n     did_request_minimize_window(u64 page_id) =|\n-    did_request_fullscreen_window(u64 page_id) =|\n+    did_request_fullscreen_window(u64 page_id) => (bool success)\n+    did_request_exit_fullscreen(u64 page_id) =|\n     did_request_file(u64 page_id, ByteString path, i32 request_id) =|\n     did_request_color_picker(u64 page_id, Color current_color) =|\n     did_request_file_picker(u64 page_id, Web::HTML::FileFilter accepted_file_types, Web::HTML::AllowMultipleFiles allow_multiple_files) =|\n\n@@ -130,4 +130,6 @@ endpoint WebContentServer\n \n     system_time_zone_changed() =|\n     cookies_changed(Vector<Web::Cookie::Cookie> cookies) =|\n+    \n+    exit_fullscreen(u64 page_id) =|\n }\n"}
{"id": 128, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Enter a brief description/summary of your PR here. What does it fix/what does it change/how was it tested (even manually, if necessary)? -->\r\n## Summary of the Pull Request\r\n\r\nSometimes the Interop Tests hung in CI. This seems to be caused by the sending of a message before the OS has set up the pipes correctly. We add a little wait to the test to reduce flakiness.\r\n\r\n<!-- Describe how you validated the behavior. Add automated tests wherever possible, but list manual validation steps taken as well -->\r\n## Validation Steps Performed\r\n![image](https://github.com/microsoft/PowerToys/assets/26118718/fa326de5-89fa-411a-b678-e36878c6cbb6)\r\nRan the \"TestSend\" with \"Run Until Failure\" and it ran 1000 times without hanging.\r\n\n\n### Code Patch:\n@@ -49,6 +49,9 @@ public void TestSend()\n                     serverPipe.Start();\n                     ClientPipe.Start();\n \n+                    // Test can be flaky as the pipes are still being set up and we end up receiving no message. Wait for a bit to avoid that.\n+                    Thread.Sleep(100);\n+\n                     ClientPipe.Send(testString);\n                     reset.WaitOne();\n \n"}
{"id": 246, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nAs a follow-up to HDFS-16935, we should provide utility to trigger heartbeat and wait until BP thread queue is fully processed. This would ensure 100% consistency w.r.t active namenode being able to receive bad block reports from the given datanode. This utility would resolve flakes for the tests that rely on namenode's awareness of the reported bad blocks by datanodes.\n\n### Code Patch:\n@@ -660,6 +660,23 @@ void triggerHeartbeatForTests() throws IOException {\n     }\n   }\n \n+  /**\n+   * Run an immediate heartbeat from all actors. Wait until heartbeat is processed and BP thread\n+   * queue is also processed. This should be used when we need to trigger the heartbeat and also\n+   * wait for bpThreadQueue to be fully processed.\n+   * Used by tests.\n+   *\n+   * @throws InterruptedException if interrupted while waiting for the queue to be processed.\n+   * @throws IOException if the retries are exhausted and the BP thread queue could not be\n+   * successfully processed.\n+   */\n+  @VisibleForTesting\n+  void triggerHeartbeatAndWaitQueueProcessedForTests() throws InterruptedException, IOException {\n+    for (BPServiceActor actor : bpServices) {\n+      actor.triggerHeartbeatAndWaitUntilQueueProcessed();\n+    }\n+  }\n+\n   boolean processCommandFromActor(DatanodeCommand cmd,\n       BPServiceActor actor) throws IOException {\n     assert bpServices.contains(actor);\n\n@@ -32,6 +32,7 @@\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n+import java.util.Queue;\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.concurrent.BlockingQueue;\n@@ -365,6 +366,35 @@ void triggerHeartbeatForTests() {\n     }\n   }\n \n+  /**\n+   * Trigger the heartbeat and wait for BP thread queue to be fully processed.\n+   * To be used as a test utility.\n+   *\n+   * @throws InterruptedException if interrupted while waiting for the queue to be processed.\n+   * @throws IOException if the retries are exhausted and the BP thread queue could not be\n+   * successfully processed.\n+   */\n+  @VisibleForTesting\n+  void triggerHeartbeatAndWaitUntilQueueProcessed() throws InterruptedException, IOException {\n+    Queue<BPServiceActorAction> bpServiceActorActions;\n+    synchronized (bpThreadQueue) {\n+      bpServiceActorActions = new LinkedList<>(bpThreadQueue);\n+    }\n+    triggerHeartbeatForTests();\n+    while (!bpServiceActorActions.isEmpty()) {\n+      BPServiceActorAction bpServiceActorAction = bpServiceActorActions.remove();\n+      int retries = 5;\n+      while (!bpServiceActorAction.isReportSuccessfullySent() && retries > 0) {\n+        LOG.info(\"{} has not yet successfully sent report\", bpServiceActorAction);\n+        Thread.sleep(1000);\n+        retries--;\n+      }\n+      if (retries == 0) {\n+        throw new IOException(\"BP service actor action could not be completed successfully\");\n+      }\n+    }\n+  }\n+\n   private int getMaxBlockReportSize() {\n     int maxBlockReportSize = 0;\n     if (!blockReportSizes.isEmpty()) {\n\n@@ -27,6 +27,10 @@\n  * to take several actions.\n  */\n public interface BPServiceActorAction {\n-  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n-    DatanodeRegistration bpRegistration) throws BPServiceActorActionException;\n+\n+  void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n+      DatanodeRegistration bpRegistration) throws BPServiceActorActionException;\n+\n+  boolean isReportSuccessfullySent();\n+\n }\n\n@@ -34,7 +34,8 @@ public class ErrorReportAction implements BPServiceActorAction {\n \n   final int errorCode;\n   final String errorMessage;\n-  \n+  private boolean isReportSuccessfullySent = false;\n+\n   public ErrorReportAction(int errorCode, String errorMessage) {\n     this.errorCode = errorCode;\n     this.errorMessage = errorMessage;\n@@ -45,6 +46,7 @@ public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n     DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n     try {\n       bpNamenode.errorReport(bpRegistration, errorCode, errorMessage);\n+      isReportSuccessfullySent = true;\n     } catch (RemoteException re) {\n       DataNode.LOG.info(\"trySendErrorReport encountered RemoteException  \"\n           + \"errorMessage: \" + errorMessage + \"  errorCode: \" + errorCode, re);\n@@ -54,6 +56,11 @@ public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n     }\n   }\n \n+  @Override\n+  public boolean isReportSuccessfullySent() {\n+    return isReportSuccessfullySent;\n+  }\n+\n   @Override\n   public int hashCode() {\n     final int prime = 31;\n\n@@ -40,6 +40,7 @@ public class ReportBadBlockAction implements BPServiceActorAction {\n   private final ExtendedBlock block;\n   private final String storageUuid;\n   private final StorageType storageType;\n+  private boolean isReportSuccessfullySent = false;\n \n   public ReportBadBlockAction(ExtendedBlock block, String storageUuid, \n       StorageType storageType) {\n@@ -63,6 +64,7 @@ public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n \n     try {\n       bpNamenode.reportBadBlocks(locatedBlock);\n+      isReportSuccessfullySent = true;\n     } catch (RemoteException re) {\n       DataNode.LOG.info(\"reportBadBlock encountered RemoteException for \"\n           + \"block:  \" + block , re);\n@@ -72,6 +74,11 @@ public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode,\n     }\n   }\n \n+  @Override\n+  public boolean isReportSuccessfullySent() {\n+    return isReportSuccessfullySent;\n+  }\n+\n   @Override\n   public int hashCode() {\n     final int prime = 31;\n\n@@ -91,7 +91,22 @@ public static void triggerHeartbeat(DataNode dn) throws IOException {\n       bpos.triggerHeartbeatForTests();\n     }\n   }\n-  \n+\n+  /**\n+   * Trigger the heartbeat and return only when all BP thread queue are successfully processed\n+   * i.e. any bad block has been successfully reported to the active namenode.\n+   * @param dn datanode\n+   * @throws InterruptedException if interrupted while waiting for the queue to be processed.\n+   * @throws IOException if the retries are exhausted and the BP thread queue could not be\n+   * successfully processed.\n+   */\n+  public static void triggerHeartbeatAndWaitQueueProcessedForTests(DataNode dn)\n+      throws InterruptedException, IOException {\n+    for (BPOfferService bpos : dn.getAllBpOs()) {\n+      bpos.triggerHeartbeatAndWaitQueueProcessedForTests();\n+    }\n+  }\n+\n   public static void triggerBlockReport(DataNode dn) throws IOException {\n     for (BPOfferService bpos : dn.getAllBpOs()) {\n       bpos.triggerBlockReportForTests();\n\n@@ -1100,7 +1100,7 @@ public void testReportBadBlocks() throws Exception {\n       block = DFSTestUtil.getFirstBlock(fs, filePath);\n       // Test for the overloaded method reportBadBlocks\n       dataNode.reportBadBlocks(block, dataNode.getFSDataset().getFsVolumeReferences().get(0));\n-      DataNodeTestUtils.triggerHeartbeat(dataNode);\n+      DataNodeTestUtils.triggerHeartbeatAndWaitQueueProcessedForTests(dataNode);\n       BlockManagerTestUtil.updateState(cluster.getNamesystem().getBlockManager());\n       assertEquals(\"Corrupt replica blocks could not be reflected with the heartbeat\", 1,\n           cluster.getNamesystem().getCorruptReplicaBlocks());\n"}
{"id": 376, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nBuild: https://dev.azure.com/monacotools/a6d41577-0fa3-498e-af22-257312ff0545/_build/results?buildId=317618\nChanges: https://github.com/Microsoft/vscode/compare/a58d48f...30dbfbb\n\nThe RandomInt, \"should include min and max\" unit test failed, which, if it were truly random and I if understand probability well enough, has a 1 in 20 trillion chance of happening (0.8 ^ 100).\n\n```\nrandomInt positive numbers max: 5, min: 0 should include min and max (reason: Expected 2,1,1,3,2,2,1,4,4,5,4,2,4,5,3,2,3,2,3,1,5,1,2,4,3,1,4,3,3,2,1,1,1,4,2,3,5,4,3,2,1,1,1,1,2,3,1,1,3,3,2,3,3,5,1,3,3,2,4,2,2,2,4,4,1,4,1,1,5,4,5,3,3,2,3,4,3,1,3,2,2,4,2,4,2,2,3,5,3,2,4,3,4,1,1,2,5,2,5,4 to include 0.\n```\n\n### Code Patch:\n@@ -102,7 +102,7 @@ export function isPointWithinTriangle(\n /**\n  * Function to get a (pseudo)random integer from a provided `max`...[`min`] range.\n  * Both `min` and `max` values are inclusive. The `min` value is optional and defaults\n- * to `0` if not explicitely specified.\n+ * to `0` if not explicitly specified.\n  *\n  * @throws in the next cases:\n  * \t- if provided `min` or `max` is not a number\n\n@@ -54,8 +54,8 @@ suite('randomInt', () => {\n \t\t\t\t});\n \t\t\t}\n \n-\t\t\ttest(`should include min and max`, async () => {\n-\t\t\t\tlet iterations = 100;\n+\t\t\ttest('should include min and max', async () => {\n+\t\t\t\tlet iterations = 125;\n \t\t\t\tconst results = [];\n \t\t\t\twhile (iterations-- > 0) {\n \t\t\t\t\tresults.push(randomInt(max, min));\n@@ -74,9 +74,9 @@ suite('randomInt', () => {\n \t};\n \n \tsuite('positive numbers', () => {\n-\t\ttestRandomIntUtil(5, 2, 'max: 5, min: 2');\n-\t\ttestRandomIntUtil(5, 0, 'max: 5, min: 0');\n-\t\ttestRandomIntUtil(5, undefined, 'max: 5, min: undefined');\n+\t\ttestRandomIntUtil(4, 2, 'max: 4, min: 2');\n+\t\ttestRandomIntUtil(4, 0, 'max: 4, min: 0');\n+\t\ttestRandomIntUtil(4, undefined, 'max: 4, min: undefined');\n \t\ttestRandomIntUtil(1, 0, 'max: 0, min: 0');\n \t});\n \n@@ -137,14 +137,14 @@ suite('randomInt', () => {\n \n \t\ttest('should throw if \"min\" is > \"max\" #5', () => {\n \t\t\tassert.throws(() => {\n-\t\t\t\trandomInt(-5, 0);\n-\t\t\t}, `\"max\"(-5) param should be greater than \"min\"(0).\"`);\n+\t\t\t\trandomInt(-4, 0);\n+\t\t\t}, `\"max\"(-4) param should be greater than \"min\"(0).\"`);\n \t\t});\n \n \t\ttest('should throw if \"min\" is > \"max\" #6', () => {\n \t\t\tassert.throws(() => {\n-\t\t\t\trandomInt(-5);\n-\t\t\t}, `\"max\"(-5) param should be greater than \"min\"(0).\"`);\n+\t\t\t\trandomInt(-4);\n+\t\t\t}, `\"max\"(-4) param should be greater than \"min\"(0).\"`);\n \t\t});\n \n \t\ttest('should throw if \"max\" is `NaN`', () => {\n@@ -155,7 +155,7 @@ suite('randomInt', () => {\n \n \t\ttest('should throw if \"min\" is `NaN`', () => {\n \t\t\tassert.throws(() => {\n-\t\t\t\trandomInt(5, NaN);\n+\t\t\t\trandomInt(4, NaN);\n \t\t\t}, `\"min\" param is not a number.\"`);\n \t\t});\n \n"}
{"id": 40, "ground_truth": "Network", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n\n| Reason | <code>client-proxy/test-https-proxy-request-invalid-char-in-url</code> |\n| - | :- |\n| Type | JS_TEST_FAILURE |\n| Failed PR | 3 ([https://github.com/nodejs/node/pull/59587/](https://ci.nodejs.org/job/node-test-pull-request/68946/), [https://github.com/nodejs/node/pull/59734/](https://ci.nodejs.org/job/node-test-pull-request/69013/), [https://github.com/nodejs/node/pull/59717/](https://ci.nodejs.org/job/node-test-pull-request/69014/)) |\n| Appeared | [vm-efv4j](https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66595/console), [vm-a49f1](https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66594/console), [vm-yy37e](https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66523/console) |\n| First CI | https://ci.nodejs.org/job/node-test-pull-request/68946/ |\n| Last CI | https://ci.nodejs.org/job/node-test-pull-request/69014/ |\n\n<details>\n<summary><a href=\"https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66595/console\">Example</a></summary>\n\n```\nnot ok 4333 client-proxy/test-https-proxy-request-invalid-char-in-url\n  ---\n  duration_ms: 413.52500\n  severity: fail\n  exitcode: 1\n  stack: |-\n    #6 eneded response for: 'https://local\\rhost:55386/carriage-return-in-host'\n    #5 eneded response for: 'https://local\\r\\nhost:55386/crlf-in-host'\n    #4 eneded response for: 'https://local\\nhost:55386/newline-in-host'\n    #3 eneded response for: 'https://localhost:5\\r5386/carriage-return-in-port'\n    #2 eneded response for: 'https://localhost:5\\n5386/newline-in-port'\n    #1 eneded response for: 'https://localhost:5\\r\\n5386/crlf-in-port'\n    node:assert:147\n      throw new AssertionError(obj);\n      ^\n    \n    AssertionError [ERR_ASSERTION]: Expected values to be strictly deep-equal:\n    + actual - expected\n    \n    + Set(8) {\n    - Set(6) {\n        {\n    +     error: Error [ERR_STREAM_WRITE_AFTER_END]: write after end\n    +         at _write (node:internal/streams/writable:487:11)\n    +         at Writable.write (node:internal/streams/writable:508:10)\n    +      ...\n```\n</details>\n\nFrom https://github.com/nodejs/reliability/blob/main/reports/2025-09-03.md\n\n### Code Patch:\n@@ -21,9 +21,10 @@ const server = https.createServer({\n   cert: fixtures.readKey('agent8-cert.pem'),\n   key: fixtures.readKey('agent8-key.pem'),\n }, (req, res) => {\n+  console.log(`[Upstream server] responding to request for ${inspect(req.url)}`);\n   requests.add(`https://localhost:${server.address().port}${req.url}`);\n   res.writeHead(200, { 'Content-Type': 'text/plain' });\n-  res.end(`Response for ${req.url}`);\n+  res.end(`Response for ${inspect(req.url)}`);\n });\n \n server.listen(0);\n@@ -54,7 +55,7 @@ https.globalAgent = new https.Agent({\n \n const severHost = `localhost:${server.address().port}`;\n \n-let counter = testCases.length;\n+let counter = 0;\n const expectedUrls = new Set();\n const expectedProxyLogs = new Set();\n for (const testCase of testCases) {\n@@ -69,15 +70,20 @@ for (const testCase of testCases) {\n   https.request(url, (res) => {\n     res.on('error', common.mustNotCall());\n     res.setEncoding('utf8');\n-    res.on('data', () => {});\n-    res.on('end', common.mustCall(() => {\n-      console.log(`#${counter--} eneded response for: ${inspect(url)}`);\n+    res.on('data', (data) => {\n+      console.log(`[Proxy client] Received response from server for ${inspect(url)}: ${data.toString()}`);\n+    });\n+    res.on('close', common.mustCall(() => {\n+      console.log(`[Proxy client] #${++counter} closed request for: ${inspect(url)}`);\n       // Finished all test cases.\n-      if (counter === 0) {\n-        proxy.close();\n-        server.close();\n-        assert.deepStrictEqual(requests, expectedUrls);\n-        assert.deepStrictEqual(new Set(logs), expectedProxyLogs);\n+      if (counter === testCases.length) {\n+        setImmediate(() => {\n+          console.log('All requests completed, shutting down.');\n+          proxy.close();\n+          server.close();\n+          assert.deepStrictEqual(requests, expectedUrls);\n+          assert.deepStrictEqual(new Set(logs), expectedProxyLogs);\n+        });\n       }\n     }));\n   }).on('error', common.mustNotCall()).end();\n\n@@ -63,7 +63,7 @@ exports.createProxyServer = function(options = {}) {\n     });\n \n     res.on('error', (err) => {\n-      logs.push({ error: err, source: 'proxy response' });\n+      logs.push({ error: err, source: 'client response for request' });\n     });\n \n     req.pipe(proxyReq, { end: true });\n@@ -75,7 +75,7 @@ exports.createProxyServer = function(options = {}) {\n     const [hostname, port] = req.url.split(':');\n \n     res.on('error', (err) => {\n-      logs.push({ error: err, source: 'proxy response' });\n+      logs.push({ error: err, source: 'client response for connect' });\n     });\n \n     const proxyReq = net.connect(port, hostname, () => {\n@@ -90,9 +90,13 @@ exports.createProxyServer = function(options = {}) {\n     });\n \n     proxyReq.on('error', (err) => {\n-      logs.push({ error: err, source: 'proxy request' });\n-      res.write('HTTP/1.1 500 Connection Error\\r\\n\\r\\n');\n-      res.end('Proxy error: ' + err.message);\n+      logs.push({ error: err, source: 'proxy connect' });\n+      // The proxy client might have already closed the connection\n+      // when the upstream connection fails.\n+      if (!res.writableEnded) {\n+        res.write('HTTP/1.1 500 Connection Error\\r\\n\\r\\n');\n+        res.end('Proxy error: ' + err.message);\n+      }\n     });\n   });\n \n"}
{"id": 369, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Please make sure the target branch is right. In most case, the target branch should be `develop`. -->\r\n\r\n### Which Issue(s) This PR Fixes\r\n\r\n<!-- Please ensure that the related issue has already been created, and [link this pull request to that issue using keywords](<https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword>) to ensure automatic closure. -->\r\n\r\nFixes #7431\r\n\r\n### Brief Description\r\n\r\nFix flaky test of DLedgerControllerTest#testBrokerLifecycleListener\r\n\r\n### How Did You Test This Change?\r\n\r\n<!-- In order to ensure the code quality of Apache RocketMQ, we expect every pull request to have undergone thorough testing. -->\r\n\n\n### Code Patch:\n@@ -63,7 +63,8 @@ public class DLedgerControllerTest {\n     private List<String> baseDirs;\n     private List<DLedgerController> controllers;\n \n-    public DLedgerController launchController(final String group, final String peers, final String selfId, final boolean isEnableElectUncleanMaster) {\n+    public DLedgerController launchController(final String group, final String peers, final String selfId,\n+        final boolean isEnableElectUncleanMaster) {\n         String tmpdir = System.getProperty(\"java.io.tmpdir\");\n         final String path = (StringUtils.endsWith(tmpdir, File.separator) ? tmpdir : tmpdir + File.separator) + group + File.separator + selfId;\n         baseDirs.add(path);\n@@ -121,11 +122,11 @@ public void registerNewBroker(Controller leader, String clusterName, String brok\n         final RegisterBrokerToControllerRequestHeader registerBrokerToControllerRequestHeader = new RegisterBrokerToControllerRequestHeader(clusterName, brokerName, nextBrokerId, brokerAddress);\n         RemotingCommand remotingCommand2 = leader.registerBroker(registerBrokerToControllerRequestHeader).get(2, TimeUnit.SECONDS);\n \n-\n         assertEquals(ResponseCode.SUCCESS, remotingCommand2.getCode());\n     }\n \n-    public void brokerTryElectMaster(Controller leader, String clusterName, String brokerName, String brokerAddress, Long brokerId,\n+    public void brokerTryElectMaster(Controller leader, String clusterName, String brokerName, String brokerAddress,\n+        Long brokerId,\n         boolean exceptSuccess) throws Exception {\n         final ElectMasterRequestHeader electMasterRequestHeader = ElectMasterRequestHeader.ofBrokerTrigger(clusterName, brokerName, brokerId);\n         RemotingCommand command = leader.electMaster(electMasterRequestHeader).get(2, TimeUnit.SECONDS);\n@@ -186,9 +187,9 @@ public DLedgerController mockMetaData(boolean enableElectUncleanMaster) throws E\n         registerNewBroker(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[1], 2L);\n         registerNewBroker(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[2], 3L);\n         // try elect\n-        brokerTryElectMaster(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[0], 1L,true);\n-        brokerTryElectMaster(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[1], 2L,  false);\n-        brokerTryElectMaster(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[2], 3L,false);\n+        brokerTryElectMaster(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[0], 1L, true);\n+        brokerTryElectMaster(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[1], 2L, false);\n+        brokerTryElectMaster(leader, DEFAULT_CLUSTER_NAME, DEFAULT_BROKER_NAME, DEFAULT_IP[2], 3L, false);\n         final RemotingCommand getInfoResponse = leader.getReplicaInfo(new GetReplicaInfoRequestHeader(DEFAULT_BROKER_NAME)).get(10, TimeUnit.SECONDS);\n         final GetReplicaInfoResponseHeader replicaInfo = (GetReplicaInfoResponseHeader) getInfoResponse.readCustomHeader();\n         assertEquals(1, replicaInfo.getMasterEpoch().intValue());\n@@ -239,6 +240,8 @@ public void testElectMaster() throws Exception {\n     @Test\n     public void testBrokerLifecycleListener() throws Exception {\n         final DLedgerController leader = mockMetaData(false);\n+\n+        assertTrue(leader.isLeaderState());\n         // Mock that master broker has been inactive, and try to elect a new master from sync-state-set\n         // But we shut down two controller, so the ElectMasterEvent will be appended to DLedger failed.\n         // So the statemachine still keep the stale master's information\n@@ -247,15 +250,20 @@ public void testBrokerLifecycleListener() throws Exception {\n             dLedgerController.shutdown();\n             controllers.remove(dLedgerController);\n         }\n+\n         final ElectMasterRequestHeader request = ElectMasterRequestHeader.ofControllerTrigger(DEFAULT_BROKER_NAME);\n         setBrokerElectPolicy(leader, 1L);\n         Exception exception = null;\n+        RemotingCommand remotingCommand = null;\n         try {\n-            leader.electMaster(request).get(5, TimeUnit.SECONDS);\n+            remotingCommand = leader.electMaster(request).get(5, TimeUnit.SECONDS);\n         } catch (Exception e) {\n             exception = e;\n         }\n-        assertNotNull(exception);\n+\n+        assertTrue(exception != null ||\n+            remotingCommand != null && remotingCommand.getCode() == ResponseCode.CONTROLLER_NOT_LEADER);\n+\n         // Shut down leader controller\n         leader.shutdown();\n         controllers.remove(leader);\n@@ -272,7 +280,7 @@ public void testBrokerLifecycleListener() throws Exception {\n         setBrokerAlivePredicate(newLeader, 1L);\n         // Check if the statemachine is stale\n         final RemotingCommand resp = newLeader.getReplicaInfo(new GetReplicaInfoRequestHeader(DEFAULT_BROKER_NAME)).\n-                get(10, TimeUnit.SECONDS);\n+            get(10, TimeUnit.SECONDS);\n         final GetReplicaInfoResponseHeader replicaInfo = (GetReplicaInfoResponseHeader) resp.readCustomHeader();\n         assertEquals(1, replicaInfo.getMasterBrokerId().longValue());\n         assertEquals(1, replicaInfo.getMasterEpoch().intValue());\n"}
{"id": 416, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n**Description**\r\n\r\nCloses #1142 \n\n### Code Patch:\n@@ -207,21 +207,14 @@ func TestRunCmdContext_writeEnvoyResourcesAndRunExtProc(t *testing.T) {\n }\n \n func Test_mustStartExtProc(t *testing.T) {\n-\tctx, cancel := context.WithCancel(t.Context())\n-\tt.Cleanup(cancel)\n+\tmockerr := errors.New(\"mock extproc error\")\n \trunCtx := &runCmdContext{\n \t\ttmpdir:          t.TempDir(),\n-\t\textProcLauncher: mainlib.Main,\n-\t\t// UNIX doesn't like a long UDS path, so we use a short one.\n-\t\t// https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars\n-\t\tudsPath:      filepath.Join(\"/tmp\", \"run.sock\"),\n-\t\tstderrLogger: slog.New(slog.NewTextHandler(os.Stderr, &slog.HandlerOptions{})),\n+\t\textProcLauncher: func(context.Context, []string, io.Writer) error { return mockerr },\n+\t\tstderrLogger:    slog.New(slog.NewTextHandler(os.Stderr, &slog.HandlerOptions{})),\n \t}\n-\tdone := runCtx.mustStartExtProc(ctx, filterapi.MustLoadDefaultConfig())\n-\ttime.Sleep(1 * time.Second)\n-\tcancel()\n-\t// Wait for the external processor to stop.\n-\trequire.NoError(t, <-done)\n+\tdone := runCtx.mustStartExtProc(t.Context(), filterapi.MustLoadDefaultConfig())\n+\trequire.ErrorIs(t, <-done, mockerr)\n }\n \n // checkIfOllamaReady checks if the Ollama server is ready and if the specified model is available.\n"}
{"id": 301, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## PR Summary\r\n\r\nSince parametrizing the test allows it to run in parallel, this makes it flaky, as one process can overwrite the test result image of another.\r\n\r\nOur standard way for dealing with tests that use the same baseline image is to pass duplicate filenames to `image_comparison`, because that is serialized.\r\n\r\nThis is basically the same as #16656 for `test_stem`.\r\n\r\n## PR Checklist\r\n\r\n- [x] Has Pytest style unit tests\r\n- [x] Code is [Flake 8](http://flake8.pycqa.org/en/latest/) compliant\r\n- [N/A] New features are documented, with examples if plot related\r\n- [N/A] Documentation is sphinx and numpydoc compliant\r\n- [N/A] Added an entry to doc/users/next_whats_new/ if major new feature (follow instructions in README.rst there)\r\n- [N/A] Documented in doc/api/api_changes.rst if API changed in a backward-incompatible way\n\n### Code Patch:\n@@ -3342,10 +3342,15 @@ def test_hist_stacked_weighted():\n     ax.hist((d1, d2), weights=(w1, w2), histtype=\"stepfilled\", stacked=True)\n \n \n-@pytest.mark.parametrize(\"use_line_collection\", [True, False],\n-                         ids=['w/ line collection', 'w/o line collection'])\n-@image_comparison(['stem.png'], style='mpl20', remove_text=True)\n-def test_stem(use_line_collection):\n+@image_comparison(['stem.png', 'stem.png'], style='mpl20', remove_text=True)\n+def test_stem():\n+    # Note, we don't use @pytest.mark.parametrize, because in parallel this\n+    # might cause one process result to overwrite another's.\n+    for use_line_collection in [True, False]:\n+        _test_stem(use_line_collection)\n+\n+\n+def _test_stem(use_line_collection):\n     x = np.linspace(0.1, 2 * np.pi, 100)\n     args = (x, np.cos(x))\n     # Label is a single space to force a legend to be drawn, but to avoid any\n"}
{"id": 115, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nIn conn_test.go, \"TestStartupTimeout\" at the very last step requires the log to contain \"no response to connection startup within timeout\".\r\n\r\nHowever, during my test, it fails sometimes, not always, with log containing \"no response received from cassandra within timeout period\" instead.\r\n\r\nFixing the flaky test by allowing log to contain either message.\n\n### Code Patch:\n@@ -238,7 +238,8 @@ func TestStartupTimeout(t *testing.T) {\n \t\tt.Fatalf(\"Expected to receive no connections error - got '%s'\", err)\n \t}\n \n-\tif !strings.Contains(log.String(), \"no response to connection startup within timeout\") {\n+\tif !strings.Contains(log.String(), \"no response to connection startup within timeout\") &&\n+\t\t!strings.Contains(log.String(), \"no response received from cassandra within timeout period\") {\n \t\tt.Fatalf(\"Expected to receive timeout log message  - got '%s'\", log.String())\n \t}\n \n"}
{"id": 285, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nPRs backported:\r\n\r\n- #9909 \r\n- #9918\r\n- #9919 \r\n- #9920\r\n- #9922 \r\n- #9937 \r\n- #9938 \r\n- #9939 \r\n- #9940 \r\n- #9941 \r\n- #9948 \r\n- #9950 \r\n- #9953 \r\n- #9954 \r\n- #9961 \r\n- #9962 \r\n- #9963 \r\n- #9966 \r\n- #9967 \r\n- #9969\r\n- #9971\r\n- #9972 \r\n- #9934 \r\n- #9968 \r\n- use extension ref `release-1.9`\n\n### Code Patch:\n@@ -1,116 +1,80 @@\n {\n     \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",\n+    \"description\": \"THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\\n\\nGenerated on 2024-12-16T18:02:24Z by kres 318187b.\\n\\n\",\n+    \"prHeader\": \"Update Request | Renovate Bot\",\n     \"extends\": [\n         \":dependencyDashboard\",\n         \":gitSignOff\",\n         \":semanticCommitScopeDisabled\",\n         \"schedule:earlyMondays\"\n     ],\n-    \"prHeader\": \"Update Request | Renovate Bot\",\n-    \"regexManagers\": [\n+    \"customManagers\": [\n         {\n+            \"customType\": \"regex\",\n+            \"versioningTemplate\": \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\",\n             \"fileMatch\": [\n                 \"Makefile\"\n             ],\n             \"matchStrings\": [\n                 \"# renovate: datasource=(?<datasource>.*?)(?:\\\\s+extractVersion=(?<extractVersion>.+?))?\\\\s+depName=(?<depName>.+?)\\\\s.*_VERSION\\\\s+\\\\?=\\\\s+(?<currentValue>.+)\"\n-            ],\n-            \"versioningTemplate\": \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\"\n+            ]\n         },\n         {\n+            \"customType\": \"regex\",\n+            \"versioningTemplate\": \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\",\n             \"fileMatch\": [\n                 \"pkg/machinery/constants/constants.go\"\n             ],\n             \"matchStrings\": [\n                 \"\\\\/\\\\/\\\\s+renovate: datasource=(?<datasource>.*?)(?:\\\\s+extractVersion=(?<extractVersion>.+?))?(?:\\\\s+versioning=(?<versioning>.+?))?\\\\s+depName=(?<depName>.+?)?\\\\s.*Version\\\\s+=\\\\s+\\\\\\\"(?<currentValue>.+?)\\\\\\\"\"\n-            ],\n-            \"versioningTemplate\": \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\"\n+            ]\n         },\n         {\n+            \"customType\": \"regex\",\n+            \"versioningTemplate\": \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\",\n             \"fileMatch\": [\n                 \"internal/integration/k8s/constants.go\"\n             ],\n             \"matchStrings\": [\n                 \"\\\\/\\\\/\\\\s+renovate: datasource=(?<datasource>.*?)(?:\\\\s+extractVersion=(?<extractVersion>.+?))?(?:\\\\s+versioning=(?<versioning>.+?))?\\\\s+depName=(?<depName>.+?)?(?:\\\\s+registryUrl=(?<registryUrl>.+?))?\\\\s.*Version\\\\s+=\\\\s+\\\\\\\"(?<currentValue>.+?)\\\\\\\"\"\n-            ],\n-            \"versioningTemplate\": \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\"\n+            ]\n         },\n         {\n+            \"customType\": \"regex\",\n+            \"datasourceTemplate\": \"docker\",\n+            \"depNameTemplate\": \"docker/dockerfile-upstream\",\n+            \"versioningTemplate\": \"docker\",\n             \"fileMatch\": [\n                 \"Dockerfile\"\n             ],\n             \"matchStrings\": [\n                 \"# syntax = docker\\\\/dockerfile-upstream:(?<currentValue>.*)\"\n-            ],\n-            \"depNameTemplate\": \"docker/dockerfile-upstream\",\n-            \"datasourceTemplate\": \"docker\",\n-            \"versioningTemplate\": \"docker\"\n+            ]\n         }\n     ],\n     \"packageRules\": [\n         {\n-            \"enabled\": false,\n-            \"matchDatasources\": [\n-                \"docker\"\n-            ],\n-            \"matchFiles\": [\n-                \"hack/test/libvirt/docker-compose.yaml\"\n+            \"groupName\": \"dependencies\",\n+            \"matchUpdateTypes\": [\n+                \"major\",\n+                \"minor\",\n+                \"patch\",\n+                \"pin\",\n+                \"digest\"\n             ]\n         },\n         {\n             \"enabled\": false,\n-            \"matchDatasources\": [\n-                \"npm\"\n-            ],\n-            \"matchPaths\": [\n-                \"website/\"\n+            \"matchFileNames\": [\n+                \"website/**\"\n             ]\n         },\n         {\n+            \"versioning\": \"regex:^(?<major>\\\\d+)\\\\.(?<minor>\\\\d+)\",\n             \"matchPackageNames\": [\n                 \"golang/go\"\n-            ],\n-            \"versioning\": \"regex:^(?<major>\\\\d+)\\\\.(?<minor>\\\\d+)\"\n-        },\n-        {\n-            \"matchPackagePatterns\": [\n-                \"*\"\n-            ],\n-            \"matchDatasources\": [\n-                \"docker\"\n-            ],\n-            \"groupName\": \"container images\"\n-        },\n-        {\n-            \"matchPackagePatterns\": [\n-                \"*\"\n-            ],\n-            \"matchDatasources\": [\n-                \"go\",\n-                \"golang-version\"\n-            ],\n-            \"groupName\": \"go packages\"\n-        },\n-        {\n-            \"matchPackagePatterns\": [\n-                \"*\"\n-            ],\n-            \"matchDatasources\": [\n-                \"npm\"\n-            ],\n-            \"groupName\": \"node packages\"\n-        },\n-        {\n-            \"matchPackagePatterns\": [\n-                \"*\"\n-            ],\n-            \"matchDatasources\": [\n-                \"git-refs\",\n-                \"git-tags\",\n-                \"github-tags\",\n-                \"github-releases\"\n-            ],\n-            \"groupName\": \"releases\"\n+            ]\n         }\n-    ]\n+    ],\n+    \"separateMajorMinor\": false\n }\n\n@@ -1,6 +1,6 @@\n # THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\n #\n-# Generated on 2024-12-06T17:21:12Z by kres 1ebe796.\n+# Generated on 2024-12-17T10:40:00Z by kres 318187b.\n \n name: default\n concurrency:\n@@ -592,7 +592,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: set variables\n         run: |\n@@ -746,7 +746,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: set variables\n         run: |\n@@ -1438,7 +1438,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: unshallow-extensions\n         run: |\n@@ -1680,7 +1680,7 @@ jobs:\n       - name: image-cache\n         env:\n           IMAGE_REGISTRY: registry.dev.siderolabs.io\n-          MORE_IMAGES: alpine;registry.k8s.io/conformance:v1.32.0-rc.1;registry.k8s.io/e2e-test-images/busybox:1.36.1-1;registry.k8s.io/e2e-test-images/agnhost:2.53;registry.k8s.io/e2e-test-images/httpd:2.4.38-4;registry.k8s.io/e2e-test-images/nonewprivs:1.3;registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7;registry.k8s.io/e2e-test-images/nautilus:1.7;registry.k8s.io/e2e-test-images/sample-apiserver:1.29.2;registry.k8s.io/e2e-test-images/nginx:1.14-4;registry.k8s.io/etcd:3.5.16-0;registry.k8s.io/e2e-test-images/httpd:2.4.39-4\n+          MORE_IMAGES: alpine;registry.k8s.io/conformance:v1.32.0;registry.k8s.io/e2e-test-images/busybox:1.36.1-1;registry.k8s.io/e2e-test-images/agnhost:2.53;registry.k8s.io/e2e-test-images/httpd:2.4.38-4;registry.k8s.io/e2e-test-images/nonewprivs:1.3;registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7;registry.k8s.io/e2e-test-images/nautilus:1.7;registry.k8s.io/e2e-test-images/sample-apiserver:1.29.2;registry.k8s.io/e2e-test-images/nginx:1.14-4;registry.k8s.io/etcd:3.5.16-0;registry.k8s.io/e2e-test-images/httpd:2.4.39-4\n           PLATFORM: linux/amd64,linux/arm64\n           PUSH: \"true\"\n         run: |\n@@ -2074,6 +2074,7 @@ jobs:\n           GITHUB_STEP_NAME: ${{ github.job}}-e2e-controlplane-port\n           IMAGE_REGISTRY: registry.dev.siderolabs.io\n           SHORT_INTEGRATION_TEST: \"yes\"\n+          WITH_CONFIG_PATCH: '@hack/test/patches/ephemeral-min-max.yaml'\n           WITH_CONTROL_PLANE_PORT: \"443\"\n         run: |\n           sudo -E make e2e-qemu\n@@ -3021,7 +3022,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: set variables\n         run: |\n\n@@ -1,6 +1,6 @@\n # THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\n #\n-# Generated on 2024-09-09T13:58:35Z by kres 8be5fa7.\n+# Generated on 2024-12-17T10:40:00Z by kres 318187b.\n \n name: integration-aws-nvidia-nonfree-cron\n concurrency:\n@@ -105,7 +105,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: set variables\n         run: |\n\n@@ -1,6 +1,6 @@\n # THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\n #\n-# Generated on 2024-09-09T13:58:35Z by kres 8be5fa7.\n+# Generated on 2024-12-17T10:40:00Z by kres 318187b.\n \n name: integration-aws-nvidia-oss-cron\n concurrency:\n@@ -105,7 +105,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: set variables\n         run: |\n\n@@ -1,6 +1,6 @@\n # THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\n #\n-# Generated on 2024-12-04T12:34:25Z by kres 232fe63.\n+# Generated on 2024-12-17T10:40:00Z by kres 318187b.\n \n name: integration-extensions-cron\n concurrency:\n@@ -86,7 +86,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: unshallow-extensions\n         run: |\n\n@@ -1,6 +1,6 @@\n # THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\n #\n-# Generated on 2024-11-28T13:53:18Z by kres 232fe63.\n+# Generated on 2024-12-11T13:30:10Z by kres 8183c20.\n \n name: integration-misc-0-cron\n concurrency:\n@@ -99,6 +99,7 @@ jobs:\n           GITHUB_STEP_NAME: ${{ github.job}}-e2e-controlplane-port\n           IMAGE_REGISTRY: registry.dev.siderolabs.io\n           SHORT_INTEGRATION_TEST: \"yes\"\n+          WITH_CONFIG_PATCH: '@hack/test/patches/ephemeral-min-max.yaml'\n           WITH_CONTROL_PLANE_PORT: \"443\"\n         run: |\n           sudo -E make e2e-qemu\n\n@@ -1,6 +1,6 @@\n # THIS FILE WAS AUTOMATICALLY GENERATED, PLEASE DO NOT EDIT.\n #\n-# Generated on 2024-11-28T13:53:18Z by kres 232fe63.\n+# Generated on 2024-12-17T10:40:00Z by kres 318187b.\n \n name: integration-qemu-csi-longhorn-cron\n concurrency:\n@@ -86,7 +86,7 @@ jobs:\n         uses: actions/checkout@v4\n         with:\n           path: _out/extensions\n-          ref: main\n+          ref: release-1.9\n           repository: siderolabs/extensions\n       - name: set variables\n         run: |\n\n@@ -684,6 +684,7 @@ spec:\n             SHORT_INTEGRATION_TEST: yes\n             WITH_CONTROL_PLANE_PORT: 443\n             IMAGE_REGISTRY: registry.dev.siderolabs.io\n+            WITH_CONFIG_PATCH: \"@hack/test/patches/ephemeral-min-max.yaml\"\n         - name: save-talos-logs\n           conditions:\n             - always\n@@ -1037,7 +1038,7 @@ spec:\n         - name: checkout extensions\n           checkoutStep:\n             repository: siderolabs/extensions\n-            ref: main\n+            ref: release-1.9\n             path: _out/extensions\n         - name: unshallow-extensions\n           nonMakeStep: true\n@@ -1378,7 +1379,7 @@ spec:\n         - name: checkout extensions\n           checkoutStep:\n             repository: siderolabs/extensions\n-            ref: main\n+            ref: release-1.9\n             path: _out/extensions\n         - name: set variables\n           nonMakeStep: true\n@@ -1548,7 +1549,7 @@ spec:\n             PLATFORM: linux/amd64,linux/arm64\n             IMAGE_REGISTRY: registry.dev.siderolabs.io\n             PUSH: true\n-            MORE_IMAGES: \"alpine;registry.k8s.io/conformance:v1.32.0-rc.1;registry.k8s.io/e2e-test-images/busybox:1.36.1-1;registry.k8s.io/e2e-test-images/agnhost:2.53;registry.k8s.io/e2e-test-images/httpd:2.4.38-4;registry.k8s.io/e2e-test-images/nonewprivs:1.3;registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7;registry.k8s.io/e2e-test-images/nautilus:1.7;registry.k8s.io/e2e-test-images/sample-apiserver:1.29.2;registry.k8s.io/e2e-test-images/nginx:1.14-4;registry.k8s.io/etcd:3.5.16-0;registry.k8s.io/e2e-test-images/httpd:2.4.39-4\"\n+            MORE_IMAGES: \"alpine;registry.k8s.io/conformance:v1.32.0;registry.k8s.io/e2e-test-images/busybox:1.36.1-1;registry.k8s.io/e2e-test-images/agnhost:2.53;registry.k8s.io/e2e-test-images/httpd:2.4.38-4;registry.k8s.io/e2e-test-images/nonewprivs:1.3;registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7;registry.k8s.io/e2e-test-images/nautilus:1.7;registry.k8s.io/e2e-test-images/sample-apiserver:1.29.2;registry.k8s.io/e2e-test-images/nginx:1.14-4;registry.k8s.io/etcd:3.5.16-0;registry.k8s.io/e2e-test-images/httpd:2.4.39-4\"\n         - name: e2e-image-cache\n           command: e2e-qemu\n           withSudo: true\n@@ -1838,7 +1839,7 @@ spec:\n         - name: checkout extensions\n           checkoutStep:\n             repository: siderolabs/extensions\n-            ref: main\n+            ref: release-1.9\n             path: _out/extensions\n         - name: set variables\n           nonMakeStep: true\n@@ -1935,7 +1936,7 @@ spec:\n         - name: checkout extensions\n           checkoutStep:\n             repository: siderolabs/extensions\n-            ref: main\n+            ref: release-1.9\n             path: _out/extensions\n         - name: set variables\n           nonMakeStep: true\n@@ -2188,3 +2189,40 @@ spec:\n             TF_SCRIPT_DIR: _out/contrib\n             TF_E2E_TEST_TYPE: equinix-metal\n             TF_E2E_ACTION: destroy\n+---\n+kind: common.Renovate\n+spec:\n+  customManagers:\n+    - customType: regex\n+      fileMatch:\n+        - Makefile\n+      matchStrings:\n+        - '# renovate: datasource=(?<datasource>.*?)(?:\\s+extractVersion=(?<extractVersion>.+?))?\\s+depName=(?<depName>.+?)\\s.*_VERSION\\s+\\?=\\s+(?<currentValue>.+)'\n+      versioningTemplate: \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\"\n+    - customType: regex\n+      fileMatch:\n+        - pkg/machinery/constants/constants.go\n+      matchStrings:\n+        - '\\/\\/\\s+renovate: datasource=(?<datasource>.*?)(?:\\s+extractVersion=(?<extractVersion>.+?))?(?:\\s+versioning=(?<versioning>.+?))?\\s+depName=(?<depName>.+?)?\\s.*Version\\s+=\\s+\\\"(?<currentValue>.+?)\\\"'\n+      versioningTemplate: \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\"\n+    - customType: regex\n+      fileMatch:\n+        - internal/integration/k8s/constants.go\n+      matchStrings:\n+        - '\\/\\/\\s+renovate: datasource=(?<datasource>.*?)(?:\\s+extractVersion=(?<extractVersion>.+?))?(?:\\s+versioning=(?<versioning>.+?))?\\s+depName=(?<depName>.+?)?(?:\\s+registryUrl=(?<registryUrl>.+?))?\\s.*Version\\s+=\\s+\\\"(?<currentValue>.+?)\\\"'\n+      versioningTemplate: \"{{#if versioning}}{{versioning}}{{else}}semver{{/if}}\"\n+    - customType: regex\n+      fileMatch:\n+        - Dockerfile\n+      matchStrings:\n+        - '# syntax = docker\\/dockerfile-upstream:(?<currentValue>.*)'\n+      depNameTemplate: docker/dockerfile-upstream\n+      datasourceTemplate: docker\n+      versioningTemplate: docker\n+  packageRules:\n+    - enabled: false\n+      matchFileNames:\n+        - \"website/**\"\n+    - matchPackageNames:\n+        - golang/go\n+      versioning: 'regex:^(?<major>\\d+)\\.(?<minor>\\d+)'\n\n@@ -22,7 +22,7 @@ TOOLS ?= ghcr.io/siderolabs/tools:v1.9.0-1-geaad82f\n DEBUG_TOOLS_SOURCE := scratch\n \n PKGS_PREFIX ?= ghcr.io/siderolabs\n-PKGS ?= v1.9.0-6-gabba1d8\n+PKGS ?= v1.9.0-12-g9576b97\n EXTRAS ?= v1.9.0\n \n KRES_IMAGE ?= ghcr.io/siderolabs/kres:latest\n@@ -107,7 +107,7 @@ INTEGRATION_TEST := integration-test\n INTEGRATION_TEST_DEFAULT_TARGET := $(INTEGRATION_TEST)-$(OPERATING_SYSTEM)\n INTEGRATION_TEST_PROVISION_DEFAULT_TARGET := integration-test-provision-$(OPERATING_SYSTEM)\n # renovate: datasource=github-releases depName=kubernetes/kubernetes\n-KUBECTL_VERSION ?= v1.32.0-rc.1\n+KUBECTL_VERSION ?= v1.32.0\n # renovate: datasource=github-releases depName=kastenhq/kubestr\n KUBESTR_VERSION ?= v0.4.46\n # renovate: datasource=github-releases depName=helm/helm\n@@ -251,7 +251,7 @@ COMMON_ARGS += --build-arg=MICROSOFT_SECUREBOOT_RELEASE=$(MICROSOFT_SECUREBOOT_R\n \n CI_ARGS ?=\n \n-EXTENSIONS_FILTER_COMMAND ?= grep -vE 'tailscale|xen-guest-agent|nvidia|vmtoolsd-guest-agent|metal-agent'\n+EXTENSIONS_FILTER_COMMAND ?= grep -vE 'tailscale|xen-guest-agent|nvidia|vmtoolsd-guest-agent|metal-agent|cloudflared'\n \n all: initramfs kernel installer imager talosctl talosctl-image talos\n \n\n@@ -184,6 +184,8 @@ message LinkStatusSpec {\n   BondMasterSpec bond_master = 28;\n   WireguardSpec wireguard = 29;\n   bytes permanent_addr = 30;\n+  string alias = 31;\n+  repeated string alt_names = 32;\n }\n \n // NfTablesAddressMatch describes the match on the IP address.\n\n@@ -119,6 +119,7 @@ var (\n \tnodeVmlinuzPath           string\n \tnodeInitramfsPath         string\n \tnodeISOPath               string\n+\tnodeUSBPath               string\n \tnodeDiskImagePath         string\n \tnodeIPXEBootScript        string\n \tapplyConfigEnabled        bool\n@@ -223,6 +224,9 @@ func downloadBootAssets(ctx context.Context) error {\n \t\t{\n \t\t\tpath: &nodeISOPath,\n \t\t},\n+\t\t{\n+\t\t\tpath: &nodeUSBPath,\n+\t\t},\n \t\t{\n \t\t\tpath: &nodeDiskImagePath,\n \t\t},\n@@ -462,6 +466,7 @@ func create(ctx context.Context) error {\n \t\tKernelPath:     nodeVmlinuzPath,\n \t\tInitramfsPath:  nodeInitramfsPath,\n \t\tISOPath:        nodeISOPath,\n+\t\tUSBPath:        nodeUSBPath,\n \t\tIPXEBootScript: nodeIPXEBootScript,\n \t\tDiskImagePath:  nodeDiskImagePath,\n \n@@ -1239,6 +1244,7 @@ func init() {\n \tcreateCmd.Flags().StringVar(&nodeInstallImage, nodeInstallImageFlag, helpers.DefaultImage(images.DefaultInstallerImageRepository), \"the installer image to use\")\n \tcreateCmd.Flags().StringVar(&nodeVmlinuzPath, \"vmlinuz-path\", helpers.ArtifactPath(constants.KernelAssetWithArch), \"the compressed kernel image to use\")\n \tcreateCmd.Flags().StringVar(&nodeISOPath, \"iso-path\", \"\", \"the ISO path to use for the initial boot (VM only)\")\n+\tcreateCmd.Flags().StringVar(&nodeUSBPath, \"usb-path\", \"\", \"the USB stick image path to use for the initial boot (VM only)\")\n \tcreateCmd.Flags().StringVar(&nodeInitramfsPath, \"initrd-path\", helpers.ArtifactPath(constants.InitramfsAssetWithArch), \"initramfs image to use\")\n \tcreateCmd.Flags().StringVar(&nodeDiskImagePath, \"disk-image-path\", \"\", \"disk image to use\")\n \tcreateCmd.Flags().StringVar(&nodeIPXEBootScript, \"ipxe-boot-script\", \"\", \"iPXE boot script (URL) to use\")\n\n@@ -48,7 +48,7 @@ func (flags imageCmdFlagsType) apiNamespace() (common.ContainerdNamespace, error\n var imageCmd = &cobra.Command{\n \tUse:     \"image\",\n \tAliases: []string{\"images\"},\n-\tShort:   \"Manage CRI containter images\",\n+\tShort:   \"Manage CRI container images\",\n \tLong:    ``,\n \tArgs:    cobra.NoArgs,\n }\n\n@@ -32,16 +32,16 @@ replace github.com/insomniacslk/dhcp => github.com/smira/dhcp v0.0.0-20241001122\n \n // Kubernetes dependencies sharing the same version.\n require (\n-\tk8s.io/api v0.32.0-rc.1\n-\tk8s.io/apimachinery v0.32.0-rc.1\n-\tk8s.io/apiserver v0.32.0-rc.1\n-\tk8s.io/client-go v0.32.0-rc.1\n-\tk8s.io/component-base v0.32.0-rc.1\n-\tk8s.io/cri-api v0.32.0-rc.1\n-\tk8s.io/kube-scheduler v0.32.0-rc.1\n-\tk8s.io/kubectl v0.32.0-rc.1\n-\tk8s.io/kubelet v0.32.0-rc.1\n-\tk8s.io/pod-security-admission v0.32.0-rc.1\n+\tk8s.io/api v0.32.0\n+\tk8s.io/apimachinery v0.32.0\n+\tk8s.io/apiserver v0.32.0\n+\tk8s.io/client-go v0.32.0\n+\tk8s.io/component-base v0.32.0\n+\tk8s.io/cri-api v0.32.0\n+\tk8s.io/kube-scheduler v0.32.0\n+\tk8s.io/kubectl v0.32.0\n+\tk8s.io/kubelet v0.32.0\n+\tk8s.io/pod-security-admission v0.32.0\n )\n \n require (\n@@ -61,7 +61,7 @@ require (\n \tgithub.com/cenkalti/backoff/v4 v4.3.0\n \tgithub.com/containerd/cgroups/v3 v3.0.4\n \tgithub.com/containerd/containerd/api v1.8.0\n-\tgithub.com/containerd/containerd/v2 v2.0.0\n+\tgithub.com/containerd/containerd/v2 v2.0.1\n \tgithub.com/containerd/errdefs v1.0.0\n \tgithub.com/containerd/log v0.1.0\n \tgithub.com/containerd/platforms v1.0.0-rc.0\n@@ -70,7 +70,7 @@ require (\n \tgithub.com/containernetworking/plugins v1.6.0\n \tgithub.com/coredns/coredns v1.11.3\n \tgithub.com/coreos/go-iptables v0.8.0\n-\tgithub.com/cosi-project/runtime v0.7.2\n+\tgithub.com/cosi-project/runtime v0.7.6\n \tgithub.com/distribution/reference v0.6.0\n \tgithub.com/docker/cli v27.3.1+incompatible\n \tgithub.com/docker/docker v27.3.1+incompatible\n@@ -104,7 +104,7 @@ require (\n \tgithub.com/hetznercloud/hcloud-go/v2 v2.17.0\n \tgithub.com/insomniacslk/dhcp v0.0.0-20240829085014-a3a4c1f04475\n \tgithub.com/jeromer/syslogparser v1.1.0\n-\tgithub.com/jsimonetti/rtnetlink/v2 v2.0.2\n+\tgithub.com/jsimonetti/rtnetlink/v2 v2.0.3-0.20241216183107-2d6e9f8ad3f2\n \tgithub.com/jxskiss/base62 v1.1.0\n \tgithub.com/klauspost/compress v1.17.11\n \tgithub.com/klauspost/cpuid/v2 v2.2.9\n@@ -142,7 +142,7 @@ require (\n \tgithub.com/siderolabs/gen v0.7.0\n \tgithub.com/siderolabs/go-api-signature v0.3.6\n \tgithub.com/siderolabs/go-blockdevice v0.4.8\n-\tgithub.com/siderolabs/go-blockdevice/v2 v2.0.6\n+\tgithub.com/siderolabs/go-blockdevice/v2 v2.0.7\n \tgithub.com/siderolabs/go-circular v0.2.1\n \tgithub.com/siderolabs/go-cmd v0.1.3\n \tgithub.com/siderolabs/go-copy v0.1.0\n@@ -179,15 +179,15 @@ require (\n \tgo.etcd.io/etcd/etcdutl/v3 v3.5.17\n \tgo.uber.org/zap v1.27.0\n \tgo4.org/netipx v0.0.0-20231129151722-fdeea329fbba\n-\tgolang.org/x/net v0.31.0\n+\tgolang.org/x/net v0.32.0\n \tgolang.org/x/oauth2 v0.24.0\n-\tgolang.org/x/sync v0.9.0\n-\tgolang.org/x/sys v0.27.0\n-\tgolang.org/x/term v0.26.0\n-\tgolang.org/x/text v0.20.0\n+\tgolang.org/x/sync v0.10.0\n+\tgolang.org/x/sys v0.28.0\n+\tgolang.org/x/term v0.27.0\n+\tgolang.org/x/text v0.21.0\n \tgolang.org/x/time v0.8.0\n \tgolang.zx2c4.com/wireguard/wgctrl v0.0.0-20230429144221-925a1e7659e6\n-\tgoogle.golang.org/grpc v1.68.0\n+\tgoogle.golang.org/grpc v1.68.1\n \tgoogle.golang.org/protobuf v1.35.2\n \tgopkg.in/yaml.v3 v3.0.1\n \tk8s.io/klog/v2 v2.130.1\n@@ -234,7 +234,7 @@ require (\n \tgithub.com/containerd/continuity v0.4.4 // indirect\n \tgithub.com/containerd/errdefs/pkg v0.3.0 // indirect\n \tgithub.com/containerd/fifo v1.1.0 // indirect\n-\tgithub.com/containerd/go-cni v1.1.10 // indirect\n+\tgithub.com/containerd/go-cni v1.1.11 // indirect\n \tgithub.com/containerd/plugin v1.0.0 // indirect\n \tgithub.com/containerd/stargz-snapshotter/estargz v0.14.3 // indirect\n \tgithub.com/containerd/ttrpc v1.2.6 // indirect\n@@ -351,18 +351,18 @@ require (\n \tgo.opentelemetry.io/otel/metric v1.31.0 // indirect\n \tgo.opentelemetry.io/otel/trace v1.31.0 // indirect\n \tgo.uber.org/multierr v1.11.0 // indirect\n-\tgolang.org/x/crypto v0.29.0 // indirect\n+\tgolang.org/x/crypto v0.30.0 // indirect\n \tgolang.org/x/exp v0.0.0-20241108190413-2d47ceb2692f // indirect\n \tgolang.org/x/mod v0.22.0 // indirect\n \tgolang.org/x/tools v0.27.0 // indirect\n \tgolang.zx2c4.com/wintun v0.0.0-20230126152724-0fa3db229ce2 // indirect\n \tgolang.zx2c4.com/wireguard v0.0.0-20231211153847-12269c276173 // indirect\n-\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20241118233622-e639e219e697 // indirect\n-\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241118233622-e639e219e697 // indirect\n+\tgoogle.golang.org/genproto/googleapis/api v0.0.0-20241206012308-a4fef0638583 // indirect\n+\tgoogle.golang.org/genproto/googleapis/rpc v0.0.0-20241206012308-a4fef0638583 // indirect\n \tgopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect\n \tgopkg.in/inf.v0 v0.9.1 // indirect\n \tgopkg.in/yaml.v2 v2.4.0 // indirect\n-\tk8s.io/cli-runtime v0.32.0-rc.1 // indirect\n+\tk8s.io/cli-runtime v0.32.0 // indirect\n \tk8s.io/kube-openapi v0.0.0-20241105132330-32ad38e42d3f // indirect\n \tk8s.io/utils v0.0.0-20241104100929-3ea5e8cea738 // indirect\n \tkernel.org/pub/linux/libs/security/libcap/psx v1.2.72 // indirect\n\n@@ -129,8 +129,8 @@ github.com/containerd/cgroups/v3 v3.0.4 h1:2fs7l3P0Qxb1nKWuJNFiwhp2CqiKzho71DQkD\n github.com/containerd/cgroups/v3 v3.0.4/go.mod h1:SA5DLYnXO8pTGYiAHXz94qvLQTKfVM5GEVisn4jpins=\n github.com/containerd/containerd/api v1.8.0 h1:hVTNJKR8fMc/2Tiw60ZRijntNMd1U+JVMyTRdsD2bS0=\n github.com/containerd/containerd/api v1.8.0/go.mod h1:dFv4lt6S20wTu/hMcP4350RL87qPWLVa/OHOwmmdnYc=\n-github.com/containerd/containerd/v2 v2.0.0 h1:qLDdFaAykQrIyLiqwQrNLLz95wiC36bAZVwioUwqShM=\n-github.com/containerd/containerd/v2 v2.0.0/go.mod h1:j25kDy9P48/ngb1sxWIFfK6GsnqOHoSqo1EpAod20VQ=\n+github.com/containerd/containerd/v2 v2.0.1 h1:xqSar9cjkGhfQ2YvanCu7FMLk6+pNCFMCAroM2ALPp0=\n+github.com/containerd/containerd/v2 v2.0.1/go.mod h1:A9DyAg+lXTiSBOsBTqQtJL4O3AGaM1aa4UHmVh4uFhM=\n github.com/containerd/continuity v0.4.4 h1:/fNVfTJ7wIl/YPMHjf+5H32uFhl63JucB34PlCpMKII=\n github.com/containerd/continuity v0.4.4/go.mod h1:/lNJvtJKUQStBzpVQ1+rasXO1LAWtUQssk28EZvJ3nE=\n github.com/containerd/errdefs v1.0.0 h1:tg5yIfIlQIrxYtu9ajqY42W3lpS19XqdxRQeEwYG8PI=\n@@ -139,8 +139,8 @@ github.com/containerd/errdefs/pkg v0.3.0 h1:9IKJ06FvyNlexW690DXuQNx2KA2cUJXx151X\n github.com/containerd/errdefs/pkg v0.3.0/go.mod h1:NJw6s9HwNuRhnjJhM7pylWwMyAkmCQvQ4GpJHEqRLVk=\n github.com/containerd/fifo v1.1.0 h1:4I2mbh5stb1u6ycIABlBw9zgtlK8viPI9QkQNRQEEmY=\n github.com/containerd/fifo v1.1.0/go.mod h1:bmC4NWMbXlt2EZ0Hc7Fx7QzTFxgPID13eH0Qu+MAb2o=\n-github.com/containerd/go-cni v1.1.10 h1:c2U73nld7spSWfiJwSh/8W9DK+/qQwYM2rngIhCyhyg=\n-github.com/containerd/go-cni v1.1.10/go.mod h1:/Y/sL8yqYQn1ZG1om1OncJB1W4zN3YmjfP/ShCzG/OY=\n+github.com/containerd/go-cni v1.1.11 h1:fWt1K15AmSLsEfa57N+qYw4NeGPiQKYq1pjNGJwV9mc=\n+github.com/containerd/go-cni v1.1.11/go.mod h1:/Y/sL8yqYQn1ZG1om1OncJB1W4zN3YmjfP/ShCzG/OY=\n github.com/containerd/log v0.1.0 h1:TCJt7ioM2cr/tfR8GPbGf9/VRAX8D2B4PjzCpfX540I=\n github.com/containerd/log v0.1.0/go.mod h1:VRRf09a7mHDIRezVKTRCrOq78v577GXq3bSa3EhrzVo=\n github.com/containerd/platforms v1.0.0-rc.0 h1:GuHWSKgVVO3POn6nRBB4sH63uPOLa87yuuhsGLWaXAA=\n@@ -165,8 +165,8 @@ github.com/coreos/go-semver v0.3.1 h1:yi21YpKnrx1gt5R+la8n5WgS0kCrsPp33dmEyHReZr\n github.com/coreos/go-semver v0.3.1/go.mod h1:irMmmIw/7yzSRPWryHsK7EYSg09caPQL03VsM8rvUec=\n github.com/coreos/go-systemd/v22 v22.5.0 h1:RrqgGjYQKalulkV8NGVIfkXQf6YYmOyiJKk8iXXhfZs=\n github.com/coreos/go-systemd/v22 v22.5.0/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=\n-github.com/cosi-project/runtime v0.7.2 h1:b8/v/YpP75LNYLyP5x0+EdqPWtNn6sfJggGGzkqZ0H4=\n-github.com/cosi-project/runtime v0.7.2/go.mod h1:EMLs8a55tJ6zA4UyDbRsTvXBd6UIlNwZfCVGvCyiXK8=\n+github.com/cosi-project/runtime v0.7.6 h1:G6w4/g6EXrMakji0fHRDHvs9wltqF9LSDU/33er8gdc=\n+github.com/cosi-project/runtime v0.7.6/go.mod h1:AmDu/IfE/Q0YYzWRnAkDw2GNuMazpNpN9qyV1IErZdc=\n github.com/cpuguy83/go-md2man/v2 v2.0.2/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\n github.com/cpuguy83/go-md2man/v2 v2.0.4/go.mod h1:tgQtvFlXSQOSOSIRvRPT7W67SCa46tRHOmNcaadrF8o=\n github.com/cpuguy83/go-md2man/v2 v2.0.5 h1:ZtcqGrnekaHpVLArFSe4HK5DoKx1T0rq2DwVB0alcyc=\n@@ -414,8 +414,8 @@ github.com/jsimonetti/rtnetlink v0.0.0-20210525051524-4cc836578190/go.mod h1:NmK\n github.com/jsimonetti/rtnetlink v0.0.0-20211022192332-93da33804786/go.mod h1:v4hqbTdfQngbVSZJVWUhGE/lbTFf9jb+ygmNUDQMuOs=\n github.com/jsimonetti/rtnetlink v1.3.5 h1:hVlNQNRlLDGZz31gBPicsG7Q53rnlsz1l1Ix/9XlpVA=\n github.com/jsimonetti/rtnetlink v1.3.5/go.mod h1:0LFedyiTkebnd43tE4YAkWGIq9jQphow4CcwxaT2Y00=\n-github.com/jsimonetti/rtnetlink/v2 v2.0.2 h1:ZKlbCujrIpp4/u3V2Ka0oxlf4BCkt6ojkvpy3nZoCBY=\n-github.com/jsimonetti/rtnetlink/v2 v2.0.2/go.mod h1:7MoNYNbb3UaDHtF8udiJo/RH6VsTKP1pqKLUTVCvToE=\n+github.com/jsimonetti/rtnetlink/v2 v2.0.3-0.20241216183107-2d6e9f8ad3f2 h1:4pspWog/mjnfv+B3rjEUfCoFL80T7J8ojK9ay8ApPCM=\n+github.com/jsimonetti/rtnetlink/v2 v2.0.3-0.20241216183107-2d6e9f8ad3f2/go.mod h1:7MoNYNbb3UaDHtF8udiJo/RH6VsTKP1pqKLUTVCvToE=\n github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=\n github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\n github.com/jxskiss/base62 v1.1.0 h1:A5zbF8v8WXx2xixnAKD2w+abC+sIzYJX+nxmhA6HWFw=\n@@ -613,8 +613,8 @@ github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=\n github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=\n github.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=\n github.com/rogpeppe/go-internal v1.6.1/go.mod h1:xXDCJY+GAPziupqXw64V24skbSoqbTEfhy4qGm1nDQc=\n-github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=\n-github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=\n+github.com/rogpeppe/go-internal v1.13.1 h1:KvO1DLK/DRN07sQ1LQKScxyZJuNnedQ5/wKSR38lUII=\n+github.com/rogpeppe/go-internal v1.13.1/go.mod h1:uMEvuHeurkdAXX61udpOXGD/AzZDWNMNyH2VO9fmH0o=\n github.com/rs/xid v1.6.0 h1:fV591PaemRlL6JfRxGDEPl69wICngIQ3shQtzfy2gxU=\n github.com/rs/xid v1.6.0/go.mod h1:7XoLgs4eV+QndskICGsho+ADou8ySMSjJKDIan90Nz0=\n github.com/russross/blackfriday/v2 v2.1.0 h1:JIOH55/0cWyOuilr9/qlrm0BSXldqnqwMsf35Ld67mk=\n@@ -645,8 +645,8 @@ github.com/siderolabs/go-api-signature v0.3.6 h1:wDIsXbpl7Oa/FXvxB6uz4VL9INA9fmr\n github.com/siderolabs/go-api-signature v0.3.6/go.mod h1:hoH13AfunHflxbXfh+NoploqV13ZTDfQ1mQJWNVSW9U=\n github.com/siderolabs/go-blockdevice v0.4.8 h1:KfdWvIx0Jft5YVuCsFIJFwjWEF1oqtzkgX9PeU9cX4c=\n github.com/siderolabs/go-blockdevice v0.4.8/go.mod h1:4PeOuk71pReJj1JQEXDE7kIIQJPVe8a+HZQa+qjxSEA=\n-github.com/siderolabs/go-blockdevice/v2 v2.0.6 h1:/NAy3MbNZhjLWo28asZyS/hmf86PEPDMc9i6wIcgbwI=\n-github.com/siderolabs/go-blockdevice/v2 v2.0.6/go.mod h1:74htzCV913UzaLZ4H+NBXkwWlYnBJIq5m/379ZEcu8w=\n+github.com/siderolabs/go-blockdevice/v2 v2.0.7 h1:OCxxA7W1xVqbEP3MrCttqhKpuV4t1KkBTzNeboYDTmc=\n+github.com/siderolabs/go-blockdevice/v2 v2.0.7/go.mod h1:74htzCV913UzaLZ4H+NBXkwWlYnBJIq5m/379ZEcu8w=\n github.com/siderolabs/go-circular v0.2.1 h1:a++iVCn9jyhICX3POQZZX8n72p2h5JGdGU6w1ulmpcA=\n github.com/siderolabs/go-circular v0.2.1/go.mod h1:ZDItzVyXK+B/XuqTBV5MtQtSv06VI+oCmWGRnNCATo8=\n github.com/siderolabs/go-cmd v0.1.3 h1:JrgZwqhJQeoec3QRON0LK+fv+0y7d0DyY7zsfkO6ciw=\n@@ -830,8 +830,8 @@ golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPh\n golang.org/x/crypto v0.0.0-20210421170649-83a5a9bb288b/go.mod h1:T9bdIzuCu7OtxOm1hfPfRQxPLYneinmdGuTeoZ9dtd4=\n golang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=\n golang.org/x/crypto v0.0.0-20220722155217-630584e8d5aa/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=\n-golang.org/x/crypto v0.29.0 h1:L5SG1JTTXupVV3n6sUqMTeWbjAyfPwoda2DLX8J8FrQ=\n-golang.org/x/crypto v0.29.0/go.mod h1:+F4F4N5hv6v38hfeYwTdx20oUvLLc+QfrE9Ax9HtgRg=\n+golang.org/x/crypto v0.30.0 h1:RwoQn3GkWiMkzlX562cLB7OxWvjH1L8xutO2WoJcRoY=\n+golang.org/x/crypto v0.30.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=\n golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\n golang.org/x/exp v0.0.0-20241108190413-2d47ceb2692f h1:XdNn9LlyWAhLVp6P/i8QYBW+hlyhrhei9uErw2B5GJo=\n golang.org/x/exp v0.0.0-20241108190413-2d47ceb2692f/go.mod h1:D5SMRVC3C2/4+F/DB1wZsLRnSNimn2Sp/NPsCrsv8ak=\n@@ -880,8 +880,8 @@ golang.org/x/net v0.0.0-20211112202133-69e39bad7dc2/go.mod h1:9nx3DQGgdP8bBQD5qx\n golang.org/x/net v0.0.0-20220127200216-cd36cc0744dd/go.mod h1:CfG3xpIq0wQ8r1q4Su4UZFWDARRcnwPjda9FqA0JpMk=\n golang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=\n golang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=\n-golang.org/x/net v0.31.0 h1:68CPQngjLL0r2AlUKiSxtQFKvzRVbnzLwMUn5SzcLHo=\n-golang.org/x/net v0.31.0/go.mod h1:P4fl1q7dY2hnZFxEk4pPSkDHF+QqjitcnDjUQyMM+pM=\n+golang.org/x/net v0.32.0 h1:ZqPmj8Kzc+Y6e0+skZsuACbx+wzMgo5MQsJh9Qd6aYI=\n+golang.org/x/net v0.32.0/go.mod h1:CwU0IoeOlnQQWJ6ioyFrfRuomB8GKF6KbYXZVyeXNfs=\n golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\n golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\n@@ -903,8 +903,8 @@ golang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJ\n golang.org/x/sync v0.0.0-20210220032951-036812b2e83c/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n-golang.org/x/sync v0.9.0 h1:fEo0HyrW1GIgZdpbhCRO0PkJajUS5H9IFUztCgEo2jQ=\n-golang.org/x/sync v0.9.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n+golang.org/x/sync v0.10.0 h1:3NQrjDixjgGwUOCaF8w2+VYHv0Ve/vGYSbdkTa98gmQ=\n+golang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\n golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\n golang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\n@@ -962,14 +962,15 @@ golang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.10.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\n golang.org/x/sys v0.16.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n golang.org/x/sys v0.17.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n-golang.org/x/sys v0.27.0 h1:wBqf8DvsY9Y/2P8gAfPDEYNuS30J4lPHJxXSb/nJZ+s=\n golang.org/x/sys v0.27.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n+golang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=\n+golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\n golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\n golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=\n golang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=\n golang.org/x/term v0.17.0/go.mod h1:lLRBjIVuehSbZlaOtGMbcMncT+aqLLLmKrsjNrUguwk=\n-golang.org/x/term v0.26.0 h1:WEQa6V3Gja/BhNxg540hBip/kkaYtRg3cxg4oXSw4AU=\n-golang.org/x/term v0.26.0/go.mod h1:Si5m1o57C5nBNQo5z1iq+XDijt21BDBDp2bK0QI8e3E=\n+golang.org/x/term v0.27.0 h1:WP60Sv1nlK1T6SupCHbXzSaN0b9wUmsPoRS9b61A23Q=\n+golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\n golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n@@ -979,8 +980,8 @@ golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=\n golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=\n golang.org/x/text v0.8.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=\n golang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\n-golang.org/x/text v0.20.0 h1:gK/Kv2otX8gz+wn7Rmb3vT96ZwuoxnQlY+HlJVj7Qug=\n-golang.org/x/text v0.20.0/go.mod h1:D4IsuqiFMhST5bX19pQ9ikHC2GsaKyk/oF+pn3ducp4=\n+golang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=\n+golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=\n golang.org/x/time v0.8.0 h1:9i3RxcPv3PZnitoVGMPDKZSq1xW1gK1Xy3ArNOGZfEg=\n golang.org/x/time v0.8.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n@@ -1038,10 +1039,10 @@ google.golang.org/genproto v0.0.0-20210828152312-66f60bf46e71/go.mod h1:eFjDcFEc\n google.golang.org/genproto v0.0.0-20210909211513-a8c4777a87af/go.mod h1:eFjDcFEctNawg4eG61bRv87N7iHBWyVhJu7u1kqDUXY=\n google.golang.org/genproto v0.0.0-20211208223120-3a66f561d7aa/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n google.golang.org/genproto v0.0.0-20211221195035-429b39de9b1c/go.mod h1:5CzLGKJ67TSI2B9POpiiyGha0AjJvZIUgRMt1dSmuhc=\n-google.golang.org/genproto/googleapis/api v0.0.0-20241118233622-e639e219e697 h1:pgr/4QbFyktUv9CtQ/Fq4gzEE6/Xs7iCXbktaGzLHbQ=\n-google.golang.org/genproto/googleapis/api v0.0.0-20241118233622-e639e219e697/go.mod h1:+D9ySVjN8nY8YCVjc5O7PZDIdZporIDY3KaGfJunh88=\n-google.golang.org/genproto/googleapis/rpc v0.0.0-20241118233622-e639e219e697 h1:LWZqQOEjDyONlF1H6afSWpAL/znlREo2tHfLoe+8LMA=\n-google.golang.org/genproto/googleapis/rpc v0.0.0-20241118233622-e639e219e697/go.mod h1:5uTbfoYQed2U9p3KIj2/Zzm02PYhndfdmML0qC3q3FU=\n+google.golang.org/genproto/googleapis/api v0.0.0-20241206012308-a4fef0638583 h1:v+j+5gpj0FopU0KKLDGfDo9ZRRpKdi5UBrCP0f76kuY=\n+google.golang.org/genproto/googleapis/api v0.0.0-20241206012308-a4fef0638583/go.mod h1:jehYqy3+AhJU9ve55aNOaSml7wUXjF9x6z2LcCfpAhY=\n+google.golang.org/genproto/googleapis/rpc v0.0.0-20241206012308-a4fef0638583 h1:IfdSdTcLFy4lqUQrQJLkLt1PB+AsqVz6lwkWPzWEz10=\n+google.golang.org/genproto/googleapis/rpc v0.0.0-20241206012308-a4fef0638583/go.mod h1:5uTbfoYQed2U9p3KIj2/Zzm02PYhndfdmML0qC3q3FU=\n google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\n google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\n google.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\n@@ -1058,8 +1059,8 @@ google.golang.org/grpc v1.39.0/go.mod h1:PImNr+rS9TWYb2O4/emRugxiyHZ5JyHW5F+RPnD\n google.golang.org/grpc v1.39.1/go.mod h1:PImNr+rS9TWYb2O4/emRugxiyHZ5JyHW5F+RPnDzfrE=\n google.golang.org/grpc v1.40.0/go.mod h1:ogyxbiOoUXAkP+4+xa6PZSE9DZgIHtSpzjDTB9KAK34=\n google.golang.org/grpc v1.40.1/go.mod h1:ogyxbiOoUXAkP+4+xa6PZSE9DZgIHtSpzjDTB9KAK34=\n-google.golang.org/grpc v1.68.0 h1:aHQeeJbo8zAkAa3pRzrVjZlbz6uSfeOXlJNQM0RAbz0=\n-google.golang.org/grpc v1.68.0/go.mod h1:fmSPC5AsjSBCK54MyHRx48kpOti1/jRfOlwEWywNjWA=\n+google.golang.org/grpc v1.68.1 h1:oI5oTa11+ng8r8XMMN7jAOmWfPZWbYpCFaMUTACxkM0=\n+google.golang.org/grpc v1.68.1/go.mod h1:+q1XYFJjShcqn0QZHvCyeR4CXPA+llXIeUIfIe00waw=\n google.golang.org/grpc/cmd/protoc-gen-go-grpc v1.1.0/go.mod h1:6Kw0yEErY5E/yWrBtf03jp27GLLJujG4z/JK95pnjjw=\n google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\n google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\n@@ -1097,32 +1098,32 @@ gvisor.dev/gvisor v0.0.0-20240331093104-8c9cbf0d9090/go.mod h1:NQHVAzMwvZ+Qe3ElS\n honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n honnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\n-k8s.io/api v0.32.0-rc.1 h1:dxXiVjLYq4JmlXFgctsK9zJaXhPA1hJLO772lHk4wKw=\n-k8s.io/api v0.32.0-rc.1/go.mod h1:3TNYUV1WB3ZF2yOeY4qlr9FDb1GVzO+OucsXW2BOi5o=\n-k8s.io/apimachinery v0.32.0-rc.1 h1:jeLQr/Dqcq0kGU/yjEmievDMHTJzzwQctlysW0Kb70U=\n-k8s.io/apimachinery v0.32.0-rc.1/go.mod h1:HqhdaJUgQqky29T1V0o2yFkt/pZqLFIDyn9Zi/8rxoY=\n-k8s.io/apiserver v0.32.0-rc.1 h1:zlrmW4wO1JYXlu4mHzQ/bQajC7OP/sgCRFF8abOWhEQ=\n-k8s.io/apiserver v0.32.0-rc.1/go.mod h1:kzVgA1c3Ig7wDnDF99bOz/dY4iEY/OWzayCY1gZPzDI=\n-k8s.io/cli-runtime v0.32.0-rc.1 h1:rQDtu2ihnv/ItkawGIewwqhgb6C6skVwNFWCxBx7loI=\n-k8s.io/cli-runtime v0.32.0-rc.1/go.mod h1:ifV1UrGAvsFhohEfFLkxoAAZPAXnp92ROioZvFG4be4=\n-k8s.io/client-go v0.32.0-rc.1 h1:ZW27sbQZ06FnkFTNqlfa9HTIW01RGoLoiOmdiLbWYK0=\n-k8s.io/client-go v0.32.0-rc.1/go.mod h1:Nlv9GR11bKYSlCcxY3B1RpVbcsfasSAwminv42knDHk=\n-k8s.io/component-base v0.32.0-rc.1 h1:c3EG8QuyozbalRmbsUxXW/ZmF2bev3Zq9PbpU3oIHT4=\n-k8s.io/component-base v0.32.0-rc.1/go.mod h1:xIvfPkeLsQbd2fkDNWezG20WQwrabBetTUxp67eycbI=\n-k8s.io/cri-api v0.32.0-rc.1 h1:e7+u84qR9Nychp2imSIIDeQSNBZut/1alF7vcskZbik=\n-k8s.io/cri-api v0.32.0-rc.1/go.mod h1:5WOp91NVV2mzTQRp732avavhFFItsWsQy8jO7DPv8d8=\n+k8s.io/api v0.32.0 h1:OL9JpbvAU5ny9ga2fb24X8H6xQlVp+aJMFlgtQjR9CE=\n+k8s.io/api v0.32.0/go.mod h1:4LEwHZEf6Q/cG96F3dqR965sYOfmPM7rq81BLgsE0p0=\n+k8s.io/apimachinery v0.32.0 h1:cFSE7N3rmEEtv4ei5X6DaJPHHX0C+upp+v5lVPiEwpg=\n+k8s.io/apimachinery v0.32.0/go.mod h1:GpHVgxoKlTxClKcteaeuF1Ul/lDVb74KpZcxcmLDElE=\n+k8s.io/apiserver v0.32.0 h1:VJ89ZvQZ8p1sLeiWdRJpRD6oLozNZD2+qVSLi+ft5Qs=\n+k8s.io/apiserver v0.32.0/go.mod h1:HFh+dM1/BE/Hm4bS4nTXHVfN6Z6tFIZPi649n83b4Ag=\n+k8s.io/cli-runtime v0.32.0 h1:dP+OZqs7zHPpGQMCGAhectbHU2SNCuZtIimRKTv2T1c=\n+k8s.io/cli-runtime v0.32.0/go.mod h1:Mai8ht2+esoDRK5hr861KRy6z0zHsSTYttNVJXgP3YQ=\n+k8s.io/client-go v0.32.0 h1:DimtMcnN/JIKZcrSrstiwvvZvLjG0aSxy8PxN8IChp8=\n+k8s.io/client-go v0.32.0/go.mod h1:boDWvdM1Drk4NJj/VddSLnx59X3OPgwrOo0vGbtq9+8=\n+k8s.io/component-base v0.32.0 h1:d6cWHZkCiiep41ObYQS6IcgzOUQUNpywm39KVYaUqzU=\n+k8s.io/component-base v0.32.0/go.mod h1:JLG2W5TUxUu5uDyKiH2R/7NnxJo1HlPoRIIbVLkK5eM=\n+k8s.io/cri-api v0.32.0 h1:pzXJfyG7Tm4acrEt5HPqAq3r4cN5guLeapAN/NM2b70=\n+k8s.io/cri-api v0.32.0/go.mod h1:DCzMuTh2padoinefWME0G678Mc3QFbLMF2vEweGzBAI=\n k8s.io/klog/v2 v2.130.1 h1:n9Xl7H1Xvksem4KFG4PYbdQCQxqc/tTUyrgXaOhHSzk=\n k8s.io/klog/v2 v2.130.1/go.mod h1:3Jpz1GvMt720eyJH1ckRHK1EDfpxISzJ7I9OYgaDtPE=\n k8s.io/kube-openapi v0.0.0-20241105132330-32ad38e42d3f h1:GA7//TjRY9yWGy1poLzYYJJ4JRdzg3+O6e8I+e+8T5Y=\n k8s.io/kube-openapi v0.0.0-20241105132330-32ad38e42d3f/go.mod h1:R/HEjbvWI0qdfb8viZUeVZm0X6IZnxAydC7YU42CMw4=\n-k8s.io/kube-scheduler v0.32.0-rc.1 h1:TrCQMuOZthP7POY+wQgPZoXQ9+wJeHZNsCbeqO26jGw=\n-k8s.io/kube-scheduler v0.32.0-rc.1/go.mod h1:SjsaRjg/7iyItYvxkKaSaTvSQNnIbXwBMYSjrp+8ZhY=\n-k8s.io/kubectl v0.32.0-rc.1 h1:hkZ9bqMyOySj8Ky8d6/Yjxa7WQBJKijaMTcGkYmYYKE=\n-k8s.io/kubectl v0.32.0-rc.1/go.mod h1:+N+xgO48ssPu8eJMm2rymUuMIDy2UgqY67Xj/p5eO0Y=\n-k8s.io/kubelet v0.32.0-rc.1 h1:vgQavYPMnDlOAECm3ApeKWrU3m7Oo2lQuGtviJ00jSA=\n-k8s.io/kubelet v0.32.0-rc.1/go.mod h1:lkKQv20UU5m23z4DKYOUbe+j3BMnhRpALR2rqDtduBo=\n-k8s.io/pod-security-admission v0.32.0-rc.1 h1:hinSdeNwLTmuyQpUaZjAPgQjSzIKfu8vo4MFPwV2uKE=\n-k8s.io/pod-security-admission v0.32.0-rc.1/go.mod h1:kMQk2qV2sdGCXMRITdPNy4dAZ9qqcbLgdkzgj/C+niM=\n+k8s.io/kube-scheduler v0.32.0 h1:FCsF/3TPvR51ptx/gLUrqcoKqAMhQKrydYCJzPz9VGM=\n+k8s.io/kube-scheduler v0.32.0/go.mod h1:yof3vmyx70TWoQ6XZruYEGIUT/r0H/ELGdnWiqPF5EE=\n+k8s.io/kubectl v0.32.0 h1:rpxl+ng9qeG79YA4Em9tLSfX0G8W0vfaiPVrc/WR7Xw=\n+k8s.io/kubectl v0.32.0/go.mod h1:qIjSX+QgPQUgdy8ps6eKsYNF+YmFOAO3WygfucIqFiE=\n+k8s.io/kubelet v0.32.0 h1:uLyiKlz195Wo4an/K2tyge8o3QHx0ZkhVN3pevvp59A=\n+k8s.io/kubelet v0.32.0/go.mod h1:lAwuVZT/Hm7EdLn0jW2D+WdrJoorjJL2rVSdhOFnegw=\n+k8s.io/pod-security-admission v0.32.0 h1:I+Og0uZIiMpIgTgPrTbW4jlwRI5RWazi8y+jrx1v10w=\n+k8s.io/pod-security-admission v0.32.0/go.mod h1:RvrcY0+5UAoCIJ7BscgDF3nbmXprgfnjTW+byCyXDvA=\n k8s.io/utils v0.0.0-20241104100929-3ea5e8cea738 h1:M3sRQVHv7vB20Xc2ybTt7ODCeFj6JSWYFzOFnYeS6Ro=\n k8s.io/utils v0.0.0-20241104100929-3ea5e8cea738/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=\n kernel.org/pub/linux/libs/security/libcap/cap v1.2.72 h1:SqLZbTqwcNxctcdM5yb6OcO3lFJNtRgDJoFeca+5hP0=\n\n@@ -18,11 +18,11 @@ preface = \"\"\"\n     [notes.updates]\n         title = \"Component Updates\"\n         description = \"\"\"\\\n-* Linux: 6.12.4\n-* containerd: 2.0.0\n+* Linux: 6.12.5\n+* containerd: 2.0.1\n * Flannel: 0.26.1\n-* Kubernetes: 1.32.0-rc.1\n-* runc: 1.2.1\n+* Kubernetes: 1.32.0\n+* runc: 1.2.3\n * CoreDNS: 1.12.0\n \n Talos is built with Go 1.23.4.\n@@ -174,6 +174,7 @@ It can be enabled with the following config patch:\n machine:\n   features:\n     nodeAddressSortAlgorithm: v2\n+```\n \"\"\"\n \n [make_deps]\n\n@@ -33,7 +33,7 @@ export TALOS_VERSION\n # Kubernetes\n \n export KUBECONFIG=\"${TMP}/kubeconfig\"\n-export KUBERNETES_VERSION=${KUBERNETES_VERSION:-1.32.0-rc.1}\n+export KUBERNETES_VERSION=${KUBERNETES_VERSION:-1.32.0}\n \n export NAME_PREFIX=\"talos-e2e-${SHA}-${PLATFORM}\"\n export TIMEOUT=1200\n\n@@ -0,0 +1,8 @@\n+apiVersion: v1alpha1\n+kind: VolumeConfig\n+name: EPHEMERAL\n+provisioning:\n+  diskSelector:\n+    match: system_disk\n+  minSize: 4GB\n+  maxSize: 100GB\n\n@@ -30,7 +30,7 @@ func Format(ctx context.Context, logger *zap.Logger, volumeContext ManagerContex\n \t\tdevPath = volumeContext.Status.Location\n \t}\n \n-\tdev, err := blockdev.NewFromPath(devPath, blockdev.OpenForWrite())\n+\tdev, err := blockdev.NewFromPath(devPath)\n \tif err != nil {\n \t\treturn xerrors.NewTaggedf[Retryable](\"error opening disk: %w\", err)\n \t}\n\n@@ -8,6 +8,7 @@ package volumes\n import (\n \t\"cmp\"\n \t\"context\"\n+\t\"math\"\n \n \t\"github.com/siderolabs/gen/optional\"\n \n@@ -18,11 +19,29 @@ import (\n \n // CompareVolumeConfigs compares two volume configs in the proposed order of provisioning.\n func CompareVolumeConfigs(a, b *block.VolumeConfig) int {\n+\t// first, sort by wave, smaller wave first\n \tif c := cmp.Compare(a.TypedSpec().Provisioning.Wave, b.TypedSpec().Provisioning.Wave); c != 0 {\n \t\treturn c\n \t}\n \n-\treturn cmpBool(a.TypedSpec().Provisioning.PartitionSpec.Grow, b.TypedSpec().Provisioning.PartitionSpec.Grow)\n+\t// prefer partitions which do not grow, as growing partitions may consume space needed by other partitions\n+\tif c := cmpBool(a.TypedSpec().Provisioning.PartitionSpec.Grow, b.TypedSpec().Provisioning.PartitionSpec.Grow); c != 0 {\n+\t\treturn c\n+\t}\n+\n+\t// prefer partitions with smaller sizes first\n+\t// e.g.: for a disk of size 1GiB, and following config with min-max requested sizes:\n+\t// 1. 100MiB - 200MiB\n+\t// 2. 300MiB - 2GiB\n+\t//\n+\t// if the order is 2-1, the second partition will grow to full disk size and will leave no space for the first partition,\n+\t// but if the order is 1-2, partition sizes will 200MiB and 800MiB respectively.\n+\t//\n+\t// we compare only max size, as it affects the resulting size of the partition\n+\tdesiredSizeA := cmp.Or(a.TypedSpec().Provisioning.PartitionSpec.MaxSize, math.MaxUint64)\n+\tdesiredSizeB := cmp.Or(b.TypedSpec().Provisioning.PartitionSpec.MaxSize, math.MaxUint64)\n+\n+\treturn cmp.Compare(desiredSizeA, desiredSizeB)\n }\n \n func cmpBool(a, b bool) int {\n\n@@ -0,0 +1,132 @@\n+// This Source Code Form is subject to the terms of the Mozilla Public\n+// License, v. 2.0. If a copy of the MPL was not distributed with this\n+// file, You can obtain one at http://mozilla.org/MPL/2.0/.\n+\n+package volumes_test\n+\n+import (\n+\t\"testing\"\n+\n+\t\"github.com/stretchr/testify/assert\"\n+\n+\t\"github.com/siderolabs/talos/internal/app/machined/pkg/controllers/block/internal/volumes\"\n+\t\"github.com/siderolabs/talos/pkg/machinery/resources/block\"\n+)\n+\n+func TestCompareVolumeConfigs(t *testing.T) {\n+\tt.Parallel()\n+\n+\tfor _, test := range []struct {\n+\t\tname string\n+\n+\t\ta *block.VolumeConfigSpec\n+\t\tb *block.VolumeConfigSpec\n+\n+\t\texpected int\n+\t}{\n+\t\t{\n+\t\t\tname: \"different wave\",\n+\n+\t\t\ta: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tb: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveUserDisks,\n+\t\t\t\t},\n+\t\t\t},\n+\n+\t\t\texpected: -1,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"prefer grow\",\n+\n+\t\t\ta: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t\tPartitionSpec: block.PartitionSpec{\n+\t\t\t\t\t\tGrow: true,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tb: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t\tPartitionSpec: block.PartitionSpec{\n+\t\t\t\t\t\tGrow: false,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\n+\t\t\texpected: 1,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"prefer smaller size\",\n+\n+\t\t\ta: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t\tPartitionSpec: block.PartitionSpec{\n+\t\t\t\t\t\tGrow:    false,\n+\t\t\t\t\t\tMinSize: 100,\n+\t\t\t\t\t\tMaxSize: 200,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tb: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t\tPartitionSpec: block.PartitionSpec{\n+\t\t\t\t\t\tGrow:    false,\n+\t\t\t\t\t\tMinSize: 150,\n+\t\t\t\t\t\tMaxSize: 1000,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\n+\t\t\texpected: -1,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"prefer max size\",\n+\n+\t\t\ta: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t\tPartitionSpec: block.PartitionSpec{\n+\t\t\t\t\t\tGrow:    false,\n+\t\t\t\t\t\tMinSize: 100,\n+\t\t\t\t\t\tMaxSize: 200,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tb: &block.VolumeConfigSpec{\n+\t\t\t\tProvisioning: block.ProvisioningSpec{\n+\t\t\t\t\tWave: block.WaveSystemDisk,\n+\t\t\t\t\tPartitionSpec: block.PartitionSpec{\n+\t\t\t\t\t\tGrow:    false,\n+\t\t\t\t\t\tMinSize: 50,\n+\t\t\t\t\t\tMaxSize: 0, // no limit\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\n+\t\t\texpected: -1,\n+\t\t},\n+\t} {\n+\t\tt.Run(test.name, func(t *testing.T) {\n+\t\t\tt.Parallel()\n+\n+\t\t\tresA := block.NewVolumeConfig(block.NamespaceName, \"A\")\n+\t\t\t*resA.TypedSpec() = *test.a\n+\n+\t\t\tresB := block.NewVolumeConfig(block.NamespaceName, \"B\")\n+\t\t\t*resB.TypedSpec() = *test.b\n+\n+\t\t\tactual := volumes.CompareVolumeConfigs(resA, resB)\n+\n+\t\t\tassert.Equal(t, test.expected, actual)\n+\t\t})\n+\t}\n+}\n\n@@ -346,7 +346,28 @@ func (ctrl *VolumeManagerController) Run(ctx context.Context, r controller.Runti\n \t\t\t}\n \n \t\t\tif prevPhase != volumeStatus.Phase || err != nil {\n-\t\t\t\tvolumeLogger.Info(\"volume status\", zap.String(\"phase\", fmt.Sprintf(\"%s -> %s\", prevPhase, volumeStatus.Phase)), zap.Error(err))\n+\t\t\t\tfields := []zap.Field{\n+\t\t\t\t\tzap.String(\"phase\", fmt.Sprintf(\"%s -> %s\", prevPhase, volumeStatus.Phase)),\n+\t\t\t\t\tzap.Error(err),\n+\t\t\t\t}\n+\n+\t\t\t\tif volumeStatus.Location != \"\" {\n+\t\t\t\t\tfields = append(fields, zap.String(\"location\", volumeStatus.Location))\n+\t\t\t\t}\n+\n+\t\t\t\tif volumeStatus.MountLocation != \"\" && volumeStatus.MountLocation != volumeStatus.Location {\n+\t\t\t\t\tfields = append(fields, zap.String(\"mountLocation\", volumeStatus.MountLocation))\n+\t\t\t\t}\n+\n+\t\t\t\tif volumeStatus.ParentLocation != \"\" {\n+\t\t\t\t\tfields = append(fields, zap.String(\"parentLocation\", volumeStatus.ParentLocation))\n+\t\t\t\t}\n+\n+\t\t\t\tif len(volumeStatus.EncryptionFailedSyncs) > 0 {\n+\t\t\t\t\tfields = append(fields, zap.Strings(\"encryptionFailedSyncs\", volumeStatus.EncryptionFailedSyncs))\n+\t\t\t\t}\n+\n+\t\t\t\tvolumeLogger.Info(\"volume status\", fields...)\n \t\t\t}\n \n \t\t\tallClosed = allClosed && volumeStatus.Phase == block.VolumePhaseClosed\n\n@@ -16,6 +16,7 @@ import (\n \t\"github.com/cosi-project/runtime/pkg/safe\"\n \t\"github.com/cosi-project/runtime/pkg/state\"\n \t\"github.com/dustin/go-humanize\"\n+\t\"github.com/google/cel-go/common/ast\"\n \t\"github.com/google/cel-go/common/operators\"\n \t\"github.com/google/cel-go/common/types\"\n \t\"github.com/siderolabs/gen/optional\"\n@@ -198,19 +199,26 @@ func (ctrl *ImageCacheConfigController) Run(ctx context.Context, r controller.Ru\n func (ctrl *ImageCacheConfigController) createVolumeConfigISO(ctx context.Context, r controller.ReaderWriter) error {\n \tbuilder := cel.NewBuilder(celenv.VolumeLocator())\n \n-\t// volume.name == \"iso9660\" && volume.label.startsWith(\"TALOS_\")\n+\t// volume.name in [\"iso9660\", \"vfat\"] && volume.label.startsWith(\"TALOS_\")\n \texpr := builder.NewCall(\n \t\tbuilder.NextID(),\n \t\toperators.LogicalAnd,\n \t\tbuilder.NewCall(\n \t\t\tbuilder.NextID(),\n-\t\t\toperators.Equals,\n+\t\t\toperators.In,\n \t\t\tbuilder.NewSelect(\n \t\t\t\tbuilder.NextID(),\n \t\t\t\tbuilder.NewIdent(builder.NextID(), \"volume\"),\n \t\t\t\t\"name\",\n \t\t\t),\n-\t\t\tbuilder.NewLiteral(builder.NextID(), types.String(\"iso9660\")),\n+\t\t\tbuilder.NewList(\n+\t\t\t\tbuilder.NextID(),\n+\t\t\t\t[]ast.Expr{\n+\t\t\t\t\tbuilder.NewLiteral(builder.NextID(), types.String(\"iso9660\")),\n+\t\t\t\t\tbuilder.NewLiteral(builder.NextID(), types.String(\"vfat\")),\n+\t\t\t\t},\n+\t\t\t\tnil,\n+\t\t\t),\n \t\t),\n \t\tbuilder.NewMemberCall(\n \t\t\tbuilder.NextID(),\n\n@@ -63,7 +63,7 @@ func (suite *ImageCacheConfigSuite) TestReconcileFeatureEnabled() {\n \tsuite.Require().NoError(suite.State().Create(suite.Ctx(), cfg))\n \n \tctest.AssertResource(suite, crictrl.VolumeImageCacheISO, func(r *block.VolumeConfig, asrt *assert.Assertions) {\n-\t\tasrt.Equal(`volume.name == \"iso9660\" && volume.label.startsWith(\"TALOS_\")`, r.TypedSpec().Locator.Match.String())\n+\t\tasrt.Equal(`volume.name in [\"iso9660\", \"vfat\"] && volume.label.startsWith(\"TALOS_\")`, r.TypedSpec().Locator.Match.String())\n \t})\n \tctest.AssertResource(suite, crictrl.VolumeImageCacheDISK, func(r *block.VolumeConfig, asrt *assert.Assertions) {\n \t\tasrt.Equal(`volume.partition_label == \"IMAGECACHE\"`, r.TypedSpec().Locator.Match.String())\n\n@@ -15,6 +15,7 @@ import (\n \t\"github.com/siderolabs/gen/optional\"\n \t\"go.uber.org/zap\"\n \n+\t\"github.com/siderolabs/talos/pkg/machinery/config/types/v1alpha1\"\n \t\"github.com/siderolabs/talos/pkg/machinery/constants\"\n \t\"github.com/siderolabs/talos/pkg/machinery/resources/config\"\n \t\"github.com/siderolabs/talos/pkg/machinery/resources/cri\"\n@@ -54,19 +55,7 @@ func NewRegistriesConfigController() *RegistriesConfigController {\n \t\t\t\t\tmr := cfg.Provider().RawV1Alpha1().MachineConfig.MachineRegistries\n \n \t\t\t\t\tfor k, v := range mr.RegistryConfig {\n-\t\t\t\t\t\tspec.RegistryConfig[k] = &cri.RegistryConfig{\n-\t\t\t\t\t\t\tRegistryTLS: &cri.RegistryTLSConfig{\n-\t\t\t\t\t\t\t\tTLSClientIdentity:     v.RegistryTLS.TLSClientIdentity,\n-\t\t\t\t\t\t\t\tTLSCA:                 v.RegistryTLS.TLSCA,\n-\t\t\t\t\t\t\t\tTLSInsecureSkipVerify: v.RegistryTLS.TLSInsecureSkipVerify,\n-\t\t\t\t\t\t\t},\n-\t\t\t\t\t\t\tRegistryAuth: &cri.RegistryAuthConfig{\n-\t\t\t\t\t\t\t\tRegistryUsername:      v.RegistryAuth.RegistryUsername,\n-\t\t\t\t\t\t\t\tRegistryPassword:      v.RegistryAuth.RegistryPassword,\n-\t\t\t\t\t\t\t\tRegistryAuth:          v.RegistryAuth.RegistryAuth,\n-\t\t\t\t\t\t\t\tRegistryIdentityToken: v.RegistryAuth.RegistryIdentityToken,\n-\t\t\t\t\t\t\t},\n-\t\t\t\t\t\t}\n+\t\t\t\t\t\tspec.RegistryConfig[k] = makeRegistryConfig(v)\n \t\t\t\t\t}\n \n \t\t\t\t\tfor k, v := range mr.RegistryMirrors {\n@@ -116,3 +105,26 @@ func clearInit[M ~map[K]V, K comparable, V any](m M) M {\n \n \treturn m\n }\n+\n+func makeRegistryConfig(cfg *v1alpha1.RegistryConfig) *cri.RegistryConfig {\n+\tresult := &cri.RegistryConfig{}\n+\n+\tif rtls := cfg.RegistryTLS; rtls != nil {\n+\t\tresult.RegistryTLS = &cri.RegistryTLSConfig{\n+\t\t\tTLSClientIdentity:     rtls.TLSClientIdentity,\n+\t\t\tTLSCA:                 rtls.TLSCA,\n+\t\t\tTLSInsecureSkipVerify: rtls.TLSInsecureSkipVerify,\n+\t\t}\n+\t}\n+\n+\tif rauth := cfg.RegistryAuth; rauth != nil {\n+\t\tresult.RegistryAuth = &cri.RegistryAuthConfig{\n+\t\t\tRegistryUsername:      rauth.RegistryUsername,\n+\t\t\tRegistryPassword:      rauth.RegistryPassword,\n+\t\t\tRegistryAuth:          rauth.RegistryAuth,\n+\t\t\tRegistryIdentityToken: rauth.RegistryIdentityToken,\n+\t\t}\n+\t}\n+\n+\treturn result\n+}\n\n@@ -8,6 +8,7 @@ import (\n \t\"testing\"\n \t\"time\"\n \n+\t\"github.com/siderolabs/go-pointer\"\n \t\"github.com/stretchr/testify/assert\"\n \t\"github.com/stretchr/testify/suite\"\n \n@@ -74,6 +75,173 @@ func (suite *ConfigSuite) TestRegistry() {\n \t})\n }\n \n+func (suite *ConfigSuite) TestRegistryAuth() {\n+\tcfg := config.NewMachineConfig(container.NewV1Alpha1(&v1alpha1.Config{\n+\t\tConfigVersion: \"v1alpha1\",\n+\t\tMachineConfig: &v1alpha1.MachineConfig{\n+\t\t\tMachineType: \"controlplane\",\n+\t\t\tMachineRegistries: v1alpha1.RegistriesConfig{\n+\t\t\t\tRegistryMirrors: map[string]*v1alpha1.RegistryMirrorConfig{\n+\t\t\t\t\t\"docker.io\": {MirrorEndpoints: []string{\"https://mirror.io\"}},\n+\t\t\t\t},\n+\t\t\t\tRegistryConfig: map[string]*v1alpha1.RegistryConfig{\n+\t\t\t\t\t\"docker.io\": {\n+\t\t\t\t\t\tRegistryAuth: &v1alpha1.RegistryAuthConfig{\n+\t\t\t\t\t\t\tRegistryUsername:      \"example\",\n+\t\t\t\t\t\t\tRegistryPassword:      \"pass\",\n+\t\t\t\t\t\t\tRegistryAuth:          \"someauth\",\n+\t\t\t\t\t\t\tRegistryIdentityToken: \"token\",\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t},\n+\t}))\n+\n+\tsuite.Require().NoError(suite.State().Create(suite.Ctx(), cfg))\n+\n+\tctest.AssertResource(suite, crires.RegistriesConfigID, func(r *crires.RegistriesConfig, a *assert.Assertions) {\n+\t\tspec := r.TypedSpec()\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryMirrorConfig{\n+\t\t\t\t\"docker.io\": {MirrorEndpoints: []string{\"https://mirror.io\"}},\n+\t\t\t},\n+\t\t\tspec.RegistryMirrors,\n+\t\t)\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryConfig{\n+\t\t\t\t\"docker.io\": {\n+\t\t\t\t\tRegistryAuth: &crires.RegistryAuthConfig{\n+\t\t\t\t\t\tRegistryUsername:      \"example\",\n+\t\t\t\t\t\tRegistryPassword:      \"pass\",\n+\t\t\t\t\t\tRegistryAuth:          \"someauth\",\n+\t\t\t\t\t\tRegistryIdentityToken: \"token\",\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tspec.RegistryConfig,\n+\t\t)\n+\t})\n+\n+\tic := crires.NewImageCacheConfig()\n+\tic.TypedSpec().Roots = []string{\"/imagecache\"}\n+\tic.TypedSpec().Status = crires.ImageCacheStatusReady\n+\n+\tsuite.Require().NoError(suite.State().Create(suite.Ctx(), ic))\n+\n+\tctest.AssertResource(suite, crires.RegistriesConfigID, func(r *crires.RegistriesConfig, a *assert.Assertions) {\n+\t\tspec := r.TypedSpec()\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryMirrorConfig{\n+\t\t\t\t\"*\": {MirrorEndpoints: []string{\n+\t\t\t\t\t\"http://\" + constants.RegistrydListenAddress,\n+\t\t\t\t}},\n+\t\t\t\t\"docker.io\": {MirrorEndpoints: []string{\n+\t\t\t\t\t\"http://\" + constants.RegistrydListenAddress,\n+\t\t\t\t\t\"https://mirror.io\",\n+\t\t\t\t}},\n+\t\t\t},\n+\t\t\tspec.RegistryMirrors,\n+\t\t)\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryConfig{\n+\t\t\t\t\"docker.io\": {\n+\t\t\t\t\tRegistryAuth: &crires.RegistryAuthConfig{\n+\t\t\t\t\t\tRegistryUsername:      \"example\",\n+\t\t\t\t\t\tRegistryPassword:      \"pass\",\n+\t\t\t\t\t\tRegistryAuth:          \"someauth\",\n+\t\t\t\t\t\tRegistryIdentityToken: \"token\",\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tspec.RegistryConfig,\n+\t\t)\n+\t})\n+}\n+\n+func (suite *ConfigSuite) TestRegistryTLS() {\n+\tcfg := config.NewMachineConfig(container.NewV1Alpha1(&v1alpha1.Config{\n+\t\tConfigVersion: \"v1alpha1\",\n+\t\tMachineConfig: &v1alpha1.MachineConfig{\n+\t\t\tMachineType: \"controlplane\",\n+\t\t\tMachineRegistries: v1alpha1.RegistriesConfig{\n+\t\t\t\tRegistryMirrors: map[string]*v1alpha1.RegistryMirrorConfig{\n+\t\t\t\t\t\"docker.io\": {MirrorEndpoints: []string{\"https://mirror.io\"}},\n+\t\t\t\t},\n+\t\t\t\tRegistryConfig: map[string]*v1alpha1.RegistryConfig{\n+\t\t\t\t\t\"docker.io\": {\n+\t\t\t\t\t\tRegistryTLS: &v1alpha1.RegistryTLSConfig{\n+\t\t\t\t\t\t\tTLSInsecureSkipVerify: pointer.To(true),\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t},\n+\t}))\n+\n+\tsuite.Require().NoError(suite.State().Create(suite.Ctx(), cfg))\n+\n+\tctest.AssertResource(suite, crires.RegistriesConfigID, func(r *crires.RegistriesConfig, a *assert.Assertions) {\n+\t\tspec := r.TypedSpec()\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryMirrorConfig{\n+\t\t\t\t\"docker.io\": {MirrorEndpoints: []string{\"https://mirror.io\"}},\n+\t\t\t},\n+\t\t\tspec.RegistryMirrors,\n+\t\t)\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryConfig{\n+\t\t\t\t\"docker.io\": {\n+\t\t\t\t\tRegistryTLS: &crires.RegistryTLSConfig{\n+\t\t\t\t\t\tTLSInsecureSkipVerify: pointer.To(true),\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tspec.RegistryConfig,\n+\t\t)\n+\t})\n+\n+\tic := crires.NewImageCacheConfig()\n+\tic.TypedSpec().Roots = []string{\"/imagecache\"}\n+\tic.TypedSpec().Status = crires.ImageCacheStatusReady\n+\n+\tsuite.Require().NoError(suite.State().Create(suite.Ctx(), ic))\n+\n+\tctest.AssertResource(suite, crires.RegistriesConfigID, func(r *crires.RegistriesConfig, a *assert.Assertions) {\n+\t\tspec := r.TypedSpec()\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryMirrorConfig{\n+\t\t\t\t\"*\": {MirrorEndpoints: []string{\n+\t\t\t\t\t\"http://\" + constants.RegistrydListenAddress,\n+\t\t\t\t}},\n+\t\t\t\t\"docker.io\": {MirrorEndpoints: []string{\n+\t\t\t\t\t\"http://\" + constants.RegistrydListenAddress,\n+\t\t\t\t\t\"https://mirror.io\",\n+\t\t\t\t}},\n+\t\t\t},\n+\t\t\tspec.RegistryMirrors,\n+\t\t)\n+\n+\t\ta.Equal(\n+\t\t\tmap[string]*crires.RegistryConfig{\n+\t\t\t\t\"docker.io\": {\n+\t\t\t\t\tRegistryTLS: &crires.RegistryTLSConfig{\n+\t\t\t\t\t\tTLSInsecureSkipVerify: pointer.To(true),\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\tspec.RegistryConfig,\n+\t\t)\n+\t})\n+}\n+\n func (suite *ConfigSuite) TestRegistryNoMachineConfig() {\n \tcfg := config.NewMachineConfig(container.NewV1Alpha1(nil))\n \n\n@@ -7,6 +7,7 @@ package k8s\n import (\n \t\"context\"\n \t\"fmt\"\n+\t\"slices\"\n \t\"strconv\"\n \t\"strings\"\n \n@@ -127,20 +128,24 @@ func NewControlPlaneAuthorizationController() *ControlPlaneAuthorizationControll\n \t\t\t\t\treturn nil\n \t\t\t\t}\n \n+\t\t\t\tvar authorizers []k8s.AuthorizationAuthorizersSpec\n+\n \t\t\t\tfor _, authorizer := range cfgProvider.Cluster().APIServer().AuthorizationConfig() {\n \t\t\t\t\t// skip Node and RBAC authorizers as we add them by default later on.\n \t\t\t\t\tif authorizer.Type() == \"Node\" || authorizer.Type() == \"RBAC\" {\n \t\t\t\t\t\tcontinue\n \t\t\t\t\t}\n \n-\t\t\t\t\tres.TypedSpec().Config = append(res.TypedSpec().Config, k8s.AuthorizationAuthorizersSpec{\n-\t\t\t\t\t\tType:    authorizer.Type(),\n-\t\t\t\t\t\tName:    authorizer.Name(),\n-\t\t\t\t\t\tWebhook: authorizer.Webhook(),\n+\t\t\t\t\tauthorizers = slices.Concat(authorizers, []k8s.AuthorizationAuthorizersSpec{\n+\t\t\t\t\t\t{\n+\t\t\t\t\t\t\tType:    authorizer.Type(),\n+\t\t\t\t\t\t\tName:    authorizer.Name(),\n+\t\t\t\t\t\t\tWebhook: authorizer.Webhook(),\n+\t\t\t\t\t\t},\n \t\t\t\t\t})\n \t\t\t\t}\n \n-\t\t\t\tres.TypedSpec().Config = append(v1alpha1.APIServerDefaultAuthorizationConfigAuthorizers, res.TypedSpec().Config...)\n+\t\t\t\tres.TypedSpec().Config = slices.Concat(v1alpha1.APIServerDefaultAuthorizationConfigAuthorizers, authorizers)\n \n \t\t\t\treturn nil\n \t\t\t},\n\n@@ -6,6 +6,7 @@ package k8s_test\n \n import (\n \t\"net/url\"\n+\t\"slices\"\n \t\"strings\"\n \t\"testing\"\n \t\"time\"\n@@ -208,16 +209,18 @@ func (suite *K8sControlPlaneSuite) TestReconcileAdditionalAuthorizationConfigAut\n \n \tsuite.setupMachine(cfg)\n \n-\texpectedAuthorizers := append(v1alpha1.APIServerDefaultAuthorizationConfigAuthorizers, k8s.AuthorizationAuthorizersSpec{ //nolint:gocritic\n-\t\tType: \"Webhook\",\n-\t\tName: \"webhook\",\n-\t\tWebhook: map[string]any{\n-\t\t\t\"timeout\":                    \"3s\",\n-\t\t\t\"subjectAccessReviewVersion\": \"v1\",\n-\t\t\t\"matchConditionSubjectAccessReviewVersion\": \"v1\",\n-\t\t\t\"failurePolicy\": \"NoOpinion\",\n-\t\t\t\"connectionInfo\": map[string]any{\n-\t\t\t\t\"type\": \"InClusterConfig\",\n+\texpectedAuthorizers := slices.Concat(v1alpha1.APIServerDefaultAuthorizationConfigAuthorizers, []k8s.AuthorizationAuthorizersSpec{\n+\t\t{\n+\t\t\tType: \"Webhook\",\n+\t\t\tName: \"webhook\",\n+\t\t\tWebhook: map[string]any{\n+\t\t\t\t\"timeout\":                    \"3s\",\n+\t\t\t\t\"subjectAccessReviewVersion\": \"v1\",\n+\t\t\t\t\"matchConditionSubjectAccessReviewVersion\": \"v1\",\n+\t\t\t\t\"failurePolicy\": \"NoOpinion\",\n+\t\t\t\t\"connectionInfo\": map[string]any{\n+\t\t\t\t\t\"type\": \"InClusterConfig\",\n+\t\t\t\t},\n \t\t\t},\n \t\t},\n \t})\n@@ -280,16 +283,18 @@ func (suite *K8sControlPlaneSuite) TestReconcileAdditionalAuthorizationConfigAut\n \n \tsuite.setupMachine(cfg)\n \n-\texpectedAuthorizers := append(v1alpha1.APIServerDefaultAuthorizationConfigAuthorizers, k8s.AuthorizationAuthorizersSpec{ //nolint:gocritic\n-\t\tType: \"Webhook\",\n-\t\tName: \"webhook\",\n-\t\tWebhook: map[string]any{\n-\t\t\t\"timeout\":                    \"3s\",\n-\t\t\t\"subjectAccessReviewVersion\": \"v1\",\n-\t\t\t\"matchConditionSubjectAccessReviewVersion\": \"v1\",\n-\t\t\t\"failurePolicy\": \"NoOpinion\",\n-\t\t\t\"connectionInfo\": map[string]any{\n-\t\t\t\t\"type\": \"InClusterConfig\",\n+\texpectedAuthorizers := slices.Concat(v1alpha1.APIServerDefaultAuthorizationConfigAuthorizers, []k8s.AuthorizationAuthorizersSpec{\n+\t\t{\n+\t\t\tType: \"Webhook\",\n+\t\t\tName: \"webhook\",\n+\t\t\tWebhook: map[string]any{\n+\t\t\t\t\"timeout\":                    \"3s\",\n+\t\t\t\t\"subjectAccessReviewVersion\": \"v1\",\n+\t\t\t\t\"matchConditionSubjectAccessReviewVersion\": \"v1\",\n+\t\t\t\t\"failurePolicy\": \"NoOpinion\",\n+\t\t\t\t\"connectionInfo\": map[string]any{\n+\t\t\t\t\t\"type\": \"InClusterConfig\",\n+\t\t\t\t},\n \t\t\t},\n \t\t},\n \t})\n\n@@ -11,12 +11,14 @@ import (\n \t\"net\"\n \t\"net/netip\"\n \t\"os\"\n+\t\"slices\"\n \n \t\"github.com/cosi-project/runtime/pkg/controller\"\n \t\"github.com/cosi-project/runtime/pkg/resource\"\n \t\"github.com/cosi-project/runtime/pkg/safe\"\n \t\"github.com/jsimonetti/rtnetlink/v2\"\n \t\"github.com/mdlayher/arp\"\n+\t\"github.com/siderolabs/go-pointer\"\n \t\"go.uber.org/zap\"\n \t\"go4.org/netipx\"\n \t\"golang.org/x/sys/unix\"\n@@ -117,12 +119,28 @@ func (ctrl *AddressSpecController) Run(ctx context.Context, r controller.Runtime\n }\n \n func resolveLinkName(links []rtnetlink.LinkMessage, linkName string) uint32 {\n+\tif linkName == \"\" {\n+\t\treturn 0 // should never match\n+\t}\n+\n+\t// first, lookup by name\n \tfor _, link := range links {\n \t\tif link.Attributes.Name == linkName {\n \t\t\treturn link.Index\n \t\t}\n \t}\n \n+\t// then, lookup by alias/altname\n+\tfor _, link := range links {\n+\t\tif pointer.SafeDeref(link.Attributes.Alias) == linkName {\n+\t\t\treturn link.Index\n+\t\t}\n+\n+\t\tif slices.Index(link.Attributes.AltNames, linkName) != -1 {\n+\t\t\treturn link.Index\n+\t\t}\n+\t}\n+\n \treturn 0\n }\n \n\n@@ -238,6 +238,74 @@ func (suite *AddressSpecSuite) TestDummy() {\n \t}\n }\n \n+func (suite *AddressSpecSuite) TestDummyAlias() {\n+\tdummyInterface := suite.uniqueDummyInterface()\n+\tdummyAlias := suite.uniqueDummyInterface()\n+\n+\tsuite.T().Logf(\"dummyInterface: %s, dummyAlias: %s\", dummyInterface, dummyAlias)\n+\n+\tconn, err := rtnetlink.Dial(nil)\n+\tsuite.Require().NoError(err)\n+\n+\tdefer conn.Close() //nolint:errcheck\n+\n+\tdummy := network.NewAddressSpec(network.NamespaceName, \"dummy/10.0.0.5/8\")\n+\t*dummy.TypedSpec() = network.AddressSpecSpec{\n+\t\tAddress:     netip.MustParsePrefix(\"10.0.0.5/8\"),\n+\t\tLinkName:    dummyAlias, // use alias name instead of the actual interface name\n+\t\tFamily:      nethelpers.FamilyInet4,\n+\t\tScope:       nethelpers.ScopeGlobal,\n+\t\tConfigLayer: network.ConfigDefault,\n+\t\tFlags:       nethelpers.AddressFlags(nethelpers.AddressPermanent),\n+\t}\n+\n+\t// it's fine to create the address before the interface is actually created\n+\tfor _, res := range []resource.Resource{dummy} {\n+\t\tsuite.Require().NoError(suite.state.Create(suite.ctx, res), \"%v\", res.Spec())\n+\t}\n+\n+\t// create dummy interface\n+\tsuite.Require().NoError(\n+\t\tconn.Link.New(\n+\t\t\t&rtnetlink.LinkMessage{\n+\t\t\t\tType: unix.ARPHRD_ETHER,\n+\t\t\t\tAttributes: &rtnetlink.LinkAttributes{\n+\t\t\t\t\tName: dummyInterface,\n+\t\t\t\t\tMTU:  1400,\n+\t\t\t\t\tInfo: &rtnetlink.LinkInfo{\n+\t\t\t\t\t\tKind: \"dummy\",\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t),\n+\t)\n+\n+\tiface, err := net.InterfaceByName(dummyInterface)\n+\tsuite.Require().NoError(err)\n+\n+\t// set alias name\n+\tsuite.Require().NoError(\n+\t\tconn.Link.Set(\n+\t\t\t&rtnetlink.LinkMessage{\n+\t\t\t\tIndex: uint32(iface.Index),\n+\t\t\t\tAttributes: &rtnetlink.LinkAttributes{\n+\t\t\t\t\tAlias: &dummyAlias,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t),\n+\t)\n+\n+\tdefer conn.Link.Delete(uint32(iface.Index)) //nolint:errcheck\n+\n+\tsuite.Assert().NoError(\n+\t\tretry.Constant(3*time.Second, retry.WithUnits(100*time.Millisecond)).Retry(\n+\t\t\tfunc() error {\n+\t\t\t\treturn suite.assertLinkAddress(dummyInterface, \"10.0.0.5/8\")\n+\t\t\t},\n+\t\t),\n+\t)\n+}\n+\n func (suite *AddressSpecSuite) TearDownTest() {\n \tsuite.T().Log(\"tear down\")\n \n\n@@ -183,27 +183,42 @@ func (ctrl *LinkConfigController) Run(ctx context.Context, r controller.Runtime,\n \t\t\treturn fmt.Errorf(\"error listing link statuses: %w\", err)\n \t\t}\n \n+\touter:\n \t\tfor _, item := range list.Items {\n \t\t\tlinkStatus := item.(*network.LinkStatus) //nolint:forcetypeassert\n \n-\t\t\tif _, configured := configuredLinks[linkStatus.Metadata().ID()]; !configured {\n-\t\t\t\tif linkStatus.TypedSpec().Physical() {\n-\t\t\t\t\tvar ids []string\n+\t\t\tif _, configured := configuredLinks[linkStatus.Metadata().ID()]; configured {\n+\t\t\t\tcontinue\n+\t\t\t}\n \n-\t\t\t\t\tids, err = ctrl.apply(ctx, r, []network.LinkSpecSpec{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tName:        linkStatus.Metadata().ID(),\n-\t\t\t\t\t\t\tUp:          true,\n-\t\t\t\t\t\t\tConfigLayer: network.ConfigDefault,\n-\t\t\t\t\t\t},\n-\t\t\t\t\t})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn fmt.Errorf(\"error applying default link up: %w\", err)\n-\t\t\t\t\t}\n+\t\t\tif linkStatus.TypedSpec().Alias != \"\" {\n+\t\t\t\tif _, configured := configuredLinks[linkStatus.TypedSpec().Alias]; configured {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t}\n \n-\t\t\t\t\tfor _, id := range ids {\n-\t\t\t\t\t\ttouchedIDs[id] = struct{}{}\n-\t\t\t\t\t}\n+\t\t\tfor _, altName := range linkStatus.TypedSpec().AltNames {\n+\t\t\t\tif _, configured := configuredLinks[altName]; configured {\n+\t\t\t\t\tcontinue outer\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif linkStatus.TypedSpec().Physical() {\n+\t\t\t\tvar ids []string\n+\n+\t\t\t\tids, err = ctrl.apply(ctx, r, []network.LinkSpecSpec{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tName:        linkStatus.Metadata().ID(),\n+\t\t\t\t\t\tUp:          true,\n+\t\t\t\t\t\tConfigLayer: network.ConfigDefault,\n+\t\t\t\t\t},\n+\t\t\t\t})\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn fmt.Errorf(\"error applying default link up: %w\", err)\n+\t\t\t\t}\n+\n+\t\t\t\tfor _, id := range ids {\n+\t\t\t\t\ttouchedIDs[id] = struct{}{}\n \t\t\t\t}\n \t\t\t}\n \t\t}\n"}
{"id": 303, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe tests that style.use() and imread() support urls can just use local\r\nurls (`file://`); this avoids failing the tests when downloading the\r\ngist fails due to a flaky connection.\r\n\r\nSince RFC8089 file: urls don't necessarily start with two slashes\r\nanymore (https://tools.ietf.org/html/rfc8089#appendix-A).\r\n\r\n(If we really want to test https we could use http.server to spin up a local server but that seems overkill.)\r\n\r\n\r\n## PR Summary\r\n\r\n## PR Checklist\r\n\r\n- [ ] Has Pytest style unit tests\r\n- [ ] Code is [Flake 8](http://flake8.pycqa.org/en/latest/) compliant\r\n- [ ] New features are documented, with examples if plot related\r\n- [ ] Documentation is sphinx and numpydoc compliant\r\n- [ ] Added an entry to doc/users/next_whats_new/ if major new feature (follow instructions in README.rst there)\r\n- [ ] Documented in doc/api/api_changes.rst if API changed in a backward-incompatible way\r\n\r\n<!--\r\nThank you so much for your PR!  To help us review your contribution, please\r\nconsider the following points:\r\n\r\n- A development guide is available at https://matplotlib.org/devdocs/devel/index.html.\r\n\r\n- Help with git and github is available at\r\n  https://matplotlib.org/devel/gitwash/development_workflow.html.\r\n\r\n- Do not create the PR out of master, but out of a separate branch.\r\n\r\n- The PR title should summarize the changes, for example \"Raise ValueError on\r\n  non-numeric input to set_xlim\".  Avoid non-descriptive titles such as\r\n  \"Addresses issue #8576\".\r\n\r\n- The summary should provide at least 1-2 sentences describing the pull request\r\n  in detail (Why is this change required?  What problem does it solve?) and\r\n  link to any relevant issues.\r\n\r\n- If you are contributing fixes to docstrings, please pay attention to\r\n  http://matplotlib.org/devel/documenting_mpl.html#formatting.  In particular,\r\n  note the difference between using single backquotes, double backquotes, and\r\n  asterisks in the markup.\r\n\r\nWe understand that PRs can sometimes be overwhelming, especially as the\r\nreviews start coming in.  Please let us know if the reviews are unclear or\r\nthe recommended next step seems overly demanding, if you would like help in\r\naddressing a reviewer's comments, or if you have been waiting too long to hear\r\nback on your PR.\r\n-->\r\n\n\n### Code Patch:\n@@ -16,7 +16,7 @@ environment:\n   global:\n     PYTHONIOENCODING: UTF-8\n     PYTEST_ARGS: -raR --numprocesses=auto --timeout=300 --durations=25\n-                 --cov-report= --cov=lib -m \"not network\" --log-level=DEBUG\n+                 --cov-report= --cov=lib --log-level=DEBUG\n \n   matrix:\n     # theoretically the CONDA_INSTALL_LOCN could be only two: one for 32bit,\n\n@@ -0,0 +1,7 @@\n+API changes\n+```````````\n+\n+The ``--no-network`` flag to ``tests.py`` has been removed (no test requires\n+internet access anymore).  If it is desired to disable internet access both for\n+old and new versions of Matplotlib, use ``tests.py -m 'not network'`` (which is\n+now a no-op).\n\n@@ -54,7 +54,6 @@ commands, such as:\n \n ========================  ===========\n ``--pep8``                Perform pep8 checks (requires pytest-pep8_)\n-``-m \"not network\"``      Disable tests that require network access\n ========================  ===========\n \n Additional arguments are passed on to pytest. See the pytest documentation for\n\n@@ -867,7 +867,7 @@ def rc_params(fail_on_error=False):\n     return rc_params_from_file(matplotlib_fname(), fail_on_error)\n \n \n-URL_REGEX = re.compile(r'http://|https://|ftp://|file://|file:\\\\')\n+URL_REGEX = re.compile(r'^http://|^https://|^ftp://|^file:')\n \n \n def is_url(filename):\n\n@@ -1379,12 +1379,20 @@ def read_png(*args, **kwargs):\n     if format is None:\n         if isinstance(fname, str):\n             parsed = urllib.parse.urlparse(fname)\n-            # If the string is a URL, assume png\n+            # If the string is a URL (Windows paths appear as if they have a\n+            # length-1 scheme), assume png.\n             if len(parsed.scheme) > 1:\n                 ext = 'png'\n             else:\n                 basename, ext = os.path.splitext(fname)\n                 ext = ext.lower()[1:]\n+        elif hasattr(fname, 'geturl'):  # Returned by urlopen().\n+            # We could try to parse the url's path and use the extension, but\n+            # returning png is consistent with the block above.  Note that this\n+            # if clause has to come before checking for fname.name as\n+            # urlopen(\"file:///...\") also has a name attribute (with the fixed\n+            # value \"<urllib response>\").\n+            ext = 'png'\n         elif hasattr(fname, 'name'):\n             basename, ext = os.path.splitext(fname.name)\n             ext = ext.lower()[1:]\n\n@@ -5,6 +5,7 @@\n import sys\n from pathlib import Path\n import platform\n+import sys\n import urllib.request\n import warnings\n \n@@ -657,9 +658,11 @@ def test_minimized_rasterized():\n                 assert False\n \n \n-@pytest.mark.network\n def test_load_from_url():\n-    url = \"http://matplotlib.org/_static/logo_sidebar_horiz.png\"\n+    path = Path(__file__).parent / \"baseline_images/test_image/imshow.png\"\n+    url = ('file:'\n+           + ('///' if sys.platform == 'win32' else '')\n+           + path.resolve().as_posix())\n     plt.imread(url)\n     plt.imread(urllib.request.urlopen(url))\n \n\n@@ -3,6 +3,7 @@\n import gc\n from pathlib import Path\n from tempfile import TemporaryDirectory\n+import sys\n \n import pytest\n \n@@ -56,10 +57,14 @@ def test_use():\n             assert mpl.rcParams[PARAM] == VALUE\n \n \n-@pytest.mark.network\n-def test_use_url():\n+def test_use_url(tmpdir):\n+    path = Path(tmpdir, 'file')\n+    path.write_text('axes.facecolor: adeade')\n     with temp_style('test', DUMMY_SETTINGS):\n-        with style.context('https://gist.github.com/adrn/6590261/raw'):\n+        url = ('file:'\n+               + ('///' if sys.platform == 'win32' else '')\n+               + path.resolve().as_posix())\n+        with style.context(url):\n             assert mpl.rcParams['axes.facecolor'] == \"#adeade\"\n \n \n\n@@ -6,5 +6,4 @@ python_files = test_*.py\n \n markers =\n     backend: Set alternate Matplotlib backend temporarily.\n-    network: Mark a test that uses the network.\n     style: Set alternate Matplotlib style temporarily.\n\n@@ -39,17 +39,10 @@\n     from matplotlib import test\n \n     parser = argparse.ArgumentParser(add_help=False)\n-    parser.add_argument('--no-network', action='store_true',\n-                        help='Run tests without network connection')\n     parser.add_argument('--recursionlimit', type=int, default=0,\n                         help='Specify recursionlimit for test run')\n     args, extra_args = parser.parse_known_args()\n \n-    if args.no_network:\n-        from matplotlib.testing import disable_internet\n-        disable_internet.turn_off_internet()\n-        extra_args.extend(['-m', 'not network'])\n-\n     print('Python byte-compilation optimization level:', sys.flags.optimize)\n \n     retcode = test(argv=extra_args, switch_backend_warn=False,\n"}
{"id": 280, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Failure cluster [46c1b1ee59dffad82fad](https://storage.googleapis.com/k8s-triage/index.html?pr=1#46c1b1ee59dffad82fad)\r\n\r\n##### Error text:\r\n```\r\n=== RUN   TestFrameworkHandler_IterateOverWaitingPods/pods_with_different_profiles_are_waiting_on_permit_stage\r\n    framework.go:381: I0229 10:48:13.714983] the scheduler starts to work with those plugins Plugins={\"PreEnqueue\":{\"Enabled\":null,\"Disabled\":null},\"QueueSort\":{\"Enabled\":[{\"Name\":\"PrioritySort\",\"Weight\":0}],\"Disabled\":null},\"PreFilter\":{\"Enabled\":null,\"Disabled\":null},\"Filter\":{\"Enabled\":null,\"Disabled\":null},\"PostFilter\":{\"Enabled\":null,\"Disabled\":null},\"PreScore\":{\"Enabled\":null,\"Disabled\":null},\"Score\":{\"Enabled\":null,\"Disabled\":null},\"Reserve\":{\"Enabled\":null,\"Disabled\":null},\"Permit\":{\"Enabled\":[{\"Name\":\"fakePermit\",\"Weight\":0}],\"Disabled\":null},\"PreBind\":{\"Enabled\":null,\"Disabled\":null},\"Bind\":{\"Enabled\":[{\"Name\":\"DefaultBinder\",\"Weight\":0}],\"Disabled\":null},\"PostBind\":{\"Enabled\":null,\"Disabled\":null},\"MultiPoint\":{\"Enabled\":null,\"Disabled\":null}}\r\n    framework.go:381: I0229 10:48:13.715309] the scheduler starts to work with those plugins Plugins={\"PreEnqueue\":{\"Enabled\":null,\"Disabled\":null},\"QueueSort\":{\"Enabled\":[{\"Name\":\"PrioritySort\",\"Weight\":0}],\"Disabled\":null},\"PreFilter\":{\"Enabled\":null,\"Disabled\":null},\"Filter\":{\"Enabled\":null,\"Disabled\":null},\"PostFilter\":{\"Enabled\":null,\"Disabled\":null},\"PreScore\":{\"Enabled\":null,\"Disabled\":null},\"Score\":{\"Enabled\":null,\"Disabled\":null},\"Reserve\":{\"Enabled\":null,\"Disabled\":null},\"Permit\":{\"Enabled\":[{\"Name\":\"fakePermit\",\"Weight\":0}],\"Disabled\":null},\"PreBind\":{\"Enabled\":null,\"Disabled\":null},\"Bind\":{\"Enabled\":[{\"Name\":\"DefaultBinder\",\"Weight\":0}],\"Disabled\":null},\"PostBind\":{\"Enabled\":null,\"Disabled\":null},\"MultiPoint\":{\"Enabled\":null,\"Disabled\":null}}\r\n    framework.go:381: I0229 10:48:13.715630] the scheduler starts to work with those plugins Plugins={\"PreEnqueue\":{\"Enabled\":null,\"Disabled\":null},\"QueueSort\":{\"Enabled\":[{\"Name\":\"PrioritySort\",\"Weight\":0}],\"Disabled\":null},\"PreFilter\":{\"Enabled\":null,\"Disabled\":null},\"Filter\":{\"Enabled\":null,\"Disabled\":null},\"PostFilter\":{\"Enabled\":null,\"Disabled\":null},\"PreScore\":{\"Enabled\":null,\"Disabled\":null},\"Score\":{\"Enabled\":null,\"Disabled\":null},\"Reserve\":{\"Enabled\":null,\"Disabled\":null},\"Permit\":{\"Enabled\":[{\"Name\":\"fakePermit\",\"Weight\":0}],\"Disabled\":null},\"PreBind\":{\"Enabled\":null,\"Disabled\":null},\"Bind\":{\"Enabled\":[{\"Name\":\"DefaultBinder\",\"Weight\":0}],\"Disabled\":null},\"PostBind\":{\"Enabled\":null,\"Disabled\":null},\"MultiPoint\":{\"Enabled\":null,\"Disabled\":null}}\r\n    eventhandlers.go:75: I0229 10:48:13.717736] Add event for node node=\"node1\"\r\n    node_tree.go:65: I0229 10:48:13.717988] Added node in listed group to NodeTree node=\"node1\" zone=\"\"\r\n    eventhandlers.go:75: I0229 10:48:13.718196] Add event for node node=\"node2\"\r\n    node_tree.go:65: I0229 10:48:13.718385] Added node in listed group to NodeTree node=\"node2\" zone=\"\"\r\n    eventhandlers.go:75: I0229 10:48:13.718588] Add event for node node=\"node3\"\r\n    node_tree.go:65: I0229 10:48:13.718793] Added node in listed group to NodeTree node=\"node3\" zone=\"\"\r\n    eventhandlers.go:128: I0229 10:48:13.718980] Add event for unscheduled pod pod=\"pod1\"\r\n    scheduling_queue.go:595: I0229 10:48:13.719138] Pod moved to an internal scheduling queue pod=\"pod1\" event=\"PodAdd\" queue=\"Active\"\r\n    schedule_one.go:84: I0229 10:48:13.719250] About to try and schedule pod pod=\"pod1\"\r\n    schedule_one.go:97: I0229 10:48:13.719321] Attempting to schedule pod pod=\"pod1\"\r\n    framework.go:1475: I0229 10:48:13.719623] Permit: One or more plugins asked to wait and no plugin rejected pod node=\"node1\" pod=\"pod1\"\r\n    framework.go:1500: I0229 10:48:13.719889] Pod waiting on permit pod=\"pod1\"\r\n    eventhandlers.go:128: I0229 10:48:13.720049] Add event for unscheduled pod pod=\"pod2\"\r\n    scheduling_queue.go:595: I0229 10:48:13.720184] Pod moved to an internal scheduling queue pod=\"pod2\" event=\"PodAdd\" queue=\"Active\"\r\n    schedule_one.go:84: I0229 10:48:13.720358] About to try and schedule pod pod=\"pod2\"\r\n    schedule_one.go:97: I0229 10:48:13.720451] Attempting to schedule pod pod=\"pod2\"\r\n    framework.go:1475: I0229 10:48:13.720791] Permit: One or more plugins asked to wait and no plugin rejected pod node=\"node2\" pod=\"pod2\"\r\n    framework.go:1500: I0229 10:48:13.721010] Pod waiting on permit pod=\"pod2\"\r\n    eventhandlers.go:128: I0229 10:48:13.721439] Add event for unscheduled pod pod=\"pod3\"\r\n    scheduling_queue.go:595: I0229 10:48:13.721732] Pod moved to an internal scheduling queue pod=\"pod3\" event=\"PodAdd\" queue=\"Active\"\r\n    eventhandlers.go:128: I0229 10:48:13.721917] Add event for unscheduled pod pod=\"pod4\"\r\n    schedule_one.go:84: I0229 10:48:13.722078] About to try and schedule pod pod=\"pod3\"\r\n    schedule_one.go:97: I0229 10:48:13.722236] Attempting to schedule pod pod=\"pod3\"\r\n    scheduling_queue.go:595: I0229 10:48:13.722449] Pod moved to an internal scheduling queue pod=\"pod4\" event=\"PodAdd\" queue=\"Active\"\r\n    framework.go:1475: I0229 10:48:13.722729] Permit: One or more plugins asked to wait and no plugin rejected pod node=\"node3\" pod=\"pod3\"\r\n    framework.go:1500: I0229 10:48:13.723032] Pod waiting on permit pod=\"pod3\"\r\n    schedule_one.go:84: I0229 10:48:13.723270] About to try and schedule pod pod=\"pod4\"\r\n    schedule_one.go:97: I0229 10:48:13.723413] Attempting to schedule pod pod=\"pod4\"\r\n    scheduler_test.go:1044: Unexpected waitingPods in scheduler profile test-scheduler-profile-1, expect: []string{\"pod1\", \"pod2\", \"pod3\", \"pod4\"}, got: []string{\"pod1\", \"pod2\", \"pod3\"}\r\n    --- FAIL: TestFrameworkHandler_IterateOverWaitingPods/pods_with_different_profiles_are_waiting_on_permit_stage (0.01s)\r\n```\r\n\r\n#### Recent failures:\r\n[3/1/2024, 7:14:16 AM pr:pull-kubernetes-unit](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/123614/pull-kubernetes-unit/1763538031354056704)\r\n[2/29/2024, 5:28:43 AM pr:pull-kubernetes-unit](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/123555/pull-kubernetes-unit/1763149087500144640)\r\n[2/28/2024, 12:17:55 PM pr:pull-kubernetes-unit](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/123562/pull-kubernetes-unit/1762889666685571072)\r\n\r\n\r\n/kind failing-test\r\n<!-- If this is a flake, please add: /kind flake -->\r\n/kind flake\r\n\r\n<!-- Please assign a SIG using: /sig SIG-NAME -->\r\n/sig scheduling\n\n### Code Patch:\n@@ -31,6 +31,7 @@ import (\n \tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n \t\"k8s.io/apimachinery/pkg/runtime\"\n \t\"k8s.io/apimachinery/pkg/util/sets\"\n+\t\"k8s.io/apimachinery/pkg/util/wait\"\n \tutilfeature \"k8s.io/apiserver/pkg/util/feature\"\n \t\"k8s.io/client-go/informers\"\n \t\"k8s.io/client-go/kubernetes\"\n@@ -977,14 +978,16 @@ func TestFrameworkHandler_IterateOverWaitingPods(t *testing.T) {\n \t\t\tfakeClient := fake.NewSimpleClientset(objs...)\n \t\t\tinformerFactory := informers.NewSharedInformerFactory(fakeClient, 0)\n \t\t\teventBroadcaster := events.NewBroadcaster(&events.EventSinkImpl{Interface: fakeClient.EventsV1()})\n+\t\t\tdefer eventBroadcaster.Shutdown()\n \t\t\teventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, fakePermit)\n \n \t\t\toutOfTreeRegistry := frameworkruntime.Registry{\n \t\t\t\tfakePermit: newFakePermitPlugin(eventRecorder),\n \t\t\t}\n \n \t\t\t_, ctx := ktesting.NewTestContext(t)\n-\t\t\tctx, cancel := context.WithCancel(ctx)\n+\t\t\t// timeout equals to the permit plugin waiting time.\n+\t\t\tctx, cancel := context.WithTimeout(ctx, 100*time.Second)\n \t\t\tdefer cancel()\n \n \t\t\tscheduler, err := New(\n@@ -1032,17 +1035,23 @@ func TestFrameworkHandler_IterateOverWaitingPods(t *testing.T) {\n \t\t\t// Wait all pods in waitSchedulingPods to be scheduled.\n \t\t\twg.Wait()\n \n-\t\t\t// Ensure that all waitingPods in scheduler can be obtained from any profiles.\n-\t\t\tfor _, fwk := range scheduler.Profiles {\n-\t\t\t\tactualPodNamesInWaitingPods := sets.NewString()\n-\t\t\t\tfwk.IterateOverWaitingPods(func(pod framework.WaitingPod) {\n-\t\t\t\t\tactualPodNamesInWaitingPods.Insert(pod.GetPod().Name)\n-\t\t\t\t})\n-\t\t\t\t// Validate the name of pods in waitingPods matches expectations.\n-\t\t\t\tif actualPodNamesInWaitingPods.Len() != len(tc.expectPodNamesInWaitingPods) ||\n-\t\t\t\t\t!actualPodNamesInWaitingPods.HasAll(tc.expectPodNamesInWaitingPods...) {\n-\t\t\t\t\tt.Fatalf(\"Unexpected waitingPods in scheduler profile %s, expect: %#v, got: %#v\", fwk.ProfileName(), tc.expectPodNamesInWaitingPods, actualPodNamesInWaitingPods.List())\n+\t\t\t// When permit plugin emits the event, pod may not been added to the waitingPods pool yet, so we use pollUntil here.\n+\t\t\tif err := wait.PollUntilContextCancel(ctx, 100*time.Microsecond, true, func(context.Context) (done bool, err error) {\n+\t\t\t\t// Ensure that all waitingPods in scheduler can be obtained from any profiles.\n+\t\t\t\tfor _, fwk := range scheduler.Profiles {\n+\t\t\t\t\tactualPodNamesInWaitingPods := sets.NewString()\n+\t\t\t\t\tfwk.IterateOverWaitingPods(func(pod framework.WaitingPod) {\n+\t\t\t\t\t\tactualPodNamesInWaitingPods.Insert(pod.GetPod().Name)\n+\t\t\t\t\t})\n+\t\t\t\t\t// Validate the name of pods in waitingPods matches expectations.\n+\t\t\t\t\tif actualPodNamesInWaitingPods.Len() != len(tc.expectPodNamesInWaitingPods) ||\n+\t\t\t\t\t\t!actualPodNamesInWaitingPods.HasAll(tc.expectPodNamesInWaitingPods...) {\n+\t\t\t\t\t\treturn false, fmt.Errorf(\"Unexpected waitingPods in scheduler profile %s, expect: %#v, got: %#v\", fwk.ProfileName(), tc.expectPodNamesInWaitingPods, actualPodNamesInWaitingPods.List())\n+\t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\treturn true, nil\n+\t\t\t}); err != nil {\n+\t\t\t\tt.Fatal(\"got unexpected result\")\n \t\t\t}\n \t\t})\n \t}\n"}
{"id": 425, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n#### What type of PR is this?\r\n/kind bug\r\n/kind flake\r\n\r\n#### What this PR does / why we need it:\r\nFixes flakiness in `[sig-network] Proxy version v1 should proxy through a service and a pod [Conformance]` caused by the changes in #129837.\r\n\r\nThe test is kind of badly written. (It's old.) It required that as soon as it became aware that the Service and its Endpoints/EndpointSlice had been updated, that the apiserver proxying codepath must _also_ be aware of them, and must _immediately_ be able to proxy connections to the service, and if not, then the test fails. Apparently it was flaky even before the changes. But #129837 made things much worse because [now the proxying codepath uses an informer and a lister rather than using a client directly](https://github.com/kubernetes/kubernetes/pull/129837#discussion_r2001237237), so it's much more possible for the apiserver proxy code to have not noticed the Service/EndpointSlice yet at the point when e2e.test has noticed it.\r\n\r\nAnyway, fix is just to poll for the apiserver to be ready before we start.\r\n\r\nOh, also, the conformance description of the test was _completely_ wrong. It looks like when the conformance comments were added, someone copied the description of the wrong test? So I fixed that too.\r\n\r\n(Also, update to use the new `e2eendpointslice.WaitForEndpointCount`.)\r\n\r\n#### Which issue(s) this PR is related to:\r\nFixes #133257\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n/sig network\r\n/priority important-soon\r\n/triage accepted\r\n/milestone v1.34\n\n### Code Patch:\n@@ -1867,11 +1867,10 @@\n     MUST be returned for each endpoint.\n   release: v1.21\n   file: test/e2e/network/proxy.go\n-- testname: Proxy, logs service endpoint\n+- testname: Proxy through apiserver to a Service\n   codename: '[sig-network] Proxy version v1 should proxy through a service and a pod\n     [Conformance]'\n-  description: Select any node in the cluster to invoke  /logs endpoint  using the\n-    /nodes/proxy subresource from the kubelet port. This endpoint MUST be reachable.\n+  description: The apiserver will proxy a connection to a Service.\n   release: v1.9\n   file: test/e2e/network/proxy.go\n - testname: Service endpoint latency, thresholds\n\n@@ -39,6 +39,7 @@ import (\n \t\"k8s.io/client-go/transport\"\n \t\"k8s.io/kubernetes/test/e2e/framework\"\n \te2edeployment \"k8s.io/kubernetes/test/e2e/framework/deployment\"\n+\te2eendpointslice \"k8s.io/kubernetes/test/e2e/framework/endpointslice\"\n \te2enode \"k8s.io/kubernetes/test/e2e/framework/node\"\n \te2epod \"k8s.io/kubernetes/test/e2e/framework/pod\"\n \te2eservice \"k8s.io/kubernetes/test/e2e/framework/service\"\n@@ -91,12 +92,10 @@ var _ = common.SIGDescribe(\"Proxy\", func() {\n \t\t*/\n \t\tginkgo.It(\"should proxy logs on node using proxy subresource \", func(ctx context.Context) { nodeProxyTest(ctx, f, prefix+\"/nodes/\", \"/proxy/logs/\") })\n \n-\t\t// using the porter image to serve content, access the content\n-\t\t// (of multiple pods?) from multiple (endpoints/services?)\n \t\t/*\n \t\t\tRelease: v1.9\n-\t\t\tTestname: Proxy, logs service endpoint\n-\t\t\tDescription: Select any node in the cluster to invoke  /logs endpoint  using the /nodes/proxy subresource from the kubelet port. This endpoint MUST be reachable.\n+\t\t\tTestname: Proxy through apiserver to a Service\n+\t\t\tDescription: The apiserver will proxy a connection to a Service.\n \t\t*/\n \t\tframework.ConformanceIt(\"should proxy through a service and a pod\", func(ctx context.Context) {\n \t\t\tstart := time.Now()\n@@ -133,7 +132,7 @@ var _ = common.SIGDescribe(\"Proxy\", func() {\n \t\t\t}, metav1.CreateOptions{})\n \t\t\tframework.ExpectNoError(err)\n \n-\t\t\t// Make a deployment with a single pod. The 'porter' image is\n+\t\t\t// Make a deployment with a single pod. 'agnhost porter' is\n \t\t\t// a simple server which serves the values of the\n \t\t\t// environmental variables below.\n \t\t\tginkgo.By(\"starting an echo server on multiple ports\")\n@@ -223,7 +222,7 @@ var _ = common.SIGDescribe(\"Proxy\", func() {\n \t\t\tframework.ExpectNoError(err)\n \t\t\tpods := podList.Items\n \n-\t\t\terr = framework.WaitForServiceEndpointsNum(ctx, f.ClientSet, f.Namespace.Name, service.Name, 1, time.Second, e2eservice.ServiceEndpointsTimeout)\n+\t\t\terr = e2eendpointslice.WaitForEndpointCount(ctx, f.ClientSet, f.Namespace.Name, service.Name, 1)\n \t\t\tframework.ExpectNoError(err)\n \n \t\t\t// table constructors\n@@ -261,6 +260,16 @@ var _ = common.SIGDescribe(\"Proxy\", func() {\n \t\t\t\t// podPrefix + \":dest2\": \"bar\",\n \t\t\t}\n \n+\t\t\t// Poll until the apiserver is aware of the service and its endpoints,\n+\t\t\t// before starting the main part of the test.\n+\t\t\tpollTestPath := subresourceServiceProxyURL(\"\", \"portname1\") + \"/\"\n+\t\t\tpollTestBody := \"foo\"\n+\t\t\terr = wait.PollUntilContextTimeout(ctx, time.Second, e2eservice.ServiceEndpointsTimeout, true, func(ctx context.Context) (bool, error) {\n+\t\t\t\tbody, _, _, _ := doProxy(ctx, f, pollTestPath, 0)\n+\t\t\t\treturn string(body) == pollTestBody, nil\n+\t\t\t})\n+\t\t\tframework.ExpectNoError(err, \"Unable to reach service through proxy\")\n+\n \t\t\twg := sync.WaitGroup{}\n \t\t\terrs := []string{}\n \t\t\terrLock := sync.Mutex{}\n"}
{"id": 427, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n### Failure cluster [fb94bb199721f2524f81](https://go.k8s.io/triage#fb94bb199721f2524f81)\r\n\r\n##### Error text:\r\n```\r\n[FAILED] deploying csi-hostpath driver: create ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"psp-csi-hostpath-role-ephemeral-9424\" already exists\r\nIn [It] at: test/e2e/storage/drivers/csi.go:280 @ 05/03/23 07:02:07.131\r\n\r\n```\r\n#### Recent failures:\r\n[5/10/2023, 11:15:07 PM ci-kubernetes-e2e-gce-cos-k8sstable1-default](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-cos-k8sstable1-default/1656407525098000384)\r\n[5/3/2023, 8:41:51 AM ci-kubernetes-e2e-gce-cos-k8sstable1-default](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-cos-k8sstable1-default/1653651043134738432)\r\n\r\n\r\n/kind failing-test\r\n/kind flake\r\n/sig storage\n\n### Code Patch:\n@@ -235,7 +235,6 @@ func (h *hostpathCSIDriver) PrepareTest(ctx context.Context, f *framework.Framew\n \t// Create secondary namespace which will be used for creating driver\n \tdriverNamespace := utils.CreateDriverNamespace(ctx, f)\n \tdriverns := driverNamespace.Name\n-\ttestns := f.Namespace.Name\n \n \tginkgo.By(fmt.Sprintf(\"deploying %s driver\", h.driverInfo.Name))\n \tcancelLogging := utils.StartPodLogs(ctx, f, driverNamespace)\n@@ -328,7 +327,6 @@ func (h *hostpathCSIDriver) PrepareTest(ctx context.Context, f *framework.Framew\n \tcleanupFunc := generateDriverCleanupFunc(\n \t\tf,\n \t\th.driverInfo.Name,\n-\t\ttestns,\n \t\tdriverns,\n \t\tcancelLogging)\n \tginkgo.DeferCleanup(cleanupFunc)\n@@ -600,7 +598,6 @@ func (m *mockCSIDriver) PrepareTest(ctx context.Context, f *framework.Framework)\n \t// Create secondary namespace which will be used for creating driver\n \tm.driverNamespace = utils.CreateDriverNamespace(ctx, f)\n \tdriverns := m.driverNamespace.Name\n-\ttestns := f.Namespace.Name\n \n \tif m.embedded {\n \t\tginkgo.By(\"deploying csi mock proxy\")\n@@ -762,7 +759,6 @@ func (m *mockCSIDriver) PrepareTest(ctx context.Context, f *framework.Framework)\n \tdriverCleanupFunc := generateDriverCleanupFunc(\n \t\tf,\n \t\t\"mock\",\n-\t\ttestns,\n \t\tdriverns,\n \t\tcancelLogging)\n \n@@ -942,7 +938,6 @@ func (g *gcePDCSIDriver) GetSnapshotClass(ctx context.Context, config *storagefr\n }\n \n func (g *gcePDCSIDriver) PrepareTest(ctx context.Context, f *framework.Framework) *storageframework.PerTestConfig {\n-\ttestns := f.Namespace.Name\n \tcfg := &storageframework.PerTestConfig{\n \t\tDriver:    g,\n \t\tPrefix:    \"gcepd\",\n@@ -1000,7 +995,6 @@ func (g *gcePDCSIDriver) PrepareTest(ctx context.Context, f *framework.Framework\n \tcleanupFunc := generateDriverCleanupFunc(\n \t\tf,\n \t\t\"gce-pd\",\n-\t\ttestns,\n \t\tdriverns,\n \t\tcancelLogging)\n \tginkgo.DeferCleanup(cleanupFunc)\n@@ -1073,17 +1067,12 @@ func tryFunc(f func()) error {\n \n func generateDriverCleanupFunc(\n \tf *framework.Framework,\n-\tdriverName, testns, driverns string,\n+\tdriverName, driverns string,\n \tcancelLogging func()) func(ctx context.Context) {\n \n \t// Cleanup CSI driver and namespaces. This function needs to be idempotent and can be\n \t// concurrently called from defer (or AfterEach) and AfterSuite action hooks.\n \tcleanupFunc := func(ctx context.Context) {\n-\t\tginkgo.By(fmt.Sprintf(\"deleting the test namespace: %s\", testns))\n-\t\t// Delete the primary namespace but it's okay to fail here because this namespace will\n-\t\t// also be deleted by framework.Aftereach hook\n-\t\t_ = tryFunc(func() { f.DeleteNamespace(ctx, testns) })\n-\n \t\tginkgo.By(fmt.Sprintf(\"uninstalling csi %s driver\", driverName))\n \t\t_ = tryFunc(cancelLogging)\n \n"}
{"id": 113, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n**Description**\r\nSteve pointed this out when he was running into flaky tests. Looking at videos of the failures, it looks like the previous dialog had not finished closing before proceeding, which threw off the tests. Fix has worked for me a few times.\r\n\r\n**Review Instructions**\r\nNot really reviewable; tests should pass.\r\n\r\n**Issue**\r\nNone\r\n\r\n**Security**\r\nIf there are any concerns that require extra attention from the security team, highlight them here.\r\n\r\nPlease make sure that you've checked the following before submitting your pull request. Thanks!\r\n\r\n- [x] Check that your code compiles by running `npm run build`\r\n- [x] Ensure that the PR targets the correct branch. Check the milestone or fix version of the ticket.\r\n- [x] If this is the first time you're submitting a PR or even if you just need a refresher, consider reviewing our [style guide](https://github.com/dockstore/dockstore/wiki/Dockstore-Frontend-Opinionated-Style-Guide#pr-checklist)\r\n- [x] Do not bypass Angular sanitization (bypassSecurityTrustHtml, etc.), or justify why you need to do so\r\n- [x] If displaying markdown, use the `markdown-wrapper` component, which does extra sanitization\r\n- [x] Do not use cookies, although this may change in the future\r\n- [x] Run `npm audit` and ensure you are not introducing new vulnerabilities\r\n- [x] Do due diligence on new 3rd party libraries, checking for CVEs\r\n- [x] Don't allow user-uploaded images to be served from the Dockstore domain\r\n- [x] If this PR is for a user-facing feature, create and link a documentation ticket for this feature (usually in the same milestone as the linked issue). Style points if you create a documentation PR directly and link that instead.\r\n- [x] Check whether this PR disables tests. If it legitimately needs to disable a test, create a new ticket to re-enable it in a specific milestone. \r\n\n\n### Code Patch:\n@@ -97,6 +97,7 @@ describe('GitHub App Tools', () => {\n       cy.get('[data-cy=install-dockstore-app]');\n       cy.contains('Tool storage type').click();\n       cy.contains('Close').click();\n+      cy.contains('Close').should('not.exist'); // Wait for dialog to close before proceeding\n \n       cy.intercept('GET', '/api/lambdaEvents/**').as('lambdaEvents1');\n       // GitHub App Logs\n\n@@ -57,10 +57,19 @@ export class VersionsDataSource implements DataSource<WorkflowVersion> {\n         pageIndex,\n         sortCol,\n         sortDirection,\n+        'metrics',\n         'response'\n       );\n     } else {\n-      workflowVersions = this.workflowsService.getWorkflowVersions(workflowId, pageSize, pageIndex, sortCol, sortDirection, 'response');\n+      workflowVersions = this.workflowsService.getWorkflowVersions(\n+        workflowId,\n+        pageSize,\n+        pageIndex,\n+        sortCol,\n+        sortDirection,\n+        'metrics',\n+        'response'\n+      );\n     }\n \n     workflowVersions.pipe(finalize(() => this.loadingSubject$.next(false))).subscribe((versions) => {\n"}
{"id": 118, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe root cause was that `andIUnlinkTheDefendant` was navigating to the defendant details screen as part of its implementation. But it was only called after `andIClickThroughToTheDefendantDetailsScreen`, which _also_ navigates to the defendant details screen. For some reason most of the time playwright wasn't complaining about the double-click, but just occasionally, in Safari, it was. Keeping `andIUnlinkTheDefendant` to just filling out the unlink form fixes the flakiness.\n\n### Code Patch:\n@@ -55,9 +55,6 @@ export class CaseDetailSteps {\n   }\n \n   async andIUnlinkTheDefendant() {\n-    await this.page.getByRole('link', { name: DEFENDANT_NAME })\n-              .click();\n-\n     await this.page.locator('summary:has-text(\"Remove link to court data\")') // Expand the summary section\n               .click();\n \n"}
{"id": 366, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Please make sure the target branch is right. In most case, the target branch should be `develop`. -->\r\n\r\n### Which Issue(s) This PR Fixes\r\n\r\n<!-- Please ensure that the related issue has already been created, and [link this pull request to that issue using keywords](<https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword>) to ensure automatic closure. -->\r\n\r\nFixes #7614 \r\n\r\n### Brief Description\r\n\r\n<!-- Write a brief description for your pull request to help the maintainer understand the reasons behind your changes. -->\r\n\r\n### How Did You Test This Change?\r\n\r\n```\r\nbazel test --config=remote  //store:src/test/java/org/apache/rocketmq/store/RocksDBMessageStoreTest --runs_per_test=1024\r\n```\r\nVerify there is no failures anymore.\r\n\r\n\n\n### Code Patch:\n@@ -69,9 +69,8 @@ java_library(\n GenTestRules(\n     name = \"GeneratedTestRules\",\n     exclude_tests = [\n-        # This test is extremely slow and flaky, exclude it.\n+        # These tests are extremely slow and flaky, exclude them before they are properly fixed.\n         \"src/test/java/org/apache/rocketmq/store/ha/autoswitch/AutoSwitchHATest\",\n-        \"src/test/java/org/apache/rocketmq/store/RocksDBMessageStoreTest\",\n     ],\n     medium_tests = [\n         \"src/test/java/org/apache/rocketmq/store/DefaultMessageStoreTest\",\n@@ -80,6 +79,7 @@ GenTestRules(\n         \"src/test/java/org/apache/rocketmq/store/MappedFileQueueTest\",\n         \"src/test/java/org/apache/rocketmq/store/queue/BatchConsumeMessageTest\",\n         \"src/test/java/org/apache/rocketmq/store/dledger/MixCommitlogTest\",\n+        \"src/test/java/org/apache/rocketmq/store/RocksDBMessageStoreTest\",\n     ],\n     test_files = glob([\"src/test/java/**/*Test.java\"]),\n     deps = [\n\n@@ -60,15 +60,18 @@\n import org.apache.rocketmq.store.queue.CqUnit;\n import org.apache.rocketmq.store.stats.BrokerStatsManager;\n import org.assertj.core.util.Strings;\n+import org.awaitility.Awaitility;\n import org.junit.After;\n-import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.mockito.junit.MockitoJUnitRunner;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.fail;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertSame;\n import static org.junit.Assert.assertTrue;\n \n @RunWith(MockitoJUnitRunner.class)\n@@ -77,7 +80,7 @@ public class RocksDBMessageStoreTest {\n     private final String messageTopic = \"FooBar\";\n     private final String storeType = StoreType.DEFAULT_ROCKSDB.getStoreType();\n     private int queueTotal = 100;\n-    private AtomicInteger queueId = new AtomicInteger(0);\n+    private final AtomicInteger queueId = new AtomicInteger(0);\n     private SocketAddress bornHost;\n     private SocketAddress storeHost;\n     private byte[] messageBody;\n@@ -171,27 +174,27 @@ public void testWriteAndRead() {\n         if (notExecuted()) {\n             return;\n         }\n-        long ipv4HostMsgs = 10;\n-        long ipv6HostMsgs = 10;\n-        long totalMsgs = ipv4HostMsgs + ipv6HostMsgs;\n+        long ipv4HostMessages = 10;\n+        long ipv6HostMessages = 10;\n+        long totalMessages = ipv4HostMessages + ipv6HostMessages;\n         queueTotal = 1;\n         messageBody = storeMessage.getBytes();\n-        for (long i = 0; i < ipv4HostMsgs; i++) {\n+        for (long i = 0; i < ipv4HostMessages; i++) {\n             messageStore.putMessage(buildMessage());\n         }\n \n-        for (long i = 0; i < ipv6HostMsgs; i++) {\n+        for (long i = 0; i < ipv6HostMessages; i++) {\n             messageStore.putMessage(buildIPv6HostMessage());\n         }\n \n         StoreTestUtil.waitCommitLogReput((RocksDBMessageStore) messageStore);\n \n-        for (long i = 0; i < totalMsgs; i++) {\n+        for (long i = 0; i < totalMessages; i++) {\n             GetMessageResult result = messageStore.getMessage(\"GROUP_A\", \"FooBar\", 0, i, 1024 * 1024, null);\n             assertThat(result).isNotNull();\n             result.release();\n         }\n-        verifyThatMasterIsFunctional(totalMsgs, messageStore);\n+        verifyThatMasterIsFunctional(totalMessages, messageStore);\n     }\n \n     @Test\n@@ -549,15 +552,13 @@ private MessageExtBrokerInner buildIPv6HostMessage(byte[] messageBody, String to\n         try {\n             msg.setBornHost(new InetSocketAddress(InetAddress.getByName(\"1050:0000:0000:0000:0005:0600:300c:326b\"), 0));\n         } catch (UnknownHostException e) {\n-            e.printStackTrace();\n-            assertThat(Boolean.FALSE).isTrue();\n+            fail(\"build IPv6 host message error\", e);\n         }\n \n         try {\n             msg.setStoreHost(new InetSocketAddress(InetAddress.getByName(\"::1\"), 0));\n         } catch (UnknownHostException e) {\n-            e.printStackTrace();\n-            assertThat(Boolean.FALSE).isTrue();\n+            fail(\"build IPv6 host message error\", e);\n         }\n         return msg;\n     }\n@@ -582,27 +583,27 @@ public MessageExtBatch buildMessageBatch(MessageBatch msgBatch) {\n     }\n \n     @Test\n-    public void testGroupCommit() throws Exception {\n+    public void testGroupCommit() {\n         if (notExecuted()) {\n             return;\n         }\n-        long totalMsgs = 10;\n+        long totalMessages = 10;\n         queueTotal = 1;\n         messageBody = storeMessage.getBytes();\n-        for (long i = 0; i < totalMsgs; i++) {\n+        for (long i = 0; i < totalMessages; i++) {\n             messageStore.putMessage(buildMessage());\n         }\n \n-        for (long i = 0; i < totalMsgs; i++) {\n+        for (long i = 0; i < totalMessages; i++) {\n             GetMessageResult result = messageStore.getMessage(\"GROUP_A\", \"TOPIC_A\", 0, i, 1024 * 1024, null);\n             assertThat(result).isNotNull();\n             result.release();\n         }\n-        verifyThatMasterIsFunctional(totalMsgs, messageStore);\n+        verifyThatMasterIsFunctional(totalMessages, messageStore);\n     }\n \n     @Test\n-    public void testMaxOffset() throws InterruptedException {\n+    public void testMaxOffset() {\n         if (notExecuted()) {\n             return;\n         }\n@@ -618,11 +619,11 @@ public void testMaxOffset() throws InterruptedException {\n             messageStore.putMessage(msg);\n         }\n \n-        while (messageStore.dispatchBehindBytes() != 0) {\n-            TimeUnit.MILLISECONDS.sleep(1);\n-        }\n-\n-        assertThat(messageStore.getMaxOffsetInQueue(messageTopic, queueId)).isEqualTo(firstBatchMessages);\n+        Awaitility.await()\n+                .with()\n+                .atMost(3, TimeUnit.SECONDS)\n+                .pollInterval(1, TimeUnit.MILLISECONDS)\n+                .until(() -> messageStore.getMaxOffsetInQueue(messageTopic, queueId) == firstBatchMessages);\n \n         // Disable the dispatcher\n         messageStore.getDispatcherList().clear();\n@@ -644,14 +645,14 @@ private MessageExtBrokerInner buildIPv6HostMessage() {\n         return buildIPv6HostMessage(messageBody, \"FooBar\");\n     }\n \n-    private void verifyThatMasterIsFunctional(long totalMsgs, MessageStore master) {\n-        for (long i = 0; i < totalMsgs; i++) {\n+    private void verifyThatMasterIsFunctional(long totalMessages, MessageStore master) {\n+        for (long i = 0; i < totalMessages; i++) {\n             master.putMessage(buildMessage());\n         }\n \n         StoreTestUtil.waitCommitLogReput((RocksDBMessageStore) messageStore);\n \n-        for (long i = 0; i < totalMsgs; i++) {\n+        for (long i = 0; i < totalMessages; i++) {\n             GetMessageResult result = master.getMessage(\"GROUP_A\", \"FooBar\", 0, i, 1024 * 1024, null);\n             assertThat(result).isNotNull();\n             result.release();\n@@ -660,7 +661,7 @@ private void verifyThatMasterIsFunctional(long totalMsgs, MessageStore master) {\n     }\n \n     @Test\n-    public void testPullSize() throws Exception {\n+    public void testPullSize() {\n         if (notExecuted()) {\n             return;\n         }\n@@ -673,9 +674,11 @@ public void testPullSize() throws Exception {\n             messageStore.putMessage(messageExtBrokerInner);\n         }\n         // wait for consume queue build\n-        // the sleep time should be great than consume queue flush interval\n-        //Thread.sleep(100);\n-        StoreTestUtil.waitCommitLogReput((RocksDBMessageStore) messageStore);\n+        Awaitility.await().atMost(10, TimeUnit.SECONDS)\n+                .with()\n+                .pollInterval(10, TimeUnit.MILLISECONDS)\n+                .until(() -> messageStore.getMaxOffsetInQueue(topic, 0) >= 32);\n+\n         String group = \"simple\";\n         GetMessageResult getMessageResult32 = messageStore.getMessage(group, topic, 0, 0, 32, null);\n         assertThat(getMessageResult32.getMessageBufferList().size()).isEqualTo(32);\n@@ -705,21 +708,25 @@ public void testRecover() throws Exception {\n             messageStore.putMessage(messageExtBrokerInner);\n         }\n \n-        // Thread.sleep(100);//wait for build consumer queue\n-        StoreTestUtil.waitCommitLogReput((RocksDBMessageStore) messageStore);\n+        // wait for build consumer queue\n+        Awaitility.await()\n+                .with()\n+                .pollInterval(100, TimeUnit.MILLISECONDS)\n+                .atMost(10, TimeUnit.SECONDS)\n+                .until(() -> messageStore.getMaxOffsetInQueue(topic, 0) >= 100);\n \n         long maxPhyOffset = messageStore.getMaxPhyOffset();\n         long maxCqOffset = messageStore.getMaxOffsetInQueue(topic, 0);\n \n         //1.just reboot\n         messageStore.shutdown();\n-        String storeRootDir = ((RocksDBMessageStore) messageStore).getMessageStoreConfig().getStorePathRootDir();\n+        String storeRootDir = messageStore.getMessageStoreConfig().getStorePathRootDir();\n         messageStore = buildMessageStore(storeRootDir, topic);\n         boolean load = messageStore.load();\n         assertTrue(load);\n         messageStore.start();\n-        assertTrue(maxPhyOffset == messageStore.getMaxPhyOffset());\n-        assertTrue(maxCqOffset == messageStore.getMaxOffsetInQueue(topic, 0));\n+        assertEquals(maxPhyOffset, messageStore.getMaxPhyOffset());\n+        assertEquals(maxCqOffset, messageStore.getMaxOffsetInQueue(topic, 0));\n \n         //2.damage commit-log and reboot normal\n         for (int i = 0; i < 100; i++) {\n@@ -728,39 +735,49 @@ public void testRecover() throws Exception {\n             messageExtBrokerInner.setQueueId(0);\n             messageStore.putMessage(messageExtBrokerInner);\n         }\n-        //Thread.sleep(100);\n-        StoreTestUtil.waitCommitLogReput((RocksDBMessageStore) messageStore);\n+\n+        Awaitility.await()\n+                .with()\n+                .pollInterval(100, TimeUnit.MILLISECONDS)\n+                .atMost(10, TimeUnit.SECONDS)\n+                .until(() -> messageStore.getMaxOffsetInQueue(topic, 0) >= 200);\n+\n         long secondLastPhyOffset = messageStore.getMaxPhyOffset();\n         long secondLastCqOffset = messageStore.getMaxOffsetInQueue(topic, 0);\n \n+        // Append a message to corrupt\n         MessageExtBrokerInner messageExtBrokerInner = buildMessage();\n         messageExtBrokerInner.setTopic(topic);\n         messageExtBrokerInner.setQueueId(0);\n         messageStore.putMessage(messageExtBrokerInner);\n \n-\n         messageStore.shutdown();\n \n-        //damage last message\n+        // Corrupt the last message\n         damageCommitLog((RocksDBMessageStore) messageStore, secondLastPhyOffset);\n \n         //reboot\n         messageStore = buildMessageStore(storeRootDir, topic);\n         load = messageStore.load();\n         assertTrue(load);\n         messageStore.start();\n-        assertTrue(secondLastPhyOffset == messageStore.getMaxPhyOffset());\n-        assertTrue(secondLastCqOffset == messageStore.getMaxOffsetInQueue(topic, 0));\n+        assertEquals(secondLastPhyOffset, messageStore.getMaxPhyOffset());\n+        assertEquals(secondLastCqOffset, messageStore.getMaxOffsetInQueue(topic, 0));\n \n-        //3.damage commitlog and reboot abnormal\n+        //3.Corrupt commit-log and reboot abnormal\n         for (int i = 0; i < 100; i++) {\n             messageExtBrokerInner = buildMessage();\n             messageExtBrokerInner.setTopic(topic);\n             messageExtBrokerInner.setQueueId(0);\n             messageStore.putMessage(messageExtBrokerInner);\n         }\n-        //Thread.sleep(100);\n-        StoreTestUtil.waitCommitLogReput((RocksDBMessageStore) messageStore);\n+\n+        Awaitility.await()\n+                .with()\n+                .pollInterval(100, TimeUnit.MILLISECONDS)\n+                .atMost(10, TimeUnit.SECONDS)\n+                .until(() -> messageStore.getMaxOffsetInQueue(topic, 0) >= 300);\n+\n         secondLastPhyOffset = messageStore.getMaxPhyOffset();\n         secondLastCqOffset = messageStore.getMaxOffsetInQueue(topic, 0);\n \n@@ -770,20 +787,20 @@ public void testRecover() throws Exception {\n         messageStore.putMessage(messageExtBrokerInner);\n         messageStore.shutdown();\n \n-        //damage last message\n+        //Corrupt the last message\n         damageCommitLog((RocksDBMessageStore) messageStore, secondLastPhyOffset);\n         //add abort file\n-        String fileName = StorePathConfigHelper.getAbortFile(((RocksDBMessageStore) messageStore).getMessageStoreConfig().getStorePathRootDir());\n+        String fileName = StorePathConfigHelper.getAbortFile(messageStore.getMessageStoreConfig().getStorePathRootDir());\n         File file = new File(fileName);\n         UtilAll.ensureDirOK(file.getParent());\n-        file.createNewFile();\n+        assertTrue(file.createNewFile());\n \n         messageStore = buildMessageStore(storeRootDir, topic);\n         load = messageStore.load();\n         assertTrue(load);\n         messageStore.start();\n-        assertTrue(secondLastPhyOffset == messageStore.getMaxPhyOffset());\n-        assertTrue(secondLastCqOffset == messageStore.getMaxOffsetInQueue(topic, 0));\n+        assertEquals(secondLastPhyOffset, messageStore.getMaxPhyOffset());\n+        assertEquals(secondLastCqOffset, messageStore.getMaxOffsetInQueue(topic, 0));\n \n         //message write again\n         for (int i = 0; i < 100; i++) {\n@@ -860,7 +877,8 @@ public void testPutMsgBatchExceedsMaxLength() {\n         MessageExtBatch msgExtBatch = buildMessageBatch(msgBatch);\n \n         try {\n-            PutMessageResult result = this.messageStore.putMessages(msgExtBatch);\n+            this.messageStore.putMessages(msgExtBatch);\n+            fail(\"Should have raised an exception\");\n         } catch (Exception e) {\n             assertThat(e.getMessage()).contains(\"message body size exceeded\");\n         }\n@@ -871,7 +889,7 @@ public void testPutMsgWhenReplicasNotEnough() {\n         if (notExecuted()) {\n             return;\n         }\n-        MessageStoreConfig messageStoreConfig = ((RocksDBMessageStore) this.messageStore).getMessageStoreConfig();\n+        MessageStoreConfig messageStoreConfig = this.messageStore.getMessageStoreConfig();\n         messageStoreConfig.setBrokerRole(BrokerRole.SYNC_MASTER);\n         messageStoreConfig.setTotalReplicas(2);\n         messageStoreConfig.setInSyncReplicas(2);\n@@ -890,7 +908,7 @@ public void testPutMsgWhenAdaptiveDegradation() {\n         if (notExecuted()) {\n             return;\n         }\n-        MessageStoreConfig messageStoreConfig = ((RocksDBMessageStore) this.messageStore).getMessageStoreConfig();\n+        MessageStoreConfig messageStoreConfig = this.messageStore.getMessageStoreConfig();\n         messageStoreConfig.setBrokerRole(BrokerRole.SYNC_MASTER);\n         messageStoreConfig.setTotalReplicas(2);\n         messageStoreConfig.setInSyncReplicas(2);\n@@ -930,44 +948,44 @@ public void testGetBulkCommitLogData() {\n     }\n \n     @Test\n-    public void testPutLongMessage() throws Exception {\n+    public void testPutLongMessage() {\n         if (notExecuted()) {\n             return;\n         }\n         MessageExtBrokerInner messageExtBrokerInner = buildMessage();\n-        CommitLog commitLog = ((RocksDBMessageStore) messageStore).getCommitLog();\n-        MessageStoreConfig messageStoreConfig = ((RocksDBMessageStore) messageStore).getMessageStoreConfig();\n+        CommitLog commitLog = messageStore.getCommitLog();\n+        MessageStoreConfig messageStoreConfig = messageStore.getMessageStoreConfig();\n         MessageExtEncoder.PutMessageThreadLocal putMessageThreadLocal = commitLog.getPutMessageThreadLocal().get();\n \n         //body size, topic size, properties size exactly equal to max size\n         messageExtBrokerInner.setBody(new byte[messageStoreConfig.getMaxMessageSize()]);\n         messageExtBrokerInner.setTopic(new String(new byte[127]));\n         messageExtBrokerInner.setPropertiesString(new String(new byte[Short.MAX_VALUE]));\n         PutMessageResult encodeResult1 = putMessageThreadLocal.getEncoder().encode(messageExtBrokerInner);\n-        assertTrue(encodeResult1 == null);\n+        assertNull(encodeResult1);\n \n         //body size exactly more than max message body size\n         messageExtBrokerInner.setBody(new byte[messageStoreConfig.getMaxMessageSize() + 1]);\n         PutMessageResult encodeResult2 = putMessageThreadLocal.getEncoder().encode(messageExtBrokerInner);\n-        assertTrue(encodeResult2.getPutMessageStatus() == PutMessageStatus.MESSAGE_ILLEGAL);\n+        assertSame(encodeResult2.getPutMessageStatus(), PutMessageStatus.MESSAGE_ILLEGAL);\n \n         //body size exactly equal to max message size\n         messageExtBrokerInner.setBody(new byte[messageStoreConfig.getMaxMessageSize() + 64 * 1024]);\n         PutMessageResult encodeResult3 = putMessageThreadLocal.getEncoder().encode(messageExtBrokerInner);\n-        assertTrue(encodeResult3.getPutMessageStatus() == PutMessageStatus.MESSAGE_ILLEGAL);\n+        assertSame(encodeResult3.getPutMessageStatus(), PutMessageStatus.MESSAGE_ILLEGAL);\n \n         //message properties length more than properties maxSize\n         messageExtBrokerInner.setBody(new byte[messageStoreConfig.getMaxMessageSize()]);\n         messageExtBrokerInner.setPropertiesString(new String(new byte[Short.MAX_VALUE + 1]));\n         PutMessageResult encodeResult4 = putMessageThreadLocal.getEncoder().encode(messageExtBrokerInner);\n-        assertTrue(encodeResult4.getPutMessageStatus() == PutMessageStatus.PROPERTIES_SIZE_EXCEEDED);\n+        assertSame(encodeResult4.getPutMessageStatus(), PutMessageStatus.PROPERTIES_SIZE_EXCEEDED);\n \n         //message length more than buffer length capacity\n         messageExtBrokerInner.setBody(new byte[messageStoreConfig.getMaxMessageSize()]);\n         messageExtBrokerInner.setTopic(new String(new byte[Short.MAX_VALUE]));\n         messageExtBrokerInner.setPropertiesString(new String(new byte[Short.MAX_VALUE]));\n         PutMessageResult encodeResult5 = putMessageThreadLocal.getEncoder().encode(messageExtBrokerInner);\n-        assertTrue(encodeResult5.getPutMessageStatus() == PutMessageStatus.MESSAGE_ILLEGAL);\n+        assertSame(encodeResult5.getPutMessageStatus(), PutMessageStatus.MESSAGE_ILLEGAL);\n     }\n \n     @Test\n@@ -976,21 +994,21 @@ public void testDynamicMaxMessageSize() {\n             return;\n         }\n         MessageExtBrokerInner messageExtBrokerInner = buildMessage();\n-        MessageStoreConfig messageStoreConfig = ((RocksDBMessageStore) messageStore).getMessageStoreConfig();\n+        MessageStoreConfig messageStoreConfig = messageStore.getMessageStoreConfig();\n         int originMaxMessageSize = messageStoreConfig.getMaxMessageSize();\n \n         messageExtBrokerInner.setBody(new byte[originMaxMessageSize + 10]);\n         PutMessageResult putMessageResult = messageStore.putMessage(messageExtBrokerInner);\n-        assertTrue(putMessageResult.getPutMessageStatus() == PutMessageStatus.MESSAGE_ILLEGAL);\n+        assertSame(putMessageResult.getPutMessageStatus(), PutMessageStatus.MESSAGE_ILLEGAL);\n \n         int newMaxMessageSize = originMaxMessageSize + 10;\n         messageStoreConfig.setMaxMessageSize(newMaxMessageSize);\n         putMessageResult = messageStore.putMessage(messageExtBrokerInner);\n-        assertTrue(putMessageResult.getPutMessageStatus() == PutMessageStatus.PUT_OK);\n+        assertSame(putMessageResult.getPutMessageStatus(), PutMessageStatus.PUT_OK);\n \n         messageStoreConfig.setMaxMessageSize(10);\n         putMessageResult = messageStore.putMessage(messageExtBrokerInner);\n-        assertTrue(putMessageResult.getPutMessageStatus() == PutMessageStatus.MESSAGE_ILLEGAL);\n+        assertSame(putMessageResult.getPutMessageStatus(), PutMessageStatus.MESSAGE_ILLEGAL);\n \n         messageStoreConfig.setMaxMessageSize(originMaxMessageSize);\n     }\n@@ -1013,11 +1031,11 @@ public void testDeleteTopics() {\n             }\n             consumeQueueTable.put(topicName, cqTable);\n         }\n-        Assert.assertEquals(consumeQueueTable.size(), 10);\n+        assertEquals(consumeQueueTable.size(), 10);\n         HashSet<String> resultSet = Sets.newHashSet(\"topic-3\", \"topic-5\");\n         messageStore.deleteTopics(Sets.difference(consumeQueueTable.keySet(), resultSet));\n-        Assert.assertEquals(consumeQueueTable.size(), 2);\n-        Assert.assertEquals(resultSet, consumeQueueTable.keySet());\n+        assertEquals(consumeQueueTable.size(), 2);\n+        assertEquals(resultSet, consumeQueueTable.keySet());\n     }\n \n     @Test\n@@ -1038,14 +1056,14 @@ public void testCleanUnusedTopic() {\n             }\n             consumeQueueTable.put(topicName, cqTable);\n         }\n-        Assert.assertEquals(consumeQueueTable.size(), 10);\n+        assertEquals(consumeQueueTable.size(), 10);\n         HashSet<String> resultSet = Sets.newHashSet(\"topic-3\", \"topic-5\");\n         messageStore.cleanUnusedTopic(resultSet);\n-        Assert.assertEquals(consumeQueueTable.size(), 2);\n-        Assert.assertEquals(resultSet, consumeQueueTable.keySet());\n+        assertEquals(consumeQueueTable.size(), 2);\n+        assertEquals(resultSet, consumeQueueTable.keySet());\n     }\n \n-    private class MyMessageArrivingListener implements MessageArrivingListener {\n+    private static class MyMessageArrivingListener implements MessageArrivingListener {\n         @Override\n         public void arriving(String topic, int queueId, long logicOffset, long tagsCode, long msgStoreTime,\n             byte[] filterBitMap, Map<String, String> properties) {\n\n@@ -426,8 +426,8 @@ public Boolean call() {\n         assertEquals(first.getCommitReadTimeMs(), second.getCommitReadTimeMs());\n         second.start(true);\n \n-        // Wait until all messages have wrote back to commitLog and consumeQueue.\n-        await().atMost(5000, TimeUnit.MILLISECONDS).until(new Callable<Boolean>() {\n+        // Wait until all messages have been written back to commitLog and consumeQueue.\n+        await().atMost(30000, TimeUnit.MILLISECONDS).until(new Callable<Boolean>() {\n             @Override\n             public Boolean call() {\n                 ConsumeQueue cq = (ConsumeQueue) messageStore.getConsumeQueue(topic, 0);\n"}
{"id": 378, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nfix flaky test directory call\n\nsometimes multiple runners share the same temp directory. When this happens there's a chance that the check for number of files output is (2) instead of (1). Fix this by creating a unique directory for this test.\n\n\n### Code Patch:\n@@ -48,7 +48,9 @@ xla_cc_test(\n         \"@com_google_absl//absl/log\",\n         \"@com_google_absl//absl/status:status_matchers\",\n         \"@com_google_absl//absl/status:statusor\",\n+        \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:string_view\",\n+        \"@com_google_absl//absl/time\",\n         \"@com_google_absl//absl/types:span\",\n         \"@com_google_googletest//:gtest_main\",\n         \"@llvm-project//mlir:IR\",\n\n@@ -26,7 +26,10 @@ limitations under the License.\n #include \"absl/log/log.h\"\n #include \"absl/status/status_matchers.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"absl/time/clock.h\"\n+#include \"absl/time/time.h\"\n #include \"absl/types/span.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n@@ -119,7 +122,11 @@ TEST(DumpTest, GetDumpSubdirPathEmptyPath) {\n }\n \n TEST(DumpTest, DumpCompileInputs) {\n-  const std::string temp_dir = tsl::testing::TmpDir();\n+  const std::string temp_test_dir = tsl::testing::TmpDir();\n+  const std::string temp_test_subdir =\n+      tsl::io::JoinPath(temp_test_dir, \"compile_dump_test\",\n+                        absl::StrCat(absl::ToUnixMillis(absl::Now())));\n+  TF_ASSERT_OK(tsl::Env::Default()->RecursivelyCreateDir(temp_test_subdir));\n   xla::CompileOptions compile_options;\n   mlir::MLIRContext context;\n   mlir::OpBuilder builder(&context);\n@@ -129,13 +136,13 @@ TEST(DumpTest, DumpCompileInputs) {\n \n   // Dump compile inputs.\n   compile_options.executable_build_options.mutable_debug_options()\n-      ->set_xla_dump_to(temp_dir);\n+      ->set_xla_dump_to(temp_test_subdir);\n \n-  TF_ASSERT_OK(pjrt::DumpCompileInputs(temp_dir, compile_options, *module,\n-                                       *topology.get()));\n+  TF_ASSERT_OK(pjrt::DumpCompileInputs(temp_test_subdir, compile_options,\n+                                       *module, *topology.get()));\n   std::vector<std::string> files;\n   TF_ASSERT_OK(tsl::Env::Default()->GetMatchingPaths(\n-      tsl::io::JoinPath(temp_dir, \"*\"), &files));\n+      tsl::io::JoinPath(temp_test_subdir, \"*\"), &files));\n \n   ASSERT_EQ(files.size(), 1);\n   std::string dump_subdir = files[0];\n"}
{"id": 384, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Description\r\n\r\n<img width=\"2048\" height=\"675\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a035d725-79a5-4da5-962e-79cf4bfee447\" />\r\nhttps://github.com/apache/airflow/actions/runs/16537752338/job/46774859176?pr=53597\r\n\r\n\r\nSee failure example: https://github.com/apache/airflow/actions/runs/16537752338/job/46774859176?pr=53597\r\n\r\nThis happened because `DagRun.get_task_instances()` returns TaskInstances in a non-deterministic order (usually depending on the database), and the test incorrectly assumed that the last TI in the returned list would be the one with the lexicographically largest `task_id`.\r\n\r\n### Solution\r\n\r\nTo ensure consistent behavior, we now explicitly sort TaskInstances by `task_id` in natural (human-friendly) order using `natsort`, so that:\r\n\r\n- `\"task2\"` comes before `\"task10\"`\r\n- `tis[-1]` now reliably refers to the correct \"last\" task when sorted by `task_id`\r\n\r\nThis ensures `DagRun.get_last_ti()` behaves deterministically and the associated test is no longer flaky.\r\n\r\n### Changes\r\n\r\n- Use `natsorted` in `get_last_ti()` to sort TIs by `task_id` in a stable natural order\r\n- Document rationale in the code\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n\n### Code Patch:\n@@ -30,6 +30,7 @@\n     overload,\n )\n \n+from natsort import natsorted\n from sqlalchemy import (\n     JSON,\n     Column,\n@@ -1373,7 +1374,10 @@ def get_last_ti(self, dag: DAG, session: Session = NEW_SESSION) -> TI | None:\n         if dag.partial:\n             tis = [ti for ti in tis if not ti.state == State.NONE]\n         # filter out removed tasks\n-        tis = [ti for ti in tis if ti.state != TaskInstanceState.REMOVED]\n+        tis = natsorted(\n+            (ti for ti in tis if ti.state != TaskInstanceState.REMOVED),\n+            key=lambda ti: ti.task_id,\n+        )\n         if not tis:\n             return None\n         ti = tis[-1]  # get last TaskInstance of DagRun\n\n@@ -69,6 +69,7 @@ version = \"3.1.0\"\n dependencies = [\n     \"apache-airflow-task-sdk<1.2.0,>=1.0.0\",\n     \"apache-airflow-core==3.1.0\",\n+    \"natsort>=8.4.0\",\n ]\n \n packages = []\n"}
{"id": 32, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nPlatforms: linux, rocm, slow\n\n\n\n  This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_aoti_fx_add&suite=AOTFxirTestCase&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/49795173420).\n\n  Over the past 6 hours, it has been determined flaky in 10 workflow(s) with 20 failures and 10 successes.\n\n  **Debugging instructions (after clicking on the recent samples link):**\n  DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\n  To find relevant log snippets:\n  1. Click on the workflow logs linked above\n  2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n  3. Grep for `test_aoti_fx_add`\n  4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n  \n  \n  Test file path: `inductor/test_fxir_backend.py`\n\n  For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.\n\ncc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben\n\n### Code Patch:\n@@ -693,7 +693,7 @@ def check(self, model, inp, dynamic_shapes=None, strict=False):\n                 model, inp, dynamic_shapes=dynamic_shapes, strict=strict\n             )\n             gm = torch._inductor.aot_compile(\n-                ep.module(), inp, options={\"fx_wrapper\": True}\n+                ep.module(), inp, options={\"fx_wrapper\": True, \"compile_threads\": 1}\n             )\n             self.assertTrue(torch.allclose(model(*inp), gm(*inp)))\n \n"}
{"id": 328, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n`contentTracing captured events include V8 samples from the main process` has been flaking a fair amount on Appveyor as of late.\r\n\r\nSee [here](https://ci.appveyor.com/project/electron-bot/electron-x64-testing/builds/47184539/job/6ycj2f45a7flb0dc/tests) for example.\n\n### Code Patch:\n@@ -5,8 +5,8 @@ import * as path from 'node:path';\n import { setTimeout } from 'node:timers/promises';\n import { ifdescribe } from './lib/spec-helpers';\n \n-// FIXME: The tests are skipped on arm/arm64 and ia32.\n-ifdescribe(!(['arm', 'arm64', 'ia32'].includes(process.arch)))('contentTracing', () => {\n+// FIXME: The tests are skipped on linux arm/arm64\n+ifdescribe(!(['arm', 'arm64'].includes(process.arch)) || (process.platform !== 'linux'))('contentTracing', () => {\n   const record = async (options: TraceConfig | TraceCategoriesAndOptions, outputFilePath: string | undefined, recordTimeInMilliseconds = 1e1) => {\n     await app.whenReady();\n \n@@ -120,9 +120,7 @@ ifdescribe(!(['arm', 'arm64', 'ia32'].includes(process.arch)))('contentTracing',\n \n   describe('captured events', () => {\n     it('include V8 samples from the main process', async function () {\n-      // This test is flaky on macOS CI.\n-      this.retries(3);\n-\n+      this.timeout(60000);\n       await contentTracing.startRecording({\n         categoryFilter: 'disabled-by-default-v8.cpu_profiler',\n         traceOptions: 'record-until-full'\n@@ -131,7 +129,7 @@ ifdescribe(!(['arm', 'arm64', 'ia32'].includes(process.arch)))('contentTracing',\n         const start = Date.now();\n         let n = 0;\n         const f = () => {};\n-        while (Date.now() - start < 200 || n < 500) {\n+        while (Date.now() - start < 200 && n < 500) {\n           await setTimeout(0);\n           f();\n           n++;\n"}
{"id": 424, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[Jira: KAFKA-18537](https://issues.apache.org/jira/browse/KAFKA-18537)\nAdd a wait for cleaner thread shutdown in `testCleanerThreadShutdown` to\neliminate flakiness. After calling `cache.close()`, the test now uses\n`TestUtils.waitForCondition` to poll until the background\n‚Äúremote-log-index-cleaner‚Äù thread has fully exited before asserting that\nno cleaner threads remain. This ensures the asynchronous shutdown always\ncompletes before the final assertions.\n\nReviewers: TengYao Chi <kitingiao@gmail.com>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n\n### Code Patch:\n@@ -376,8 +376,10 @@ public void testCleanerThreadShutdown() throws IOException, InterruptedException\n         // close the cache properly\n         cache.close();\n         // verify that the thread is closed properly\n-        threads = getRunningCleanerThread();\n-        assertTrue(threads.isEmpty(), \"Found unexpected \" + threads.size() + \" threads=\" + threads.stream().map(Thread::getName).collect(Collectors.joining(\", \")));\n+        TestUtils.waitForCondition(\n+                () -> getRunningCleanerThread().isEmpty(),\n+                () -> \"Failed while waiting for cleaner threads to shutdown. Remaining threads: \" +\n+                        getRunningCleanerThread().stream().map(Thread::getName).collect(Collectors.joining(\", \")));\n         // if the thread is correctly being shutdown it will not be running\n         assertFalse(cache.cleanerScheduler().isStarted(), \"Unexpected thread state=running. Check error logs.\");\n     }\n"}
{"id": 131, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Summary of the Pull Request\r\n\r\n**What is this about:**\r\nThe pdf unit tests use a mock of the IStream interface which fails if it can't copy the contents of the stream in a single pass.\r\n\r\n**What is include in the PR:** \r\nFixes for the mock to run correctly even when it receives a smaller buffer side.\r\nAlso run Pdf Thumbnail tests as part of the automated tests.\r\nTook the chance to also add PDF modules to the bug report template.\r\n\r\n**How does someone test / validate:** \r\nTests run successfully.\r\n\r\n## Quality Checklist\r\n\r\n- [x] **Linked issue:** #13247\r\n- [ ] **Communication:** I've discussed this with core contributors in the issue. \r\n- [ ] **Tests:** Added/updated and all pass\r\n- [ ] **Installer:** Added/updated and all pass\r\n- [ ] **Localization:** All end user facing strings can be localized\r\n- [ ] **Docs:** Added/ updated\r\n- [x] **Binaries:** Any new files are added to WXS / YML\r\n   - [x] No new binaries\r\n   - [ ] [YML for signing](https://github.com/microsoft/PowerToys/blob/master/.pipelines/pipeline.user.windows.yml#L68) for new binaries\r\n   - [ ] [WXS for installer](https://github.com/microsoft/PowerToys/blob/master/installer/PowerToysSetup/Product.wxs) for new binaries\r\n\n\n### Code Patch:\n@@ -39,6 +39,8 @@ body:\n       - Image Resizer\n       - Keyboard Manager\n       - MD Preview\n+      - PDF Preview\n+      - PDF Thumbnail\n       - PowerRename\n       - PowerToys Run\n       - Shortcut Guide\n\n@@ -121,6 +121,7 @@ steps:\n     testSelector: 'testAssemblies'\n     testAssemblyVer2: |\n       **\\UnitTests-SvgThumbnailProvider.dll\n+      **\\UnitTests-PdfThumbnailProvider.dll\n       **\\Microsoft.PowerToys.Settings.UI.UnitTests.dll\n       **\\UnitTests-MarkdownPreviewHandler.dll\n       **\\UnitTests-PdfPreviewHandler.dll\n\n@@ -66,16 +66,18 @@ public void PdfPreviewHandlerControlShouldAddValidInfoBarIfPdfPreviewThrows()\n         private static IStream GetMockStream(byte[] sourceArray)\n         {\n             var streamMock = new Mock<IStream>();\n-            var firstCall = true;\n+            int bytesRead = 0;\n+\n             streamMock\n                 .Setup(x => x.Read(It.IsAny<byte[]>(), It.IsAny<int>(), It.IsAny<IntPtr>()))\n                 .Callback<byte[], int, IntPtr>((buffer, countToRead, bytesReadPtr) =>\n                 {\n-                    if (firstCall)\n+                    int actualCountToRead = Math.Min(sourceArray.Length - bytesRead, countToRead);\n+                    if (actualCountToRead > 0)\n                     {\n-                        Array.Copy(sourceArray, 0, buffer, 0, sourceArray.Length);\n-                        Marshal.WriteInt32(bytesReadPtr, sourceArray.Length);\n-                        firstCall = false;\n+                        Array.Copy(sourceArray, bytesRead, buffer, 0, actualCountToRead);\n+                        Marshal.WriteInt32(bytesReadPtr, actualCountToRead);\n+                        bytesRead += actualCountToRead;\n                     }\n                     else\n                     {\n\n@@ -70,16 +70,18 @@ public void GetThumbnailToBigPDF()\n         private static IStream GetMockStream(byte[] sourceArray)\n         {\n             var streamMock = new Mock<IStream>();\n-            var firstCall = true;\n+            int bytesRead = 0;\n+\n             streamMock\n                 .Setup(x => x.Read(It.IsAny<byte[]>(), It.IsAny<int>(), It.IsAny<IntPtr>()))\n                 .Callback<byte[], int, IntPtr>((buffer, countToRead, bytesReadPtr) =>\n                 {\n-                    if (firstCall)\n+                    int actualCountToRead = Math.Min(sourceArray.Length - bytesRead, countToRead);\n+                    if (actualCountToRead > 0)\n                     {\n-                        Array.Copy(sourceArray, 0, buffer, 0, sourceArray.Length);\n-                        Marshal.WriteInt32(bytesReadPtr, sourceArray.Length);\n-                        firstCall = false;\n+                        Array.Copy(sourceArray, bytesRead, buffer, 0, actualCountToRead);\n+                        Marshal.WriteInt32(bytesReadPtr, actualCountToRead);\n+                        bytesRead += actualCountToRead;\n                     }\n                     else\n                     {\n"}
{"id": 45, "ground_truth": "Logic", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Motivation\r\n\r\nRunning ClusterMigrationTest could fail with OOME.\r\nWhen investigating the problem, it showed up that ClusterMigrationTest's TestBroker inner class doesn't clean up resources properly. The admin clients weren't closed.\r\n\r\n### Modifications\r\n\r\n* Call `super.internalCleanup()` in the `cleanup` method.\r\n* Close admin clients in the test cleanup method\r\n\r\n### Documentation\r\n\r\n<!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->\r\n\r\n- [ ] `doc` <!-- Your PR contains doc changes. -->\r\n- [ ] `doc-required` <!-- Your PR changes impact docs and you will update later -->\r\n- [x] `doc-not-needed` <!-- Your PR changes do not impact docs -->\r\n- [ ] `doc-complete` <!-- Docs have been already added -->\n\n### Code Patch:\n@@ -25,13 +25,13 @@\n import static org.testng.Assert.assertNotNull;\n import static org.testng.Assert.assertNull;\n import static org.testng.Assert.assertTrue;\n-\n+import com.google.common.collect.Sets;\n import java.lang.reflect.Method;\n import java.net.URL;\n import java.util.List;\n import java.util.Optional;\n import java.util.concurrent.TimeUnit;\n-\n+import lombok.Cleanup;\n import org.apache.pulsar.broker.BrokerTestUtil;\n import org.apache.pulsar.broker.PulsarService;\n import org.apache.pulsar.broker.auth.MockedPulsarServiceBaseTest;\n@@ -55,10 +55,6 @@\n import org.testng.annotations.DataProvider;\n import org.testng.annotations.Test;\n \n-import com.google.common.collect.Sets;\n-\n-import lombok.Cleanup;\n-\n @Test(groups = \"broker\")\n public class ClusterMigrationTest {\n \n@@ -232,9 +228,13 @@ public void setup() throws Exception {\n     protected void cleanup() throws Exception {\n         log.info(\"--- Shutting down ---\");\n         broker1.cleanup();\n+        admin1.close();\n         broker2.cleanup();\n+        admin2.close();\n         broker3.cleanup();\n+        admin3.close();\n         broker4.cleanup();\n+        admin4.close();\n     }\n \n     @BeforeMethod(alwaysRun = true)\n@@ -459,7 +459,7 @@ public void testClusterMigrationWithReplicationBacklog(boolean persistent, Subsc\n         assertEquals(topic1.getReplicators().size(), 1);\n \n        // stop service in the replication cluster to build replication backlog\n-        broker3.cleanup();\n+        broker3.stop();\n         retryStrategically((test) -> broker3.getPulsarService() == null, 10, 1000);\n         assertNull(pulsar3.getBrokerService());\n \n@@ -529,7 +529,7 @@ public void testClusterMigrationWithReplicationBacklog(boolean persistent, Subsc\n     /**\n      * This test validates that blue cluster first creates list of subscriptions into green cluster so, green cluster\n      * will not lose the data if producer migrates.\n-     * \n+     *\n      * @throws Exception\n      */\n     @Test\n@@ -928,7 +928,7 @@ public void testNamespaceMigrationWithReplicationBacklog(boolean persistent, Sub\n         assertEquals(blueTopicNs2.getReplicators().size(), 1);\n \n         // stop service in the replication cluster to build replication backlog\n-        broker3.cleanup();\n+        broker3.stop();\n         retryStrategically((test) -> broker3.getPulsarService() == null, 10, 1000);\n         assertNull(pulsar3.getBrokerService());\n \n@@ -1063,9 +1063,13 @@ public String getClusterName() {\n             return configClusterName;\n         }\n \n+        public void stop() throws Exception {\n+            stopBroker();\n+        }\n+\n         @Override\n         protected void cleanup() throws Exception {\n-            stopBroker();\n+            internalCleanup();\n         }\n \n         public void restart() throws Exception {\n"}
{"id": 297, "ground_truth": "Concurrency", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n**Build Scans:**\n- [elasticsearch-periodic-platform-support #9255 / windows-2025_checkpart1_platform-support-windows](https://gradle-enterprise.elastic.co/s/wrqmjkr2q5tv2)\n- [elasticsearch-periodic-platform-support #9227 / amazonlinux-2023_platform-support-aws](https://gradle-enterprise.elastic.co/s/lnzqxbgk5pjyi)\n- [elasticsearch-periodic-platform-support #9227 / oraclelinux-9_platform-support-unix](https://gradle-enterprise.elastic.co/s/bc573kel6n2ae)\n\n**Reproduction Line:**\n```\ngradlew \":server:internalClusterTest\" --tests \"org.elasticsearch.search.aggregations.bucket.FiltersCancellationIT.testFiltersSubAggsCancellation\" -Dtests.seed=E591B05A93049B8B -Dtests.locale=fr-TG -Dtests.timezone=US/Eastern -Druntime.java=24\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nN/A\n\n**Failure History:**\n[See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.search.aggregations.bucket.FiltersCancellationIT),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!('testFiltersSubAggsCancellation'),title:'Test',type:optionsListControl)))))\n\n**Failure Message:**\n```\njava.lang.AssertionError: null\n```\n\n**Issue Reasons:**\n- [main] 3 failures in test testFiltersSubAggsCancellation (1.1% fail rate in 263 executions)\n- [main] 2 failures in pipeline elasticsearch-periodic-platform-support (40.0% fail rate in 5 executions)\n\n**Note:**\nThis issue was created using new test triage automation. Please report issues or feedback to es-delivery.\n\n### Code Patch:\n@@ -28,6 +28,7 @@\n import org.elasticsearch.test.ESIntegTestCase;\n import org.elasticsearch.xcontent.XContentBuilder;\n import org.elasticsearch.xcontent.json.JsonXContent;\n+import org.junit.Before;\n \n import java.util.Collection;\n import java.util.List;\n@@ -39,9 +40,22 @@\n import static org.elasticsearch.search.aggregations.AggregationBuilders.filters;\n import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;\n import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.greaterThan;\n import static org.hamcrest.Matchers.not;\n \n+/**\n+ * Ensures the filters aggregation checks task cancellation, by ensuring it doesn't process all the docs.\n+ * <p>\n+ *   The CancellableBulkScorer we use to break the execution is called per search thread in the query.\n+ *   It currently breaks the \"for each doc\" into blocks of 4096 docs (x2 every iteration), and checks for cancellation between blocks.\n+ *   This test creates N docs and releases N - X permits, to ensure the search request gets cancelled before grabbing all the permits.\n+ * </p>\n+ * <p>\n+ *   Also, if the search thread pool size is too high, it can lead to them trying to process too many documents anyway (pool size * 4096),\n+ *   eventually blocking the threads (And failing the test). So it's explicitly set to a small number to avoid this.\n+ * </p>\n+ */\n @ESIntegTestCase.SuiteScopeTestCase\n public class FiltersCancellationIT extends ESIntegTestCase {\n \n@@ -55,11 +69,12 @@ public class FiltersCancellationIT extends ESIntegTestCase {\n \n     @Override\n     protected Collection<Class<? extends Plugin>> nodePlugins() {\n-        return CollectionUtils.appendToCopy(super.nodePlugins(), pausableFieldPluginClass());\n+        return CollectionUtils.appendToCopy(super.nodePlugins(), PauseScriptPlugin.class);\n     }\n \n-    protected Class<? extends Plugin> pausableFieldPluginClass() {\n-        return PauseScriptPlugin.class;\n+    @Override\n+    public Settings nodeSettings(int nodeOrdinal, Settings otherSettings) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal, otherSettings)).put(\"thread_pool.search.size\", 4).build();\n     }\n \n     @Override\n@@ -99,6 +114,11 @@ public void setupSuiteScopeCluster() throws Exception {\n         client().admin().indices().prepareForceMerge(INDEX).setMaxNumSegments(1).get();\n     }\n \n+    @Before\n+    public void reset() {\n+        SCRIPT_SEMAPHORE.drainPermits();\n+    }\n+\n     public void testFiltersCountCancellation() throws Exception {\n         ensureProperCancellation(\n             client().prepareSearch(INDEX)\n@@ -129,14 +149,14 @@ public void testFiltersSubAggsCancellation() throws Exception {\n \n     private void ensureProperCancellation(SearchRequestBuilder searchRequestBuilder) throws Exception {\n         var searchRequestFuture = searchRequestBuilder.setTimeout(TimeValue.timeValueSeconds(1)).execute();\n-        assertFalse(searchRequestFuture.isCancelled());\n-        assertFalse(searchRequestFuture.isDone());\n+        assertThat(searchRequestFuture.isCancelled(), equalTo(false));\n+        assertThat(searchRequestFuture.isDone(), equalTo(false));\n \n         // Check that there are search tasks running\n         assertThat(getSearchTasks(), not(empty()));\n \n         // Wait for the script field to get blocked\n-        assertBusy(() -> { assertThat(SCRIPT_SEMAPHORE.getQueueLength(), greaterThan(0)); });\n+        assertBusy(() -> assertThat(SCRIPT_SEMAPHORE.getQueueLength(), greaterThan(0)));\n \n         // Cancel the tasks\n         // Warning: Adding a waitForCompletion(true)/execute() here sometimes causes tasks to not get canceled and threads to get stuck\n@@ -146,8 +166,8 @@ private void ensureProperCancellation(SearchRequestBuilder searchRequestBuilder)\n \n         // Ensure the search request finished and that there are no more search tasks\n         assertBusy(() -> {\n-            assertTrue(searchRequestFuture.isDone());\n-            assertThat(getSearchTasks(), empty());\n+            assertThat(\"Search request didn't finish\", searchRequestFuture.isDone(), equalTo(true));\n+            assertThat(\"There are dangling search tasks\", getSearchTasks(), empty());\n         });\n     }\n \n"}
{"id": 11, "ground_truth": "Concurrency", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nSince python does not have a library for max heap and usual tricks like inverting values etc.. does not work for all cases. \n\nWe have our own implementation of max heap. \n\n\n### Code Patch:\n@@ -29,7 +29,7 @@\n from tempfile import NamedTemporaryFile\n from threading import Thread\n import warnings\n-from heapq import heappush, heappop, heappushpop\n+import heapq\n \n from pyspark.serializers import NoOpSerializer, CartesianDeserializer, \\\n     BatchedSerializer, CloudPickleSerializer, PairDeserializer, pack_long\n@@ -41,9 +41,9 @@\n \n from py4j.java_collections import ListConverter, MapConverter\n \n-\n __all__ = [\"RDD\"]\n \n+\n def _extract_concise_traceback():\n     \"\"\"\n     This function returns the traceback info for a callsite, returns a dict\n@@ -91,6 +91,73 @@ def __exit__(self, type, value, tb):\n         if _spark_stack_depth == 0:\n             self._context._jsc.setCallSite(None)\n \n+class MaxHeapQ(object):\n+    \"\"\"\n+    An implementation of MaxHeap.\n+    >>> import pyspark.rdd\n+    >>> heap = pyspark.rdd.MaxHeapQ(5)\n+    >>> [heap.insert(i) for i in range(10)]\n+    [None, None, None, None, None, None, None, None, None, None]\n+    >>> sorted(heap.getElements())\n+    [0, 1, 2, 3, 4]\n+    >>> heap = pyspark.rdd.MaxHeapQ(5)\n+    >>> [heap.insert(i) for i in range(9, -1, -1)]\n+    [None, None, None, None, None, None, None, None, None, None]\n+    >>> sorted(heap.getElements())\n+    [0, 1, 2, 3, 4]\n+    >>> heap = pyspark.rdd.MaxHeapQ(1)\n+    >>> [heap.insert(i) for i in range(9, -1, -1)]\n+    [None, None, None, None, None, None, None, None, None, None]\n+    >>> heap.getElements()\n+    [0]\n+    \"\"\"\n+\n+    def __init__(self, maxsize):\n+        # we start from q[1], this makes calculating children as trivial as 2 * k\n+        self.q = [0]\n+        self.maxsize = maxsize\n+\n+    def _swim(self, k):\n+        while (k > 1) and (self.q[k/2] < self.q[k]):\n+            self._swap(k, k/2)\n+            k = k/2\n+\n+    def _swap(self, i, j):\n+        t = self.q[i]\n+        self.q[i] = self.q[j]\n+        self.q[j] = t\n+\n+    def _sink(self, k):\n+        N = self.size()\n+        while 2 * k <= N:\n+            j = 2 * k\n+            # Here we test if both children are greater than parent\n+            # if not swap with larger one.\n+            if j < N and self.q[j] < self.q[j + 1]:\n+                j = j + 1\n+            if(self.q[k] > self.q[j]):\n+                break\n+            self._swap(k, j)\n+            k = j\n+\n+    def size(self):\n+        return len(self.q) - 1\n+\n+    def insert(self, value):\n+        if (self.size()) < self.maxsize:\n+            self.q.append(value)\n+            self._swim(self.size())\n+        else:\n+            self._replaceRoot(value)\n+\n+    def getElements(self):\n+        return self.q[1:]\n+\n+    def _replaceRoot(self, value):\n+        if(self.q[1] > value):\n+            self.q[1] = value\n+            self._sink(1)\n+\n class RDD(object):\n     \"\"\"\n     A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n@@ -696,23 +763,53 @@ def top(self, num):\n         Note: It returns the list sorted in descending order.\n         >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n         [12]\n-        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().top(2)\n+        >>> sc.parallelize([2, 3, 4, 5, 6], 2).cache().top(2)\n         [6, 5]\n         \"\"\"\n         def topIterator(iterator):\n             q = []\n             for k in iterator:\n                 if len(q) < num:\n-                    heappush(q, k)\n+                    heapq.heappush(q, k)\n                 else:\n-                    heappushpop(q, k)\n+                    heapq.heappushpop(q, k)\n             yield q\n \n         def merge(a, b):\n             return next(topIterator(a + b))\n \n         return sorted(self.mapPartitions(topIterator).reduce(merge), reverse=True)\n \n+    def takeOrdered(self, num, key=None):\n+        \"\"\"\n+        Get the N elements from a RDD ordered in ascending order or as specified\n+        by the optional key function. \n+\n+        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n+        [1, 2, 3, 4, 5, 6]\n+        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n+        [10, 9, 7, 6, 5, 4]\n+        \"\"\"\n+\n+        def topNKeyedElems(iterator, key_=None):\n+            q = MaxHeapQ(num)\n+            for k in iterator:\n+                if key_ != None:\n+                    k = (key_(k), k)\n+                q.insert(k)\n+            yield q.getElements()\n+\n+        def unKey(x, key_=None):\n+            if key_ != None:\n+                x = [i[1] for i in x]\n+            return x\n+        \n+        def merge(a, b):\n+            return next(topNKeyedElems(a + b))\n+        result = self.mapPartitions(lambda i: topNKeyedElems(i, key)).reduce(merge)\n+        return sorted(unKey(result, key), key=key)\n+\n+\n     def take(self, num):\n         \"\"\"\n         Take the first num elements of the RDD.\n"}
{"id": 198, "ground_truth": "Network", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nIdeas and help of Alexander Motin ;)\r\n\r\nTicket: #28198\r\n(cherry picked from commit ef3eb51d9b80715c45401827316ab1ef7fc6933b)\n\n### Code Patch:\n@@ -210,11 +210,13 @@ static VNET_DEFINE(int, carp_senderr_adj) = CARP_MAXSKEW;\n static VNET_DEFINE(int, carp_ifdown_adj) = CARP_MAXSKEW;\n #define\tV_carp_ifdown_adj\tVNET(carp_ifdown_adj)\n \n+static int carp_allow_sysctl(SYSCTL_HANDLER_ARGS);\n static int carp_demote_adj_sysctl(SYSCTL_HANDLER_ARGS);\n \n SYSCTL_NODE(_net_inet, IPPROTO_CARP,\tcarp,\tCTLFLAG_RW, 0,\t\"CARP\");\n-SYSCTL_INT(_net_inet_carp, OID_AUTO, allow, CTLFLAG_VNET | CTLFLAG_RW,\n-    &VNET_NAME(carp_allow), 0, \"Accept incoming CARP packets\");\n+SYSCTL_PROC(_net_inet_carp, OID_AUTO, allow,\n+    CTLFLAG_VNET | CTLTYPE_INT | CTLFLAG_RW, 0, 0, carp_allow_sysctl, \"I\",\n+    \"Accept incoming CARP packets\");\n SYSCTL_INT(_net_inet_carp, OID_AUTO, preempt, CTLFLAG_VNET | CTLFLAG_RW,\n     &VNET_NAME(carp_preempt), 0, \"High-priority backup preemption mode\");\n SYSCTL_INT(_net_inet_carp, OID_AUTO, log, CTLFLAG_VNET | CTLFLAG_RW,\n@@ -1197,7 +1199,8 @@ carp_setrun(struct carp_softc *sc, sa_family_t af)\n \n \tif ((sc->sc_carpdev->if_flags & IFF_UP) == 0 ||\n \t    sc->sc_carpdev->if_link_state != LINK_STATE_UP ||\n-\t    (sc->sc_naddrs == 0 && sc->sc_naddrs6 == 0))\n+\t    (sc->sc_naddrs == 0 && sc->sc_naddrs6 == 0) ||\n+\t    !V_carp_allow)\n \t\treturn;\n \n \tswitch (sc->sc_state) {\n@@ -1969,7 +1972,8 @@ carp_sc_state(struct carp_softc *sc)\n \tCARP_LOCK_ASSERT(sc);\n \n \tif (sc->sc_carpdev->if_link_state != LINK_STATE_UP ||\n-\t    !(sc->sc_carpdev->if_flags & IFF_UP)) {\n+\t    !(sc->sc_carpdev->if_flags & IFF_UP) ||\n+\t    !V_carp_allow) {\n \t\tcallout_stop(&sc->sc_ad_tmo);\n #ifdef INET\n \t\tcallout_stop(&sc->sc_md_tmo);\n@@ -1999,6 +2003,33 @@ carp_demote_adj(int adj, char *reason)\n \ttaskqueue_enqueue(taskqueue_swi, &carp_sendall_task);\n }\n \n+static int\n+carp_allow_sysctl(SYSCTL_HANDLER_ARGS)\n+{\n+\tint new, error;\n+\tstruct carp_softc *sc;\n+\n+\tnew = V_carp_allow;\n+\terror = sysctl_handle_int(oidp, &new, 0, req);\n+\tif (error || !req->newptr)\n+\t\treturn (error);\n+\n+\tif (V_carp_allow != new) {\n+\t\tV_carp_allow = new;\n+\n+\t\tmtx_lock(&carp_mtx);\n+\t\tLIST_FOREACH(sc, &carp_list, sc_next) {\n+\t\t\tCARP_LOCK(sc);\n+\t\t\tif (curvnet == sc->sc_carpdev->if_vnet)\n+\t\t\t\tcarp_sc_state(sc);\n+\t\t\tCARP_UNLOCK(sc);\n+\t\t}\n+\t\tmtx_unlock(&carp_mtx);\n+\t}\n+\n+\treturn (0);\n+}\n+\n static int\n carp_demote_adj_sysctl(SYSCTL_HANDLER_ARGS)\n {\n"}
{"id": 304, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nGhostscript doesn't obey SOURCE_DATE_EPOCH\r\n(https://bugs.ghostscript.com/show_bug.cgi?id=696765) so locally\r\ntest_savefig_to_stringio is a bit flaky -- sometimes the string\r\ncomparisons fail due to differences in timestamps between two\r\nghostscript invocations (I'm actually a bit curious why this never fails\r\non CI).\r\nFix that by stripping out the timestamp.\r\n\r\n## PR Summary\r\n\r\n## PR Checklist\r\n\r\n- [ ] Has Pytest style unit tests\r\n- [ ] Code is [Flake 8](http://flake8.pycqa.org/en/latest/) compliant\r\n- [ ] New features are documented, with examples if plot related\r\n- [ ] Documentation is sphinx and numpydoc compliant\r\n- [ ] Added an entry to doc/users/next_whats_new/ if major new feature (follow instructions in README.rst there)\r\n- [ ] Documented in doc/api/api_changes.rst if API changed in a backward-incompatible way\r\n\r\n<!--\r\nThank you so much for your PR!  To help us review your contribution, please\r\nconsider the following points:\r\n\r\n- A development guide is available at https://matplotlib.org/devdocs/devel/index.html.\r\n\r\n- Help with git and github is available at\r\n  https://matplotlib.org/devel/gitwash/development_workflow.html.\r\n\r\n- Do not create the PR out of master, but out of a separate branch.\r\n\r\n- The PR title should summarize the changes, for example \"Raise ValueError on\r\n  non-numeric input to set_xlim\".  Avoid non-descriptive titles such as\r\n  \"Addresses issue #8576\".\r\n\r\n- The summary should provide at least 1-2 sentences describing the pull request\r\n  in detail (Why is this change required?  What problem does it solve?) and\r\n  link to any relevant issues.\r\n\r\n- If you are contributing fixes to docstrings, please pay attention to\r\n  http://matplotlib.org/devel/documenting_mpl.html#formatting.  In particular,\r\n  note the difference between using single backquotes, double backquotes, and\r\n  asterisks in the markup.\r\n\r\nWe understand that PRs can sometimes be overwhelming, especially as the\r\nreviews start coming in.  Please let us know if the reviews are unclear or\r\nthe recommended next step seems overly demanding, if you would like help in\r\naddressing a reviewer's comments, or if you have been waiting too long to hear\r\nback on your PR.\r\n-->\r\n\n\n### Code Patch:\n@@ -1,5 +1,6 @@\n import io\n from pathlib import Path\n+import re\n import tempfile\n \n import pytest\n@@ -69,6 +70,13 @@ def test_savefig_to_stringio(format, use_log, rcParams, orientation,\n         s_val = s_buf.getvalue().encode('ascii')\n         b_val = b_buf.getvalue()\n \n+        if rcParams.get(\"ps.usedistiller\") or rcParams.get(\"text.usetex\"):\n+            # Strip out CreationDate betcase ghostscript doesn't obey\n+            # SOURCE_DATE_EPOCH.  Note that in usetex mode, we *always* call\n+            # gs_distill, even if ps.usedistiller is unset.\n+            s_val = re.sub(b\"(?<=\\n%%CreationDate: ).*\", b\"\", s_val)\n+            b_val = re.sub(b\"(?<=\\n%%CreationDate: ).*\", b\"\", b_val)\n+\n         assert s_val == b_val.replace(b'\\r\\n', b'\\n')\n \n \n"}
{"id": 99, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nTry fixing a UI test that has been flaky for a while recently.\n\n### Code Patch:\n@@ -1,8 +1,6 @@\n // Copyright (c) Jupyter Development Team.\n // Distributed under the terms of the Modified BSD License.\n \n-import path from 'path';\n-\n import { expect } from '@playwright/test';\n \n import { test } from './fixtures';\n@@ -28,26 +26,29 @@ test.describe('Mobile', () => {\n     tmpPath,\n     browserName,\n   }) => {\n-    const notebook = 'empty.ipynb';\n-    await page.contents.uploadFile(\n-      path.resolve(__dirname, `./notebooks/${notebook}`),\n-      `${tmpPath}/${notebook}`\n-    );\n-    await page.goto(`notebooks/${tmpPath}/${notebook}`);\n+    await page.goto(`tree/${tmpPath}`);\n+\n+    // Create a new notebook\n+    const [notebook] = await Promise.all([\n+      page.waitForEvent('popup'),\n+      page.click('text=\"New\"'),\n+      page.click('text=\"Python 3 (ipykernel)\"'),\n+    ]);\n \n     // wait for the kernel status animations to be finished\n-    await waitForKernelReady(page);\n+    await waitForKernelReady(notebook);\n \n     // force switching back to command mode to avoid capturing the cursor in the screenshot\n-    await page.evaluate(async () => {\n+    await notebook.evaluate(async () => {\n       await window.jupyterapp.commands.execute('notebook:enter-command-mode');\n     });\n \n     // TODO: remove\n     if (browserName === 'firefox') {\n-      await hideAddCellButton(page);\n+      await hideAddCellButton(notebook);\n     }\n \n-    expect(await page.screenshot()).toMatchSnapshot('notebook.png');\n+    expect(await notebook.screenshot()).toMatchSnapshot('notebook.png');\n+    await notebook.close();\n   });\n });\n\n\n\n\n"}
{"id": 37, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nPlatforms: asan, linux, mac, macos, rocm, slow, win, windows, dynamo\n\n\n\n\n\nThis test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_get_mode_stack&suite=TestTorchFunctionMode&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/28989307006).\n\nOver the past 3 hours, it has been determined flaky in 69 workflow(s) with 210 failures and 69 successes.\n\n**Debugging instructions (after clicking on the recent samples link):**\nDO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\nTo find relevant log snippets:\n1. Click on the workflow logs linked above\n2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n3. Grep for `test_get_mode_stack`\n4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n\n\n\n<details><summary>Sample error message</summary>\n\n```\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/test/test_overrides.py\", line 1328, in test_get_mode_stack\n    self.assertEqual(_get_current_function_mode_stack(), [])\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py\", line 3785, in assertEqual\n    error_metas = not_close_error_metas(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_comparison.py\", line 1227, in not_close_error_metas\n    raise error_meta.to_error() from None  # noqa: RSE102\nAssertionError: The length of the sequences mismatch: 1 != 0\n\nTo execute this test, run the following from the base repo dir:\n    python test/test_overrides.py TestTorchFunctionMode.test_get_mode_stack\n\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n```\n\n</details>\n\n\nTest file path: `test_overrides.py`\n\ncc @clee2000 @hameerabbasi @rgommers @ezyang\n\n### Code Patch:\n@@ -1621,22 +1621,25 @@ def __torch_function__(self, func, types, args=(), kwargs=None):\n     def test_device_context_semantics(self):\n         from torch._C import _len_torch_function_stack\n         from torch.utils._device import DeviceContext\n-        torch.set_default_device(\"cuda\")\n+        try:\n+            torch.set_default_device(\"cuda\")\n \n-        def get_stack():\n-            return [torch._C._get_function_stack_at(i) for i in range(_len_torch_function_stack())]\n+            def get_stack():\n+                return [torch._C._get_function_stack_at(i) for i in range(_len_torch_function_stack())]\n+\n+            base_mode = BaseTorchFunctionMode()\n+            with base_mode:\n+                torch.set_default_device(\"cpu\")\n+                x = torch.ones(2, 2)\n+                stack = get_stack()\n+                self.assertIsInstance(stack[0], DeviceContext)\n+                self.assertEqual(stack[0].device, torch.device(\"cpu\"))\n \n-        base_mode = BaseTorchFunctionMode()\n-        with base_mode:\n-            torch.set_default_device(\"cpu\")\n-            x = torch.ones(2, 2)\n             stack = get_stack()\n             self.assertIsInstance(stack[0], DeviceContext)\n             self.assertEqual(stack[0].device, torch.device(\"cpu\"))\n-\n-        stack = get_stack()\n-        self.assertIsInstance(stack[0], DeviceContext)\n-        self.assertEqual(stack[0].device, torch.device(\"cpu\"))\n+        finally:\n+            torch.set_default_device(None)\n \n \n \n"}
{"id": 67, "ground_truth": "Logic", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nThese tests are the same ones that run against other analytics DBs, but for some reason when we run them against Presto/Hive we get false positives (or are they false negatives? ü§î) at a much greater rate. These tests also seem to take much longer to run, which may be related.\r\n\r\nIf we can get to the bottom of this it will massively improve the experience of contributing to Superset.\r\n\r\n#### How to reproduce the bug\r\n\r\nRun CI a few times.\r\n\r\n### Expected results\r\n\r\nCI should pass if the PR hasn't introduced any bugs.\r\n\r\n### Actual results\r\n\r\nIt fails a lot. Often multiple times in a row, with different tests.\r\n\r\n### Additional context\r\n\r\nI want to start putting together a list of the tests that have failed, to see if there is some sort of pattern. Feel free to add new entries to this list (only if you are certain that it really is a false positive!). Please also link to a relevant line of the GitHub Action where the test failed.\r\n\r\n- [ERROR tests/integration_tests/dashboards/filter_state/api_tests.py::test_delete_not_owner](https://github.com/apache/superset/runs/4524150250?check_suite_focus=true#step:8:2363)\r\n- [ERROR tests/integration_tests/security_tests.py::TestRolePermission::test_admin_permissions](https://github.com/apache/superset/runs/4525283552?check_suite_focus=true#step:8:2420)\r\n\n\n### Code Patch:\n@@ -80,7 +80,7 @@ jobs:\n       - name: Python unit tests (PostgreSQL)\n         if: steps.check.outcome == 'failure'\n         run: |\n-          ./scripts/python_tests.sh\n+          ./scripts/python_tests.sh -m 'chart_data_flow or sql_json_flow'\n       - name: Upload code coverage\n         if: steps.check.outcome == 'failure'\n         run: |\n@@ -158,7 +158,7 @@ jobs:\n       - name: Python unit tests (PostgreSQL)\n         if: steps.check.outcome == 'failure'\n         run: |\n-          ./scripts/python_tests.sh\n+          ./scripts/python_tests.sh -m 'chart_data_flow or sql_json_flow'\n       - name: Upload code coverage\n         if: steps.check.outcome == 'failure'\n         run: |\n\n@@ -31,4 +31,5 @@ superset db upgrade\n superset init\n \n echo \"Running tests\"\n-pytest --durations=0 --maxfail=1 --cov=superset $@\n+\n+pytest --durations=0 --maxfail=1 --cov=superset \"$@\"\n\n@@ -66,6 +66,7 @@\n QUERY_3 = \"SELECT * FROM birth_names LIMIT 10\"\n \n \n+@pytest.mark.sql_json_flow\n class TestSqlLab(SupersetTestCase):\n     \"\"\"Testings for Sql Lab\"\"\"\n \n"}
{"id": 190, "ground_truth": "Network", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\nFew of recent ci runs have failures for `//test/extensions/filters/http/alternate_protocols_cache:filter_integration_test`:\nhttps://github.com/envoyproxy/envoy/actions/runs/15444829526/job/43471494796\nhttps://github.com/envoyproxy/envoy/actions/runs/15439887326/job/43454992923\nci log:\n```\n[ RUN      ] Protocols/MixedUpstreamIntegrationTest.BasicRequestAutoWithHttp3/IPv4_Http2Downstream_Http3UpstreamHttpParserNghttp2Legacy\ntest/extensions/filters/http/alternate_protocols_cache/filter_integration_test.cc:536: Failure\nExpected: (getSrtt(alt_svc, timeSystem())) != (0), actual: 0 vs 0\nAlt-svc entry :''\nStack trace:\n  0xaaaad262aa0c: Envoy::(anonymous namespace)::MixedUpstreamIntegrationTest_BasicRequestAutoWithHttp3_Test::TestBody()\n  0xaaaad3ad4224: testing::internal::HandleExceptionsInMethodIfSupported<>()\n  0xaaaad3ad40c0: testing::Test::Run()\n  0xaaaad3ad528c: testing::TestInfo::Run()\n... Google Test internal frames ...\n\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_quic_always_support_server_preferred_address to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_enable_universal_header_validator to: false\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http1_use_balsa_parser to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http2_use_oghttp2 to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_no_extension_lookup_by_name to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_runtime_initialized to: false\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_quic_always_support_server_preferred_address to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_enable_universal_header_validator to: false\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http1_use_balsa_parser to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http2_use_oghttp2 to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_no_extension_lookup_by_name to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_runtime_initialized to: false\n[  FAILED  ] Protocols/MixedUpstreamIntegrationTest.BasicRequestAutoWithHttp3/IPv4_Http2Downstream_Http3UpstreamHttpParserNghttp2Legacy, where GetParam() = 24-byte object <00-00 00-00 01-00 00-00 02-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00> (571 ms)\n```\n\n### Code Patch:\n@@ -523,7 +523,7 @@ TEST_P(MixedUpstreamIntegrationTest, BasicRequestAutoWithHttp3) {\n   std::string alt_svc;\n \n   // Make sure the srtt gets updated to a non-zero value.\n-  for (int i = 0; i < 5; ++i) {\n+  for (int i = 0; i < 20; ++i) {\n     // Make sure that srtt is updated.\n     const std::string filename = TestEnvironment::temporaryPath(\"alt_svc_cache.txt\");\n     alt_svc = TestEnvironment::readFileToStringForTest(filename);\n"}
{"id": 104, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe changes in this pull request address issue SUREFIRE-756. Namely, the changes include\r\n\r\n- Output of the random seed used to generate a particular random test order when -Dsurefire.runOrder=random or -Dfailsafe.runOrder=random is set\r\n- Ability to replay a previously observed random test order by setting -Dsurefire.seed and -Dfailsafe.seed to the seed that observed the random test order\r\n- Tests to ensure that the setting of the _same_ random seeds do create the _same_ test orders and _different_ random seeds do create _different_ test orders. Note that the inherent randomness of the orders does mean that the tests can be flaky (nondeterministically pass or fail without changes to the code). The current tests have a rate of 0.4% ```(1/3)^5``` of failing. Increasing the number of tests (```3```) or the number of times to loop (```5```) would decrease the odds of the tests failing.\n\n### Code Patch:\n@@ -310,6 +310,22 @@ public class IntegrationTestMojo\n     @Parameter( property = \"failsafe.runOrder\", defaultValue = \"filesystem\" )\n     private String runOrder;\n \n+    /**\n+     * Sets the random seed that will be used to order the tests if {@code failsafe.runOrder} is set to {@code random}.\n+     * <br>\n+     * <br>\n+     * If no seeds are set and {@code failsafe.runOrder} is set to {@code random}, then the seed used will be\n+     * outputted (search for \"To reproduce ordering use flag -Dfailsafe.runOrder.random.seed\").\n+     * <br>\n+     * <br>\n+     * To deterministically reproduce any random test order that was run before, simply set the seed to \n+     * be the same value.\n+     *\n+     * @since 3.0.0-M6\n+     */\n+    @Parameter( property = \"failsafe.runOrder.random.seed\" )\n+    private Long runOrderRandomSeed;\n+\n     /**\n      * A file containing include patterns, each in a next line. Blank lines, or lines starting with # are ignored.\n      * If {@code includes} are also specified, these patterns are appended. Example with path, simple and regex\n@@ -891,6 +907,18 @@ public void setRunOrder( String runOrder )\n         this.runOrder = runOrder;\n     }\n \n+    @Override\n+    public Long getRunOrderRandomSeed()\n+    {\n+        return runOrderRandomSeed;\n+    }\n+\n+    @Override\n+    public void setRunOrderRandomSeed( Long runOrderRandomSeed )\n+    {\n+        this.runOrderRandomSeed = runOrderRandomSeed;\n+    }\n+\n     @Override\n     public File getIncludesFile()\n     {\n\n@@ -865,6 +865,10 @@ public abstract class AbstractSurefireMojo\n \n     public abstract void setRunOrder( String runOrder );\n \n+    public abstract Long getRunOrderRandomSeed();\n+\n+    public abstract void setRunOrderRandomSeed( Long runOrderRandomSeed );\n+\n     protected abstract void handleSummary( RunResult summary, Exception firstForkException )\n         throws MojoExecutionException, MojoFailureException;\n \n@@ -1129,6 +1133,7 @@ boolean verifyParameters()\n             warnIfNotApplicableSkipAfterFailureCount();\n             warnIfIllegalTempDir();\n             warnIfForkCountIsZero();\n+            printDefaultSeedIfNecessary();\n         }\n         return true;\n     }\n@@ -1285,7 +1290,7 @@ private RunResult executeProvider( @Nonnull ProviderInfo provider, @Nonnull Defa\n         ClassLoaderConfiguration classLoaderConfiguration = getClassLoaderConfiguration();\n         provider.addProviderProperties();\n         RunOrderParameters runOrderParameters =\n-            new RunOrderParameters( getRunOrder(), getStatisticsFile( getConfigChecksum() ) );\n+            new RunOrderParameters( getRunOrder(), getStatisticsFile( getConfigChecksum() ), getRunOrderRandomSeed() );\n \n         if ( isNotForking() )\n         {\n@@ -3051,6 +3056,17 @@ private void warnIfIllegalTempDir() throws MojoFailureException\n         }\n     }\n \n+    private void printDefaultSeedIfNecessary()\n+    {\n+        if ( getRunOrderRandomSeed() == null && getRunOrder().equals( RunOrder.RANDOM.name() ) )\n+        {\n+            setRunOrderRandomSeed( System.nanoTime() );\n+            getConsoleLogger().info(\n+                \"Tests will run in random order. To reproduce ordering use flag -D\"\n+                    + getPluginName() + \".runOrder.random.seed=\" + getRunOrderRandomSeed() );\n+        }\n+    }\n+\n     final class TestNgProviderInfo\n         implements ProviderInfo\n     {\n\n@@ -56,6 +56,7 @@\n import static org.apache.maven.surefire.booter.BooterConstants.PLUGIN_PID;\n import static org.apache.maven.surefire.booter.BooterConstants.PROCESS_CHECKER;\n import static org.apache.maven.surefire.booter.BooterConstants.PROVIDER_CONFIGURATION;\n+import static org.apache.maven.surefire.booter.BooterConstants.RUN_ORDER_RANDOM_SEED;\n import static org.apache.maven.surefire.booter.BooterConstants.REPORTSDIRECTORY;\n import static org.apache.maven.surefire.booter.BooterConstants.REQUESTEDTEST;\n import static org.apache.maven.surefire.booter.BooterConstants.RERUN_FAILING_TESTS_COUNT;\n@@ -161,6 +162,7 @@ File serialize( KeyValueSource sourceProperties, ProviderConfiguration providerC\n         {\n             properties.setProperty( RUN_ORDER, RunOrder.asString( runOrderParameters.getRunOrder() ) );\n             properties.setProperty( RUN_STATISTICS_FILE, runOrderParameters.getRunStatisticsFile() );\n+            properties.setProperty( RUN_ORDER_RANDOM_SEED, runOrderParameters.getRunOrderRandomSeed() );\n         }\n \n         ReporterConfiguration reporterConfiguration = providerConfiguration.getReporterConfiguration();\n\n@@ -839,6 +839,18 @@ public void setRunOrder( String runOrder )\n \n         }\n \n+        @Override\n+        public Long getRunOrderRandomSeed()\n+        {\n+            return null;\n+        }\n+\n+        @Override\n+        public void setRunOrderRandomSeed( Long runOrderRandomSeed )\n+        {\n+\n+        }\n+\n         @Override\n         protected void handleSummary( RunResult summary, Exception firstForkException )\n         {\n\n@@ -2381,6 +2381,18 @@ public void setRunOrder( String runOrder )\n \n         }\n \n+        @Override\n+        public Long getRunOrderRandomSeed()\n+        {\n+            return null;\n+        }\n+\n+        @Override\n+        public void setRunOrderRandomSeed( Long runOrderRandomSeed )\n+        {\n+\n+        }\n+\n         @Override\n         protected void handleSummary( RunResult summary, Exception firstForkException )\n         {\n\n@@ -709,6 +709,18 @@ public void setRunOrder( String runOrder )\n \n         }\n \n+        @Override\n+        public Long getRunOrderRandomSeed()\n+        {\n+            return null;\n+        }\n+\n+        @Override\n+        public void setRunOrderRandomSeed( Long runOrderRandomSeed )\n+        {\n+\n+        }\n+\n         @Override\n         public String[] getDependenciesToScan()\n         {\n\n@@ -292,6 +292,22 @@ public class SurefirePlugin\n     @Parameter( property = \"surefire.runOrder\", defaultValue = \"filesystem\" )\n     private String runOrder;\n \n+    /**\n+     * Sets the random seed that will be used to order the tests if {@code surefire.runOrder} is set to {@code random}.\n+     * <br>\n+     * <br>\n+     * If no seeds are set and {@code surefire.runOrder} is set to {@code random}, then the seed used will be\n+     * outputted (search for \"To reproduce ordering use flag -Dsurefire.runOrder.random.seed\").\n+     * <br>\n+     * <br>\n+     * To deterministically reproduce any random test order that was run before, simply set the seed to \n+     * be the same value.\n+     *\n+     * @since 3.0.0-M6\n+     */\n+    @Parameter( property = \"surefire.runOrder.random.seed\" )\n+    private Long runOrderRandomSeed;\n+\n     /**\n      * A file containing include patterns. Blank lines, or lines starting with # are ignored. If {@code includes} are\n      * also specified, these patterns are appended. Example with path, simple and regex includes:\n@@ -794,6 +810,18 @@ public void setRunOrder( String runOrder )\n         this.runOrder = runOrder;\n     }\n \n+    @Override\n+    public Long getRunOrderRandomSeed()\n+    {\n+        return runOrderRandomSeed;\n+    }\n+\n+    @Override\n+    public void setRunOrderRandomSeed( Long runOrderRandomSeed )\n+    {\n+        this.runOrderRandomSeed = runOrderRandomSeed;\n+    }\n+\n     @Override\n     public File getIncludesFile()\n     {\n\n@@ -31,16 +31,34 @@ public class RunOrderParameters\n \n     private File runStatisticsFile;\n \n+    private Long runOrderRandomSeed;\n+\n     public RunOrderParameters( RunOrder[] runOrder, File runStatisticsFile )\n     {\n         this.runOrder = runOrder;\n         this.runStatisticsFile = runStatisticsFile;\n+        this.runOrderRandomSeed = null;\n     }\n \n     public RunOrderParameters( String runOrder, File runStatisticsFile )\n     {\n         this.runOrder = runOrder == null ? RunOrder.DEFAULT : RunOrder.valueOfMulti( runOrder );\n         this.runStatisticsFile = runStatisticsFile;\n+        this.runOrderRandomSeed = null;\n+    }\n+\n+    public RunOrderParameters( RunOrder[] runOrder, File runStatisticsFile, Long runOrderRandomSeed )\n+    {\n+        this.runOrder = runOrder;\n+        this.runStatisticsFile = runStatisticsFile;\n+        this.runOrderRandomSeed = runOrderRandomSeed;\n+    }\n+\n+    public RunOrderParameters( String runOrder, File runStatisticsFile, Long runOrderRandomSeed )\n+    {\n+        this.runOrder = runOrder == null ? RunOrder.DEFAULT : RunOrder.valueOfMulti( runOrder );\n+        this.runStatisticsFile = runStatisticsFile;\n+        this.runOrderRandomSeed = runOrderRandomSeed;\n     }\n \n     public static RunOrderParameters alphabetical()\n@@ -53,6 +71,16 @@ public RunOrder[] getRunOrder()\n         return runOrder;\n     }\n \n+    public Long getRunOrderRandomSeed()\n+    {\n+        return runOrderRandomSeed;\n+    }\n+\n+    public void setRunOrderRandomSeed( Long runOrderRandomSeed )\n+    {\n+        this.runOrderRandomSeed = runOrderRandomSeed;\n+    }\n+\n     public File getRunStatisticsFile()\n     {\n         return runStatisticsFile;\n\n@@ -28,6 +28,7 @@\n import java.util.Comparator;\n import java.util.LinkedHashSet;\n import java.util.List;\n+import java.util.Random;\n \n /**\n  * Applies the final runorder of the tests\n@@ -45,12 +46,21 @@ public class DefaultRunOrderCalculator\n \n     private final int threadCount;\n \n+    private final Random random;\n+\n     public DefaultRunOrderCalculator( RunOrderParameters runOrderParameters, int threadCount )\n     {\n         this.runOrderParameters = runOrderParameters;\n         this.threadCount = threadCount;\n         this.runOrder = runOrderParameters.getRunOrder();\n         this.sortOrder = this.runOrder.length > 0 ? getSortOrderComparator( this.runOrder[0] ) : null;\n+        Long runOrderRandomSeed = runOrderParameters.getRunOrderRandomSeed();\n+        if ( runOrderRandomSeed == null )\n+        {\n+            runOrderRandomSeed = System.nanoTime();\n+            runOrderParameters.setRunOrderRandomSeed( runOrderRandomSeed );\n+        }\n+        this.random = new Random( runOrderRandomSeed );\n     }\n \n     @Override\n@@ -72,7 +82,7 @@ private void orderTestClasses( List<Class<?>> testClasses, RunOrder runOrder )\n     {\n         if ( RunOrder.RANDOM.equals( runOrder ) )\n         {\n-            Collections.shuffle( testClasses );\n+            Collections.shuffle( testClasses, random );\n         }\n         else if ( RunOrder.FAILEDFIRST.equals( runOrder ) )\n         {\n\n@@ -46,6 +46,7 @@ private BooterConstants()\n     public static final String SOURCE_DIRECTORY = \"testSuiteDefinitionTestSourceDirectory\";\n     public static final String TEST_CLASSES_DIRECTORY = \"testClassesDirectory\";\n     public static final String RUN_ORDER = \"runOrder\";\n+    public static final String RUN_ORDER_RANDOM_SEED = \"runOrderRandomSeed\";\n     public static final String RUN_STATISTICS_FILE = \"runStatisticsFile\";\n     public static final String TEST_SUITE_XML_FILES = \"testSuiteXmlFiles\";\n     public static final String PROVIDER_CONFIGURATION = \"providerConfiguration\";\n\n@@ -102,6 +102,7 @@ public ProviderConfiguration deserialize()\n         final List<String> testSuiteXmlFiles = properties.getStringList( TEST_SUITE_XML_FILES );\n         final File testClassesDirectory = properties.getFileProperty( TEST_CLASSES_DIRECTORY );\n         final String runOrder = properties.getProperty( RUN_ORDER );\n+        final Long runOrderRandomSeed = properties.getLongProperty( RUN_ORDER_RANDOM_SEED );\n         final String runStatisticsFile = properties.getProperty( RUN_STATISTICS_FILE );\n \n         final int rerunFailingTestsCount = properties.getIntProperty( RERUN_FAILING_TESTS_COUNT );\n@@ -111,7 +112,8 @@ public ProviderConfiguration deserialize()\n                                             properties.getBooleanProperty( FAILIFNOTESTS ), runOrder );\n \n         RunOrderParameters runOrderParameters\n-                = new RunOrderParameters( runOrder, runStatisticsFile == null ? null : new File( runStatisticsFile ) );\n+                = new RunOrderParameters( runOrder, runStatisticsFile == null ? null : new File( runStatisticsFile ),\n+                                          runOrderRandomSeed );\n \n         TestArtifactInfo testNg = new TestArtifactInfo( testNgVersion, testArtifactClassifier );\n         TestRequest testSuiteDefinition =\n\n@@ -178,10 +178,11 @@ private Object createRunOrderParameters( RunOrderParameters runOrderParameters )\n             return null;\n         }\n         //Can't use the constructor with the RunOrder parameter. Using it causes some integration tests to fail.\n-        Class<?>[] arguments = { String.class, File.class };\n+        Class<?>[] arguments = { String.class, File.class, Long.class };\n         Constructor<?> constructor = getConstructor( this.runOrderParameters, arguments );\n         File runStatisticsFile = runOrderParameters.getRunStatisticsFile();\n-        return newInstance( constructor, RunOrder.asString( runOrderParameters.getRunOrder() ), runStatisticsFile );\n+        return newInstance( constructor, RunOrder.asString( runOrderParameters.getRunOrder() ), runStatisticsFile,\n+                            runOrderParameters.getRunOrderRandomSeed() );\n     }\n \n     private Object createTestArtifactInfo( TestArtifactInfo testArtifactInfo )\n\n@@ -119,6 +119,20 @@ public void testRunOrderParameters()\n         assertTrue( isCalled( foo ) );\n     }\n \n+    public void testRunOrderParametersWithRunOrderRandomSeed()\n+    {\n+        SurefireReflector surefireReflector = getReflector();\n+        Object foo = getFoo();\n+\n+        // Arbitrary random seed that should be ignored because RunOrder is not RANDOM\n+        Long runOrderRandomSeed = 5L;\n+\n+        RunOrderParameters runOrderParameters = new RunOrderParameters( RunOrder.DEFAULT, new File( \".\" ),\n+                                                                        runOrderRandomSeed );\n+        surefireReflector.setRunOrderParameters( foo, runOrderParameters );\n+        assertTrue( isCalled( foo ) );\n+    }\n+\n     public void testNullRunOrderParameters()\n     {\n         SurefireReflector surefireReflector = getReflector();\n\n@@ -19,6 +19,7 @@\n  * under the License.\n  */\n \n+import java.util.Arrays;\n import java.util.Calendar;\n import org.apache.maven.it.VerificationException;\n import org.apache.maven.surefire.its.fixture.OutputValidator;\n@@ -60,6 +61,43 @@ public void testAlphabeticalJUnit5()\n         assertTestnamesAppearInSpecificOrder( validator, TESTS_IN_ALPHABETICAL_ORDER );\n     }\n \n+    @Test\n+    public void testRandomJUnit4DifferentSeed()\n+        throws Exception\n+    {\n+        long seed = 0L;\n+        OutputValidator validator = executeWithRandomOrder( \"junit4\", seed );\n+        String[] expected = validator.getStringsOrderInLog( TESTS_IN_ALPHABETICAL_ORDER );\n+        for ( long i = seed; i < 5 + seed; i++ )\n+        {\n+            OutputValidator validator2 = executeWithRandomOrder( \"junit4\", i );\n+            String[] observed = validator2.getStringsOrderInLog( TESTS_IN_ALPHABETICAL_ORDER );\n+            if ( ! Arrays.equals( expected, observed ) )\n+            {\n+                return;\n+            }\n+        }\n+        throw new VerificationException( \"All random orders with the different seeds produced the same orders\" );\n+    }\n+\n+    @Test\n+    public void testRandomJUnit4SameSeed()\n+        throws Exception\n+    {\n+        long seed = 0L;\n+        OutputValidator validator = executeWithRandomOrder( \"junit4\", seed );\n+        String[] expected = validator.getStringsOrderInLog( TESTS_IN_ALPHABETICAL_ORDER );\n+        for ( long i = 0; i < 5; i++ )\n+        {\n+            OutputValidator validator2 = executeWithRandomOrder( \"junit4\", seed );\n+            String[] observed = validator2.getStringsOrderInLog( TESTS_IN_ALPHABETICAL_ORDER );\n+            if ( ! Arrays.equals( expected, observed ) )\n+            {\n+                throw new VerificationException( \"Random orders with the same seed produced different orders\" );\n+            }\n+        }\n+    }\n+    \n     @Test\n     public void testReverseAlphabeticalJUnit4()\n         throws Exception\n@@ -149,6 +187,17 @@ private OutputValidator executeWithRunOrder( String runOrder, String profile )\n             .verifyErrorFree( 3 );\n     }\n \n+    private OutputValidator executeWithRandomOrder( String profile, long seed  )\n+    {\n+        return unpack()\n+            .activateProfile( profile )\n+            .forkMode( getForkMode() )\n+            .runOrder( \"random\" )\n+            .runOrderRandomSeed( String.valueOf( seed ) )\n+            .executeTest()\n+            .verifyErrorFree( 3 );\n+    }\n+\n     protected String getForkMode()\n     {\n         return \"once\";\n\n@@ -23,6 +23,7 @@\n import java.io.IOException;\n import java.nio.charset.Charset;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n \n import org.apache.commons.io.FileUtils;\n@@ -223,6 +224,32 @@ public File getBaseDir()\n         return baseDir;\n     }\n \n+    public String[] getStringsOrderInLog( String[] strings )\n+        throws VerificationException\n+    {\n+        String[] retArr = new String[strings.length];\n+        List<String> strList = new ArrayList<String>( Arrays.asList( strings ) );\n+        int i = 0;\n+        for ( String line : loadLogLines() )\n+        {\n+            for ( int j = 0; j < strList.size(); j++ )\n+            {\n+                if ( line.startsWith( strList.get( j ) ) ) \n+                {\n+                    retArr[i] = strList.get( j );\n+                    ++i;\n+                    if ( i == strings.length )\n+                    {\n+                        return retArr;\n+                    }\n+                    strList.remove( j );\n+                    break;\n+                }\n+            }\n+        }\n+        return retArr;\n+    }\n+\n     public boolean stringsAppearInSpecificOrderInLog( String[] strings )\n         throws VerificationException\n     {\n\n@@ -289,6 +289,12 @@ public SurefireLauncher runOrder( String runOrder )\n         return this;\n     }\n \n+    public SurefireLauncher runOrderRandomSeed( String runOrderRandomSeed )\n+    {\n+        mavenLauncher.sysProp( \"surefire.runOrder.random.seed\", runOrderRandomSeed );\n+        return this;\n+    }\n+\n     public SurefireLauncher failIfNoTests( boolean fail )\n     {\n         mavenLauncher.sysProp( \"failIfNoTests\", fail );\n"}
{"id": 165, "ground_truth": "Async Wait", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nThe test `test_installed_modules` appears to not be all that useful.\n\nThe test exists to verify the behavior of the [`_generate_installed_modules` function](https://github.com/getsentry/sentry-python/blob/9b66f3b51502ca600554c711bc3f599c18f8f18b/sentry_sdk/utils.py#L1689). However, all the test does is essentially check the output of `_generate_installed_modules` against a refactored version of the function call itself.\n\nIn short, in its current form, the test appears to not make too much sense. As the test recently started failing, let's just delete it.\n\n<!-- Describe your PR here -->\n\n---\n\nThank you for contributing to `sentry-python`! Please add tests to validate your changes, and lint your code using `tox -e linters`.\n\nRunning the test suite on your PR might require maintainer approval.\n\n### Code Patch:\n@@ -30,7 +30,6 @@\n     serialize_frame,\n     is_sentry_url,\n     _get_installed_modules,\n-    _generate_installed_modules,\n     ensure_integration_enabled,\n     to_string,\n     exc_info_from_error,\n@@ -667,47 +666,6 @@ def __str__(self):\n     assert result == repr(obj)\n \n \n-def test_installed_modules():\n-    try:\n-        from importlib.metadata import distributions, version\n-\n-        importlib_available = True\n-    except ImportError:\n-        importlib_available = False\n-\n-    try:\n-        import pkg_resources\n-\n-        pkg_resources_available = True\n-    except ImportError:\n-        pkg_resources_available = False\n-\n-    installed_distributions = {\n-        _normalize_distribution_name(dist): version\n-        for dist, version in _generate_installed_modules()\n-    }\n-\n-    if importlib_available:\n-        importlib_distributions = {\n-            _normalize_distribution_name(dist.metadata.get(\"Name\", None)): version(\n-                dist.metadata.get(\"Name\", None)\n-            )\n-            for dist in distributions()\n-            if dist.metadata.get(\"Name\", None) is not None\n-            and version(dist.metadata.get(\"Name\", None)) is not None\n-        }\n-        assert installed_distributions == importlib_distributions\n-\n-    elif pkg_resources_available:\n-        pkg_resources_distributions = {\n-            _normalize_distribution_name(dist.key): dist.version\n-            for dist in pkg_resources.working_set\n-        }\n-        assert installed_distributions == pkg_resources_distributions\n-    else:\n-        pytest.fail(\"Neither importlib nor pkg_resources is available\")\n-\n-\n def test_installed_modules_caching():\n     mock_generate_installed_modules = mock.Mock()\n     mock_generate_installed_modules.return_value = {\"package\": \"1.0.0\"}\n"}
{"id": 117, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nSome tests may randomly fail du to debounce timer on field validation.\n\n### Code Patch:\n@@ -3,7 +3,7 @@\n  */\n \n import React from 'react'\n-import { describe, expect, jest, test } from '@jest/globals';\n+import { afterEach, beforeEach, describe, expect, jest, test } from '@jest/globals';\n import { render, screen, waitFor } from '@testing-library/react';\n import userEvent from '@testing-library/user-event'\n import '@testing-library/jest-dom/jest-globals'\n@@ -64,8 +64,18 @@ const theme: Theme = buildTheme({\n type Model = { simple: string }\n \n describe('DOM testing', () => {\n+\n+    beforeEach(() => {\n+        jest.useFakeTimers();\n+    })\n+\n+    afterEach(() => {\n+        jest.runOnlyPendingTimers()\n+        jest.useRealTimers();\n+    });\n+\n     test('default settings', async () => {\n-        const user = userEvent.setup()\n+        const user = userEvent.setup({ advanceTimers: jest.advanceTimersByTime })\n \n         const key = 'simple'\n         const label = 'simple'\n@@ -172,7 +182,7 @@ describe('DOM testing', () => {\n             hint: 'value.match'\n         })\n \n-        const user = userEvent.setup()\n+        const user = userEvent.setup({ advanceTimers: jest.advanceTimersByTime })\n \n         const key = 'simple'\n         const label = 'simple'\n@@ -215,6 +225,9 @@ describe('DOM testing', () => {\n         const invalidValue = 'ILoveApples'\n         await user.clear(input)\n         await user.type(input, invalidValue)\n+                \n+        // Fast-forward until all timers have been executed (handle year debounced value)\n+        await jest.runOnlyPendingTimersAsync();\n \n         expect(onFieldChange).toHaveBeenLastCalledWith(\n             expect.objectContaining({\n"}
{"id": 100, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nhttps://github.com/apache/airflow/actions/runs/16658216543/job/47149178223#step:10:696\r\n\r\nSometimes the test failing with cause of other test stored cache.\r\nWhen we run entire test file its working if we run only `task-sdk/tests/task_sdk/io/test_path.py::TestAttach::test_alias` , this was failing due to there is no previously cached object and its asserting for two fields.\r\n \r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n\n### Code Patch:\n@@ -74,6 +74,8 @@ def test_lazy_load():\n \n     assert o.fs is not None\n     assert o._fs_cached\n+    # Clear the cache to avoid side effects in other tests below\n+    _STORE_CACHE.clear()\n \n \n class _FakeRemoteFileSystem(MemoryFileSystem):\n@@ -123,7 +125,7 @@ def fake_files(self):\n     def test_alias(self):\n         store = attach(\"file\", alias=\"local\")\n         assert isinstance(store.fs, LocalFileSystem)\n-        assert {\"local\": store, \"file\": store} == _STORE_CACHE\n+        assert {\"local\": store} == _STORE_CACHE\n \n     def test_objectstoragepath_init_conn_id_in_uri(self):\n         attach(protocol=\"fake\", conn_id=\"fake\", fs=_FakeRemoteFileSystem(conn_id=\"fake\"))\n"}
{"id": 283, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nResolves : https://issues.apache.org/jira/browse/KAFKA-16518\r\n\r\n- Adds a new argument \"standalone\" to kafka-storage.sh\r\n- If standalone mode, creates a checkpoint file in metadata dir ${kafkaConfig.metadataLogDir}/__cluster_metadata-0/\r\n\r\n\r\n### Committer Checklist (excluded from commit message)\r\n- [ ] Verify design and implementation \r\n- [ ] Verify test coverage and CI build status\r\n- [ ] Verify documentation (including upgrade notes)\r\n\n\n### Code Patch:\n@@ -202,27 +202,13 @@ def determineCommitId() {\n }\n \n def excludedSpotlessModules = [':clients',\n-                               ':connect:api',\n-                               ':connect:basic-auth-extension',\n-                               ':connect:file',\n-                               ':connect:json',\n-                               ':connect:mirror',\n-                               ':connect:mirror-client',\n                                ':connect:runtime',\n-                               ':connect:test-plugins',\n-                               ':connect:transforms',\n                                ':core',\n-                               ':examples',\n-                               ':generator',\n                                ':group-coordinator:group-coordinator-api', // https://github.com/apache/kafka/pull/16198\n                                ':group-coordinator',\n-                               ':jmh-benchmarks',\n-                               ':log4j-appender',\n                                ':metadata',\n                                ':raft',\n                                ':server',\n-                               ':server-common',\n-                               ':shell',\n                                ':storage',\n                                ':storage:storage-api', //  rename in settings.gradle\n                                ':streams',\n@@ -251,11 +237,7 @@ def excludedSpotlessModules = [':clients',\n                                ':streams:upgrade-system-tests-34',\n                                ':streams:upgrade-system-tests-35',\n                                ':streams:upgrade-system-tests-36',\n-                               ':streams:upgrade-system-tests-37',\n-                               ':tools',\n-                               ':tools:tools-api',\n-                               ':transaction-coordinator',\n-                               ':trogdor']\n+                               ':streams:upgrade-system-tests-37']\n \n \n apply from: file('wrapper.gradle')\n@@ -445,9 +427,6 @@ subprojects {\n     }\n   }\n \n-  // Remove the relevant project name once it's converted to JUnit 5\n-  def shouldUseJUnit5 = !([\"runtime\"].contains(it.project.name))\n-\n   def testLoggingEvents = [\"passed\", \"skipped\", \"failed\"]\n   def testShowStandardStreams = false\n   def testExceptionFormat = 'full'\n@@ -522,13 +501,6 @@ subprojects {\n   // The suites are for running sets of tests in IDEs.\n   // Gradle will run each test class, so we exclude the suites to avoid redundantly running the tests twice.\n   def testsToExclude = ['**/*Suite.class']\n-  // Exclude PowerMock tests when running with Java 16 or newer until a version of PowerMock that supports the relevant versions is released\n-  // The relevant issues are https://github.com/powermock/powermock/issues/1094 and https://github.com/powermock/powermock/issues/1099\n-  if (JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_16)) {\n-    testsToExclude.addAll([\n-      // connect tests\n-    ])\n-  }\n \n   test {\n     maxParallelForks = maxTestForks\n@@ -547,8 +519,7 @@ subprojects {\n \n     exclude testsToExclude\n \n-    if (shouldUseJUnit5)\n-      useJUnitPlatform()\n+    useJUnitPlatform()\n \n     retry {\n       maxRetries = userMaxTestRetries\n@@ -575,23 +546,18 @@ subprojects {\n \n     exclude testsToExclude\n \n-    if (shouldUseJUnit5) {\n-      if (project.name == 'streams') {\n-        useJUnitPlatform {\n-          includeTags \"integration\"\n-          includeTags \"org.apache.kafka.test.IntegrationTest\"\n-\t  // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n-          // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n-          includeEngines \"junit-vintage\", \"junit-jupiter\"\n-        }\n-      } else {\n-        useJUnitPlatform {\n-          includeTags \"integration\"\n-        }\n+    if (project.name == 'streams') {\n+      useJUnitPlatform {\n+        includeTags \"integration\"\n+        includeTags \"org.apache.kafka.test.IntegrationTest\"\n+        // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n+        // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n+        includeEngines \"junit-vintage\", \"junit-jupiter\"\n       }\n     } else {\n-      useJUnit {\n-        includeCategories 'org.apache.kafka.test.IntegrationTest'\n+      useJUnitPlatform {\n+        includeTags \"integration\"\n+        includeTags 'org.apache.kafka.test.IntegrationTest'\n       }\n     }\n \n@@ -618,23 +584,18 @@ subprojects {\n \n     exclude testsToExclude\n \n-    if (shouldUseJUnit5) {\n-      if (project.name == 'streams') {\n-        useJUnitPlatform {\n-          excludeTags \"integration\"\n-          excludeTags \"org.apache.kafka.test.IntegrationTest\"\n-\t  // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n-          // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n-          includeEngines \"junit-vintage\", \"junit-jupiter\"\n-        }\n-      } else {\n-        useJUnitPlatform {\n-          excludeTags \"integration\"\n-        }\n+    if (project.name == 'streams') {\n+      useJUnitPlatform {\n+        excludeTags \"integration\"\n+        excludeTags \"org.apache.kafka.test.IntegrationTest\"\n+        // Both engines are needed to run JUnit 4 tests alongside JUnit 5 tests.\n+        // junit-vintage (JUnit 4) can be removed once the JUnit 4 migration is complete.\n+        includeEngines \"junit-vintage\", \"junit-jupiter\"\n       }\n     } else {\n-      useJUnit {\n-        excludeCategories 'org.apache.kafka.test.IntegrationTest'\n+      useJUnitPlatform {\n+        excludeTags \"integration\"\n+        excludeTags 'org.apache.kafka.test.IntegrationTest'\n       }\n     }\n \n@@ -825,10 +786,7 @@ subprojects {\n       jacoco {\n         toolVersion = versions.jacoco\n       }\n-\n-      // NOTE: Jacoco Gradle plugin does not support \"offline instrumentation\" this means that classes mocked by PowerMock\n-      // may report 0 coverage, since the source was modified after initial instrumentation.\n-      // See https://github.com/jacoco/jacoco/issues/51\n+      \n       jacocoTestReport {\n         dependsOn tasks.test\n         sourceSets sourceSets.main\n@@ -852,8 +810,10 @@ subprojects {\n     skipProjects = [ \":jmh-benchmarks\", \":trogdor\" ]\n     skipConfigurations = [ \"zinc\" ]\n   }\n-  \n-  if(JavaVersion.current().isJava11Compatible() && project.path !in excludedSpotlessModules) {\n+  //  the task `removeUnusedImports` is implemented by google-java-format, \n+  //  and unfortunately the google-java-format version used by spotless 6.14.0 can't work with JDK 21. \n+  //  Hence, we apply spotless tasks only if the env is either JDK11 or JDK17\n+  if ((JavaVersion.current().isJava11() || (JavaVersion.current() == JavaVersion.VERSION_17)) && project.path !in excludedSpotlessModules) {\n     apply plugin: 'com.diffplug.spotless'\n     spotless {\n       java {\n@@ -2177,7 +2137,6 @@ project(':tools') {\n   }\n \n   dependencies {\n-    compileOnly libs.slf4jlog4j\n     implementation project(':clients')\n     implementation project(':storage')\n     implementation project(':server-common')\n@@ -2188,6 +2147,7 @@ project(':tools') {\n     implementation libs.jacksonDataformatCsv\n     implementation libs.jacksonJDK8Datatypes\n     implementation libs.slf4jApi\n+    implementation libs.slf4jlog4j\n     implementation libs.joptSimple\n \n     implementation libs.jose4j                    // for SASL/OAUTHBEARER JWT validation\n@@ -2214,7 +2174,6 @@ project(':tools') {\n       exclude group: 'junit', module: 'junit'\n     }\n     testImplementation libs.log4j\n-    testRuntimeOnly libs.slf4jlog4j\n   }\n \n   javadoc {\n@@ -2376,7 +2335,6 @@ project(':streams') {\n     testImplementation libs.log4j\n     testImplementation libs.junitJupiter\n     testImplementation libs.junitVintageEngine\n-    testImplementation libs.easymock\n     testImplementation libs.bcpkix\n     testImplementation libs.hamcrest\n     testImplementation libs.mockitoCore\n@@ -3258,12 +3216,12 @@ project(':connect:runtime') {\n     testImplementation project(':connect:test-plugins')\n     testImplementation project(':group-coordinator')\n \n-    testImplementation libs.easymock\n-    testImplementation libs.junitJupiterApi\n+    testImplementation libs.junitJupiter\n     testImplementation libs.junitVintageEngine\n-    testImplementation libs.powermockJunit4\n-    testImplementation libs.powermockEasymock\n+    testImplementation libs.mockitoJunitJupiter\n     testImplementation libs.mockitoCore\n+    testImplementation libs.hamcrest\n+    testImplementation libs.mockitoJunitJupiter\n     testImplementation libs.httpclient\n \n     testRuntimeOnly libs.slf4jlog4j\n\n@@ -150,6 +150,9 @@\n \n     <!-- Allows the use of the @SuppressWarnings annotation in the code -->\n     <module name=\"SuppressWarningsHolder\"/>\n+\n+    <module name=\"ModifierOrder\"/>\n+    \n   </module>\n \n   <module name=\"SuppressionFilter\">\n\n@@ -38,6 +38,7 @@\n   <allow pkg=\"org.apache.kafka.common\" />\n   <allow pkg=\"org.mockito\" class=\"AssignmentsManagerTest\"/>\n   <allow pkg=\"org.apache.kafka.server\"/>\n+  <allow pkg=\"org.opentest4j\" class=\"RemoteLogManagerTest\"/>\n   <!-- see KIP-544 for why KafkaYammerMetrics should be used instead of the global default yammer metrics registry\n        https://cwiki.apache.org/confluence/display/KAFKA/KIP-544%3A+Make+metrics+exposed+via+JMX+configurable -->\n   <disallow class=\"com.yammer.metrics.Metrics\" />\n\n@@ -30,8 +30,6 @@\n   <allow pkg=\"org.opentest4j\" />\n   <allow pkg=\"org.hamcrest\" />\n   <allow pkg=\"org.mockito\" />\n-  <allow pkg=\"org.easymock\" />\n-  <allow pkg=\"org.powermock\" />\n   <allow pkg=\"java.security\" />\n   <allow pkg=\"javax.net.ssl\" />\n   <allow pkg=\"javax.security\" />\n@@ -622,9 +620,6 @@\n     <subpackage name=\"file\">\n       <allow pkg=\"org.apache.kafka.connect\" />\n       <allow pkg=\"org.apache.kafka.clients.consumer\" />\n-      <!-- for tests -->\n-      <allow pkg=\"org.easymock\" />\n-      <allow pkg=\"org.powermock\" />\n     </subpackage>\n \n     <subpackage name=\"tools\">\n\n@@ -200,7 +200,7 @@\n               files=\"StreamThread.java\"/>\n \n     <suppress checks=\"ClassDataAbstractionCoupling\"\n-              files=\"(InternalTopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl).java\"/>\n+              files=\"(InternalTopologyBuilder|KafkaStreams|KStreamImpl|KTableImpl|StreamsPartitionAssignor).java\"/>\n \n     <suppress checks=\"CyclomaticComplexity\"\n               files=\"(KafkaStreams|StreamsPartitionAssignor|StreamThread|TaskManager|PartitionGroup|SubscriptionWrapperSerde|AssignorConfiguration).java\"/>\n@@ -209,7 +209,7 @@\n               files=\"StreamsMetricsImpl.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n-              files=\"(KafkaStreams|StreamsPartitionAssignor|StreamThread|TaskManager|GlobalStateManagerImpl|KStreamImplJoin|TopologyConfig|KTableKTableOuterJoin).java\"/>\n+              files=\"(KafkaStreams|StreamsPartitionAssignor|StreamThread|TaskManager|TaskAssignmentUtils|GlobalStateManagerImpl|KStreamImplJoin|TopologyConfig|KTableKTableOuterJoin).java\"/>\n \n     <suppress checks=\"(FinalLocalVariable|UnnecessaryParentheses|BooleanExpressionComplexity|CyclomaticComplexity|WhitespaceAfter|LocalVariableName)\"\n               files=\"Murmur3.java\"/>\n\n@@ -245,7 +245,9 @@ public static NetworkClient createNetworkClient(AbstractConfig config,\n                     throttleTimeSensor,\n                     logContext,\n                     hostResolver,\n-                    clientTelemetrySender);\n+                    clientTelemetrySender,\n+                    MetadataRecoveryStrategy.forName(config.getString(CommonClientConfigs.METADATA_RECOVERY_STRATEGY_CONFIG))\n+            );\n         } catch (Throwable t) {\n             closeQuietly(selector, \"Selector\");\n             closeQuietly(channelBuilder, \"ChannelBuilder\");\n\n@@ -37,10 +37,10 @@\n  *\n  */\n final class ClusterConnectionStates {\n-    final static int RECONNECT_BACKOFF_EXP_BASE = 2;\n-    final static double RECONNECT_BACKOFF_JITTER = 0.2;\n-    final static int CONNECTION_SETUP_TIMEOUT_EXP_BASE = 2;\n-    final static double CONNECTION_SETUP_TIMEOUT_JITTER = 0.2;\n+    static final int RECONNECT_BACKOFF_EXP_BASE = 2;\n+    static final double RECONNECT_BACKOFF_JITTER = 0.2;\n+    static final int CONNECTION_SETUP_TIMEOUT_EXP_BASE = 2;\n+    static final double CONNECTION_SETUP_TIMEOUT_JITTER = 0.2;\n     private final Map<String, NodeConnectionState> nodeState;\n     private final Logger log;\n     private final HostResolver hostResolver;\n\n@@ -219,6 +219,19 @@ public class CommonClientConfigs {\n     public static final String DEFAULT_API_TIMEOUT_MS_DOC = \"Specifies the timeout (in milliseconds) for client APIs. \" +\n             \"This configuration is used as the default timeout for all client operations that do not specify a <code>timeout</code> parameter.\";\n \n+    public static final String METADATA_RECOVERY_STRATEGY_CONFIG = \"metadata.recovery.strategy\";\n+    public static final String METADATA_RECOVERY_STRATEGY_DOC = \"Controls how the client recovers when none of the brokers known to it is available. \" +\n+            \"If set to <code>none</code>, the client fails. If set to <code>rebootstrap</code>, \" +\n+            \"the client repeats the bootstrap process using <code>bootstrap.servers</code>. \" +\n+            \"Rebootstrapping is useful when a client communicates with brokers so infrequently \" +\n+            \"that the set of brokers may change entirely before the client refreshes metadata. \" +\n+            \"Metadata recovery is triggered when all last-known brokers appear unavailable simultaneously. \" +\n+            \"Brokers appear unavailable when disconnected and no current retry attempt is in-progress. \" +\n+            \"Consider increasing <code>reconnect.backoff.ms</code> and <code>reconnect.backoff.max.ms</code> and \" +\n+            \"decreasing <code>socket.connection.setup.timeout.ms</code> and <code>socket.connection.setup.timeout.max.ms</code> \" +\n+            \"for the client.\";\n+    public static final String DEFAULT_METADATA_RECOVERY_STRATEGY = MetadataRecoveryStrategy.NONE.name;\n+\n     /**\n      * Postprocess the configuration so that exponential backoff is disabled when reconnect backoff\n      * is explicitly configured but the maximum reconnect backoff is not explicitly configured.\n\n@@ -130,7 +130,7 @@ public interface KafkaClient extends Closeable {\n      * @param now The current time in ms\n      * @return The node with the fewest in-flight requests.\n      */\n-    Node leastLoadedNode(long now);\n+    LeastLoadedNode leastLoadedNode(long now);\n \n     /**\n      * The number of currently in-flight requests for which we have not yet returned a response\n\n@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients;\n+\n+import org.apache.kafka.common.Node;\n+\n+public class LeastLoadedNode {\n+    private final Node node;\n+    private final boolean atLeastOneConnectionReady;\n+\n+    public LeastLoadedNode(Node node, boolean atLeastOneConnectionReady) {\n+        this.node = node;\n+        this.atLeastOneConnectionReady = atLeastOneConnectionReady;\n+    }\n+\n+    public Node node() {\n+        return node;\n+    }\n+\n+    /**\n+     * Indicates if the least loaded node is available or at least a ready connection exists.\n+     *\n+     * <p>There may be no node available while ready connections to live nodes exist. This may happen when\n+     * the connections are overloaded with in-flight requests. This function takes this into account.\n+     */\n+    public boolean hasNodeAvailableOrConnectionReady() {\n+        return node != null || atLeastOneConnectionReady;\n+    }\n+}\n\n@@ -82,6 +82,8 @@ public class Metadata implements Closeable {\n     private final ClusterResourceListeners clusterResourceListeners;\n     private boolean isClosed;\n     private final Map<TopicPartition, Integer> lastSeenLeaderEpochs;\n+    /** Addresses with which the metadata was originally bootstrapped. */\n+    private List<InetSocketAddress> bootstrapAddresses;\n \n     /**\n      * Create a new Metadata instance\n@@ -304,6 +306,12 @@ public synchronized void bootstrap(List<InetSocketAddress> addresses) {\n         this.needFullUpdate = true;\n         this.updateVersion += 1;\n         this.metadataSnapshot = MetadataSnapshot.bootstrap(addresses);\n+        this.bootstrapAddresses = addresses;\n+    }\n+\n+    public synchronized void rebootstrap() {\n+        log.info(\"Rebootstrapping with {}\", this.bootstrapAddresses);\n+        this.bootstrap(this.bootstrapAddresses);\n     }\n \n     /**\n\n@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Defines the strategies which clients can follow to deal with the situation when none of the known nodes is available.\n+ */\n+public enum MetadataRecoveryStrategy {\n+    NONE(\"none\"),\n+    REBOOTSTRAP(\"rebootstrap\");\n+\n+    public final String name;\n+\n+    MetadataRecoveryStrategy(String name) {\n+        this.name = name;\n+    }\n+\n+    public static MetadataRecoveryStrategy forName(String name) {\n+        if (name == null) {\n+            throw new IllegalArgumentException(\"Illegal MetadataRecoveryStrategy: null\");\n+        }\n+        try {\n+            return MetadataRecoveryStrategy.valueOf(name.toUpperCase(Locale.ROOT));\n+        } catch (IllegalArgumentException e) {\n+            throw new IllegalArgumentException(\"Illegal MetadataRecoveryStrategy: \" + name);\n+        }\n+    }\n+}\n\n@@ -114,6 +114,8 @@ private enum State {\n     /* time in ms to wait before retrying to create connection to a server */\n     private final long reconnectBackoffMs;\n \n+    private final MetadataRecoveryStrategy metadataRecoveryStrategy;\n+\n     private final Time time;\n \n     /**\n@@ -147,7 +149,8 @@ public NetworkClient(Selectable selector,\n                          Time time,\n                          boolean discoverBrokerVersions,\n                          ApiVersions apiVersions,\n-                         LogContext logContext) {\n+                         LogContext logContext,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         this(selector,\n              metadata,\n              clientId,\n@@ -163,7 +166,8 @@ public NetworkClient(Selectable selector,\n              discoverBrokerVersions,\n              apiVersions,\n              null,\n-             logContext);\n+             logContext,\n+             metadataRecoveryStrategy);\n     }\n \n     public NetworkClient(Selectable selector,\n@@ -181,7 +185,8 @@ public NetworkClient(Selectable selector,\n                          boolean discoverBrokerVersions,\n                          ApiVersions apiVersions,\n                          Sensor throttleTimeSensor,\n-                         LogContext logContext) {\n+                         LogContext logContext,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         this(null,\n              metadata,\n              selector,\n@@ -200,7 +205,8 @@ public NetworkClient(Selectable selector,\n              throttleTimeSensor,\n              logContext,\n              new DefaultHostResolver(),\n-             null);\n+             null,\n+             metadataRecoveryStrategy);\n     }\n \n     public NetworkClient(Selectable selector,\n@@ -217,7 +223,8 @@ public NetworkClient(Selectable selector,\n                          Time time,\n                          boolean discoverBrokerVersions,\n                          ApiVersions apiVersions,\n-                         LogContext logContext) {\n+                         LogContext logContext,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         this(metadataUpdater,\n              null,\n              selector,\n@@ -236,7 +243,8 @@ public NetworkClient(Selectable selector,\n              null,\n              logContext,\n              new DefaultHostResolver(),\n-             null);\n+             null,\n+             metadataRecoveryStrategy);\n     }\n \n     public NetworkClient(MetadataUpdater metadataUpdater,\n@@ -257,7 +265,8 @@ public NetworkClient(MetadataUpdater metadataUpdater,\n                          Sensor throttleTimeSensor,\n                          LogContext logContext,\n                          HostResolver hostResolver,\n-                         ClientTelemetrySender clientTelemetrySender) {\n+                         ClientTelemetrySender clientTelemetrySender,\n+                         MetadataRecoveryStrategy metadataRecoveryStrategy) {\n         /* It would be better if we could pass `DefaultMetadataUpdater` from the public constructor, but it's not\n          * possible because `DefaultMetadataUpdater` is an inner class and it can only be instantiated after the\n          * super constructor is invoked.\n@@ -288,6 +297,7 @@ public NetworkClient(MetadataUpdater metadataUpdater,\n         this.log = logContext.logger(NetworkClient.class);\n         this.state = new AtomicReference<>(State.ACTIVE);\n         this.telemetrySender = (clientTelemetrySender != null) ? new TelemetrySender(clientTelemetrySender) : null;\n+        this.metadataRecoveryStrategy = metadataRecoveryStrategy;\n     }\n \n     /**\n@@ -695,7 +705,7 @@ public void close() {\n      * @return The node with the fewest in-flight requests.\n      */\n     @Override\n-    public Node leastLoadedNode(long now) {\n+    public LeastLoadedNode leastLoadedNode(long now) {\n         List<Node> nodes = this.metadataUpdater.fetchNodes();\n         if (nodes.isEmpty())\n             throw new IllegalStateException(\"There are no nodes in the Kafka cluster\");\n@@ -705,16 +715,25 @@ public Node leastLoadedNode(long now) {\n         Node foundCanConnect = null;\n         Node foundReady = null;\n \n+        boolean atLeastOneConnectionReady = false;\n+\n         int offset = this.randOffset.nextInt(nodes.size());\n         for (int i = 0; i < nodes.size(); i++) {\n             int idx = (offset + i) % nodes.size();\n             Node node = nodes.get(idx);\n+\n+            if (!atLeastOneConnectionReady\n+                    && connectionStates.isReady(node.idString(), now)\n+                    && selector.isChannelReady(node.idString())) {\n+                atLeastOneConnectionReady = true;\n+            }\n+\n             if (canSendRequest(node.idString(), now)) {\n                 int currInflight = this.inFlightRequests.count(node.idString());\n                 if (currInflight == 0) {\n                     // if we find an established connection with no in-flight requests we can stop right away\n                     log.trace(\"Found least loaded node {} connected with no in-flight requests\", node);\n-                    return node;\n+                    return new LeastLoadedNode(node, true);\n                 } else if (currInflight < inflight) {\n                     // otherwise if this is the best we have found so far, record that\n                     inflight = currInflight;\n@@ -738,16 +757,16 @@ public Node leastLoadedNode(long now) {\n         // which are being established before connecting to new nodes.\n         if (foundReady != null) {\n             log.trace(\"Found least loaded node {} with {} inflight requests\", foundReady, inflight);\n-            return foundReady;\n+            return new LeastLoadedNode(foundReady, atLeastOneConnectionReady);\n         } else if (foundConnecting != null) {\n             log.trace(\"Found least loaded connecting node {}\", foundConnecting);\n-            return foundConnecting;\n+            return new LeastLoadedNode(foundConnecting, atLeastOneConnectionReady);\n         } else if (foundCanConnect != null) {\n             log.trace(\"Found least loaded node {} with no active connection\", foundCanConnect);\n-            return foundCanConnect;\n+            return new LeastLoadedNode(foundCanConnect, atLeastOneConnectionReady);\n         } else {\n             log.trace(\"Least loaded node selection failed to find an available node\");\n-            return null;\n+            return new LeastLoadedNode(null, atLeastOneConnectionReady);\n         }\n     }\n \n@@ -821,9 +840,8 @@ private void processDisconnection(List<ClientResponse> responses,\n                 break;\n             case AUTHENTICATE:\n                 log.warn(\"Connection to node {} ({}) terminated during authentication. This may happen \" +\n-                    \"due to any of the following reasons: (1) Authentication failed due to invalid \" +\n-                    \"credentials with brokers older than 1.0.0, (2) Firewall blocking Kafka TLS \" +\n-                    \"traffic (eg it may only allow HTTPS traffic), (3) Transient network issue.\",\n+                    \"due to any of the following reasons: (1) Firewall blocking Kafka TLS \" +\n+                    \"traffic (eg it may only allow HTTPS traffic), (2) Transient network issue.\",\n                     nodeId, disconnectState.remoteAddress());\n                 break;\n             case NOT_CONNECTED:\n@@ -1122,13 +1140,22 @@ public long maybeUpdate(long now) {\n \n             // Beware that the behavior of this method and the computation of timeouts for poll() are\n             // highly dependent on the behavior of leastLoadedNode.\n-            Node node = leastLoadedNode(now);\n-            if (node == null) {\n+            LeastLoadedNode leastLoadedNode = leastLoadedNode(now);\n+\n+            // Rebootstrap if needed and configured.\n+            if (metadataRecoveryStrategy == MetadataRecoveryStrategy.REBOOTSTRAP\n+                    && !leastLoadedNode.hasNodeAvailableOrConnectionReady()) {\n+                metadata.rebootstrap();\n+\n+                leastLoadedNode = leastLoadedNode(now);\n+            }\n+\n+            if (leastLoadedNode.node() == null) {\n                 log.debug(\"Give up sending metadata request since no node is available\");\n                 return reconnectBackoffMs;\n             }\n \n-            return maybeUpdate(now, node);\n+            return maybeUpdate(now, leastLoadedNode.node());\n         }\n \n         @Override\n@@ -1266,7 +1293,7 @@ public long maybeUpdate(long now) {\n \n             // Per KIP-714, let's continue to re-use the same broker for as long as possible.\n             if (stickyNode == null) {\n-                stickyNode = leastLoadedNode(now);\n+                stickyNode = leastLoadedNode(now).node();\n                 if (stickyNode == null) {\n                     log.debug(\"Give up sending telemetry request since no node is available\");\n                     return reconnectBackoffMs;\n\n@@ -19,6 +19,7 @@\n \n import org.apache.kafka.clients.ClientDnsLookup;\n import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.MetadataRecoveryStrategy;\n import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n import org.apache.kafka.common.config.ConfigDef.Importance;\n@@ -139,6 +140,10 @@ public class AdminClientConfig extends AbstractConfig {\n     public static final String RETRIES_CONFIG = CommonClientConfigs.RETRIES_CONFIG;\n     public static final String DEFAULT_API_TIMEOUT_MS_CONFIG = CommonClientConfigs.DEFAULT_API_TIMEOUT_MS_CONFIG;\n \n+    public static final String METADATA_RECOVERY_STRATEGY_CONFIG = CommonClientConfigs.METADATA_RECOVERY_STRATEGY_CONFIG;\n+    public static final String METADATA_RECOVERY_STRATEGY_DOC = CommonClientConfigs.METADATA_RECOVERY_STRATEGY_DOC;\n+    public static final String DEFAULT_METADATA_RECOVERY_STRATEGY = CommonClientConfigs.DEFAULT_METADATA_RECOVERY_STRATEGY;\n+\n     /**\n      * <code>security.providers</code>\n      */\n@@ -262,7 +267,14 @@ public class AdminClientConfig extends AbstractConfig {\n                                         Importance.MEDIUM,\n                                         SECURITY_PROTOCOL_DOC)\n                                 .withClientSslSupport()\n-                                .withClientSaslSupport();\n+                                .withClientSaslSupport()\n+                                .define(METADATA_RECOVERY_STRATEGY_CONFIG,\n+                                        Type.STRING,\n+                                        DEFAULT_METADATA_RECOVERY_STRATEGY,\n+                                        ConfigDef.CaseInsensitiveValidString\n+                                                .in(Utils.enumOptions(MetadataRecoveryStrategy.class)),\n+                                        Importance.LOW,\n+                                        METADATA_RECOVERY_STRATEGY_DOC);\n     }\n \n     @Override\n\n@@ -32,7 +32,7 @@\n  */\n @InterfaceStability.Evolving\n public class CreateTopicsResult {\n-    final static int UNKNOWN = -1;\n+    static final int UNKNOWN = -1;\n \n     private final Map<String, KafkaFuture<TopicMetadataAndConfig>> futures;\n \n\n@@ -66,7 +66,7 @@ public KafkaFuture<Map<TopicPartitionReplica, ReplicaLogDirInfo>> all() {\n             });\n     }\n \n-    static public class ReplicaLogDirInfo {\n+    public static class ReplicaLogDirInfo {\n         // The current log directory of the replica of this partition on the given broker.\n         // Null if no replica is not found for this partition on the given broker.\n         private final String currentReplicaLogDir;\n\n@@ -28,5 +28,5 @@\n  * The API of this class is evolving, see {@link Admin} for details.\n  */\n @InterfaceStability.Evolving\n-final public class ElectLeadersOptions extends AbstractOptions<ElectLeadersOptions> {\n+public final class ElectLeadersOptions extends AbstractOptions<ElectLeadersOptions> {\n }\n\n@@ -34,7 +34,7 @@\n  * The API of this class is evolving, see {@link Admin} for details.\n  */\n @InterfaceStability.Evolving\n-final public class ElectLeadersResult {\n+public final class ElectLeadersResult {\n     private final KafkaFuture<Map<TopicPartition, Optional<Throwable>>> electionFuture;\n \n     ElectLeadersResult(KafkaFuture<Map<TopicPartition, Optional<Throwable>>> electionFuture) {\n\n@@ -25,6 +25,8 @@\n import org.apache.kafka.clients.DefaultHostResolver;\n import org.apache.kafka.clients.HostResolver;\n import org.apache.kafka.clients.KafkaClient;\n+import org.apache.kafka.clients.LeastLoadedNode;\n+import org.apache.kafka.clients.MetadataRecoveryStrategy;\n import org.apache.kafka.clients.NetworkClient;\n import org.apache.kafka.clients.StaleMetadataException;\n import org.apache.kafka.clients.admin.CreateTopicsResult.TopicMetadataAndConfig;\n@@ -398,6 +400,7 @@ public class KafkaAdminClient extends AdminClient {\n     private final long retryBackoffMaxMs;\n     private final ExponentialBackoff retryBackoff;\n     private final boolean clientTelemetryEnabled;\n+    private final MetadataRecoveryStrategy metadataRecoveryStrategy;\n \n     /**\n      * The telemetry requests client instance id.\n@@ -611,6 +614,7 @@ private KafkaAdminClient(AdminClientConfig config,\n             retryBackoffMaxMs,\n             CommonClientConfigs.RETRY_BACKOFF_JITTER);\n         this.clientTelemetryEnabled = config.getBoolean(AdminClientConfig.ENABLE_METRICS_PUSH_CONFIG);\n+        this.metadataRecoveryStrategy = MetadataRecoveryStrategy.forName(config.getString(AdminClientConfig.METADATA_RECOVERY_STRATEGY_CONFIG));\n         config.logUnused();\n         AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());\n         log.debug(\"Kafka admin client initialized\");\n@@ -697,7 +701,13 @@ private interface NodeProvider {\n     private class MetadataUpdateNodeIdProvider implements NodeProvider {\n         @Override\n         public Node provide() {\n-            return client.leastLoadedNode(time.milliseconds());\n+            LeastLoadedNode leastLoadedNode = client.leastLoadedNode(time.milliseconds());\n+            if (metadataRecoveryStrategy == MetadataRecoveryStrategy.REBOOTSTRAP\n+                    && !leastLoadedNode.hasNodeAvailableOrConnectionReady()) {\n+                metadataManager.rebootstrap(time.milliseconds());\n+            }\n+\n+            return leastLoadedNode.node();\n         }\n \n         @Override\n@@ -779,7 +789,7 @@ public Node provide() {\n             if (metadataManager.isReady()) {\n                 // This may return null if all nodes are busy.\n                 // In that case, we will postpone node assignment.\n-                return client.leastLoadedNode(time.milliseconds());\n+                return client.leastLoadedNode(time.milliseconds()).node();\n             }\n             metadataManager.requestUpdate();\n             return null;\n@@ -834,7 +844,7 @@ public Node provide() {\n                 } else {\n                     // This may return null if all nodes are busy.\n                     // In that case, we will postpone node assignment.\n-                    return client.leastLoadedNode(time.milliseconds());\n+                    return client.leastLoadedNode(time.milliseconds()).node();\n                 }\n             }\n             metadataManager.requestUpdate();\n@@ -2245,7 +2255,7 @@ void handleResponse(AbstractResponse abstractResponse) {\n                         continue;\n                     }\n \n-                    TopicDescription currentTopicDescription = getTopicDescriptionFromDescribeTopicsResponseTopic(topic, nodes);\n+                    TopicDescription currentTopicDescription = getTopicDescriptionFromDescribeTopicsResponseTopic(topic, nodes, options.includeAuthorizedOperations());\n \n                     if (partiallyFinishedTopicDescription != null && partiallyFinishedTopicDescription.name().equals(topicName)) {\n                         // Add the partitions for the cursor topic of the previous batch.\n@@ -2326,7 +2336,7 @@ private Map<String, KafkaFuture<TopicDescription>> handleDescribeTopicsByNamesWi\n         clusterResult.nodes().whenComplete(\n             (nodes, exception) -> {\n                 if (exception != null) {\n-                    completeAllExceptionally(topicFutures.values(), exception.getCause());\n+                    completeAllExceptionally(topicFutures.values(), exception);\n                     return;\n                 }\n \n@@ -2408,14 +2418,16 @@ void handleFailure(Throwable throwable) {\n \n     private TopicDescription getTopicDescriptionFromDescribeTopicsResponseTopic(\n         DescribeTopicPartitionsResponseTopic topic,\n-        Map<Integer, Node> nodes\n+        Map<Integer, Node> nodes,\n+        boolean includeAuthorizedOperations\n     ) {\n         List<DescribeTopicPartitionsResponsePartition> partitionInfos = topic.partitions();\n         List<TopicPartitionInfo> partitions = new ArrayList<>(partitionInfos.size());\n         for (DescribeTopicPartitionsResponsePartition partitionInfo : partitionInfos) {\n             partitions.add(DescribeTopicPartitionsResponse.partitionToTopicPartitionInfo(partitionInfo, nodes));\n         }\n-        return new TopicDescription(topic.name(), topic.isInternal(), partitions, validAclOperations(topic.topicAuthorizedOperations()), topic.topicId());\n+        Set<AclOperation> authorisedOperations = includeAuthorizedOperations ? validAclOperations(topic.topicAuthorizedOperations()) : null;\n+        return new TopicDescription(topic.name(), topic.isInternal(), partitions, authorisedOperations, topic.topicId());\n     }\n \n     private TopicDescription getTopicDescriptionFromCluster(Cluster cluster, String topicName, Uuid topicId,\n@@ -3471,7 +3483,7 @@ private Set<AclOperation> validAclOperations(final int authorizedOperations) {\n             .collect(Collectors.toSet());\n     }\n \n-    private final static class ListConsumerGroupsResults {\n+    private static final class ListConsumerGroupsResults {\n         private final List<Throwable> errors;\n         private final HashMap<String, ConsumerGroupListing> listings;\n         private final HashSet<Node> remaining;\n@@ -4414,12 +4426,13 @@ public DescribeMetadataQuorumResult describeMetadataQuorum(DescribeMetadataQuoru\n             private QuorumInfo.ReplicaState translateReplicaState(DescribeQuorumResponseData.ReplicaState replica) {\n                 return new QuorumInfo.ReplicaState(\n                         replica.replicaId(),\n+                        replica.replicaDirectoryId() == null ? Uuid.ZERO_UUID : replica.replicaDirectoryId(),\n                         replica.logEndOffset(),\n                         replica.lastFetchTimestamp() == -1 ? OptionalLong.empty() : OptionalLong.of(replica.lastFetchTimestamp()),\n                         replica.lastCaughtUpTimestamp() == -1 ? OptionalLong.empty() : OptionalLong.of(replica.lastCaughtUpTimestamp()));\n             }\n \n-            private QuorumInfo createQuorumResult(final DescribeQuorumResponseData.PartitionData partition) {\n+            private QuorumInfo createQuorumResult(final DescribeQuorumResponseData.PartitionData partition, DescribeQuorumResponseData.NodeCollection nodeCollection) {\n                 List<QuorumInfo.ReplicaState> voters = partition.currentVoters().stream()\n                     .map(this::translateReplicaState)\n                     .collect(Collectors.toList());\n@@ -4428,12 +4441,21 @@ private QuorumInfo createQuorumResult(final DescribeQuorumResponseData.Partition\n                     .map(this::translateReplicaState)\n                     .collect(Collectors.toList());\n \n+                Map<Integer, QuorumInfo.Node> nodes = nodeCollection.stream().map(n -> {\n+                    List<RaftVoterEndpoint> endpoints = n.listeners().stream()\n+                        .map(l -> new RaftVoterEndpoint(l.name(), l.host(), l.port()))\n+                        .collect(Collectors.toList());\n+\n+                    return new QuorumInfo.Node(n.nodeId(), endpoints);\n+                }).collect(Collectors.toMap(QuorumInfo.Node::nodeId, Function.identity()));\n+\n                 return new QuorumInfo(\n                     partition.leaderId(),\n                     partition.leaderEpoch(),\n                     partition.highWatermark(),\n                     voters,\n-                    observers\n+                    observers,\n+                    nodes\n                 );\n             }\n \n@@ -4447,7 +4469,7 @@ DescribeQuorumRequest.Builder createRequest(int timeoutMs) {\n             void handleResponse(AbstractResponse response) {\n                 final DescribeQuorumResponse quorumResponse = (DescribeQuorumResponse) response;\n                 if (quorumResponse.data().errorCode() != Errors.NONE.code()) {\n-                    throw Errors.forCode(quorumResponse.data().errorCode()).exception();\n+                    throw Errors.forCode(quorumResponse.data().errorCode()).exception(quorumResponse.data().errorMessage());\n                 }\n                 if (quorumResponse.data().topics().size() != 1) {\n                     String msg = String.format(\"DescribeMetadataQuorum received %d topics when 1 was expected\",\n@@ -4476,9 +4498,9 @@ void handleResponse(AbstractResponse response) {\n                     throw new UnknownServerException(msg);\n                 }\n                 if (partition.errorCode() != Errors.NONE.code()) {\n-                    throw Errors.forCode(partition.errorCode()).exception();\n+                    throw Errors.forCode(partition.errorCode()).exception(partition.errorMessage());\n                 }\n-                future.complete(createQuorumResult(partition));\n+                future.complete(createQuorumResult(partition, quorumResponse.data().nodes()));\n             }\n \n             @Override\n\n@@ -16,7 +16,10 @@\n  */\n package org.apache.kafka.clients.admin;\n \n+import org.apache.kafka.common.Uuid;\n+\n import java.util.List;\n+import java.util.Map;\n import java.util.Objects;\n import java.util.OptionalLong;\n \n@@ -29,19 +32,22 @@ public class QuorumInfo {\n     private final long highWatermark;\n     private final List<ReplicaState> voters;\n     private final List<ReplicaState> observers;\n+    private final Map<Integer, Node> nodes;\n \n     QuorumInfo(\n         int leaderId,\n         long leaderEpoch,\n         long highWatermark,\n         List<ReplicaState> voters,\n-        List<ReplicaState> observers\n+        List<ReplicaState> observers,\n+        Map<Integer, Node> nodes\n     ) {\n         this.leaderId = leaderId;\n         this.leaderEpoch = leaderEpoch;\n         this.highWatermark = highWatermark;\n         this.voters = voters;\n         this.observers = observers;\n+        this.nodes = nodes;\n     }\n \n     public int leaderId() {\n@@ -64,6 +70,13 @@ public List<ReplicaState> observers() {\n         return observers;\n     }\n \n+    /**\n+     * @return The voter nodes in the Raft cluster, or an empty map if KIP-853 is not enabled.\n+     */\n+    public Map<Integer, Node> nodes() {\n+        return nodes;\n+    }\n+\n     @Override\n     public boolean equals(Object o) {\n         if (this == o) return true;\n@@ -73,12 +86,13 @@ public boolean equals(Object o) {\n             && leaderEpoch == that.leaderEpoch\n             && highWatermark == that.highWatermark\n             && Objects.equals(voters, that.voters)\n-            && Objects.equals(observers, that.observers);\n+            && Objects.equals(observers, that.observers)\n+            && Objects.equals(nodes, that.nodes);\n     }\n \n     @Override\n     public int hashCode() {\n-        return Objects.hash(leaderId, leaderEpoch, highWatermark, voters, observers);\n+        return Objects.hash(leaderId, leaderEpoch, highWatermark, voters, observers, nodes);\n     }\n \n     @Override\n@@ -89,26 +103,30 @@ public String toString() {\n             \", highWatermark=\" + highWatermark +\n             \", voters=\" + voters +\n             \", observers=\" + observers +\n+            \", nodes=\" + nodes +\n             ')';\n     }\n \n     public static class ReplicaState {\n         private final int replicaId;\n+        private final Uuid replicaDirectoryId;\n         private final long logEndOffset;\n         private final OptionalLong lastFetchTimestamp;\n         private final OptionalLong lastCaughtUpTimestamp;\n \n         ReplicaState() {\n-            this(0, 0, OptionalLong.empty(), OptionalLong.empty());\n+            this(0, Uuid.ZERO_UUID, 0, OptionalLong.empty(), OptionalLong.empty());\n         }\n \n         ReplicaState(\n             int replicaId,\n+            Uuid replicaDirectoryId,\n             long logEndOffset,\n             OptionalLong lastFetchTimestamp,\n             OptionalLong lastCaughtUpTimestamp\n         ) {\n             this.replicaId = replicaId;\n+            this.replicaDirectoryId = replicaDirectoryId;\n             this.logEndOffset = logEndOffset;\n             this.lastFetchTimestamp = lastFetchTimestamp;\n             this.lastCaughtUpTimestamp = lastCaughtUpTimestamp;\n@@ -122,6 +140,13 @@ public int replicaId() {\n             return replicaId;\n         }\n \n+        /**\n+         * Return the directory id of the replica if configured, or Uuid.ZERO_UUID if not.\n+         */\n+        public Uuid replicaDirectoryId() {\n+            return replicaDirectoryId;\n+        }\n+\n         /**\n          * Return the logEndOffset known by the leader for this replica.\n          * @return The logEndOffset for this replica\n@@ -154,24 +179,65 @@ public boolean equals(Object o) {\n             if (o == null || getClass() != o.getClass()) return false;\n             ReplicaState that = (ReplicaState) o;\n             return replicaId == that.replicaId\n+                && Objects.equals(replicaDirectoryId, that.replicaDirectoryId)\n                 && logEndOffset == that.logEndOffset\n                 && lastFetchTimestamp.equals(that.lastFetchTimestamp)\n                 && lastCaughtUpTimestamp.equals(that.lastCaughtUpTimestamp);\n         }\n \n         @Override\n         public int hashCode() {\n-            return Objects.hash(replicaId, logEndOffset, lastFetchTimestamp, lastCaughtUpTimestamp);\n+            return Objects.hash(replicaId, replicaDirectoryId, logEndOffset, lastFetchTimestamp, lastCaughtUpTimestamp);\n         }\n \n         @Override\n         public String toString() {\n             return \"ReplicaState(\" +\n                 \"replicaId=\" + replicaId +\n+                \", replicaDirectoryId=\" + replicaDirectoryId +\n                 \", logEndOffset=\" + logEndOffset +\n                 \", lastFetchTimestamp=\" + lastFetchTimestamp +\n                 \", lastCaughtUpTimestamp=\" + lastCaughtUpTimestamp +\n                 ')';\n         }\n     }\n+\n+    public static class Node {\n+        private final int nodeId;\n+        private final List<RaftVoterEndpoint> endpoints;\n+\n+        Node(int nodeId, List<RaftVoterEndpoint> endpoints) {\n+            this.nodeId = nodeId;\n+            this.endpoints = endpoints;\n+        }\n+\n+        public int nodeId() {\n+            return nodeId;\n+        }\n+\n+        public List<RaftVoterEndpoint> endpoints() {\n+            return endpoints;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) return true;\n+            if (o == null || getClass() != o.getClass()) return false;\n+            Node node = (Node) o;\n+            return nodeId == node.nodeId && Objects.equals(endpoints, node.endpoints);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(nodeId, endpoints);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Node{\" +\n+                \"nodeId=\" + nodeId +\n+                \", endpoints=\" + endpoints +\n+                '}';\n+        }\n+    }\n }\n\n@@ -34,7 +34,7 @@ public enum TransactionState {\n     PREPARE_EPOCH_FENCE(\"PrepareEpochFence\"),\n     UNKNOWN(\"Unknown\");\n \n-    private final static Map<String, TransactionState> NAME_TO_ENUM = Arrays.stream(values())\n+    private static final Map<String, TransactionState> NAME_TO_ENUM = Arrays.stream(values())\n         .collect(Collectors.toMap(state -> state.name, Function.identity()));\n \n     private final String name;\n\n@@ -27,7 +27,7 @@\n import java.util.List;\n import java.util.Objects;\n \n-final public class AdminBootstrapAddresses {\n+public final class AdminBootstrapAddresses {\n     private final boolean usingBootstrapControllers;\n     private final List<InetSocketAddress> addresses;\n \n\n@@ -92,6 +92,11 @@ public class AdminMetadataManager {\n      */\n     private ApiException fatalException = null;\n \n+    /**\n+     * The cluster with which the metadata was bootstrapped.\n+     */\n+    private Cluster bootstrapCluster;\n+\n     public class AdminMetadataUpdater implements MetadataUpdater {\n         @Override\n         public List<Node> fetchNodes() {\n@@ -275,6 +280,7 @@ public void updateFailed(Throwable exception) {\n     public void update(Cluster cluster, long now) {\n         if (cluster.isBootstrapConfigured()) {\n             log.debug(\"Setting bootstrap cluster metadata {}.\", cluster);\n+            bootstrapCluster = cluster;\n         } else {\n             log.debug(\"Updating cluster metadata to {}\", cluster);\n             this.lastMetadataUpdateMs = now;\n@@ -287,4 +293,12 @@ public void update(Cluster cluster, long now) {\n             this.cluster = cluster;\n         }\n     }\n+\n+    /**\n+     * Rebootstrap metadata with the cluster previously used for bootstrapping.\n+     */\n+    public void rebootstrap(long now) {\n+        log.info(\"Rebootstrapping with {}\", this.bootstrapCluster);\n+        update(bootstrapCluster, now);\n+    }\n }\n\n@@ -134,6 +134,10 @@ private ApiResult<CoordinatorKey, ProducerIdAndEpoch> handleError(\n                                 \"coordinator is still in the process of loading state. Will retry\",\n                         transactionalIdKey.idValue);\n                 return ApiResult.empty();\n+            case CONCURRENT_TRANSACTIONS:\n+                log.debug(\"InitProducerId request for transactionalId `{}` failed because of \" +\n+                                \"a concurrent transaction. Will retry\", transactionalIdKey.idValue);\n+                return ApiResult.empty();\n \n             case NOT_COORDINATOR:\n             case COORDINATOR_NOT_AVAILABLE:\n\n@@ -18,6 +18,7 @@\n \n import org.apache.kafka.clients.ClientDnsLookup;\n import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.MetadataRecoveryStrategy;\n import org.apache.kafka.common.IsolationLevel;\n import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n@@ -656,7 +657,14 @@ public class ConsumerConfig extends AbstractConfig {\n                                         Importance.MEDIUM,\n                                         CommonClientConfigs.SECURITY_PROTOCOL_DOC)\n                                 .withClientSslSupport()\n-                                .withClientSaslSupport();\n+                                .withClientSaslSupport()\n+                                .define(CommonClientConfigs.METADATA_RECOVERY_STRATEGY_CONFIG,\n+                                        Type.STRING,\n+                                        CommonClientConfigs.DEFAULT_METADATA_RECOVERY_STRATEGY,\n+                                        ConfigDef.CaseInsensitiveValidString\n+                                                .in(Utils.enumOptions(MetadataRecoveryStrategy.class)),\n+                                        Importance.LOW,\n+                                        CommonClientConfigs.METADATA_RECOVERY_STRATEGY_DOC);\n     }\n \n     @Override\n\n@@ -26,10 +26,10 @@\n  * Note: Any change to this class is considered public and requires a KIP.\n  */\n public class ConsumerGroupMetadata {\n-    final private String groupId;\n-    final private int generationId;\n-    final private String memberId;\n-    final private Optional<String> groupInstanceId;\n+    private final String groupId;\n+    private final int generationId;\n+    private final String memberId;\n+    private final Optional<String> groupInstanceId;\n \n     public ConsumerGroupMetadata(String groupId,\n                                  int generationId,\n\n@@ -523,7 +523,7 @@\n  */\n public class KafkaConsumer<K, V> implements Consumer<K, V> {\n \n-    private final static ConsumerDelegateCreator CREATOR = new ConsumerDelegateCreator();\n+    private static final ConsumerDelegateCreator CREATOR = new ConsumerDelegateCreator();\n \n     private final ConsumerDelegate<K, V> delegate;\n \n@@ -1803,4 +1803,4 @@ KafkaConsumerMetrics kafkaConsumerMetrics() {\n     boolean updateAssignmentMetadataIfNeeded(final Timer timer) {\n         return delegate.updateAssignmentMetadataIfNeeded(timer);\n     }\n-}\n\\ No newline at end of file\n+}\n\n@@ -332,7 +332,7 @@\n @InterfaceStability.Evolving\n public class KafkaShareConsumer<K, V> implements ShareConsumer<K, V> {\n \n-    private final static ShareConsumerDelegateCreator CREATOR = new ShareConsumerDelegateCreator();\n+    private static final ShareConsumerDelegateCreator CREATOR = new ShareConsumerDelegateCreator();\n \n     private final ShareConsumerDelegate<K, V> delegate;\n \n\n@@ -85,7 +85,7 @@ public MemberData(List<TopicPartition> partitions, Optional<Integer> generation)\n         }\n     }\n \n-    abstract protected MemberData memberData(Subscription subscription);\n+    protected abstract MemberData memberData(Subscription subscription);\n \n     @Override\n     public Map<String, List<TopicPartition>> assignPartitions(Map<String, List<PartitionInfo>> partitionsPerTopic,\n\n@@ -331,7 +331,8 @@ private void process(final ConsumerRebalanceListenerCallbackNeededEvent event) {\n                     apiVersions,\n                     metrics,\n                     fetchMetricsManager,\n-                    clientTelemetryReporter.map(ClientTelemetryReporter::telemetrySender).orElse(null));\n+                    clientTelemetryReporter.map(ClientTelemetryReporter::telemetrySender).orElse(null),\n+                    backgroundEventHandler);\n             this.offsetCommitCallbackInvoker = new OffsetCommitCallbackInvoker(interceptors);\n             this.asyncCommitFenced = new AtomicBoolean(false);\n             this.groupMetadata.set(initializeGroupMetadata(config, groupRebalanceConfig));\n@@ -504,7 +505,9 @@ private void process(final ConsumerRebalanceListenerCallbackNeededEvent event) {\n             time,\n             config,\n             logContext,\n-            client\n+            client,\n+            metadata,\n+            backgroundEventHandler\n         );\n         this.offsetCommitCallbackInvoker = new OffsetCommitCallbackInvoker(interceptors);\n         this.asyncCommitFenced = new AtomicBoolean(false);\n@@ -1231,7 +1234,7 @@ private void close(Duration timeout, boolean swallowException) {\n         clientTelemetryReporter.ifPresent(reporter -> reporter.initiateClose(timeout.toMillis()));\n         closeTimer.update();\n         // Prepare shutting down the network thread\n-        prepareShutdown(closeTimer, firstException);\n+        releaseAssignmentAndLeaveGroup(closeTimer, firstException);\n         closeTimer.update();\n         swallow(log, Level.ERROR, \"Failed invoking asynchronous commit callback.\",\n             () -> awaitPendingAsyncCommitsAndExecuteCommitCallbacks(closeTimer, false), firstException);\n@@ -1267,12 +1270,12 @@ private void close(Duration timeout, boolean swallowException) {\n      * 2. revoke all partitions\n      * 3. if partition revocation completes successfully, send leave group\n      */\n-    void prepareShutdown(final Timer timer, final AtomicReference<Throwable> firstException) {\n+    void releaseAssignmentAndLeaveGroup(final Timer timer, final AtomicReference<Throwable> firstException) {\n         if (!groupMetadata.get().isPresent())\n             return;\n \n         if (autoCommitEnabled)\n-            autoCommitSync(timer);\n+            commitSyncAllConsumed(timer);\n \n         applicationEventHandler.add(new CommitOnCloseEvent());\n         completeQuietly(\n@@ -1284,7 +1287,7 @@ void prepareShutdown(final Timer timer, final AtomicReference<Throwable> firstEx\n     }\n \n     // Visible for testing\n-    void autoCommitSync(final Timer timer) {\n+    void commitSyncAllConsumed(final Timer timer) {\n         Map<TopicPartition, OffsetAndMetadata> allConsumed = subscriptions.allConsumed();\n         log.debug(\"Sending synchronous auto-commit of offsets {} on closing\", allConsumed);\n         try {\n"}
{"id": 306, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[IFRT] Remove the error message mentioning an invalid output target\n\nA log message was being generated when an invalid output target was specified\nand being ignored. This caused some OSS tests to fail in a flaky manner, so\nlogging is removed in this change.\n\n\n### Code Patch:\n@@ -46,8 +46,8 @@ namespace {\n std::string GetDotDumpDir(std::string dot_graph_dump_to) {\n   if (dot_graph_dump_to == \"sponge\") {\n     if (!tsl::io::GetTestUndeclaredOutputsDir(&dot_graph_dump_to)) {\n-      LOG(ERROR) << \"compile option `dot_graph_dump_to=sponge` is specified \"\n-                    \"outside of a test; ignoring the value\";\n+      // Compile option `dot_graph_dump_to=sponge` is specified outside of a\n+      // test. Ignore the value.\n       return \"\";\n     }\n   }\n"}
{"id": 236, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n## Why are these changes needed?\r\n\r\nTSAN failure is a data race only in the test's `FakeShutdownExecutor`, not production code. Fake was writing shared `std::string` fields from two threads without synchronization -- https://buildkite.com/ray-project/postmerge/builds/12666#019907c0-3bdd-4401-9aa8-6f13215ce819/176-796\r\n\r\n- Added a `std::mutex` to `FakeShutdownExecutor` and guarded assignments to `last_exit_type` and `last_detail` in all `Execute*` methods. No production code changes.\r\n- Added mutex-guarded getters in `FakeShutdownExecutor` and used them in the assertion to eliminate remaining unsynchronized reads in the TSAN test.\r\n\r\n## Related issue number\r\n\r\nCloses #55801\r\n\n\n### Code Patch:\n@@ -18,6 +18,7 @@ ray_cc_test(\n     tags = [\"team:core\"],\n     deps = [\n         \"//src/ray/core_worker:shutdown_coordinator\",\n+        \"@com_google_absl//absl/synchronization\",\n         \"@com_google_googletest//:gtest\",\n         \"@com_google_googletest//:gtest_main\",\n     ],\n\n@@ -24,6 +24,7 @@\n #include <utility>\n #include <vector>\n \n+#include \"absl/synchronization/mutex.h\"\n #include \"ray/common/buffer.h\"\n #include \"src/ray/protobuf/common.pb.h\"\n \n@@ -41,42 +42,68 @@ class FakeShutdownExecutor : public ShutdownExecutorInterface {\n \n   std::string last_exit_type;\n   std::string last_detail;\n+  mutable absl::Mutex mu_;\n+\n+  std::string GetLastExitType() const {\n+    absl::MutexLock lk(&mu_);\n+    return last_exit_type;\n+  }\n+\n+  std::string GetLastDetail() const {\n+    absl::MutexLock lk(&mu_);\n+    return last_detail;\n+  }\n \n   void ExecuteGracefulShutdown(std::string_view exit_type,\n                                std::string_view detail,\n                                std::chrono::milliseconds timeout_ms) override {\n     graceful_calls++;\n-    last_exit_type = std::string(exit_type);\n-    last_detail = std::string(detail);\n+    {\n+      absl::MutexLock lk(&mu_);\n+      last_exit_type = std::string(exit_type);\n+      last_detail = std::string(detail);\n+    }\n   }\n   void ExecuteForceShutdown(std::string_view exit_type,\n                             std::string_view detail) override {\n     force_calls++;\n-    last_exit_type = std::string(exit_type);\n-    last_detail = std::string(detail);\n+    {\n+      absl::MutexLock lk(&mu_);\n+      last_exit_type = std::string(exit_type);\n+      last_detail = std::string(detail);\n+    }\n   }\n   void ExecuteWorkerExit(std::string_view exit_type,\n                          std::string_view detail,\n                          std::chrono::milliseconds timeout_ms) override {\n     worker_exit_calls++;\n-    last_exit_type = std::string(exit_type);\n-    last_detail = std::string(detail);\n+    {\n+      absl::MutexLock lk(&mu_);\n+      last_exit_type = std::string(exit_type);\n+      last_detail = std::string(detail);\n+    }\n   }\n   void ExecuteExit(std::string_view exit_type,\n                    std::string_view detail,\n                    std::chrono::milliseconds timeout_ms,\n                    const std::shared_ptr<::ray::LocalMemoryBuffer>\n                        &creation_task_exception_pb_bytes) override {\n     worker_exit_calls++;\n-    last_exit_type = std::string(exit_type);\n-    last_detail = std::string(detail);\n+    {\n+      absl::MutexLock lk(&mu_);\n+      last_exit_type = std::string(exit_type);\n+      last_detail = std::string(detail);\n+    }\n   }\n   void ExecuteHandleExit(std::string_view exit_type,\n                          std::string_view detail,\n                          std::chrono::milliseconds timeout_ms) override {\n     handle_exit_calls++;\n-    last_exit_type = std::string(exit_type);\n-    last_detail = std::string(detail);\n+    {\n+      absl::MutexLock lk(&mu_);\n+      last_exit_type = std::string(exit_type);\n+      last_detail = std::string(detail);\n+    }\n   }\n   void KillChildProcessesImmediately() override {}\n   bool ShouldWorkerIdleExit() const override { return idle_exit_allowed.load(); }\n@@ -366,7 +393,8 @@ TEST_F(ShutdownCoordinatorTest, Concurrent_DoubleForce_ForceExecutesOnce) {\n   // Verify that only one forced shutdown was called\n   EXPECT_EQ(fake_ptr->force_calls.load(), 1);\n   EXPECT_EQ(fake_ptr->graceful_calls.load(), 0);\n-  EXPECT_TRUE(fake_ptr->last_detail == \"force1\" || fake_ptr->last_detail == \"force2\");\n+  EXPECT_TRUE(fake_ptr->GetLastDetail() == \"force1\" ||\n+              fake_ptr->GetLastDetail() == \"force2\");\n }\n \n }  // namespace core\n"}
{"id": 234, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nTunableOp will sometimes find a less precise solution due to the small input vectors used in this UT. Bumping op tolerance to eliminate flakiness.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang\n\n### Code Patch:\n@@ -4762,6 +4762,7 @@ def test_matmul_small_brute_force_3d_Nd(self, device, dtype):\n     @onlyCUDA\n     @skipCUDAIfNotRocm  # Skipping due to SM89 OOM in CI, UT doesn't do much on NV anyways\n     @dtypes(*floating_types_and(torch.half))\n+    @precisionOverride({torch.float16: 1e-1})  # TunableOp may occasionally find less precise solution\n     def test_matmul_small_brute_force_tunableop(self, device, dtype):\n         # disable tunableop buffer rotation for all tests everywhere, it can be slow\n         # We set the TunableOp numerical check environment variable here because it is\n"}
{"id": 428, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n/kind flake\r\n<!--\r\nAdd one of the following kinds:\r\n/kind bug\r\n/kind cleanup\r\n/kind documentation\r\n/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n/kind api-change\r\n/kind deprecation\r\n/kind failing-test\r\n/kind flake\r\n/kind regression\r\n-->\r\n\r\n#### What this PR does / why we need it:\r\nThis pr fixes a race problem introduced by #128642\r\n\r\nWhen using `exit 0` as the command of a container. After the container is terminated,`finishedAt - startedAt` can be -1 second, which violates the validation after.\r\n\r\nRelated job: https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/129680/pull-kubernetes-e2e-gce/1933002039273459712\r\n#### Which issue(s) this PR is related to:\r\n<!--\r\nPlease link relevant issues to help with tracking.\r\n\r\nTo automatically close the linked issue(s) when this PR is merged,\r\nadd the word \"Fixes\" before the issue number or link.\r\nDo not use \"Fixes\" if the PR is of kind `failing-test` or `flake`.\r\n\r\nReference KEPs when applicable in addition to specific issues.\r\n\r\nExamples:\r\nFixes #<issue number>\r\n<issue link> (issue in a different repository)\r\nKEP: https://github.com/kubernetes/enhancements/issues/<kep-issue-number>\r\n\r\nIf there is no associated issue, then write \"N/A\".\r\n-->\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n\n### Code Patch:\n@@ -684,7 +684,10 @@ var _ = SIGDescribe(\"Lifecycle Sleep Hook\", func() {\n \t\t\tpodWithHook := getPodWithHook(name, imageutils.GetE2EImage(imageutils.BusyBox), lifecycle)\n \t\t\tpodWithHook.Spec.TerminationGracePeriodSeconds = ptr.To[int64](gracePeriod)\n \t\t\tpodWithHook.Spec.Containers[0].Command = []string{\"/bin/sh\"}\n-\t\t\tpodWithHook.Spec.Containers[0].Args = []string{\"-c\", \"exit 0\"}\n+\t\t\t// If we exit the container as soon as it's created,\n+\t\t\t// finishAt - startedAt can be negative due to some internal race\n+\t\t\t// so we need to keep it running for a while\n+\t\t\tpodWithHook.Spec.Containers[0].Args = []string{\"-c\", \"sleep 3\"}\n \t\t\tpodWithHook.Spec.RestartPolicy = v1.RestartPolicyNever\n \t\t\tginkgo.By(\"create the pod with lifecycle hook using sleep action\")\n \t\t\tp := podClient.Create(ctx, podWithHook)\n"}
{"id": 316, "ground_truth": "OS", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n### Motivation\n\nI'm making this change for clarification in the Getting Started guide which is the gateway for new users to Pulsar.\n### Modifications\n\nUpdated documentation.\n### Result\n\nJust the documentation.\n\n\n### Code Patch:\n@@ -122,6 +122,8 @@ while (true) {\n   // Acknowledge the message so that it can be deleted by broker\n   consumer.acknowledge(msg);\n }\n+\n+client.close();\n ```\n \n \n@@ -137,4 +139,6 @@ Producer producer = client.createProducer(\n for (int i = 0; i < 10; i++) {\n     producer.send(\"my-message\".getBytes());\n }\n+\n+client.close();\n ```\n"}
{"id": 367, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n<!-- Please make sure the target branch is right. In most case, the target branch should be `develop`. -->\r\n\r\n### Which Issue(s) This PR Fixes\r\n\r\n<!-- Please ensure that the related issue has already been created, and [link this pull request to that issue using keywords](<https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword>) to ensure automatic closure. -->\r\n\r\nFixes #7627\r\n\r\n### Brief Description\r\n\r\n<!-- Write a brief description for your pull request to help the maintainer understand the reasons behind your changes. -->\r\n\r\n### How Did You Test This Change?\r\n\r\n<!-- In order to ensure the code quality of Apache RocketMQ, we expect every pull request to have undergone thorough testing. -->\r\n\n\n### Code Patch:\n@@ -213,12 +213,15 @@ public void testSemiSyncReplicaWhenAdaptiveDegradation() throws Exception {\n             assertEquals(PutMessageStatus.PUT_OK, result.getPutMessageStatus());\n             //message has been replicated to slave's commitLog, but maybe not dispatch to ConsumeQueue yet\n             //so direct read from commitLog by physical offset\n-            MessageExt slaveMsg = slaveMessageStore.lookMessageByOffset(result.getAppendMessageResult().getWroteOffset());\n-            assertNotNull(slaveMsg);\n-            assertArrayEquals(msg.getBody(), slaveMsg.getBody());\n-            assertEquals(msg.getTopic(), slaveMsg.getTopic());\n-            assertEquals(msg.getTags(), slaveMsg.getTags());\n-            assertEquals(msg.getKeys(), slaveMsg.getKeys());\n+            final MessageExt[] slaveMsg = {null};\n+            await().atMost(Duration.ofSeconds(3)).until(() -> {\n+                slaveMsg[0] = slaveMessageStore.lookMessageByOffset(result.getAppendMessageResult().getWroteOffset());\n+                return slaveMsg[0] != null;\n+            });\n+            assertArrayEquals(msg.getBody(), slaveMsg[0].getBody());\n+            assertEquals(msg.getTopic(), slaveMsg[0].getTopic());\n+            assertEquals(msg.getTags(), slaveMsg[0].getTags());\n+            assertEquals(msg.getKeys(), slaveMsg[0].getKeys());\n         }\n \n         //shutdown slave, putMessage should return IN_SYNC_REPLICAS_NOT_ENOUGH\n"}
{"id": 260, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nFor example this test fails randomly and needs to be re-run multiple times: https://github.com/mastra-ai/mastra/actions/runs/15888293944/job/44806477266?pr=5482\r\n\r\nSeems to be flawed test logic (fixed here)\n\n### Code Patch:\n@@ -279,15 +279,18 @@ describe('Memory Streaming Tests', () => {\n             content: message,\n           });\n         });\n-        const uiMessages = result.current.messages;\n+\n+        // Wait for message count to increase\n         await waitFor(\n           () => {\n             expect(error).toBeNull();\n             expect(result.current.messages.length).toBeGreaterThan(messageCountBefore);\n           },\n-          { timeout: 1000 },\n+          { timeout: 2000 },\n         );\n \n+        // Get fresh reference to messages after all waits complete\n+        const uiMessages = result.current.messages;\n         const latestMessage = uiMessages.at(-1);\n         if (!latestMessage) throw new Error(`No latest message`);\n         if (\n"}
{"id": 296, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n[PR Linked Issue]\n**Build Scans:**\n- [elasticsearch-intake #27158 / part3](https://gradle-enterprise.elastic.co/s/akaigagd47vke)\n- [elasticsearch-periodic-platform-support #10137 / rocky-9_platform-support-unix](https://gradle-enterprise.elastic.co/s/7266jgkv5xaqy)\n\n**Reproduction Line:**\n```\n./gradlew \":x-pack:plugin:esql:internalClusterTest\" --tests \"org.elasticsearch.xpack.esql.action.RandomizedTimeSeriesIT.testGroupByNothing\" -Dtests.seed=3050CE9CE9791881 -Dtests.locale=sa -Dtests.timezone=Australia/LHI -Druntime.java=24\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nN/A\n\n**Failure History:**\n[See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.xpack.esql.action.RandomizedTimeSeriesIT),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!('testGroupByNothing'),title:'Test',type:optionsListControl)))))\n\n**Failure Message:**\n```\norg.elasticsearch.action.bulk.IndexDocFailureStoreStatus$ExceptionWithFailureStoreStatus: [.ds-tsit_ds-2025.08.20-000001/TEfC-UbzS46u74MtBQWqXA][[.ds-tsit_ds-2025.08.20-000001][0]] org.elasticsearch.index.engine.VersionConflictEngineException: [C49S8X6g-QjW9uynAAABmMd3qH4][KBkJUGhjrYx0kKOigCXrLeK0RcVVTL_itQLi69ku_AIJqqsrijofpOU@2025-08-20T12:32:44.670Z]: version conflict, document already exists (current version [1])\n```\n\n**Issue Reasons:**\n- [main] 2 failures in test testGroupByNothing (1.7% fail rate in 118 executions)\n\n**Note:**\nThis issue was created using new test triage automation. Please report issues or feedback to es-delivery.\n\n### Code Patch:\n@@ -546,12 +546,6 @@ tests:\n - class: org.elasticsearch.gradle.internal.transport.TransportVersionValidationFuncTest\n   method: definitions have primary ids which cannot change\n   issue: https://github.com/elastic/elasticsearch/issues/133131\n-- class: org.elasticsearch.xpack.esql.action.RandomizedTimeSeriesIT\n-  method: testGroupBySubset\n-  issue: https://github.com/elastic/elasticsearch/issues/133220\n-- class: org.elasticsearch.xpack.esql.action.RandomizedTimeSeriesIT\n-  method: testGroupByNothing\n-  issue: https://github.com/elastic/elasticsearch/issues/133225\n - class: org.elasticsearch.gradle.internal.transport.TransportVersionValidationFuncTest\n   method: named and unreferenced definitions cannot have the same name\n   issue: https://github.com/elastic/elasticsearch/issues/133255\n\n@@ -183,10 +183,10 @@ public void populateIndex() throws IOException {\n             if (documents == null) {\n                 documents = new ArrayList<>();\n             }\n-            documents.add(document);\n             var indexRequest = client().prepareIndex(DATASTREAM_NAME).setOpType(DocWriteRequest.OpType.CREATE).setSource(document);\n             indexRequest.setRefreshPolicy(org.elasticsearch.action.support.WriteRequest.RefreshPolicy.IMMEDIATE);\n             indexRequest.get();\n+            documents.add(document);\n         }\n     }\n \n\n@@ -24,6 +24,7 @@\n \n import java.io.IOException;\n import java.time.Instant;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n@@ -65,6 +66,29 @@ private static Object randomDimensionValue(String dimensionName) {\n             return dimensionsInMetric.stream().map(attr -> new Tuple<>(attr, randomDimensionValue(attr))).collect(Collectors.toList());\n         }).toList();\n \n+        // We want to ensure that all documents have different timestamps.\n+        var now = Instant.now();\n+        var timestampSet = new HashSet<Instant>();\n+        var regens = 0;\n+        for (int i = 0; i < numDocs; i++) {\n+            // Random timestamps within the last 90 days.\n+            while (true) {\n+                var randomIns = Instant.ofEpochMilli(\n+                    ESTestCase.randomLongBetween(now.minusSeconds(60 * 60 * 2).toEpochMilli(), now.toEpochMilli())\n+                );\n+                if (timestampSet.add(randomIns)) {\n+                    break;\n+                }\n+                regens++;\n+                if (regens > numDocs) {\n+                    throw new IllegalStateException(\"Too many collisions when generating timestamps\");\n+                }\n+            }\n+        }\n+        // Timestampset should have exactly numDocs entries - this works as long as the random number generator\n+        // does not cycle early.\n+        assert timestampSet.size() == numDocs : \"Expected [\" + numDocs + \"] timestamps but got [\" + timestampSet.size() + \"]\";\n+        var timestampsIter = timestampSet.iterator();\n         spec = DataGeneratorSpecification.builder()\n             .withMaxFieldCountPerLevel(0)\n             .withPredefinedFields(\n@@ -73,7 +97,7 @@ private static Object randomDimensionValue(String dimensionName) {\n                         \"@timestamp\",\n                         FieldType.DATE,\n                         Map.of(\"type\", \"date\"),\n-                        fieldMapping -> ESTestCase.randomInstantBetween(Instant.now().minusSeconds(2 * 60 * 60), Instant.now())\n+                        fieldMapping -> timestampsIter.next()\n                     ),\n                     new PredefinedField.WithGenerator(\n                         \"attributes\",\n"}
{"id": 289, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\n\r\nFixes #1291\r\n\r\n### Motivation\r\n\r\nThe `MaxTime` setting for AckGroupingTracker is too short at only 10,000 nanoseconds.\r\nThis causes the test very flaky.\r\n\r\n### Modifications\r\n\r\n- Change the `MaxTime` to 10 seconds\r\n\r\n### Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n\r\n### Does this pull request potentially affect one of the following parts:\r\n\r\n*If `yes` was chosen, please highlight the changes*\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / no)\r\n  - The public API: (yes / no)\r\n  - The schema: (yes / no / don't know)\r\n  - The default values of configurations: (yes / no)\r\n  - The wire protocol: (yes / no)\r\n\r\n### Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / GoDocs / not documented)\r\n  - If a feature is not applicable for documentation, explain why?\r\n  - If a feature is not documented yet in this PR, please create a followup issue for adding the documentation\r\n\n\n### Code Patch:\n@@ -243,7 +243,7 @@ func TestCloseFlushWithoutTimer(t *testing.T) {\n func TestCloseFlushWithTimer(t *testing.T) {\n \tvar acker mockAcker\n \ttracker := newAckGroupingTracker(\n-\t\t&AckGroupingOptions{MaxSize: 1000, MaxTime: 10 * 1000},\n+\t\t&AckGroupingOptions{MaxSize: 1000, MaxTime: 10 * time.Second},\n \t\tnil,\n \t\tfunc(id MessageID) { acker.ackCumulative(id) },\n \t\tfunc(ids []*pb.MessageIdData) { acker.ack(ids) },\n"}
{"id": 126, "ground_truth": "Resource Leak", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\n{'Test order dependency': 'The test outcome depends on the order in which the tests are run.', 'OS': 'Tests can be flaky due to operating system-level issues, such as dependencies on specific system types or configurations.', 'Floating point operations': 'Dealing with floating point operations can lead to non-determinism.', 'Resource leak': 'A resource leak occurs whenever the application does not properly manage (acquire or release) one or more of its resources, e.g., memory allocations or database connections, leading to intermittent test failures.', 'Async wait': 'The test execution makes an asynchronous call and does not properly wait for the result of the call to become available before using it.', 'Concurrency': 'In Java, The test non-determinism is due to diÔ¨Äerent threads interacting in a non-desirable manner (but not due to asynchronous calls from the Async Wait category), e.g., due to data races, atomicity violations, or deadlocks. In some other languages, such as Python or Rust, flakiness is more common due to multiple tests running concurrently on different threads (e.g., when using the `pytest-xdist` plugin or `cargo test`).', 'Network': 'Tests whose execution depends on network can be flaky because the network is a resource that is hard to control, e.g., due to remote connection failures or local bad socket management.', 'Time': 'Tests that depend on system time or platform time can fail non-deterministically due to time zone changes, precision of time reported, or other time-related issues.', 'I/O': 'I/O operations can introduce flakiness when resources like files are not properly managed.', 'Randomness': 'The use of random objects can make some tests flaky.', 'Unordered collections': 'Tests that assume a specific order in unordered collections can be flaky.'}\n\nRespond only with the exact category name. The category has some detailed descriptions to help you understand, but do not get confused by them.\n\n### Issue Description:\nfixes #36695\r\n\n\n### Code Patch:\n@@ -8,7 +8,6 @@ import (\n \t. \"bytes\"\n \t\"io\"\n \t\"math/rand\"\n-\t\"runtime\"\n \t\"testing\"\n \t\"unicode/utf8\"\n )\n@@ -501,14 +500,13 @@ func TestGrow(t *testing.T) {\n \t\t\tbuf := NewBuffer(xBytes)\n \t\t\t// If we read, this affects buf.off, which is good to test.\n \t\t\treadBytes, _ := buf.Read(tmp)\n-\t\t\tbuf.Grow(growLen)\n \t\t\tyBytes := Repeat(y, growLen)\n+\t\t\tallocs := testing.AllocsPerRun(100, func() {\n+\t\t\t\tbuf.Grow(growLen)\n+\t\t\t\tbuf.Write(yBytes)\n+\t\t\t})\n \t\t\t// Check no allocation occurs in write, as long as we're single-threaded.\n-\t\t\tvar m1, m2 runtime.MemStats\n-\t\t\truntime.ReadMemStats(&m1)\n-\t\t\tbuf.Write(yBytes)\n-\t\t\truntime.ReadMemStats(&m2)\n-\t\t\tif runtime.GOMAXPROCS(-1) == 1 && m1.Mallocs != m2.Mallocs {\n+\t\t\tif allocs != 0 {\n \t\t\t\tt.Errorf(\"allocation occurred during write\")\n \t\t\t}\n \t\t\t// Check that buffer has correct data.\n"}
