{"id": 4, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nCurrently `fedimint-cli spend <amount>` will give us `<amount>` if it has those notes on hand, and give more than we ask for if it doesn't.\r\n\r\nI was just writing a test and this behavior caused my test to be flaky. It would be nice for this reason (and probably in general) to be able to request exact amounts from `fedimint-cli spend`. \n"}
{"id": 9, "ground_truth": "Logic", "predicted": "Unordered data", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nCurrent algo to merge default CLI options with custom ones is primitive and flaky. E.g. it relies on sorting which doesn't work well for this use case in the first place.\n"}
{"id": 78, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nAfter a1e25956b6e04e9c678293318967db3f2e4b905a some of the cancellation tests have become a bit flaky. It seems io_uring doesn't behave consistent when `IOSQE_ASYNC` is and isn't set, but I'm a 100% sure yet.\n"}
{"id": 81, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI am attempting to switch from the standard library's RwLock to parking_lot's. Unfortunately when making this switch thread sanitizer started to complain about data races. I was able to isolate that this is just a matter of the RwLock, since this diff fixes the issue by switching to std::sync::RwLock - https://github.com/matanmarkind/active_standby/commit/bb1cd755e37d9ebb42bc0871a721d4d492dd59c7.\r\n\r\nChecking out the base commit, ee51de677f1722b9b1a0ec70ac9e9462d87bf4a1, and running the following results in failures about half the time with parking_lot, while never failing with the std RwLock.\r\n\r\n`\r\nRUST_BACKTRACE=full RUSTFLAGS=\"-Zsanitizer=thread -g\" cargo +nightly bench shared_wguard_rw_contention -Z build-std --target x86_64-unknown-linux-gnu --quiet\r\n`\r\n\r\n```\r\nrunning 0 tests\r\n\r\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 127 filtered out; finished in 0.00s\r\n\r\n\r\nrunning 1 test\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=516081)\r\n  Write of size 4 at 0x7b04000016b8 by thread T11:\r\n    #0 <bench::AddOne as active_standby::types::UpdateTables<i32,()>>::apply_first /home/matan/rust/active_standby/benches/bench.rs:29 (bench-d7c10858bf64896f+0xbbae9) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 <bench::AddOne as active_standby::types::UpdateTables<i32,()>>::apply_second /home/matan/rust/active_standby/benches/bench.rs:32 (bench-d7c10858bf64896f+0xbbae9)\r\n    #2 active_standby::shared::aslock::AsLockWriteGuard<T>::update_tables::{{closure}} /home/matan/rust/active_standby/src/shared/aslock.rs:169 (bench-d7c10858bf64896f+0xbbae9)\r\n    #3 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbbae9)\r\n    #4 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0xc44cc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #5 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0xc44cc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #6 active_standby::shared::aslock::AsLock<T>::write /home/matan/rust/active_standby/src/shared/aslock.rs:99 (bench-d7c10858bf64896f+0xbbc50) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 bench::shared::AsLock::write /home/matan/rust/active_standby/src/macros.rs:231 (bench-d7c10858bf64896f+0xbebc3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #8 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:181 (bench-d7c10858bf64896f+0xbd50a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #9 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbd50a)\r\n    #10 std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:498 (bench-d7c10858bf64896f+0xc3b6e) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #11 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0xc3b6e)\r\n    #12 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0xc3b6e)\r\n    #13 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #14 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0xbb9eb) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #16 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xbb9eb)\r\n    #17 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ee2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #18 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ee2)\r\n    #19 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ee2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ee2)\r\n    #21 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f)\r\n    #23 std::sys::unix::thread::Thread::new::thread_start /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:108 (bench-d7c10858bf64896f+0x1310bc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n\r\n  Previous read of size 4 at 0x7b04000016b8 by thread T4:\r\n    #0 bench::benchmarks::shared_wguard_rw_contention::{{closure}}::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:169 (bench-d7c10858bf64896f+0xbdb1a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdb1a)\r\n    #2 std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:498 (bench-d7c10858bf64896f+0xc368e) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #3 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0xc368e)\r\n    #4 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0xc368e)\r\n    #5 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #6 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0xbb4ab) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #8 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xbb4ab)\r\n    #9 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ae2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #10 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ae2)\r\n    #11 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ae2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #12 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ae2)\r\n    #13 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #14 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f)\r\n    #15 std::sys::unix::thread::Thread::new::thread_start /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:108 (bench-d7c10858bf64896f+0x1310bc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n\r\n  Location is heap block of size 16 at 0x7b04000016b0 allocated by main thread:\r\n    #0 malloc /rustc/llvm/src/llvm-project/compiler-rt/lib/tsan/rtl/tsan_interceptors_posix.cpp:667 (bench-d7c10858bf64896f+0x29ac1) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys::unix::alloc::<impl core::alloc::global::GlobalAlloc for std::alloc::System>::alloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/alloc.rs:14 (bench-d7c10858bf64896f+0x13c21c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #2 __rdl_alloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/alloc.rs:355 (bench-d7c10858bf64896f+0x13c21c)\r\n    #3 alloc::alloc::alloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:87 (bench-d7c10858bf64896f+0xbbdbe) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #4 alloc::alloc::Global::alloc_impl /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:169 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #5 <alloc::alloc::Global as core::alloc::Allocator>::allocate /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:229 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #6 alloc::alloc::exchange_malloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:318 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #7 alloc::boxed::Box<T>::new /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:195 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #8 active_standby::shared::aslock::AsLock<T>::from_identical /home/matan/rust/active_standby/src/shared/aslock.rs:61 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #9 active_standby::shared::aslock::AsLock<T>::new /home/matan/rust/active_standby/src/shared/aslock.rs:128 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #10 bench::shared::AsLock::new /home/matan/rust/active_standby/src/macros.rs:242 (bench-d7c10858bf64896f+0xbec20) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #11 bench::benchmarks::shared_wguard_rw_contention /home/matan/rust/active_standby/benches/bench.rs:162 (bench-d7c10858bf64896f+0xc5954) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #12 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:161 (bench-d7c10858bf64896f+0xbcb55) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #13 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbcb55)\r\n    #14 core::ops::function::FnMut::call_mut /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:150 (bench-d7c10858bf64896f+0xf0a6d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 test::bench::Bencher::bench /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:56 (bench-d7c10858bf64896f+0xf0a6d)\r\n    #16 test::bench::benchmark::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0x10c609) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #17 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0x10c609)\r\n    #18 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x10c609)\r\n    #19 __rust_try.llvm.16455064722209264270 :? (bench-d7c10858bf64896f+0x10ca21) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x10c3a3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #21 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xea395) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 test::bench::benchmark /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0xf1388) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #23 test::run_test /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:546 (bench-d7c10858bf64896f+0xce11f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #24 test::run_tests /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:381 (bench-d7c10858bf64896f+0xcc9ff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #25 test::console::run_tests_console /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:286 (bench-d7c10858bf64896f+0x103ccd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #26 test::test_main /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:116 (bench-d7c10858bf64896f+0xcaa60) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #27 test::test_main_static /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:135 (bench-d7c10858bf64896f+0xcac7c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #28 bench::main /home/matan/rust/active_standby/benches/bench.rs:1 (bench-d7c10858bf64896f+0xbcc4c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #29 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbdc0f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #30 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdc0f)\r\n    #31 std::rt::lang_start::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145 (bench-d7c10858bf64896f+0xbf75d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #32 core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &F>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259 (bench-d7c10858bf64896f+0x135f45) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #33 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135f45)\r\n    #34 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #35 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #36 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x135da0) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #37 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x135da0)\r\n    #38 std::rt::lang_start_internal::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x135da0)\r\n    #39 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135da0)\r\n    #40 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #41 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x13464a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #42 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x13464a)\r\n    #43 std::rt::lang_start_internal /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x123159) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #44 main ??:? (bench-d7c10858bf64896f+0xbd2fa) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #45 __libc_start_main /build/glibc-eX1tMB/glibc-2.31/csu/../csu/libc-start.c:308 (libc.so.6+0x270b2) (BuildId: 099b9225bcb0d019d9d60884be583eb31bb5f44e)\r\n\r\n  Thread T11 (tid=516093, running) created by main thread at:\r\n    #0 pthread_create /rustc/llvm/src/llvm-project/compiler-rt/lib/tsan/rtl/tsan_interceptors_posix.cpp:1022 (bench-d7c10858bf64896f+0x2aecd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys::unix::thread::Thread::new /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:87 (bench-d7c10858bf64896f+0x130ea1) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #2 std::thread::Builder::spawn_unchecked_ /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:526 (bench-d7c10858bf64896f+0xb55bf) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #3 std::thread::Builder::spawn_unchecked /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:456 (bench-d7c10858bf64896f+0xb55bf)\r\n    #4 std::thread::Builder::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:388 (bench-d7c10858bf64896f+0xb55bf)\r\n    #5 std::thread::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:649 (bench-d7c10858bf64896f+0xb55bf)\r\n    #6 bench::benchmarks::shared_wguard_rw_contention /home/matan/rust/active_standby/benches/bench.rs:180 (bench-d7c10858bf64896f+0xc5a05) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:161 (bench-d7c10858bf64896f+0xbcb55) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #8 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbcb55)\r\n    #9 core::ops::function::FnMut::call_mut /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:150 (bench-d7c10858bf64896f+0xf0a6d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #10 test::bench::Bencher::bench /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:56 (bench-d7c10858bf64896f+0xf0a6d)\r\n    #11 test::bench::benchmark::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0x10c609) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #12 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0x10c609)\r\n    #13 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x10c609)\r\n    #14 __rust_try.llvm.16455064722209264270 :? (bench-d7c10858bf64896f+0x10ca21) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x10c3a3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #16 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xea395) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #17 test::bench::benchmark /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0xf1388) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #18 test::run_test /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:546 (bench-d7c10858bf64896f+0xce11f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #19 test::run_tests /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:381 (bench-d7c10858bf64896f+0xcc9ff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 test::console::run_tests_console /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:286 (bench-d7c10858bf64896f+0x103ccd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #21 test::test_main /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:116 (bench-d7c10858bf64896f+0xcaa60) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 test::test_main_static /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:135 (bench-d7c10858bf64896f+0xcac7c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #23 bench::main /home/matan/rust/active_standby/benches/bench.rs:1 (bench-d7c10858bf64896f+0xbcc4c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #24 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbdc0f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #25 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdc0f)\r\n    #26 std::rt::lang_start::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145 (bench-d7c10858bf64896f+0xbf75d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #27 core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &F>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259 (bench-d7c10858bf64896f+0x135f45) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #28 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135f45)\r\n    #29 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #30 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #31 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x135da0) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #32 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x135da0)\r\n    #33 std::rt::lang_start_internal::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x135da0)\r\n    #34 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135da0)\r\n    #35 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #36 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x13464a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #37 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x13464a)\r\n    #38 std::rt::lang_start_internal /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x123159) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #39 main ??:? (bench-d7c10858bf64896f+0xbd2fa) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #40 __libc_start_main /build/glibc-eX1tMB/glibc-2.31/csu/../csu/libc-start.c:308 (libc.so.6+0x270b2) (BuildId: 099b9225bcb0d019d9d60884be583eb31bb5f44e)\r\n\r\n  Thread T4 (tid=516086, running) created by main thread at:\r\n    #0 pthread_create /rustc/llvm/src/llvm-project/compiler-rt/lib/tsan/rtl/tsan_interceptors_posix.cpp:1022 (bench-d7c10858bf64896f+0x2aecd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys::unix::thread::Thread::new /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:87 (bench-d7c10858bf64896f+0x130ea1) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #2 std::thread::Builder::spawn_unchecked_ /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:526 (bench-d7c10858bf64896f+0xb7eff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #3 std::thread::Builder::spawn_unchecked /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:456 (bench-d7c10858bf64896f+0xb7eff)\r\n    #4 std::thread::Builder::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:388 (bench-d7c10858bf64896f+0xb7eff)\r\n    #5 std::thread::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:649 (bench-d7c10858bf64896f+0xb7eff)\r\n    #6 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:167 (bench-d7c10858bf64896f+0xbf47a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 core::iter::adapters::map::map_fold::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/adapters/map.rs:84 (bench-d7c10858bf64896f+0xbf47a)\r\n    #8 core::iter::traits::iterator::Iterator::fold /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:2285 (bench-d7c10858bf64896f+0xbf47a)\r\n    #9 <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/adapters/map.rs:124 (bench-d7c10858bf64896f+0xbf47a)\r\n    #10 core::iter::traits::iterator::Iterator::for_each /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:773 (bench-d7c10858bf64896f+0xc044b) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #11 <alloc::vec::Vec<T,A> as alloc::vec::spec_extend::SpecExtend<T,I>>::spec_extend /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_extend.rs:40 (bench-d7c10858bf64896f+0xc044b)\r\n    #12 <alloc::vec::Vec<T> as alloc::vec::spec_from_iter_nested::SpecFromIterNested<T,I>>::from_iter /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_from_iter_nested.rs:62 (bench-d7c10858bf64896f+0xc044b)\r\n    #13 <alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_from_iter.rs:33 (bench-d7c10858bf64896f+0xc044b)\r\n    #14 <alloc::vec::Vec<T> as core::iter::traits::collect::FromIterator<T>>::from_iter /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/mod.rs:2552 (bench-d7c10858bf64896f+0xc59d4) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 core::iter::traits::iterator::Iterator::collect /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:1778 (bench-d7c10858bf64896f+0xc59d4)\r\n    #16 bench::benchmarks::shared_wguard_rw_contention /home/matan/rust/active_standby/benches/bench.rs:164 (bench-d7c10858bf64896f+0xc59d4)\r\n    #17 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:161 (bench-d7c10858bf64896f+0xbcb55) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #18 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbcb55)\r\n    #19 core::ops::function::FnMut::call_mut /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:150 (bench-d7c10858bf64896f+0xf0a6d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 test::bench::Bencher::bench /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:56 (bench-d7c10858bf64896f+0xf0a6d)\r\n    #21 test::bench::benchmark::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0x10c609) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0x10c609)\r\n    #23 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x10c609)\r\n    #24 __rust_try.llvm.16455064722209264270 :? (bench-d7c10858bf64896f+0x10ca21) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #25 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x10c3a3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #26 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xea395) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #27 test::bench::benchmark /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0xf1388) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #28 test::run_test /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:546 (bench-d7c10858bf64896f+0xce11f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #29 test::run_tests /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:381 (bench-d7c10858bf64896f+0xcc9ff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #30 test::console::run_tests_console /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:286 (bench-d7c10858bf64896f+0x103ccd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #31 test::test_main /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:116 (bench-d7c10858bf64896f+0xcaa60) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #32 test::test_main_static /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:135 (bench-d7c10858bf64896f+0xcac7c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #33 bench::main /home/matan/rust/active_standby/benches/bench.rs:1 (bench-d7c10858bf64896f+0xbcc4c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #34 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbdc0f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #35 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdc0f)\r\n    #36 std::rt::lang_start::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145 (bench-d7c10858bf64896f+0xbf75d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #37 core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &F>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259 (bench-d7c10858bf64896f+0x135f45) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #38 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135f45)\r\n    #39 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #40 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #41 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x135da0) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #42 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x135da0)\r\n    #43 std::rt::lang_start_internal::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x135da0)\r\n    #44 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135da0)\r\n    #45 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #46 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x13464a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #47 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x13464a)\r\n    #48 std::rt::lang_start_internal /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x123159) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #49 main ??:? (bench-d7c10858bf64896f+0xbd2fa) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #50 __libc_start_main /build/glibc-eX1tMB/glibc-2.31/csu/../csu/libc-start.c:308 (libc.so.6+0x270b2) (BuildId: 099b9225bcb0d019d9d60884be583eb31bb5f44e)\r\n\r\nSUMMARY: ThreadSanitizer: data race /home/matan/rust/active_standby/benches/bench.rs:29 in <bench::AddOne as active_standby::types::UpdateTables<i32,()>>::apply_first\r\n==================\r\ntest benchmarks::shared_wguard_rw_contention         ... bench:       5,859 ns/iter (+/- 4,566)\r\n\r\ntest result: ok. 0 passed; 0 failed; 0 ignored; 1 measured; 20 filtered out; finished in 5.00s\r\n\r\nThreadSanitizer: reported 1 warnings\r\nerror: bench failed\r\n```\r\n\r\n\r\n\n"}
{"id": 50, "ground_truth": "Logic", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nLooks like [`test_swap_offchain()` is flaky right now](https://github.com/0xPolygonMiden/miden-client/actions/runs/9591340149/job/26448074126#step:8:352). We should review and fix it.\n"}
{"id": 26, "ground_truth": "Randomness", "predicted": "Environment", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### Problem\r\n #18278 ignores 2 local-cluster tests failing consistently in CI\r\n\r\n\r\n#### Proposed Solution\r\nIdentify root cause of test failures, fix, and re-enable them\r\n\r\n\n"}
{"id": 19, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Flakiness Type\n\nAssertion failure\n\n### Name of Test\n\nextractors::start_time::tests::start_time_from_timestamp\n\n### Link to Test Run\n\nhttps://github.com/getsentry/relay/actions/runs/8050926586/job/21987677539\n\n### Details\n\nSeems like an issue between the generation of the `now` timestamp and the `system_time` timestamp.  We're likely crossing from second `n` to second `n+1` between calls.\r\n\r\n```\r\n---- extractors::start_time::tests::start_time_from_timestamp stdout ----\r\nthread 'extractors::start_time::tests::start_time_from_timestamp' panicked at relay-server\\src\\extractors\\start_time.rs:74:9:\r\nassertion `left == right` failed\r\n  left: 9\r\n right: 10\r\n\r\n\r\nfailures:\r\n    extractors::start_time::tests::start_time_from_timestamp\r\n\r\ntest result: FAILED. 225 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 4.77s\r\n\r\nerror: test failed, to rerun pass `-p relay-server --lib`\r\n```\n"}
{"id": 80, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n`PendingAcks::reordering_threshold` defaults to 1. This causes an ACK to be sent immediately when a packet number is skipped:\r\n\r\nhttps://github.com/quinn-rs/quinn/blob/8076ffe94d38813ce0220af9d3438e7bfb5e8429/quinn-proto/src/connection/spaces.rs#L607-L609\r\n\r\nAs of #1613, we can randomly skip packet numbers, especially early in a connection. This causes a few extra ACKs to be sent at unpredictable times. This causes tests which count the number of ACKs sent in an interval to fail. We should modify such tests to use a reordering threshold greater than 1 to prevent these extra ACKs from being sent, or perhaps expose the number of skipped packets to allow them to account for the induced ACKs.\n"}
{"id": 57, "ground_truth": "Time", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Summary**\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/6321697555/job/17166517616?pr=12998\r\n\r\n```\r\n0: statement failed: ClickHouse client error: . Code: 500, Text = TableLockExpired. Code: 2016, Text = the acquired table lock has expired..\r\n[SQL] merge into t_stored_merge as t1 using (select * from t_source_merge as t2)  on t1.a = t2.a  when matched then update set t1.c = t2.c  when not matched then insert (a,c) values(t2.a,t2.c);\r\n```\r\nat tests/sqllogictests/suites/ee/01_ee_system/01_0001_computed_column:250\n"}
{"id": 52, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nDetected by: https://github.com/paritytech/subxt/actions/runs/6745755385/job/18338194383\r\n\r\n\r\n```bash\r\n--- STDERR:              integration-tests full_client::frame::staking::tx_bond ---\r\nthread 'full_client::frame::staking::tx_bond' panicked at testing/integration-tests/src/full_client/frame/staking.rs:216:5:\r\nassertion failed: `Err(Transaction(Invalid(\"Invalid transaction: Transaction is outdated\")))` does not match `Err(Error::Runtime(DispatchError::Module(err)))`\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n```\n"}
{"id": 47, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n> Uhmm this seems to make the HTTP node tests fail sometimes, but not consistently. Not sure why, will look into it. @rbehjati let me know if you have any suggestions on why that may be happening\r\n\r\nso actually I tried again from `main`, and it seems that tests are flaky there too. To reproduce:\r\n\r\n```bash\r\nexport RUST_LOG=debug\r\ncd oak_runtime\r\ncargo test --package oak_runtime --lib -- node --nocapture\r\n```\r\n\r\nand then rerun the `cargo test` a few times. I got a failure roughtly 1 out of 5 times.\r\n\r\n<details><summary>logs</summary>\r\n<p>\r\n\r\n```\r\n    Finished test [unoptimized + debuginfo] target(s) in 0.10s\r\n     Running target/debug/deps/oak_runtime-f41ce2eeb10d11e5\r\n\r\nrunning 19 tests\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 10859620126471780718 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 14154969170481411718 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 6028826501016594447 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 16096544177349775597 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((10859620126471780718, 14154969170481411718))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 16698263373585990938 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9092960943208256323 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 244963718117020840 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((16096544177349775597, 9092960943208256323))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((16698263373585990938, 244963718117020840))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 2638176703059791071 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 15754702411688475858 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 16409535109310484427 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((6028826501016594447, 15754702411688475858))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11927899315350453922 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9360967439716317465 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((2638176703059791071, 16409535109310484427))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11888636142326083607 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_other_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export not found\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((11927899315350453922, 11888636142326083607))\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 10552541542430126522 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 1\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 12335443945007934671 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 15583588925823706518 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 2196097866962909478 maps to Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((10552541542430126522, 12335443945007934671))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 1767916906041923217 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 1\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 3120311075007141291 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((15583588925823706518, 2196097866962909478))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 1291357445641949921 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9571224628934017081 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 1\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11927899315350453922, Message { bytes: [10, 9, 9, 150, 229, 110, 9, 144, 7, 68, 216], handles: [15583588925823706518] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(15754702411688475858)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((3120311075007141291, 9571224628934017081))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 3555956587273005122 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11927899315350453922, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(15754702411688475858) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 6330247248751785548 maps to Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11927899315350453922)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 14923236747254485945 maps to Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((3555956587273005122, 14923236747254485945))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6028826501016594447 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(2638176703059791071, Message { bytes: [10, 9, 9, 66, 40, 30, 5, 173, 75, 89, 49], handles: [3555956587273005122] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6028826501016594447 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((1291357445641949921, 6330247248751785548))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(2638176703059791071, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11927899315350453922) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(10552541542430126522, Message { bytes: [10, 9, 9, 225, 82, 139, 65, 193, 210, 235, 17], handles: [1291357445641949921] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(2638176703059791071)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(10552541542430126522, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(2638176703059791071) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(10552541542430126522)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\ntest node::wasm::tests::wasm_starting_module_without_content_fails ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(10552541542430126522) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 580815582432483639 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((1767916906041923217, 580815582432483639))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): wait_on_channels: channels not ready, parking thread Thread { id: ThreadId(16), name: None }\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9155368652591746135 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Waiting for invocation channel\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): wait_on_channels: channels not ready, parking thread Thread { id: ThreadId(19), name: None }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(12335443945007934671)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((9360967439716317465, 9155368652591746135))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11080559098524874464 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(12335443945007934671) -> Ok(Some(Message { bytes: [10, 9, 9, 225, 82, 139, 65, 193, 210, 235, 17], handles: [11080559098524874464] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(12335443945007934671)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(12335443945007934671) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Starting HTTP server pseudo-Node on: [::]:2527\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2527\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connected to [::1]:2527\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] ThreadId(18): Started HTTP server pseudo-node on port 2527\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] No cached session for DNSNameRef(\"localhost\")\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Not resuming any session\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Received incoming TLS stream: TcpStream { addr: V6([::1]:2527), peer: V6([::1]:33922), fd: 21 }\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2525\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Waiting for invocation channel\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to 127.0.0.1:2525\r\nthread 'node::http::tests::test_https_server_can_serve_https_requests' panicked at 'assertion failed: resp.is_ok()', src/node/http/tests.rs:91[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(11888636142326083607)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 15642419903814064830 maps to Channel 1 WRITE\r\n:5\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\ntest node::http::tests::test_https_server_can_serve_https_requests ... FAILED\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(11888636142326083607) -> Ok(Some(Message { bytes: [10, 9, 9, 150, 229, 110, 9, 144, 7, 68, 216], handles: [15642419903814064830] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11888636142326083607)\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Waiting for invocation channel\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }, Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [2, 2, 2] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11888636142326083607) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2526\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Starting HTTP server pseudo-Node on: [::]:2526\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to 127.0.0.1:2526\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }, Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [2, 2, 2] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connected to 127.0.0.1:2526\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] ThreadId(14): Started HTTP server pseudo-node on port 2526\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_channel_with_more_confidential_label_from_non_public_node_with_privilege_err ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Received incoming TLS stream: TcpStream { addr: V6([::ffff:127.0.0.1]:2526), peer: V6([::ffff:127.0.0.1]:33940), fd: 22 }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11705927381946070715 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 490605741076439707 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 5932870466176810717 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 13595452974177765549 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG rustls::server::hs] decided upon suite SupportedCipherSuite { suite: TLS13_CHACHA20_POLY1305_SHA256, kx: BulkOnly, bulk: CHACHA20_POLY1305, hash: SHA256, sign: None, enc_key_len: 32, fixed_iv_len: 12, explicit_nonce_len: 0 }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((11705927381946070715, 13595452974177765549))\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] flushed 57 bytes\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((490605741076439707, 5932870466176810717))\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Client-connection error: Custom { kind: InvalidData, error: CorruptMessage }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Error when processing TLS stream: Custom { kind: Other, error: \"TLS Error: Custom { kind: InvalidData, error: CorruptMessage }\" }\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(16409535109310484427)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] read 0 bytes\r\n[2020-09-10T20:47:15Z ERROR oak_runtime] Wasm module signature verification failed for oak_module: ring::error::Unspecified\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 10709409905608189541 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::conn] parse error (connection closed before message completed) with 0 bytes\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::dispatch] read_head error: connection closed before message completed\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(16409535109310484427) -> Ok(Some(Message { bytes: [10, 9, 9, 66, 40, 30, 5, 173, 75, 89, 49], handles: [10709409905608189541] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 13595452974177765549 => Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 11705927381946070715 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(16409535109310484427)\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG hyper::server::shutdown] signal received, starting graceful shutdown\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(16409535109310484427) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] HTTP server pseudo-node terminated with Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Starting HTTP server pseudo-Node on: [::]:2525\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_verify_module_signature_fails ... [2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\nok[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15583588925823706518 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 2196097866962909478 => Channel 1 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] ThreadId(15): Started HTTP server pseudo-node on port 2525\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15642419903814064830 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG hyper::server::shutdown] signal received, starting graceful shutdown\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] HTTP server pseudo-node terminated with Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15583588925823706518 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 2196097866962909478 => Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 8188998132612729810 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15642419903814064830 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 5748044951194180441 maps to Channel 0 READ\r\n\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Ok((8188998132612729810, 5748044951194180441))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 1 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(8188998132612729810, Message { bytes: [14, 12, 88], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(8188998132612729810, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(5748044951194180441)\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): cannot read from Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(5748044951194180441) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [8188998132612729810, 5748044951194180441]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Using ciphersuite TLS13_CHACHA20_POLY1305_SHA256\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\ntest node::http::tests::test_https_server_cannot_serve_http_requests ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\ntest tests::create_channel_with_more_confidential_label_from_public_untrusted_node_ok ... ok\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] Not resuming\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] TLS1.3 encrypted extensions: [ServerNameAck]\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] ALPN protocol is None\r\n[2020-09-10T20:47:15Z WARN  rustls::session] Sending fatal alert BadCertificate\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Client-connection error: Custom { kind: UnexpectedEof, error: \"tls handshake eof\" }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Error when processing TLS stream: Custom { kind: Other, error: \"TLS Error: Custom { kind: UnexpectedEof, error: \\\"tls handshake eof\\\" }\" }\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2527\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Received incoming TLS stream: TcpStream { addr: V6([::1]:2527), peer: V6([::1]:33932), fd: 5 }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connected to [::1]:2527\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] No cached session for DNSNameRef(\"localhost\")\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Not resuming any session\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\ntest tests::create_node_more_confidential_label_ok ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_node_invalid_configuration_err ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 1052340951842084367 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 279083569327136254 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Ok((1052340951842084367, 279083569327136254))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(1052340951842084367, Message { bytes: [14, 12, 88], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(1052340951842084367, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(279083569327136254)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(279083569327136254) -> Ok(Some(Message { bytes: [14, 12, 88], handles: [] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [279083569327136254, 1052340951842084367]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG rustls::server::hs] decided upon suite SupportedCipherSuite { suite: TLS13_CHACHA20_POLY1305_SHA256, kx: BulkOnly, bulk: CHACHA20_POLY1305, hash: SHA256, sign: None, enc_key_len: 32, fixed_iv_len: 12, explicit_nonce_len: 0 }\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_channel_with_more_confidential_label_from_public_node_with_privilege_ok ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_node_less_confidential_label_err ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Using ciphersuite TLS13_CHACHA20_POLY1305_SHA256\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] Not resuming\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_node_same_label_ok ... ok\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] TLS1.3 encrypted extensions: [ServerNameAck]\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] ALPN protocol is None\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] Ticket saved\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] flushed 57 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] read 57 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] parsed 2 headers\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::conn] incoming body is empty\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] Handling HTTP request; request size: 0 bytes, label: Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Inject the request into the Oak Node\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 2\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9961052006950199576 maps to Channel 2 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 1791300487480186213 maps to Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((9961052006950199576, 1791300487480186213))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 3\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 14881253896451425314 maps to Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 3680328595663417379 maps to Channel 3 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((14881253896451425314, 3680328595663417379))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(9961052006950199576, Message { bytes: [10, 1, 47, 18, 3, 103, 101, 116, 34, 15, 10, 13, 111, 97, 107, 45, 108, 97, 98, 101, 108, 45, 98, 105, 110, 34, 22, 10, 4, 104, 111, 115, 116, 18, 14, 108, 111, 99, 97, 108, 104, 111, 115, 116, 58, 50, 53, 50, 55], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(9961052006950199576, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11080559098524874464, Message { bytes: [10, 9, 9, 101, 149, 61, 122, 77, 250, 219, 24, 18, 9, 9, 34, 132, 98, 22, 147, 213, 132, 206], handles: [1791300487480186213, 14881253896451425314] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11080559098524874464, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): wait_on_channels: thread Thread { id: ThreadId(19), name: None } re-woken\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9961052006950199576)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 2 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9961052006950199576) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(1791300487480186213)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(6330247248751785548)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(1791300487480186213) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14881253896451425314)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 7424337281389512419 maps to Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 12021382004607466382 maps to Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(6330247248751785548) -> Ok(Some(Message { bytes: [10, 9, 9, 101, 149, 61, 122, 77, 250, 219, 24, 18, 9, 9, 34, 132, 98, 22, 147, 213, 132, 206], handles: [7424337281389512419, 12021382004607466382] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(12021382004607466382, Message { bytes: [16, 200, 1], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(12021382004607466382, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14881253896451425314) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] Generating response for runtime 0 and reader 3680328595663417379.\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(3680328595663417379)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(3680328595663417379) -> Ok(Some(Message { bytes: [16, 200, 1], handles: [] }))\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] flushed 75 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] read 75 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] parsed 2 headers\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::conn] incoming body is empty\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::pool] pooling idle connection for (\"https\", localhost:2527)\r\n[2020-09-10T20:47:15Z DEBUG hyper::server::shutdown] signal received, starting graceful shutdown\r\n[2020-09-10T20:47:15Z DEBUG rustls::session] Sending warning alert CloseNotify\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] HTTP server pseudo-node terminated with Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 7424337281389512419 => Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 12021382004607466382 => Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3680328595663417379 => Channel 3 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1291357445641949921 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6330247248751785548 => Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 11080559098524874464 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 7424337281389512419 => Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 12021382004607466382 => Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3680328595663417379 => Channel 3 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1291357445641949921 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6330247248751785548 => Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 11080559098524874464 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=2, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=2, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 3 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=3, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=3, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 1 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::http::tests::test_https_server_does_not_terminate_after_a_bad_request ... ok\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export has incorrect function signature: Signature { params: [], return_type: None }\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9571224628934017081)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9571224628934017081) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3120311075007141291 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3120311075007141291 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_signature_2_fails ... ok\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export has incorrect function signature: Signature { params: [I64, I32, I32, I32, I32, I32, I32], return_type: Some(I32) }\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9155368652591746135)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9155368652591746135) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 9360967439716317465 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 9360967439716317465 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_signature_fails ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::wasm] entrypoint 'oak_main' export validated\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] Wasm module SHA-256 hash: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 4251925570913386371 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] NodeId(0): start node instance NodeId(1) with privilege NodePrivilege { can_declassify_confidentiality_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }}, can_endorse_integrity_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }} }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(5932870466176810717)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: running entrypoint 'oak_main'\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(5932870466176810717) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 490605741076439707 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(1) handle 4251925570913386371 => Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1) ...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export not found\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_other_main' export not found\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(244963718117020840)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_other_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(244963718117020840) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9092960943208256323)\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9092960943208256323) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16698263373585990938 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::wasm] entrypoint 'oak_main' export validated\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16096544177349775597 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16698263373585990938 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16096544177349775597 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] Wasm module SHA-256 hash: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115]\r\ntest node::wasm::tests::wasm_starting_module_missing_an_export_fails ... ok\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_export_fails ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 5475638980536319391 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] NodeId(0): start node instance NodeId(1) with privilege NodePrivilege { can_declassify_confidentiality_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }}, can_endorse_integrity_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }} }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: running entrypoint 'oak_main'\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14154969170481411718)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14154969170481411718) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 10859620126471780718 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(1) handle 5475638980536319391 => Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1) ...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export has incorrect function signature: Signature { params: [I64], return_type: Some(I32) }\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(580815582432483639)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(580815582432483639) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1767916906041923217 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1767916906041923217 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_signature_3_fails ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: entrypoint 'oak_main' completed\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [4251925570913386371]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 490605741076439707 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_verify_module_signature_succeeds ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: entrypoint 'oak_main' completed\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [5475638980536319391]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 10859620126471780718 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_minimal_module_succeeds ... ok\r\n\r\n```\r\n\r\n</p>\r\n</summary>\r\n\r\n_Originally posted by @tiziano88 in https://github.com/project-oak/oak/pull/1461#issuecomment-690563634_\n"}
{"id": 85, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nmark message as seen on imap in this case.\r\nspams log with events even though the jobs can not be executed because the device is offline.\r\n\r\nproposed solution: don't run the mark msg as read on imap job if offline\n"}
{"id": 86, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/metalbear-co/mirrord/actions/runs/5066110644/jobs/9095518984?pr=1451\r\nI think it's similar to the #1451 just need to do same in flask\r\napplication_1_Application__PythonFlaskHTTP\r\n```\r\nthread 'mirroring_with_http::application_1_Application__PythonFlaskHTTP' panicked at 'Timeout 60s expired', /Users/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rstest-0.17.0/src/timeout.rs:42:21\r\nstack backtrace:\r\n   0: rust_begin_unwind\r\n             at /rustc/478cbb42b730ba4739351b72ce2aa928e78e2f81/library/std/src/panicking.rs:577:5\r\n   1: core::panicking::panic_fmt\r\n             at /rustc/478cbb42b730ba4739351b72ce2aa928e78e2f81/library/core/src/panicking.rs:67:14\r\n   2: rstest::timeout::execute_with_timeout_async::{{closure}}\r\n   3: http_mirroring::mirroring_with_http::application_1_Application__PythonFlaskHTTP::{{closure}}\r\n   4: <core::pin::Pin<P> as core::future::future::Future>::poll\r\n   5: tokio::runtime::park::CachedParkThread::block_on::{{closure}}\r\n   6: tokio::runtime::park::CachedParkThread::block_on\r\n   7: tokio::runtime::context::BlockingRegionGuard::block_on\r\n   8: tokio::runtime::scheduler::multi_thread::MultiThread::block_on\r\n   9: tokio::runtime::runtime::Runtime::block_on\r\n  10: http_mirroring::mirroring_with_http::application_1_Application__PythonFlaskHTTP\r\n  11: http_mirroring::mirroring_with_http::application_1_Application__PythonFlaskHTTP::{{closure}}\r\n  12: core::ops::function::FnOnce::call_once\r\n  13: core::ops::function::FnOnce::call_once\r\n             at /rustc/478cbb42b730ba4739351b72ce2aa928e78e2f81/library/core/src/ops/function.rs:250:5\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\n```\n"}
{"id": 3, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Describe the bug\r\n\r\nhttps://github.com/paradigmxyz/reth/actions/runs/5405202612/jobs/9820416442?pr=3455#step:8:789\r\n\r\n```console\r\n--- STDERR:              reth-network::it connect::test_incoming_node_id_blacklist ---\r\nthread 'connect::test_incoming_node_id_blacklist' panicked at 'called `Result::unwrap()` on an `Err` value: HTTPError(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Ipv4(127.0.0.1)), port: Some(44187), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"tcp connect error\", Os { code: 111, kind: ConnectionRefused, message: \"Connection refused\" })) })', crates/net/network/tests/it/connect.rs:325:70\r\n```\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow the Code of Conduct\n"}
{"id": 49, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis seems to be flaky, and unrelated to the PR:\nhttps://drone-auto-casper-network.casperlabs.io/casper-network/casper-node/6206/2/4\n"}
{"id": 18, "ground_truth": "Logic", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Issue summary\r\n\r\nThe RPC test for `Filecoin.EthGetBlockByNumber` fails randomly. Investigate and fix this. Until then, the test is ignored.\r\n\r\n## Other information and links\r\n\r\n<!-- Add any other context or screenshots about the issue here. -->\r\n\r\n<!-- Thank you  -->\r\n\n"}
{"id": 33, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nLooks like the new `fuzz_threaded_connections` is flaky, it can hit an unwrap on the first read_event, which shouldn't fail, but CI managed to make it. This isn't an immediate correctness concern, however, as that erroring isnt a problem, just surprising.\n"}
{"id": 43, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n```shell\r\nfailures:\r\n    api::http_service::test_http_service_tls_server_mutual_tls\r\n    servers::http::http_query_handlers::test_http_service_tls_server_mutual_tls\r\n```\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/3057687821/jobs/4933098585\n"}
{"id": 38, "ground_truth": "Environment", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe `text\\split-batch.yaml` fails with 25 pixel difference, and the `image\\tile-repeat-prim-or-decompose.yaml` crashes, because we run out of descriptors. The fail was introduced in https://github.com/szeged/webrender/commit/1e81eaf8fa8ddbed1f8beda20a18bfc8787c7e80, where we started using the `max_image_array_layers` limit from gfx-hal. Previously we had a constant `2048` value there. This only affects the Vulkan backend. \n"}
{"id": 71, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Summary**\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/8168665641/job/22331179267?pr=14855\r\n\r\n```\r\n   FAIL [   0.257s] databend-meta::it grpc::metasrv_grpc_kv_read_v1::test_kv_read_v1_on_follower\r\n```\r\n\r\n```\r\n                             expire_at: Some(\r\n<                                1709712582,\r\n>                                1709712583,\r\n                             ),\r\n\r\n```\n"}
{"id": 48, "ground_truth": "Unordered Data", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nIt appears that after version 0.32, executing the GraphQL query `estimateGasPrice(10)` immediately after starting fuel-core will often throw the error below, making tests very flaky on [`fuels-ts`](https://github.com/FuelLabs/fuels-ts).\r\n\r\n```console\r\nFailed to estimate gas price for block, algorithm not yet set: 10\r\n```\r\n\r\n\r\n\r\nQuoting suspicions below:\r\n\r\n> @MitchTurner: I believe this is a race condition where the graphql services task is started before the gas price service is fully initialized. Each is run in a separate thread and we don't have any extra synchronization to make sure one is started before the other.\r\n\r\n> @Voxelot: hmm we need the graphql service up and running early otherwise the service may look unhealthy if other tasks take a while to initialize. We could split the graphql request handling from the health endpoint so they could be initialized at different times? (edited) \r\n\n"}
{"id": 51, "ground_truth": "Environment", "predicted": "Environment", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Describe the bug\n\nGithub CI for MacOS x86_64 is currently flaky with message:\r\n\r\n```\r\nWARNING: /Users/runner/hostedtoolcache/Java_Zulu_jdk/8.0.402-6/x64/jre/bin/java is loading libcrypto in an unsafe way\r\n```\r\n\r\nSome attempts have been made in #55 and #41, but it is still not resolved.\r\n\n\n### Steps to reproduce\n\nCheck the post-commit CI runs and occasionally it still fails with the above message.\n\n### Expected behavior\n\nThe CI for MacOS x86_64 should pass consistently.\n\n### Additional context\n\n_No response_\n"}
{"id": 84, "ground_truth": "Async Wait", "predicted": "Environment", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\ncc @Simon-Laux \r\n\r\nStill not fixed. I fixed python docs by pinning sphinx meanwhile.\r\n\r\nHere is a test failure: https://github.com/deltachat/deltachat-core-rust/actions/runs/8721401263/job/23925048947\n"}
{"id": 31, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nExamples (from yesterday 10 of 78 fails)\r\n\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7528201\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7527201\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7526808\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7526157\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7523749\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7519971\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7518972\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7518884\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7518515\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7517649\r\n\r\ncc: @paritytech/sdk-node \n"}
{"id": 20, "ground_truth": "Async Wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nFound in main\r\n```\r\nAssertionError: assert (['pg_xact/0000'], []) == ([], [])\r\n  At index 0 diff: ['pg_xact/0000'] != []\r\n  Full diff:\r\n  - ([], [])\r\n  + (['pg_xact/0000'], [])\r\nbatch_pg_regress/test_pg_regress.py:58: in test_pg_regress\r\n    check_restored_datadir_content(zenith_cli, test_output_dir, pg)\r\nfixtures/zenith_fixtures.py:1011: in check_restored_datadir_content\r\n    assert (mismatch, error) == ([], [])\r\nE   AssertionError: assert (['pg_xact/0000'], []) == ([], [])\r\nE     At index 0 diff: ['pg_xact/0000'] != []\r\nE     Full diff:\r\nE     - ([], [])\r\nE     + (['pg_xact/0000'], [])\r\n```\r\n\r\n@lubennikovaav is it because of the recent changes or this is a spurious error?\n"}
{"id": 69, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nLooks like weve got a flaky batcher test:\r\n\r\n```\r\nthread 'sync::tests::send_full_capacity' panicked at batcher/src/sync.rs:303:9:\r\n  assertion `left == right` failed\r\n    left: [5, 6, 7, 8, 9]\r\n   right: [0, 1, 2, 3, 4]\r\n```\n"}
{"id": 74, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Summary**\r\n\r\n```shell\r\n--- /workspace/tests/suites/0_stateless/13_tpch/13_0007_q7.result        2022-09-30 05:37:31.856084133 +0000\r\n+++ /workspace/tests/suites/0_stateless/13_tpch/13_0007_q7.stdout        2022-09-30 05:38:54.725091643 +0000\r\n@@ -1,4 +1 @@\r\n-FRANCE        GERMANY        1995        4637235.15\r\n-FRANCE        GERMANY        1996        5224779.573\r\n-GERMANY        FRANCE        1995        6232818.703\r\n-GERMANY        FRANCE        1996        5557312.112\r\n+ERROR 1105 (HY000) at line 2: Code: 1067, displayText = error reading a body from connection: broken pipe (while in processor thread 0).\r\n```\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/3156329493/jobs/5136357403\n"}
{"id": 66, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/denoland/deno/runs/555522900\r\n```\r\n---- inspector_pause stdout ----\r\ntarget_dir D:\\a\\deno\\deno\\target\\release\r\nws_url ws://127.0.0.1:9230/ws/32db3e15-0d08-4ee4-af90-b3f9ca5a7273\r\n##[error]thread 'inspector_pause' panicked at 'called `Result::unwrap()` on an `Err` value: Io(Os { code: 10054, kind: ConnectionReset, message: \"An existing connection was forcibly closed by the remote host.\" })', cli\\tests\\integration_tests.rs:2076:17\r\n```\r\n\r\n\n"}
{"id": 35, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test `Payload Index Test test_is_empty_conditions` is flaky.\r\n\r\n```\r\nfailures:\r\n\r\n---- tests::test_is_empty_conditions stdout ----\r\nthread 'tests::test_is_empty_conditions' panicked at 'assertion failed: (estimation_struct.exp as f64 - real_number as f64).abs() <\\n    (estimation_plain.exp as f64 - real_number as f64).abs()', lib/segment/tests/payload_index_test.rs:107:9\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n\r\nfailures:\r\n    tests::test_is_empty_conditions\r\n```\r\n\r\n## Current Behavior\r\nThe issue has been observed both locally and on [CI](https://github.com/qdrant/qdrant/runs/5993313794?check_suite_focus=true#step%3A7%3A202=)\r\n\r\n## Steps to Reproduce\r\nNo clear reproduction steps due to the flaky aspect.\n"}
{"id": 14, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI have twice seen this error when running `scripts/cli-test.sh`:\r\n\r\n```\r\nError: Peg-out address received 0.000005 BTC, expected 0.00000500\r\n```\r\n\r\nThe two numbers are equal, but padded differently with zeros.\n"}
{"id": 39, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nSee: https://gitlab.parity.io/parity/mirrors/polkadot/-/jobs/1530940 from the totally unrelated https://github.com/paritytech/polkadot/pull/5405 \r\n\r\ncc @drahnr @eskimor \n"}
{"id": 73, "ground_truth": "Logic", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nOften fail with connection refused, e.g:\r\n\r\ngc_after_sync.py (connection refused):\r\nhttp://52.149.162.182:3000/#/test/7746\r\nhttp://52.149.162.182:3000/#/test/7626\r\n\r\ngc_after_sync1.py (rpc timeout)\r\nhttp://52.149.162.182:3000/#/test/4627\r\n\r\ngc_sync_after_sync.py (connection refused):\r\nhttp://52.149.162.182:3000/#/test/7748\r\nhttp://52.149.162.182:3000/#/test/7628\r\nhttp://52.149.162.182:3000/#/test/7508\n"}
{"id": 41, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nWe use [svm-rs](https://github.com/roynalnaruto/svm-rs) by @roynalnaruto, which is used for managing the global `solc` binary's version, by installing each version under `~/.svm/4version/solc-$version`. For some reason, when running on CI the installation process seems to be flaky, e.g. below:\r\n\r\n![telegram-cloud-photo-size-2-5366071301866894295-y](https://user-images.githubusercontent.com/17802178/134912737-ae1f3389-ec6a-49b0-b279-aecbf587ae7a.jpg)\r\n\r\nWe should fix that. Maybe there's a bug downstream in `svm-rs` (e.g. there should be some sort of global mutex that says \"hey a solc version is being installed right now, wait for it to unlock before you try running solc\"\r\n\r\nI tried doing that manually with the [lock inside solc/lib.rs](https://github.com/gakonst/dapptools-rs/blob/master/solc/src/lib.rs#L118-L120) but it was not enough apparently.\n"}
{"id": 2, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/near/nearcore/pull/10015 introduced a flaky test test_trie_consistency_random, fails about 2% of the time; will fix at first convenience.\n"}
{"id": 25, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI encounter this time and time again: you open a PR, wait for checks for 10/20/30 minutes, and then you see that same workflows fail on the same tests. During that time I get distracted on something else and forget to re-start checks again just in time. Sometimes it takes 5-6 restarts to get them work. This way, the moment of all green lights might delay for days. And it delays development in my case.\r\n\r\nFrom my observation, the following workflows are flaky:\r\n\r\n- `I2::Dev::Tests` > `with_coverage`, `integration`, `unstable`\r\n\r\nAnd these are particular flaky tests:\r\n\r\n- `integration::extra_functional::offline_peers::genesis_block_is_committed_with_some_offline_peers`\r\n- `integration::extra_functional::unstable_network::soft_fork`\r\n- And _maybe_ some others, haven't collected much data\r\n\r\nAre these tests worth it?\r\n\r\nMy another concern is that I don't see the rationale behind having so many workflows:\r\n\r\n1. `I2::Dev::Static`\r\n    1. smart contracts\r\n    2. workspace\r\n2. `I2::Tests::UI`\r\n    1. test with all features\r\n    2. test with no default features\r\n3. `I2::Dev::Tests`\r\n    1. consistency\r\n    2. with_coverage\r\n    3. integration\r\n    4. unstable\r\n    5. client-cli-tests\r\n\r\n(there are some others too)\r\n\r\nThese workflows all run Cargo and compile more or less the same stuff. Yes, there are variations in features presets, but Cargo handles it for us. It can granularly reuse compilation artifacts depending on the context (apart from cases with different RUSTC flags, I suppose).\r\n\r\nSo, I guess that it is worth trying to combine all these workflows into a single one, and build it in a way so that it can report as many useful information as possible in a single run. I wonder how much more/less it would be efficient.\r\n\r\nAnother useful implication of this would be a shorter feedback on some early errors. For example, a certain change in PR introduces something and Iroha cannot even compile. Currently, all 8+ workflows will run and fail on the same error. In the case of a unified CI, there will be less work repetition.\r\n\r\n### Proposals\r\n\r\n- Prioritise zero-tolerance to flaky tests from development side\r\n- If flaky tests couldn't be easily fixed, possibly move them away from PR checks to after-merge checks.\r\n- Create a single unified workflow, and research the performance impact of it.\r\n- Explore ways to use a sane scripting language for CI, not Shell. That's for a separate issue, maybe.\n"}
{"id": 55, "ground_truth": "Randomness", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI've been experiencing a significant increase in flaky tests recently. I'm guessing this is due to the `FindNodes` test added to `peertest`. Would be curious to get confirmation from others. But, this has also raised an issue that when CI runs peertest, if there is a failure, then it doesn't display the error message, it simply just says that the test run failed ([example](https://app.circleci.com/pipelines/github/ethereum/trin/1267/workflows/11b900c3-b67e-40d7-9de7-9c516303c714/jobs/2473)). This makes it more difficult to diagnose the source of flakiness, so we should update how the tests are run to make sure that an error message / stack trace is displayed if it occurs.\n"}
{"id": 1, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n##  Bug Report\r\n\r\nThe following error likes to pop up in the CircleCI `tests` job from time to time:\r\n\r\n```\r\nrunning 1 test\r\ntest helpers::block_requests::tests::test_block_requests_case_2cbb ... FAILED\r\n\r\nfailures:\r\n\r\n---- helpers::block_requests::tests::test_block_requests_case_2cbb stdout ----\r\nthread 'helpers::block_requests::tests::test_block_requests_case_2cbb' panicked at 'assertion failed: `(left == right)`\r\n  left: `AbortAndDisconnect(TwoCBA, ExceededForkRange)`,\r\n right: `Proceed(TwoCBB, BlockRequestHandlerProceed { start_block_height: 966, end_block_height: 1215, ledger_is_on_fork: true })`', src/helpers/block_requests.rs:572:13\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n\r\nfailures:\r\n    helpers::block_requests::tests::test_block_requests_case_2cbb\r\n```\r\n\r\nIt's likely an off-by one in the test, but I'm not sure.\n"}
{"id": 60, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Is your feature request related to a problem?\n\nhttps://github.com/xmtp/libxmtp/blob/75ee4aeb869d66340226893ff98d8daf188166ff/bindings_ffi/src/mls.rs#L1044\r\n\r\nTest is flaking b/c the streamed messages are arriving too slow to one end of the `stream`. Calls to `tokio::sleep` are supposed to give time for messages to arrive but depending on architecture/system speed this varies. \r\n\r\n\n\n### Describe the solution to the problem\n\nIt would be great to use `async-barrier` for these tests in order to guarantee sync points and avoid flakiness\n\n### Describe the uses cases for the feature\n\n_No response_\n\n### Additional details\n\n_No response_\n"}
{"id": 54, "ground_truth": "Logic", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n`test_process_certificate` (https://github.com/MystenLabs/sui/blob/main/crates/sui-core/src/unit_tests/authority_aggregator_tests.rs#L815) is flaky. The first run usually succeeds, but if you iterate the test it will quickly fail.\r\n\r\nTo reproduce, run: \r\n\r\n     $ MSIM_TEST_NUM=10 cargo simtest test_process_certificate\r\n\r\nThis will usually fail in the first few iterations. If we do nothing, this will likely eventually fail on the first try during an unlucky CI run.\r\n\r\n I took a look at the logs and it appears this may be a real bug, not just a fragile test - passing runs indicate that `sync_certificate_to_authority` is called more often when the test passes then when it fails: \r\n\r\n<img width=\"1713\" alt=\"image\" src=\"https://user-images.githubusercontent.com/103447440/194340895-4a1d0b79-38ee-4a47-a6c0-a30631af0c75.png\">\r\n\r\n(After https://github.com/MystenLabs/sui/pull/5020 merges, it will be possible to reproduce the failed iteration exactly by specifying the random seed - for now, that does not work).\n"}
{"id": 40, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/get10101/10101/actions/runs/7969843098/job/21756644996?pr=2070#step:10:493\n"}
{"id": 34, "ground_truth": "Unordered data", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n              This looks like a flaky test. I will fix it.\r\n```\r\n05_0001_set_var:                                                        [ FAIL ] - result differs with:\r\n--- /runner/_work/databend/databend/tests/suites/0_stateless/05_hints/05_0001_set_var.result\t2023-04-28 00:15:37.866486923 +0000\r\n+++ /runner/_work/databend/databend/tests/suites/0_stateless/05_hints/05_0001_set_var.stdout\t2023-04-28 00:19:13.660031675 +0000\r\n@@ -1,6 +1,6 @@\r\n-x\tx\r\n timezone\tAmerica/Los_Angeles\r\n x\tx\r\n+x\tx\r\n timezone\tAsia/Shanghai\r\n timezone\tAsia/Shanghai\tUTC\tSESSION\tSets the timezone.\tString\r\n storage_read_buffer_size\t1048576\r\n```\r\n\r\n_Originally posted by @TCeason in https://github.com/datafuselabs/databend/issues/11258#issuecomment-1526826926_\r\n            \n"}
{"id": 75, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nEncountered on CI of a PR modifying purely github metadata:\r\nhttps://github.com/MystenLabs/sui/runs/6200152939?check_suite_focus=true\r\n\r\nFlaky and non-reproducible.\r\n\r\n```\r\nrunning 2 tests\r\ntest shared_object_transaction ... ok\r\ntest call_shared_object_contract ... FAILED\r\n\r\nfailures:\r\n\r\n---- call_shared_object_contract stdout ----\r\nthread 'call_shared_object_contract' panicked at 'Error acquiring lock for object(s): [UnexpectedSequenceNumber { object_id: 6EE48C2E2A362479E1F5DD836BA27D48C5546E7A, expected_sequence: SequenceNumber(2), given_sequence: SequenceNumber(1) }]', sui/tests/shared_objects_tests.rs:75:30\r\nstack backtrace:\r\n   0: rust_begin_unwind\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/std/src/panicking.rs:584:5\r\n   1: core::panicking::panic_fmt\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/panicking.rs:143:14\r\n   2: shared_objects_tests::submit_shared_object_transaction::{{closure}}\r\n             at ./tests/shared_objects_tests.rs:75:30\r\n   3: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/future/mod.rs:91:19\r\n   4: shared_objects_tests::call_shared_object_contract::{{closure}}\r\n             at ./tests/shared_objects_tests.rs:168:72\r\n   5: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/future/mod.rs:91:19\r\n   6: <core::pin::Pin<P> as core::future::future::Future>::poll\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/future/future.rs:124:9\r\n   7: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}::{{closure}}::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:498:48\r\n   8: tokio::coop::with_budget::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:102:9\r\n   9: std::thread::local::LocalKey<T>::try_with\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/std/src/thread/local.rs:442:16\r\n  10: std::thread::local::LocalKey<T>::with\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/std/src/thread/local.rs:418:9\r\n  11: tokio::coop::with_budget\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:95:5\r\n  12: tokio::coop::budget\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:72:5\r\n  13: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:498:25\r\n  14: tokio::runtime::basic_scheduler::Context::enter\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:356:19\r\n  15: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:497:36\r\n  16: tokio::runtime::basic_scheduler::CoreGuard::enter::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555:57\r\n  17: tokio::macros::scoped_tls::ScopedKey<T>::set\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/macros/scoped_tls.rs:61:9\r\n  18: tokio::runtime::basic_scheduler::CoreGuard::enter\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555:27\r\n  19: tokio::runtime::basic_scheduler::CoreGuard::block_on\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:488:9\r\n  20: tokio::runtime::basic_scheduler::BasicScheduler::block_on\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:168:24\r\n  21: tokio::runtime::Runtime::block_on\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/mod.rs:475:46\r\n  22: shared_objects_tests::call_shared_object_contract\r\n             at ./tests/shared_objects_tests.rs:186:5\r\n  23: shared_objects_tests::call_shared_object_contract::{{closure}}\r\n             at ./tests/shared_objects_tests.rs:104:7\r\n  24: core::ops::function::FnOnce::call_once\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/ops/function.rs:[227](https://github.com/MystenLabs/sui/runs/6200152939?check_suite_focus=true#step:5:227):5\r\n  25: core::ops::function::FnOnce::call_once\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/ops/function.rs:227:5\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\n\r\n\r\nfailures:\r\n    call_shared_object_contract\r\n\r\ntest result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 8.10s\r\n\r\nerror: test failed, to rerun pass '-p sui --test shared_objects_tests'\r\nError: Process completed with exit code 101.\r\n```\r\n\r\nFull log : https://gist.github.com/huitseeker/e5333b31a45b20fe6e56d1d6e425f78b\r\n\r\ncc @asonnino for shared objects, @velvia for locking service.\n"}
{"id": 63, "ground_truth": "I/O", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Describe the bug**\r\n<!--\r\nA clear and concise description of what the bug is.\r\n-->\r\n\r\nI have seen this test fail a couple of times in CI lately, most recently\r\n\r\nhttps://github.com/apache/arrow-rs/actions/runs/6324042685/job/17172783580\r\n\r\nThis test was added in apache/arrow-rs#3824 by @metesynnada \r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the behavior:\r\n-->\r\n\r\n**Expected behavior**\r\n<!--\r\nA clear and concise description of what you expected to happen.\r\n-->\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\n"}
{"id": 29, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n#  Bug\r\n\r\nI have seen a few PRs fail with no code changes in `storage::command_adapter::tests::test_save_list_metadata_files`. Two runs are:\r\n- https://circleci.com/gh/libra/libra/167227\r\n- https://circleci.com/gh/libra/libra/166268\r\n\n"}
{"id": 28, "ground_truth": "Time", "predicted": "Async Wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nWe have a bunch of flaky tests to fix:\r\n\r\n- [x] `test_indexing_service_does_not_shut_down_pipelines_on_indexing_pipeline_timeout` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3448688279/jobs/5755916183)) @fmassot \r\n- [x] `test_run_gc_expires_stale_staged_splits_after_grace_period` @guilload \r\n- [x] `test_ingest_api_garbage_collector` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3464851848/jobs/5786887789)) @evanxg852000 \r\n- [x] `grpc_metastore::common_tests::test_metastore_list_splits` @ChillFish8 \r\n- [x] `test_garbage_collect_index_cli` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3473230518/jobs/5805017278)) @boraarslan \r\n- [x] `test_search_dynamic_mode_expand_dots` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3516424778/jobs/5892968003)) @fulmicoton \n"}
{"id": 13, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n`src/smtp.rs::send_smtp_messages` logs and ignores SMTP errors when sending MDNs, then returns `Ok(())`. Then in `src/scheduler::smtp_loop` we reset `timeout` to `None`:\r\nhttps://github.com/deltachat/deltachat-core-rust/blob/21845ca5ea17acaad192989605b3688a46d908f5/src/scheduler.rs#L773\r\nand then log \"smtp has no messages to retry\" and wait for interrupt forever:\r\nhttps://github.com/deltachat/deltachat-core-rust/blob/21845ca5ea17acaad192989605b3688a46d908f5/src/scheduler.rs#L815-L816\r\nIt is not true that SMTP has no messages to retry in this case.\r\n\r\nThe worst part of this is that during high load postfix returns temporary error \"421 4.4.2 *** Error: timeout exceeded\": https://github.com/deltachat/chatmail/issues/315 Then tests time out in CI because they never retry sending.\n"}
{"id": 32, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test was added recently, and is failing ~3% of the time.\r\n\r\nAn example failure:\r\n- https://neon-github-public-dev.s3.amazonaws.com/reports/pr-7663/9022743601/index.html#testresult/45c20a5d0b81d6cd/retries\r\n\r\n```\r\ntest_runner/regress/test_storage_controller.py:1312: in test_lock_time_tracing\r\n    env.storage_controller.assert_log_contains(\r\ntest_runner/fixtures/neon_fixtures.py:2003: in assert_log_contains\r\n    assert res is not None\r\nE   assert None is not None\r\n```\n"}
{"id": 45, "ground_truth": "Logic", "predicted": "Environment", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nAttached is a log from CI:\r\n[flaky-test.txt](https://github.com/deltachat/deltachat-core-rust/files/14000065/flaky-test.txt)\r\n\r\n\r\nMaybe similar issue to #5191 but better understand what exactly happened.\n"}
{"id": 12, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### Problem\r\n#5659\r\n\r\n#### Proposed Solution\r\nDebug the race condition exposed by test_banking_stage_entryfication, fix it, then revert #5659\r\n\n"}
{"id": 5, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe fuzzer causes some ungraceful shutdown of the tokio tasks, which makes CI fail in a flaky way.\n"}
{"id": 67, "ground_truth": "Logic", "predicted": "Unordered data", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Describe the bug\r\n\r\n`bodies::bodies::tests::streams_bodies_in_order_after_range_reset` occasionally fails on main. A failure can be seen in CI here:\r\nhttps://github.com/paradigmxyz/reth/actions/runs/4077743007/jobs/7027133041#step:8:203\r\n\r\n### Steps to reproduce\r\n\r\nSometimes this can be reproduced locally with `cargo nextest run --workspace --locked --all-features`\r\n\r\n### Node logs\r\n\r\n```text\r\n--- STDERR:              reth-downloaders bodies::bodies::tests::streams_bodies_in_order_after_range_reset ---\r\nthread 'bodies::bodies::tests::streams_bodies_in_order_after_range_reset' panicked at 'assertion failed: `(left == right)`\r\n  left: `8`,\r\n right: `7`', crates/net/downloaders/src/bodies/bodies.rs:616:9\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n   Canceling due to test failure: 0 tests still running\r\n```\r\n\r\n\r\n### Platform(s)\r\n\r\n_No response_\r\n\r\n### What version/commit are you on?\r\n\r\n66aa947\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow the Code of Conduct\n"}
{"id": 62, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nDetected by https://github.com/paritytech/subxt/actions/runs/6771722241/job/18402732995?pr=1238.\r\n\r\n\r\n```\r\n--- STDOUT:              integration-tests full_client::frame::balances::tx_basic_transfer ---\r\nfailures:\r\n    full_client::frame::balances::tx_basic_transfer\r\n\r\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 77 filtered out; finished in 50.34s\r\n\r\n\r\n--- STDERR:              integration-tests full_client::frame::balances::tx_basic_transfer ---\r\nthread 'full_client::frame::balances::tx_basic_transfer' panicked at testing/integration-tests/src/full_client/frame/balances.rs:83:5:\r\nassertion failed: alice_pre.data.free - 10_000 >= alice_post.data.free\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n```\n"}
