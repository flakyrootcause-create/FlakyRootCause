{"id": 421, "ground_truth": "Unordered collections", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### What changes are proposed in this pull request?\r\n\r\nFix the flaky test in the two-choice random cache eviction policy\r\n\r\n### Why are the changes needed?\r\n\r\nAs the test is flaky, it will influence other PRs.\r\n\r\n### Does this PR introduce any user facing changes?\r\n\r\nNo.\r\n\n"}
{"id": 240, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test is flaky likely due to synchronization of the file ingestion thread and the live write thread with test sync points are not working as expected sometimes. Very occasionally, the live write thread can enter the write queue after file ingestion job already dequeued. Or it entered and waited for a very short period of time and quickly returned in the fast path: https://github.com/facebook/rocksdb/blob/833a2266a394fe5f140d2a22f406c82bb605c726/db/write_thread.cc#L83-L86\r\n\r\nTo fix the flakiness, I moved the test sync points to make sure the write thread is already linked into the write queue before the file ingestion writer get dequeued, so it definitely would need to wait some time in order to do its write.\r\n\r\nTest plan:\r\nI'm able to reproduce the flakiness with this command before the fix  with every two or three runs:\r\n./gtest-parallel external_sst_file_basic_test --gtest_filter=ExternalSSTFileBasicTest.Basic --repeat=10000 --workers=100\r\n\r\nAfter the fix, I have tried the command for 10 runs, and there is no failure detected.\n"}
{"id": 310, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe changes in #4577 introduced a bit of flakiness on pre-3.10 due to a weird interaction of `capsys`, `stderr` logging and our object lifecycles.\r\n\r\nIn this PR, I'm removing all the explicit `__del__` `kill`s since we [call them all explicitly in `client.close`](https://github.com/getsentry/sentry-python/blob/09c2e32cc7a618e49f5d8ae59e22d8b12f253687/sentry_sdk/client.py#L1001-L1021) anyway and that's already cleaner. Having logic in `__del__` causes non-deterministic GC behavior, especially with threaded code.\r\n\r\nStacktrace is linked in a comment below where you can see the `transport.__del__` method is causing the weird `reentrant` logging bug to `stderr`.\r\n  \n"}
{"id": 430, "ground_truth": "Randomness", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe bucket counts can vary run to run. Also don't hardcode the port\n"}
{"id": 160, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test JedisPoolTest.testCloseConnectionOnMakeObject occasionally fails with the following error:\r\n\r\n    `JedisPoolTest.testCloseConnectionOnMakeObject:403 expected:<4> but was:<5>`\r\n\r\n**Root Cause:**\r\nThe assertion is based on the output of getClientCount(jedis.clientList()), which is called immediately after closing a connection. However, Redis may take a short time to remove the closed connection from the client list, leading to a transient count mismatch and a flaky test result.\r\n\r\n**Example stack trace:**\r\n```\r\nError:  redis.clients.jedis.JedisPoolTest.testCloseConnectionOnMakeObject -- Time elapsed: 0.006 s <<< FAILURE!\r\njava.lang.AssertionError: expected:<4> but was:<5>\r\n\tat org.junit.Assert.fail(Assert.java:89)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:835)\r\n\tat org.junit.Assert.assertEquals(Assert.java:647)\r\n\tat org.junit.Assert.assertEquals(Assert.java:633)\r\n\tat redis.clients.jedis.JedisPoolTest.testCloseConnectionOnMakeObject(JedisPoolTest.java:403)\r\n```\r\n\t\r\ncloses #4135\n"}
{"id": 30, "ground_truth": "Async wait", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nWe currently use `package.json` for our configuration. I think we should move away from using `package.json` and use something like `next.config.js` since it gives us more power than JSON.\r\n\r\nAn example is the issue for custom Webpack support: https://github.com/zeit/next.js/issues/40.\r\nWith a config like this you could do this for example:\r\n\r\n```js\r\n// next.config.js\r\n\r\nexport default {\r\n  webpack: (webpackConfig) => {\r\n    const newConfig = { ...webpackConfig };\r\n    newConfig.module.preloaders.push({ test: /\\.js$/, loader: 'eslint-loader' });\r\n    return newConfig;\r\n  },\r\n  cdn: false\r\n}\r\n```\r\n\r\nWhich is in my opinion better than creating a new file for webpack since this is more centralised. This does however still give users the option to use different files, for the webpack example:\r\n\r\n```js\r\n// next.config.js\r\n\r\nexport default {\r\n  webpack: require('./webpack').default,\r\n  cdn: false\r\n}\r\n```\r\n\r\nThese are just my thoughts, I found out that I needed a sort of config file in `.js` when starting a PR for this project. Very curious what others think of this üòÑ.\n"}
{"id": 212, "ground_truth": "Concurrency", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #2495: **Kubelet doesn't kill old pods when BoundPods is empty**\n\nThe first time kubelet starts, while there are no updates, `kl.pods` is `nil` ([pkg/kubelet/kubelet.go#L1118](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1118)):\n\n``` GO\ncase <-time.After(kl.resyncInterval):\n    glog.V(4).Infof(\"Periodic sync\")\n    if kl.pods == nil {\n        continue\n    }\n```\n\nwhich guarantees that [`syncPods(...)`](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L992), which contains the container killing logic is never called.\n\nSo the first part of the fix is to remove this check and allow `syncPods(...)` to be called even when nil, so that the first periodic sync can clean up old containers.\n\nHowever, this isn't enough because inside `syncPods(...)` there is a check to make sure that all sources have been \"seen\" before it will kill old containers ([pkg/kubelet/kubelet.go#L1026](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/kubelet.go#L1026)):\n\n``` GO\nif !kl.sourcesReady() {\n    // If the sources aren't ready, skip deletion, as we may accidentally delete pods\n    // for sources that haven't reported yet.\n    glog.V(4).Infof(\"Skipping deletes, sources aren't ready yet.\")\n    return nil\n}\n```\n\nThis check is performed in the [`SeenAllSources()`](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L87) function, and is `true` only if a `SET` update has been seen for all registered kubelet sources.\n\nHowever, no `SET` update is emitted by any of the sources until they successfully read a value, which means the original problem still exists.\n\nSo the second part of the fix is to have the kubelet boundpod sources (etcd, file, and http) all emit a `SET` update with an empty pod list when the source is first read but no value exists; the empty update will not make it back to the kubelet sync loop because it will be filtered out by the [update merge logic](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L140); but it will cause the source to be [marked as seen](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/kubelet/config/config.go#L238) so the periodic sync (enabled by the first fix) will clean up the old containers (since all sources will now be marked as seen after the first read attempt).\n\n"}
{"id": 415, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n# Description\r\nFixes 3 flaky tests.\r\n\r\n# Changes\r\n- `local_node_store_filtered_solutions`\r\n    - it failed due to timeouts => increased the http delay buffer to give a bit of wiggle room\r\n    - sometimes an auction ran while only 1 of the 2 solutions was included => configure hardcoded solver solutions after both orders have been included in the auction\r\n- `local_node_partially_fillable_pool`\r\n    - solver got deny listed because it failed to produce a working solution for some reason\r\n        - updated the flags to enable that feature to work like `--flag=true` instead of `--flag` (to enable)\r\n        - disabled the feature on all tests except tests specifically for those features\r\n- `local_node_replace_order`\r\n    - order got settled before it could be replaced\r\n        - ban solver until the original order is no longer inside the auction\r\n        - allow unverified quotes for that test due to the banned solver account\r\n\r\n## How to test\r\nused the flaky test runner\r\n- ran all local node tests for 2h -> ~30 successful executions\r\n- `local_node_replace_order` -> flaky test [job](https://github.com/cowprotocol/services/actions/runs/17316920910/job/49161443945) with 369 successful runs\n"}
{"id": 409, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nI moved this in a previous PR, but I think firstly it's only actually required by one of the tests, and more importantly it might be reloading the object before the page has finished submitting.\n"}
{"id": 93, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nReference: https://spec.openapis.org/arazzo/v1.0.0#literals\r\n\r\nThis enables basic parsing of string literals as values in conditions, which in Arazzo are contained in single quote (') characters.\n"}
{"id": 341, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nThe test class has a 20% failure rate.\n\nhttps://github.com/xBis7/airflow/actions/runs/16086980198\n\nhttps://github.com/xBis7/airflow/actions/runs/16087424463\n\nhttps://github.com/xBis7/airflow/actions/runs/16087596566\n\nI'm attaching a file with the logs\n\n[fail_logs.txt](https://github.com/user-attachments/files/21076055/fail_logs.txt)\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nI can't reproduce it locally, it's occurring only on the remote CI. It can be reproduced by running the test on the CI on repeat. Increasing execution concurrency makes it more likely to occur. For example, for 10 iterations\n\n```\nmax-parallel: 10\n```\n\n\n\n### Operating System\n\nThe CI uses ubuntu-22.04.\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n"}
{"id": 419, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test checks if loading an app in the background will catch and re-raise an exception. It occasionally fails on pypy3. Possibly, the background loading thread does not finish before the assert line, and so there is no exception yet. Try to address this by waiting for the lock before asserting. This could still fail if the thread starts *very* slowly.\r\n\r\ncloses #4292 \n"}
{"id": 70, "ground_truth": "OS", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test has recently become flaky on windows CI, and before investigating further, see if it's just because the CI machines are overloaded and subprocesses are slower on windows.\n"}
{"id": 284, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nWhat I observed is that if I run both combinations useNewProtocol=true,\nuseNewProtocol=false it would often fail the second time, but if I only\nrun the second variation useNewProtocol=false it works, and only the\nfirst variation useNewProtocol=true also works. So this points to some\nstate that is not cleared between the tests  - and indeed, the test\ncreates a topic ‚ÄúinputTopic‚Äù, produces to it, but doesn‚Äôt delete it, so\nthe second variation will run with produce to it again and then run with\ntwice the data.\n\nI also reduced heartbeat interval and session timeout since some of the\ntests need to wait for the old consumer to leave which (sigh) Kafka\nStreams doesn't do, so we have to wait that it gets kicked out by\nsession timeout. So previously we waited for 45 seconds, now, we at\nleast wait only 1 second.\n\nReviewers: Bill Bejeck <bbejeck@apache.org>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n"}
{"id": 65, "ground_truth": "Time", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n - [x] closes #14626 \r\n - [x] tests added / passed\r\n - [x] passes ``git diff upstream/master | flake8 --diff``\r\n - [ ] whatsnew entry\r\n\r\n\n"}
{"id": 204, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nWhen you have a rewrite in middleware like this:\r\n```ts\r\nreturn NextResponse.rewrite(u, {\r\n  status: 403,\r\n});\r\n```\r\nThe rewrite status code is not propogated to the cache interceptor's responses when you have an external middleware. \r\n\r\nWe can also safely remove the `x-next-cache-tags` header on the response.\r\n\r\nI did update some tests that were flakey aswell. We need to register the event listener before the navigation.\n"}
{"id": 101, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nLink to issue: #8390\r\n\r\n## What is the purpose of the change\r\n\r\nThis PR is to fix a non-idempotent test `org.apache.dubbo.rpc.filter.ExecuteLimitFilterTest.testExecuteLimitInvokeWitException` \r\n- The test does not remove `RpcStatus` after running, which pollutes the state shared among tests.\r\n- It may be better to clean state pollutions so that some other tests won't fail in the future due to the shared state pollution.\r\n\r\n## Brief changelog\r\nRemoves the status of RPC when the test ends.\r\n\r\n## Verifying this change\r\n\r\nWith the proposed fix, the test does not pollute the shared state (and passes when run twice in the same JVM).\r\n\r\n<!-- Follow this checklist to help us incorporate your contribution quickly and easily: -->\r\n\r\n## Checklist\r\n- [x] Make sure there is a [GitHub_issue](https://github.com/apache/dubbo/issues) field for the change (usually before you start working on it). Trivial changes like typos do not require a GitHub issue. Your pull request should address just this issue, without pulling in other changes - one PR resolves one issue.\r\n- [x] Each commit in the pull request should have a meaningful subject line and body.\r\n- [x] Write a pull request description that is detailed enough to understand what the pull request does, how, and why.\r\n- [x] Check if is necessary to patch to Dubbo 3 if you are work on Dubbo 2.7\r\n- [x] Write necessary unit-test to verify your logic correction, more mock a little better when cross module dependency exist. If the new feature or significant change is committed, please remember to add sample in [dubbo samples](https://github.com/apache/dubbo-samples) project.\r\n- [x] Add some description to [dubbo-website](https://github.com/apache/dubbo-website) project if you are requesting to add a feature.\r\n- [x] GitHub Actions works fine on your own branch.\r\n- [x] If this contribution is large, please follow the [Software Donation Guide](https://github.com/apache/dubbo/wiki/Software-donation-guide).\n"}
{"id": 408, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nTest was failing the 15th of each month\n"}
{"id": 136, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'common/utils/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThe listener test in `ClientStreamingQuerySuite` is flaky.\r\n\r\nFor client side listeners, the terminated events might take a while before arriving to the client. This test is currently flaky, example: https://github.com/anishshri-db/spark/actions/runs/9785389228/job/27018350836\r\n\r\nThis PR tries to deflake it by waiting for a longer time.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nDeflake test\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nTest only change\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\n<!--\r\nIf generative AI tooling has been used in the process of authoring this patch, please include the\r\nphrase: 'Generated-by: ' followed by the name of the tool and its version.\r\nIf no, write 'No'.\r\nPlease refer to the [ASF Generative Tooling Guidance](https://www.apache.org/legal/generative-tooling.html) for details.\r\n-->\r\nNo\n"}
{"id": 191, "ground_truth": "Async wait", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nhttps://circleci.com/gh/square/okhttp/8330?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\r\n\r\ncloseReasonMaximumLength - okhttp3.internal.ws.WebSocketHttpTest\r\n```\r\norg.junit.ComparisonFailure: expected:<[0]> but was:<[1]>\r\n\tat okhttp3.OkHttpClientTestRule.ensureAllConnectionsReleased(OkHttpClientTestRule.kt:91)\r\n\tat okhttp3.OkHttpClientTestRule$apply$1.evaluate(OkHttpClientTestRule.kt:121)\r\n```\r\n\r\nserverCloseThenClientClose - okhttp3.internal.ws.RealWebSocketTest\r\n```\r\njava.lang.AssertionError: \r\nExpecting empty but was:<[Closed[1000 Hello!]]>\r\n\tat okhttp3.internal.ws.WebSocketRecorder.assertExhausted(WebSocketRecorder.java:163)\r\n\tat okhttp3.internal.ws.RealWebSocketTest.tearDown(RealWebSocketTest.java:58)\r\n```\n"}
{"id": 202, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe `getContext` seem to randomly timing out on github action. \r\n<img width=\"494\" alt=\"image\" src=\"https://github.com/radarlabs/radar-sdk-ios/assets/139801512/9de1176f-0d6b-434e-b63a-f28b242da8e2\">\r\n<img width=\"417\" alt=\"image\" src=\"https://github.com/radarlabs/radar-sdk-ios/assets/139801512/ec3ec059-f3cc-47aa-8f3f-54ac4a7e72ad\">\r\nThis is likely an issue with github action because\r\n- This does not occur on circle CI\r\n- This does not occur on local testing\r\nAs a sanity check, the SDK endpoint for `getContext` was called on waypoint with no noticeable issue. \n"}
{"id": 379, "ground_truth": "Resource leak", "predicted": "Resource leak", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nReduce flakiness/sensitivity of PyFunc cleanup test.\n\nOccasionally, unrelated-seeming changes would cause the GC-based cleanup test to fail, because `gc.collect()` did not collect all garbage. Calling `gc.collect()` twice in sequence will perform a more thorough collection, including cycles that were broken in the first call to `gc.collect()`. There is a plausible theory that the finalizers used in the `WeakValueDictionary` in `script_ops._py_funcs` could be contributing to the issue.\n\n"}
{"id": 107, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Problem\nThe\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutTopic()`,\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutGroup()`,\n`ResetStreamsGroupOffsetTest.testResetOffsetsWithoutGroupOption()`,\n`DeleteStreamsGroupTest.testDeleteWithoutGroupOption()`,\n`DescribeStreamsGroupTest.testDescribeWithoutGroupOption()` tests were\nflaky due to a dependency on Set iteration order in error message\ngeneration.\n\n### Root Cause\nThe cleanup [commit](https://github.com/apache/kafka/pull/20091) that\nreplaced `new HashSet<>(Arrays.asList(...))` with `Set.of(...)` in\nShareGroupCommandOptions and StreamsGroupCommandOptions changed the\niteration characteristics of collections used for error message\ngeneration:\n\nThis produces different orders like `[topic], [group]` vs `[group],\n[topic]`, but the tests expected a specific order, causing intermittent\nfailures.\n\n### Solution\nFix the root cause by ensuring deterministic error message generation\nthrough alphabetical sorting of option names.\n\nReviewers: ShivsundarR <shr@confluent.io>, Ken Huang\n <s7133700@gmail.com>, TengYao Chi <frankvicky@apache.org>\n\n"}
{"id": 235, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nFixing existing runs:\r\n- [x] Don't run Jedi on PyPy. (see https://github.com/python-trio/trio/pull/2887 and https://github.com/python-trio/trio/pull/3075) (need to update `test_static_tool_sees_class_members` too)\r\n- [x] Only run autodeps in `python-trio/trio` (see https://github.com/python-trio/trio/pull/2892)\r\n- [x] Add a `gc.collect()` to the top of `test_for_leaking_fds` (this seems to address test flakiness I've seen elsewhere) (see https://github.com/python-trio/trio/pull/2888)\r\n- [x] Start running `mypy` on PyPy. (see https://github.com/python-trio/trio/pull/3075)\r\n- [x] switch macos to run on m1 (see https://github.com/python-trio/trio/pull/2953)\r\n\r\nAdding more runs:\r\n- [ ] Add an anaconda run\r\n- [x] Alpine run (see https://github.com/python-trio/trio/pull/2933)\r\n- [ ] Try out qemu for a freebsd\r\n- [x] Cython run (see https://github.com/python-trio/trio/pull/2942)\r\n  - [ ] Run full test suite on Cython\r\n- [x] Update MacOS and Windows Python matrix to support 3.11 and 3.12. (see https://github.com/python-trio/trio/pull/3017)\r\n- [x] Add CPython 3.13 (see https://github.com/python-trio/trio/pull/3005)\r\n- [ ] ... Android run??? (https://github.com/marketplace/actions/android-emulator-runner ??)\r\n- [x] pypy nightly 3.11 (not possible)\r\n- [x] add pypy runs for macos (see https://github.com/python-trio/trio/pull/3074)\r\n- [x] add pypy runs for windows (see #2776 and #2678) (see https://github.com/python-trio/trio/pull/3074)\r\n\r\nRemoving runs:\r\n- [x] Don't run PyPy nightlies less than 3.11 (see https://github.com/python-trio/trio/pull/2952)\r\n\r\nThese feel like they will take more motivation than I currently have :(\n"}
{"id": 36, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #135370\n\nSummary: This test is flaky when run after `test_dynamic_shapes_persistent_reduction_mixed_x_dim_cuda_cuda_wrapper` because the TestCase sets config options globally in its setUp() that stick around for subsequent tests. For test isolation, we use a contextlib.ExitStack pattern in other tests to patch the config options and restore them in tearDown(). Update all TestCases in `test/inductor/test_combo_kernels.py` to use that pattern.\n\nTest Plan:\n```\npython test/inductor/test_combo_kernels.py\npython test/inductor/test_cuda_cpp_wrapper.py TestCudaWrapper.test_dynamic_shapes_persistent_reduction_mixed_x_dim_cuda_cuda_wrapper TestCudaWrapper.test_randint_cuda_cuda_wrapper\n```\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang\n"}
{"id": 42, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\r\n\r\n`test/parallel/test-zlib-brotli-16GB.js`\r\n\r\n### Platform\r\n\r\nAIX\r\n\r\n### Console output\r\n\r\n```console\r\n17:07:37 not ok 3588 parallel/test-zlib-brotli-16GB\r\n17:07:37   ---\r\n17:07:37   duration_ms: 412.84800\r\n17:07:37   severity: fail\r\n17:07:37   exitcode: 1\r\n17:07:37   stack: |-\r\n17:07:37     node:assert:126\r\n17:07:37       throw new AssertionError(obj);\r\n17:07:37       ^\r\n17:07:37     \r\n17:07:37     AssertionError [ERR_ASSERTION]: Expected values to be strictly equal:\r\n17:07:37     \r\n17:07:37     0 !== 1\r\n17:07:37     \r\n17:07:37         at Timeout.<anonymous> (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/parallel/test-zlib-brotli-16GB.js:21:3)\r\n17:07:37         at Timeout._onTimeout (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/common/index.js:476:15)\r\n17:07:37         at listOnTimeout (node:internal/timers:573:17)\r\n17:07:37         at process.processTimers (node:internal/timers:514:7) {\r\n17:07:37       generatedMessage: true,\r\n17:07:37       code: 'ERR_ASSERTION',\r\n17:07:37       actual: 0,\r\n17:07:37       expected: 1,\r\n17:07:37       operator: 'strictEqual'\r\n17:07:37     }\r\n17:07:37     \r\n17:07:37     Node.js v21.6.3-pre\r\n17:07:37   ...\r\n```\r\n\r\n\r\n### Build links\r\n\r\n- https://ci.nodejs.org/job/node-test-commit-aix/50223/nodes=aix72-ppc64/console\r\n\r\n### Additional information\r\n\r\n_No response_\n"}
{"id": 76, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n##### SUMMARY\r\n\r\nThis PR fixes various issues with the ansible-playbook-callbacks integration test.\r\n\r\n##### ISSUE TYPE\r\n\r\n- Test Pull Request\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nThe timing of the async tasks was a little unpredictable, meaning that sometimes we would get an unexpected number of v2_runner_on_async_poll callbacks, and fail the test. This change fixes the issue by increasing the poll interval to 2 seconds and the sleep duration to 3 seconds, such that on a reasonably responsive system we will poll twice per task, with the sleep ending in the middle of the two polls.\r\n    \r\nThe include_me.yml file does not exist in this integration test. It has been added.\r\n    \r\nThe remote_tmp_dir.path expression is invalid - the setup_remote_tmp_dir role uses set_fact to set remote_tmp_dir to remote_tmp_dir.path.\r\n    \r\nThe integration tests run with ANSIBLE_HOST_PATTERN_MISMATCH=error, meaning that the final play was never reached. Set ANSIBLE_HOST_PATTERN_MISMATCH=warning to continue past the play and trigger the v2_playbook_on_no_hosts_matched callback.\n"}
{"id": 423, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### About\nThis PR attempts to removed the flakiness in\n`testAlterReadCommittedToReadUncommittedIsolationLevelWithReleaseAck`\nand `testAlterReadCommittedToReadUncommittedIsolationLevelWithRejectAck`\n([link](https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.test=testAlterReadCommittedToReadUncommittedIsolationLevelWithReleaseAck()%5B2%5D)).\nThis flakiness could potentially be caused because we were not ensuring\nthat the aborted transaction record produce happened. In this PR, I have\nadded a blocking call to make sure the produce future completes before\nwe abort the transaction.\n\n### Testing\nThe fix was tested locally by running the tests\n\nReviewers: Andrew Schofield <aschofield@confluent.io>\n\n"}
{"id": 130, "ground_truth": "I/O", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Summary of the Pull Request\r\nThis might be the problem why from time to time `MarkdownPreviewHandlerControlAddsBrowserToFormWhenDoPreviewIsCalled ` test fails - timeouts after 10s\r\n\r\n**What is this about:**\r\n\r\n**What is included in the PR:** \r\n\r\n**How does someone test / validate:** \r\n\r\n## Quality Checklist\r\n\r\n- [ ] **Linked issue:** #xxx\r\n- [ ] **Communication:** I've discussed this with core contributors in the issue. \r\n- [ ] **Tests:** Added/updated and all pass\r\n- [ ] **Installer:** Added/updated and all pass\r\n- [ ] **Localization:** All end user facing strings can be localized\r\n- [ ] **Docs:** Added/ updated\r\n- [ ] **Binaries:** Any new files are added to WXS / YML\r\n   - [ ] No new binaries\r\n   - [ ] [YML for signing](https://github.com/microsoft/PowerToys/blob/main/.pipelines/pipeline.user.windows.yml#L68) for new binaries\r\n   - [ ] [WXS for installer](https://github.com/microsoft/PowerToys/blob/main/installer/PowerToysSetup/Product.wxs) for new binaries\r\n\r\n## Contributor License Agreement (CLA)\r\nA CLA must be signed. If not, go over [here](https://cla.opensource.microsoft.com/microsoft/PowerToys) and sign the CLA.\r\n\n"}
{"id": 178, "ground_truth": "Concurrency", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #13547\n"}
{"id": 141, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/apache/airflow/actions/runs/16658216543/job/47149178223#step:10:696\r\n\r\nSometimes the test failing with cause of other test stored cache.\r\nWhen we run entire test file its working if we run only `task-sdk/tests/task_sdk/io/test_path.py::TestAttach::test_alias` , this was failing due to there is no previously cached object and its asserting for two fields.\r\n \r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n"}
{"id": 291, "ground_truth": "Concurrency", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nCloses #12889 \n"}
{"id": 382, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Summary\n- Fixed flaky combobox tests that were failing due to CSS transitions\n\n## Problem\nThe combobox tests were failing with \"Unable to find an accessible element with the role 'listbox'\" because they were using `getByRole` which expects the element to be immediately visible. However, the Menu component applies CSS transitions (invisible scale-95 opacity-0) when closed, making the listbox technically present but not visible.\n\n## Solution\nChanged from `getByRole('listbox')` to `findByRole('listbox')` which waits for the element to become visible, accounting for the CSS transition timing.\n\n## Test Plan\n[ ] Run `pnpm stories:test` - combobox tests should pass\n[ ] Verify Async Select test passes\n[ ] Verify Multiselect test passes\n"}
{"id": 330, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### Description of Change\nI'm not sure what event to hook here, so poll instead.\n\n#### Checklist\n<!-- Remove items that do not apply. For completed items, change [ ] to [x]. -->\n\n- [x] PR description included and stakeholders cc'd\n- [x] `npm test` passes\n- [x] tests are [changed or added](https://github.com/electron/electron/blob/master/docs/development/testing.md)\n- [x] PR title follows semantic [commit guidelines](https://github.com/electron/electron/blob/master/docs/development/pull-requests.md#commit-message-guidelines)\n- [x] This is **NOT A BREAKING CHANGE**. Breaking changes may not be merged to master until 11-x-y is branched.\n\n#### Release Notes\n\nNotes: none\n\n"}
{"id": 19, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n- **Google Hangouts Video**: http://www.youtube.com/watch?v=Z0KHYPlFI3E\n- **GitHub Issue**: https://github.com/iojs/io.js/issues/300\n- **Original Minutes Google Doc**: https://docs.google.com/document/d/1HDUayYxTjolYZhu_hGm9hc-6MGAwWrHlNGT2Zo708ck\n\n"}
{"id": 194, "ground_truth": "Test order dependency", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThese tests were causing flakes where the mock method was being called more than once. The tests were also difficult to understand.\n\nThis change removes the need for mocking (hopefully increasing test stability) and also should hopefully make it easier to understand what these tests are meant to be checking\n"}
{"id": 137, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nWhat I observed is that if I run both combinations useNewProtocol=true,\nuseNewProtocol=false it would often fail the second time, but if I only\nrun the second variation useNewProtocol=false it works, and only the\nfirst variation useNewProtocol=true also works. So this points to some\nstate that is not cleared between the tests  - and indeed, the test\ncreates a topic ‚ÄúinputTopic‚Äù, produces to it, but doesn‚Äôt delete it, so\nthe second variation will run with produce to it again and then run with\ntwice the data.\n\nI also reduced heartbeat interval and session timeout since some of the\ntests need to wait for the old consumer to leave which (sigh) Kafka\nStreams doesn't do, so we have to wait that it gets kicked out by\nsession timeout. So previously we waited for 45 seconds, now, we at\nleast wait only 1 second.\n\nReviewers: Bill Bejeck <bbejeck@apache.org>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n"}
{"id": 395, "ground_truth": "OS", "predicted": "OS", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\ntest-child-process-cwd\n\n### Platform\n\nWindows\n\n### Console output\n\n```console\nnode:assert:126\r\n  throw new AssertionError(obj);\r\n  ^\r\n\r\nAssertionError [ERR_ASSERTION]: Expected values to be strictly equal:\r\n+ actual - expected\r\n\r\n+ 'C:\\\\Windows'\r\n- 'C:\\\\WINDOWS'\r\n        ^\r\n    at ChildProcess.<anonymous> (C:\\Users\\admin\\WebstormProjects\\node\\test\\parallel\\test-child-process-cwd.js:55:26)\r\n    at ChildProcess.<anonymous> (C:\\Users\\admin\\WebstormProjects\\node\\test\\common\\index.js:487:15)\r\n    at ChildProcess.emit (node:events:520:28)\r\n    at maybeClose (node:internal/child_process:1105:16)\r\n    at Socket.<anonymous> (node:internal/child_process:457:11)\r\n    at Socket.emit (node:events:520:28)\r\n    at Pipe.<anonymous> (node:net:337:12) {\r\n  generatedMessage: true,\r\n  code: 'ERR_ASSERTION',\r\n  actual: 'C:\\\\Windows',\r\n  expected: 'C:\\\\WINDOWS',\r\n  operator: 'strictEqual'\r\n}\r\n\r\nNode.js v23.0.0-pre\n```\n\n\n### Build links\n\n- \n\n### Additional information\n\n### Issue Description\r\n\r\nI built Node.js on my local computer using the `vcbuild.bat` script. After the build was completed, I verified it through the `Release\\node` directory. However, when running `.\\vcbuild test`, I consistently encountered the same error. The error seems to be caused by Windows not distinguishing between uppercase and lowercase folder names.\r\n\n"}
{"id": 229, "ground_truth": "Time", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nAlso change cloudcfg.sh, to make it possible to call from the examples/guestbook directory.\n\n"}
{"id": 17, "ground_truth": "Network", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nNo out-of-tree patches, yay!\n\nR=@indutny\n\n"}
{"id": 163, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nNot too sure what the problem is exactly but my suspicion is that the profiler runs in a separate thread and needs time to flush the chunk, the test wasn't waiting long enough.\n"}
{"id": 349, "ground_truth": "Randomness", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThere is some flakiness for the checked error messages, as some include a \"verification_error:\" prefix. I've used regexes to avoid that, and removing the line / character information on the error being checked. \n"}
{"id": 387, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### Trac ticket number\r\n\r\nN/A\r\n\r\n#### Branch description\r\n\r\nTwo separate calls to look up access time can result in sub-second differences which cause the test to fail.\r\n\r\nWhen running the full test suite locally, this is consistently failing for me.\r\n\r\n#### Checklist\r\n\r\n- [x] This PR targets the `main` branch. <!-- Backports will be evaluated and done by mergers, when necessary. -->\r\n- [x] The commit message is written in past tense, mentions the ticket number, and ends with a period.\r\n- [ ] I have checked the \"Has patch\" ticket flag in the Trac system.\r\n- [x] I have added or updated relevant tests.\r\n- [ ] I have added or updated relevant docs, including release notes if applicable.\r\n- [ ] I have attached screenshots in both light and dark modes for any UI changes.\n"}
{"id": 143, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nIn Postgres especially (but in generaly in all databases, if\r\nthere is no order specified in select query, the rows might\r\ncome in random order. It depends on many factors.\r\n\r\nThe test query here retrieved the dags without any order but\r\nexpected the list to be in specific order.\r\n\r\nThis PR adds ordering - it also removes side-effects of the\r\ntest by using fixture that clears the datasets before and after\r\nthe tests that rely/modify datasets - because othrwise failure of\r\none of the tests can create side effects that fail the other\r\ntests (this is what happened in this case)\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n"}
{"id": 237, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Thank you for your contribution! Please review https://github.com/ray-project/ray/blob/master/CONTRIBUTING.rst before opening a pull request. -->\r\n\r\n<!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->\r\n\r\n## Why are these changes needed?\r\n\r\n<!-- Please give a short summary of the change and the problem this solves. -->\r\n\r\nFixing [flakey](https://buildkite.com/anyscale/rayturbo/builds/5914#0198edc9-d671-4075-b788-4c0c555b29db) test_operators by explicitly sorting before comparing the results.\r\n\r\n## Related issue number\r\n\r\n<!-- For example: \"Closes #1234\" -->\r\n\r\n## Checks\r\n\r\n- [ ] I've signed off every commit(by using the -s flag, i.e., `git commit -s`) in this PR.\r\n- [ ] I've run `scripts/format.sh` to lint the changes in this PR.\r\n- [ ] I've included any doc changes needed for https://docs.ray.io/en/master/.\r\n    - [ ] I've added any new APIs to the API Reference. For example, if I added a\r\n           method in Tune, I've added it in `doc/source/tune/api/` under the\r\n           corresponding `.rst` file.\r\n- [ ] I've made sure the tests are passing. Note that there might be a few flaky tests, see the recent failures at https://flakey-tests.ray.io/\r\n- Testing Strategy\r\n   - [ ] Unit tests\r\n   - [ ] Release tests\r\n   - [ ] This PR is not tested :(\r\n\n"}
{"id": 90, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\nThank you for your contribution!\r\nUnless your change is trivial, please create an issue to discuss the change before creating a PR.\r\n-->\r\n\r\n### Describe your changes:\r\n\r\nFixes <issue-number>\r\n\r\n<!--\r\nShort blurb explaining:\r\n- What changes did you make?\r\n- Why did you make them?\r\n- How did you test your changes?\r\n-->\r\n\r\nI worked on ... because ...\r\n\r\n<!-- For frontend related change, please add screenshots and/or videos of your changes preview! -->\r\n\r\n#\r\n### Type of change:\r\n<!-- You should choose 1 option and delete options that aren't relevant -->\r\n- [ ] Bug fix\r\n- [ ] Improvement\r\n- [ ] New feature\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] Documentation\r\n\r\n#\r\n### Checklist:\r\n<!-- add an x in [] if done, don't mark items that you didn't do !-->\r\n- [x] I have read the [**CONTRIBUTING**](https://docs.open-metadata.org/developers/contribute) document.\r\n- [ ] My PR title is `Fixes <issue-number>: <short explanation>`\r\n- [ ] I have commented on my code, particularly in hard-to-understand areas. \r\n- [ ] For JSON Schema changes: I updated the migration scripts or explained why it is not needed.\r\n\r\n<!-- Based on the type(s) of your change, uncomment the required checklist üëá -->\r\n\r\n<!-- Bug fix\r\n- [ ] I have added a test that covers the exact scenario we are fixing. For complex issues, comment the issue number in the test for future reference.\r\n-->\r\n\r\n<!-- Improvement\r\n- [ ] I have added tests around the new logic.\r\n- [ ] For connector/ingestion changes: I updated the documentation.\r\n-->\r\n\r\n<!-- New feature\r\n- [ ] The issue properly describes why the new feature is needed, what's the goal, and how we are building it. Any discussion\r\n    or decision-making process is reflected in the issue.\r\n- [ ] I have updated the documentation.\r\n- [ ] I have added tests around the new logic.\r\n-->\r\n\r\n<!-- Breaking change\r\n- [ ] I have added the tag `Backward-Incompatible-Change`.\r\n-->\r\n\n"}
{"id": 373, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://scans.gradle.com/s/bfgjiqwioneey/tests/task/:profiler:test/details/com.splunk.opentelemetry.profiler.snapshot.LongRunningBackgroundTaskTest/traceBackgroundThreadProfilingContinuesAfterEntrySpanEnds()?top-execution=1\n"}
{"id": 52, "ground_truth": "Async Wait", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Rationale for this change\nFixing flaky bloom filter memory check test\n\n### What changes are included in this PR?\nAdded an additional runtime.GC() call to ensure the releases are called.\n\n\n"}
{"id": 299, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n##### SUMMARY\r\n\r\nThis PR fixes various issues with the ansible-playbook-callbacks integration test.\r\n\r\n##### ISSUE TYPE\r\n\r\n- Test Pull Request\r\n\r\n##### ADDITIONAL INFORMATION\r\n\r\nThe timing of the async tasks was a little unpredictable, meaning that sometimes we would get an unexpected number of v2_runner_on_async_poll callbacks, and fail the test. This change fixes the issue by increasing the poll interval to 2 seconds and the sleep duration to 3 seconds, such that on a reasonably responsive system we will poll twice per task, with the sleep ending in the middle of the two polls.\r\n    \r\nThe include_me.yml file does not exist in this integration test. It has been added.\r\n    \r\nThe remote_tmp_dir.path expression is invalid - the setup_remote_tmp_dir role uses set_fact to set remote_tmp_dir to remote_tmp_dir.path.\r\n    \r\nThe integration tests run with ANSIBLE_HOST_PATTERN_MISMATCH=error, meaning that the final play was never reached. Set ANSIBLE_HOST_PATTERN_MISMATCH=warning to continue past the play and trigger the v2_playbook_on_no_hosts_matched callback.\n"}
{"id": 418, "ground_truth": "Randomness", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nExperiment with a new background task library that uses Redis Streams\n"}
{"id": 47, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis was straightforward to reproduce locally, both with and without scheduler-side queuing. Unsure why on CI it seems to only fail without queuing.\r\n\r\n![image](https://github.com/dask/distributed/assets/6213168/ae93bdb1-0624-4543-8b0d-cd003e0570f1)\r\n\r\nNow the test is rock solid (ran 200 times per CI environment).\n"}
{"id": 426, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n/kind failing-test\r\n<!--\r\nAdd one of the following kinds:\r\n/kind bug\r\n/kind cleanup\r\n/kind documentation\r\n/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n/kind api-change\r\n/kind deprecation\r\n/kind failing-test\r\n/kind flake\r\n/kind regression\r\n-->\r\n\r\n#### What this PR does / why we need it:\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes part of #128113 #129036\r\n\r\n#### Special notes for your reviewer:\r\nIn the PreStop hook of a pod, we continuously output \"Looping\" in a loop and sleep for 1 second. The script declared in PreStop is as follows:\r\n```\r\nsh -c '\r\ntouch /persistent/my-container.log;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Starting 0'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\n_term() { sleep 0; echo $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Exiting'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1; exit 0; };\r\ntrap _term TERM;\r\ntouch started;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Started'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Delaying 0'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\nwhile true; do\r\n  echo $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Looping'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\n  sleep 1;\r\ndone;\r\necho $(date -u +%FT$(nmeter -d0 '%3t' | head -n1)Z) '\\''prestop-my-container Exiting'\\'' | tee -a /persistent/my-container.log >> /proc/1/fd/1;\r\nexit 0'\r\n```\r\nIt exhibits the following issues:\r\n\r\n1. The while loop execution time isn't consistently 1 second - it often takes slightly longer by tens of milliseconds\r\n\r\n2. The start/end times of the while loop vary across containers (not just loop execution variance, but also hundreds of milliseconds difference in when kubelet initiates preStop execution)\r\n\r\n3. When kubelet forcibly terminates containers, due to the inconsistent loop timings across containers, some loops might execute N times while others execute N+1 times\r\n\r\nTo mitigate these, I modified the loop timing to be configurable externally through ExecCommand parameters, which should help alleviate these issues.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n"}
{"id": 248, "ground_truth": "Time", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n\r\n<!--\r\n  Thanks for sending a pull request!\r\n    1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n    2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n-->\r\n\r\n### Description of PR\r\nPlease refer to YARN-11816 for details.\r\n\r\n### How was this patch tested?\r\n\r\n\r\n### For code changes:\r\n\r\n- [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n- [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n- [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n- [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n\r\n\n"}
{"id": 241, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nSummary: `DBErrorHandlingFSTest.AtomicFlushNoSpaceError` is flaky due to seg fault during error recovery:\r\n```\r\n...\r\nframe #5: 0x00007f0b3ea0a9d6 librocksdb.so.9.10`rocksdb::VersionSet::GetObsoleteFiles(std::vector<rocksdb::ObsoleteFileInfo, std::allocator<rocksdb::ObsoleteFileInfo>>*, std::vector<rocksdb::ObsoleteBlobFileInfo, std::allocator<rocksdb::ObsoleteBlobFileInfo>>*, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>>*, unsigned long) [inlined] std::vector<rocksdb::ObsoleteFileInfo, std::allocator<rocksdb::ObsoleteFileInfo>>::begin(this=<unavailable>) at stl_vector.h:812:16\r\nframe #6: 0x00007f0b3ea0a9d6 librocksdb.so.9.10`rocksdb::VersionSet::GetObsoleteFiles(this=0x0000000000000000, files=size=0, blob_files=size=0, manifest_filenames=size=0, min_pending_output=18446744073709551615) at version_set.cc:7258:18\r\nframe #7: 0x00007f0b3e8ccbc0 librocksdb.so.9.10`rocksdb::DBImpl::FindObsoleteFiles(this=<unavailable>, job_context=<unavailable>, force=<unavailable>, no_full_scan=<unavailable>) at db_impl_files.cc:162:30\r\nframe #8: 0x00007f0b3e85e698 librocksdb.so.9.10`rocksdb::DBImpl::ResumeImpl(this=<unavailable>, context=<unavailable>) at db_impl.cc:434:20\r\nframe #9: 0x00007f0b3e921516 librocksdb.so.9.10`rocksdb::ErrorHandler::RecoverFromBGError(this=<unavailable>, is_manual=<unavailable>) at error_handler.cc:632:46\r\n```\r\n\r\nI suspect that this is due to DB being destructed and reopened during recovery. Specifically, the [ClearBGError() call](https://github.com/facebook/rocksdb/blob/c72e79a262bf696faf5f8becabf92374fc14b464/db/db_impl/db_impl.cc#L425) can release and re-acquire mutex, and DB can be closed during this time. So it's not safe to access DB states after ClearBGError(). There was a similar story in #9496. [Moving the obsolete files logic after ClearBGError()](https://github.com/facebook/rocksdb/pull/11955) probably makes the seg fault more easily triggered.\r\n\r\nThis PR updates `ClearBGError()` to guarantee that db close cannot finish until the method returns and the mutex is released. So that we can safely access DB states after calling it.\r\n\r\n\r\nTest plan: I could not trigger the seg fault locally, will just monitor future test failures.\r\n\r\n\n"}
{"id": 153, "ground_truth": "Async Wait", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n**What type of PR is this?**\r\n\r\n/kind bug\r\n\r\n**What this PR does / why we need it**:\r\n\r\nPod with PVC will not be scheduled if the PVC is being deleted.\r\nThis can happen when the PVC has finalizers of storage plugins.\r\n\r\nSuch a pod becomes pending.  Unfortunately, after the finalizer\r\nfinishes and PVC is deleted, the pod remains pending forever.\r\nThe StatefulSet controller does nothing for this pending pod.\r\n\r\nThis commit prevents the StatefulSet controller from creating\r\nsuch pods when PVC is to be deleted.\r\n\r\nReprocedure:\r\n\r\n1. Create a single node cluster with [kind](https://github.com/kubernetes-sigs/kind).\r\n2. Create a StatefulSet with the following manifests:\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: test\r\n  namespace: default\r\nspec:\r\n  clusterIP: None\r\n  selector:\r\n    app: test\r\n---\r\napiVersion: apps/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  name: test\r\n  namespace: default\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: test\r\n  serviceName: test\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: test\r\n    spec:\r\n      containers:\r\n      - command:\r\n        - pause\r\n        image: quay.io/cybozu/ubuntu:18.04\r\n        name: ubuntu\r\n        volumeMounts:\r\n        - mountPath: /usr/share/nginx/html\r\n          name: www\r\n  volumeClaimTemplates:\r\n  - metadata:\r\n      name: www\r\n    spec:\r\n      accessModes:\r\n      - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      volumeMode: Filesystem\r\n```\r\n\r\n3. Once the pod gets running, edit PVC to add a finalizer `aaa.bbb/ccc`.\r\n4. Run `kubectl delete --wait=false pvc/www-test-0`.\r\n5. Run `kubectl delete --wait=false pods/test-0`.\r\n6. See a new pod is created and become pending.\r\n7. Edit the PVC to remove the `aaa.bbb/ccc` finalizer.\r\n8. See the PVC is get deleted.\r\n9. See the Pod remains pending forever.\r\n\r\n**Does this PR introduce a user-facing change?**:\r\n```release-note\r\nAdding fix to statefulset controller to wait for pvc deletion before creating pods.\r\n```\r\n\n"}
{"id": 392, "ground_truth": "OS", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\n`test/async-hooks/test-improper-order.js`\n\n### Platform\n\nAIX\n\n### Console output\n\n```console\nnot ok 68 async-hooks/test-improper-order\n  ---\n  duration_ms: 459.21600\n  severity: fail\n  exitcode: 1\n  stack: |-\n    node:assert:94\n      throw new AssertionError(obj);\n      ^\n    \n    AssertionError [ERR_ASSERTION]: Expected values to be strictly equal:\n    \n    null !== 1\n    \n        at ChildProcess.<anonymous> (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/async-hooks/test-improper-order.js:55:12)\n        at ChildProcess.<anonymous> (/home/iojs/build/workspace/node-test-commit-aix/nodes/aix72-ppc64/test/common/index.js:437:15)\n        at ChildProcess.emit (node:events:507:28)\n        at maybeClose (node:internal/child_process:1101:16)\n        at ChildProcess._handle.onexit (node:internal/child_process:305:5) {\n      generatedMessage: true,\n      code: 'ERR_ASSERTION',\n      actual: null,\n      expected: 1,\n      operator: 'strictEqual'\n    }\n    \n    Node.js v25.0.0-pre\n  ...\n```\n\n### Build links\n\n- https://ci.nodejs.org/job/node-test-commit-aix/57496/nodes=aix72-ppc64/testReport/junit/(root)/async-hooks/test_improper_order/\n\n### Additional information\n\nThis flake is appearing in the reliability report 3 times daily from 2025-05-31 - https://github.com/nodejs/reliability/issues/1217 till today 2025-06-03 - https://github.com/nodejs/reliability/issues/1220.\n\nThe same strategy in https://github.com/nodejs/node/pull/58478 can be used to deflake this test.\n"}
{"id": 273, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nTake 3: on win32 we use empty.txt in the fixtures directory, otherwise we use a file constructed specifically for this test due to POSIX socket path length limitations, in which case we need to do appropriate cleanup. Cleaning up empty.txt in fixtures causes downstream problems for other tests using it (currently the next test is test-zlib).\n\n"}
{"id": 313, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis improves the SmokeTestDriverIntegrationTest in three ways:\n\n1) If a SmokeTestClient fails (enters a terminal ERROR state), the\nSmokeTestDriverIntegrationTest currently times out, because it keeps\nwaiting for state NOT_RUNNING. This makes debugging quite difficult.\nThis minor  change makes sure to just fail the test immediately, if a\nSmokeTestClient enters the ERROR state.\n\n2) If a test times out or fails prematurely, because a SmokeTestClient\ncrashed, the SmokeTestClients aren't shut down correctly, which will\naffect the following test runs. Therefore, I am adding clean-up logic\nthat running SmokeTestClients in `@AfterAll`.\n\n3) Finally, I found that the processingThread variation of this thread\ntriggers a subtle race condition. Since this features is currently not\nactively developed, I disabled those variations and created a ticket to\nreactivate the test.\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Chia-Ping Tsai\n <chia7712@gmail.com>, Bill Bejeck <bill@confluent.io>\n\n"}
{"id": 122, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Thank you for submitting a Pull Request. Please:\n* Read our Pull Request guidelines:\n  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests\n* Associate an issue with the Pull Request.\n* Ensure that the code is up-to-date with the `main` branch.\n* Include a description of the proposed changes and how to test them.\n-->\n\n"}
{"id": 253, "ground_truth": "Unordered collections", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nticket-23842\n"}
{"id": 247, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Description of PR\r\ntestDecommissionStatus keeps failing intermittently.\r\n```\r\n[ERROR] testDecommissionStatus(org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor)  Time elapsed: 3.299 s  <<< FAILURE!\r\njava.lang.AssertionError: Unexpected num under-replicated blocks expected:<4> but was:<3>\r\n\tat org.junit.Assert.fail(Assert.java:89)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:835)\r\n\tat org.junit.Assert.assertEquals(Assert.java:647)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.checkDecommissionStatus(TestDecommissioningStatus.java:169)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor.testDecommissionStatus(TestDecommissioningStatusWithBackoffMonitor.java:136)\r\n```\r\n\r\n### How was this patch tested?\r\nLocal run of unit test\r\n\r\n### For code changes:\r\n\r\n- [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n\n"}
{"id": 286, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #10853\r\n\r\nI think the issue is that we do DNS client request to the DNS caching server, and expect the request to fail on client side, as caching server doesn't respond in time (as the server is waiting for a response).\r\n\r\nMake sure we use a short timeout on our client side, so that we time out before a server does.\r\n\n"}
{"id": 389, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis fixes a failure that I have with #16965.\n"}
{"id": 123, "ground_truth": "I/O", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n 1) VSCode Smoke Tests (Electron)\r\n       Terminal\r\n         Terminal Persistence\r\n           detach/attach\r\n             should persist buffer content:\r\n     Error: Timeout: get element '.quick-input-widget' after 20 seconds.\r\n      at Code.poll (/Users/runner/work/1/s/test/automation/out/code.js:204:23)\r\n      at async Code.waitForElement (/Users/runner/work/1/s/test/automation/out/code.js:163:16)\r\n      at async QuickInput.waitForQuickInputClosed (/Users/runner/work/1/s/test/automation/out/quickinput.js:32:9)\r\n      at async Terminal.runCommandWithValue (/Users/runner/work/1/s/test/automation/out/terminal.js:109:9)\r\n      at async Context.<anonymous> (out/areas/terminal/terminal-persistence.test.js:64:17)\r\n\r\nhttps://dev.azure.com/monacotools/Monaco/_build/results?buildId=278997&view=logs&j=260a512b-e5ca-5cf1-3f8e-20c0fa41efc9&t=dc7a7a05-ae89-521b-55db-0cb2ded34668\n"}
{"id": 110, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nI've seen this fail a couple times\n\n```\n=== Failed\n=== FAIL: codegen/pcl TestRangeTraversalFromObjectOfObjectsDoesNotError (0.43s)\n    binder_test.go:1110: \n        \tError Trace:\t/home/runner/work/pulumi/pulumi/pkg/codegen/pcl/binder_test.go:1110\n        \tError:      \t\"unknown property 'aws_region' among [something awsRegion]\" does not contain \"unknown property 'aws_region' among [awsRegion something]\"\n        \tTest:       \tTestRangeTraversalFromObjectOfObjectsDoesNotError\n        \tMessages:   \tThe diagnostic contains the correct message about the unknown property\n```\n\nhttps://github.com/pulumi/pulumi/actions/runs/17270052509/job/49012394297?pr=20403\n"}
{"id": 256, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### What type of PR is this?\r\n\r\n/kind flake\r\n\r\n#### What this PR does / why we need it:\r\n\r\nThis PR fixes issue #133763 where the ResourceSlice Controller test in the DRA (Dynamic Resource Allocation) e2e tests was experiencing flakiness due to insufficient timeouts.\r\n\r\n**Root Cause**: The test was failing intermittently because:\r\n- ResourceSlice operations were taking longer than expected\r\n- Default timeouts were too short for the test environment\r\n- Network latency and resource allocation delays caused test failures\r\n\r\n**Solution**: Increased timeouts for ResourceSlice Controller test to:\r\n- Provide sufficient time for resource operations to complete\r\n- Reduce test flakiness in various network conditions\r\n- Improve test reliability and stability\r\n\r\n**Impact**: More reliable DRA e2e tests, reducing false failures and improving CI stability.\r\n\r\n#### Which issue(s) this PR is related to:\r\n\r\nFixes #133763\r\n\r\n#### Special notes for your reviewer:\r\n\r\nThis fix addresses test reliability without changing test logic:\r\n\r\n- Increased timeouts to accommodate network and resource allocation delays\r\n- No changes to test behavior or assertions\r\n- Improves CI stability by reducing flaky test failures\r\n- Minimal change that only affects test timing\r\n\r\nThe fix is focused on improving test reliability rather than changing functionality.\r\n\r\n#### Does this PR introduce a user-facing change?\r\n\r\nNONE\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\nN/A\n"}
{"id": 140, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test testShareGroupHeartbeatInitializeOnPartitionUpdate was flaky\nearlier. The shareGroupStatePartitionMetadataRecord that is created\nduring heartbeat contains 2 topics to be initialized,  but the order in\nwhich they appear in the list is not deterministic. The test is changed\nto simply see whether the contents of the record is correct instead of\ndirectly comparing it with an expected record which may contains the\ncorrect topics, but in some different order.\n\n"}
{"id": 121, "ground_truth": "Randomness", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n<!-- ‚ö†Ô∏è‚ö†Ô∏è Do Not Delete This! bug_report_template ‚ö†Ô∏è‚ö†Ô∏è -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- üïÆ Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- üîé Search existing issues to avoid creating duplicates. -->\n<!-- üß™ Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- üí° Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- üîß Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- ü™ì If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- üì£ Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: \n- OS Version: \n\nMy PR is failing to be merged due to windows unit tests failing surrounding composite tokens\n\nhttps://dev.azure.com/vscode/VSCode/_build/results?buildId=152385&view=logs&j=e4714b0f-e6ff-5e04-bac1-2dcbc509f151&t=5d351641-7442-59b2-8726-6d905a792029\n\n```\n  4442 passing (37s)\n  96 pending\n  1 failing\n\n  1) Composite***\n       ‚Ä¢ equals\n         ‚Ä¢ true\n           ‚Ä¢ composite tokens:\n     Error: Method not implemented.\n      at Test***.toString (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:20:13)\n      at Base***.fullRange (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/base***.js:87:54)\n      at new Composite*** (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/composite***.js:4:21)\n      at new Test*** (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:17:7)\n      at Context.<anonymous> (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:126:24)\n      at process.processImmediate (node:internal/timers:483:21)\n\n```\n"}
{"id": 134, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\n\r\nSimplify org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.\"reattach after connection expired\" to make it more deterministic.\r\n\r\n### Why are the changes needed?\r\n\r\nThe test previously involved execution and interruption that made the test unnecessarily flaky, e.g., an exception was thrown when releasing the corresponding [execution](https://github.com/apache/spark/actions/runs/12296721038/job/34316344940), not when reattaching the execution.\r\n- The test's sole purpose is to check whether the lack of 'session' results in the correct error code.\r\n- The involvement of actual query execution only makes the test flaky and complicated.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nRepeatedly ran testOnly org.apache.spark.sql.connect.execution.ReattachableExecuteSuite.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\n\r\nNo.\r\n\n"}
{"id": 388, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nMy theory is that since 0ebea6e5c07485a36862e9b6e2be18d1694ad2c5, the saving of objects has become slightly slower\r\nThis caused some failures in the selenium test suite: https://github.com/django/django/actions/runs/13826117388/job/38698352291\r\n\n"}
{"id": 139, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[Jira: KAFKA-18537](https://issues.apache.org/jira/browse/KAFKA-18537)\nAdd a wait for cleaner thread shutdown in `testCleanerThreadShutdown` to\neliminate flakiness. After calling `cache.close()`, the test now uses\n`TestUtils.waitForCondition` to poll until the background\n‚Äúremote-log-index-cleaner‚Äù thread has fully exited before asserting that\nno cleaner threads remain. This ensures the asynchronous shutdown always\ncompletes before the final assertions.\n\nReviewers: TengYao Chi <kitingiao@gmail.com>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n"}
{"id": 363, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n\r\n/kind flake\r\n\r\n#### What this PR does / why we need it:\r\n\r\n**Kubelet logs:**\r\n\r\n```\r\n// The container probe is failed\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.197112     285 prober.go:153] \"Exec-Probe runProbe\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" containerName=\"test-container-subpath-configmap-h8zg\" execCommand=[\"sh\",\"-c\",\"cat /probe-volume/probe-file || test `cat /test-volume` = 'configmap-modified-value'\"]\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.905819     285 prober.go:120] \"Probe failed\" probeType=\"Liveness\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" containerName=\"test-container-subpath-configmap-h8zg\" probeResult=\"failure\" output=<\r\n\r\n// kubelet tried to stop the container via calling container runtime\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.910529     285 kuberuntime_manager.go:1148] \"computePodActions got for pod\" podActions=\"KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [0], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[{containerd 89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1}:{0xc000b59c08 test-container-subpath-configmap-h8zg Container test-container-subpath-configmap-h8zg failed liveness probe, will be restarted LivenessProbe}]\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.910732     285 kuberuntime_manager.go:1182] \"Killing unwanted container for pod\" containerName=\"test-container-subpath-configmap-h8zg\" containerID={\"Type\":\"containerd\",\"ID\":\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\"} pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.910919     285 kuberuntime_container.go:809] \"Killing container with a grace period\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" containerName=\"test-container-subpath-configmap-h8zg\" containerID=\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\" gracePeriod=2\r\nFeb 15 08:13:03 kind-worker kubelet[285]: I0215 08:13:03.912334     285 event.go:389] \"Event occurred\" object=\"subpath-2059/pod-subpath-test-configmap-h8zg\" fieldPath=\"spec.containers{test-container-subpath-configmap-h8zg}\" kind=\"Pod\" apiVersion=\"v1\" type=\"Normal\" reason=\"Killing\" message=\"Container test-container-subpath-configmap-h8zg failed liveness probe, will be restarted\"\r\n```\r\n\r\n**Containerd logs:** \r\n\r\n```\r\n# Killing container with a grace period but timeout is reached\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.912504992Z\" level=info msg=\"StopContainer for \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" with timeout 2 (s)\"\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.913797826Z\" level=info msg=\"Stop container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" with signal terminated\"\r\n\r\n# Force killing the container but the container is already exited\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.976463751Z\" level=info msg=\"received exit event container_id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" pid:225289 exited_at:{seconds:1739607183 nanos:962757445}\"\r\nFeb 15 08:13:03 kind-worker containerd[185]: time=\"2025-02-15T08:13:03.977725541Z\" level=info msg=\"TaskExit event in podsandbox handler container_id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" id:\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" pid:225289 exited_at:{seconds:1739607183 nanos:962757445}\"\r\nFeb 15 08:13:05 kind-worker containerd[185]: time=\"2025-02-15T08:13:05.954244737Z\" level=info msg=\"Kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\"\"\r\nFeb 15 08:13:13 kind-worker containerd[185]: time=\"2025-02-15T08:13:12.797190089Z\" level=error msg=\"StopContainer for \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\" failed\" error=\"rpc error: code = Unknown desc = failed to kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\": ttrpc: closed\"\r\n```\r\n\r\n**Kubelet logs:** \r\n\r\n```\r\n// kubelet fails to stop the container and the containerd returns an error\r\nFeb 15 08:13:13 kind-worker kubelet[285]: E0215 08:13:12.800578     285 kuberuntime_container.go:814] \"Container termination failed with gracePeriod\" err=\"rpc error: code = Unknown desc = failed to kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\": ttrpc: closed\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" containerName=\"test-container-subpath-configmap-h8zg\" containerID=\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\" gracePeriod=2\r\nFeb 15 08:13:13 kind-worker kubelet[285]: E0215 08:13:12.800970     285 kuberuntime_manager.go:1187] \"killContainer for pod failed\" err=\"rpc error: code = Unknown desc = failed to kill container \\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\": ttrpc: closed\" containerName=\"test-container-subpath-configmap-h8zg\" containerID={\"Type\":\"containerd\",\"ID\":\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\"} pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:13 kind-worker kubelet[285]: I0215 08:13:12.801016     285 kubelet.go:1859] \"SyncPod exit\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" isTerminal=false\r\nFeb 15 08:13:13 kind-worker kubelet[285]: E0215 08:13:12.801142     285 pod_workers.go:1301] \"Error syncing pod, skipping\" err=\"failed to \\\"KillContainer\\\" for \\\"test-container-subpath-configmap-h8zg\\\" with KillContainerError: \\\"rpc error: code = Unknown desc = failed to kill container \\\\\\\"89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\\\\\": ttrpc: closed\\\"\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\"\r\nFeb 15 08:13:13 kind-worker kubelet[285]: I0215 08:13:12.801177     285 pod_workers.go:1338] \"Processing pod event done\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" updateType=\"sync\"\r\n\r\n// The exitCode of The container `test-container-subpath-configmap-h8zg` is 0.\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.214763     285 helpers.go:104] \"Already successfully ran container, do nothing\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" containerName=\"test-container-subpath-configmap-h8zg\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.214879     285 kuberuntime_manager.go:1148] \"computePodActions got for pod\" podActions=\"KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215172     285 container_manager.go:238] \"Pod contains no container with pinned cpus\" podName=\"pod-subpath-test-configmap-h8zg\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215210     285 kuberuntime_sandbox_linux.go:62] \"Enforcing CFS quota\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" unlimited=false\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215232     285 kubelet.go:1859] \"SyncPod exit\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" isTerminal=false\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.215258     285 pod_workers.go:1338] \"Processing pod event done\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" updateType=\"sync\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.259606     285 status_manager.go:935] \"Patch status for pod\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" podUID=\"c9f2f574-b2f4-49c0-a94f-c452d6290215\" patch=\"{\\\"metadata\\\":{\\\"uid\\\":\\\"c9f2f574-b2f4-49c0-a94f-c452d6290215\\\"},\\\"status\\\":{\\\"$setElementOrder/conditions\\\":[{\\\"type\\\":\\\"PodReadyToStartContainers\\\"},{\\\"type\\\":\\\"Initialized\\\"},{\\\"type\\\":\\\"Ready\\\"},{\\\"type\\\":\\\"ContainersReady\\\"},{\\\"type\\\":\\\"PodScheduled\\\"}],\\\"conditions\\\":[{\\\"lastTransitionTime\\\":\\\"2025-02-15T08:13:14Z\\\",\\\"message\\\":\\\"containers with unready status: [test-container-subpath-configmap-h8zg]\\\",\\\"reason\\\":\\\"ContainersNotReady\\\",\\\"status\\\":\\\"False\\\",\\\"type\\\":\\\"Ready\\\"},{\\\"lastTransitionTime\\\":\\\"2025-02-15T08:13:14Z\\\",\\\"message\\\":\\\"containers with unready status: [test-container-subpath-configmap-h8zg]\\\",\\\"reason\\\":\\\"ContainersNotReady\\\",\\\"status\\\":\\\"False\\\",\\\"type\\\":\\\"ContainersReady\\\"}],\\\"containerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\",\\\"image\\\":\\\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\\\",\\\"imageID\\\":\\\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"test-container-subpath-configmap-h8zg\\\",\\\"ready\\\":false,\\\"restartCount\\\":0,\\\"started\\\":false,\\\"state\\\":{\\\"terminated\\\":{\\\"containerID\\\":\\\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\\\",\\\"exitCode\\\":0,\\\"finishedAt\\\":\\\"2025-02-15T08:13:03Z\\\",\\\"reason\\\":\\\"Completed\\\",\\\"startedAt\\\":\\\"2025-02-15T08:12:57Z\\\"}},\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/test-volume\\\",\\\"name\\\":\\\"test-volume\\\"},{\\\"mountPath\\\":\\\"/probe-volume\\\",\\\"name\\\":\\\"liveness-probe-volume\\\"},{\\\"mountPath\\\":\\\"/var/run/secrets/kubernetes.io/serviceaccount\\\",\\\"name\\\":\\\"kube-api-access-d7bjr\\\",\\\"readOnly\\\":true,\\\"recursiveReadOnly\\\":\\\"Disabled\\\"}]},{\\\"containerID\\\":\\\"containerd://b83d07f353e6c10960e144839d90c658f1f40aea77ea78a9483620e33f98b67b\\\",\\\"image\\\":\\\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\\\",\\\"imageID\\\":\\\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"test-container-volume-configmap-h8zg\\\",\\\"ready\\\":true,\\\"restartCount\\\":0,\\\"started\\\":true,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2025-02-15T08:12:58Z\\\"}},\\\"volumeMounts\\\":[{\\\"mountPath\\\":\\\"/test-volume\\\",\\\"name\\\":\\\"test-volume\\\"},{\\\"mountPath\\\":\\\"/probe-volume\\\",\\\"name\\\":\\\"liveness-probe-volume\\\"},{\\\"mountPath\\\":\\\"/var/run/secrets/kubernetes.io/serviceaccount\\\",\\\"name\\\":\\\"kube-api-access-d7bjr\\\",\\\"readOnly\\\":true,\\\"recursiveReadOnly\\\":\\\"Disabled\\\"}]}]}}\"\r\nFeb 15 08:13:14 kind-worker kubelet[285]: I0215 08:13:14.259762     285 status_manager.go:944] \"Status for pod updated successfully\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" statusVersion=5 status={\"phase\":\"Running\",\"conditions\":[{\"type\":\"PodReadyToStartContainers\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:12:55Z\"},{\"type\":\"Initialized\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:12:56Z\"},{\"type\":\"Ready\",\"status\":\"False\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:13:14Z\",\"reason\":\"ContainersNotReady\",\"message\":\"containers with unready status: [test-container-subpath-configmap-h8zg]\"},{\"type\":\"ContainersReady\",\"status\":\"False\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:13:14Z\",\"reason\":\"ContainersNotReady\",\"message\":\"containers with unready status: [test-container-subpath-configmap-h8zg]\"},{\"type\":\"PodScheduled\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2025-02-15T08:12:52Z\"}],\"hostIP\":\"fc00:f853:ccd:e793::3\",\"hostIPs\":[{\"ip\":\"fc00:f853:ccd:e793::3\"}],\"podIP\":\"fd00:10:244:1::27f\",\"podIPs\":[{\"ip\":\"fd00:10:244:1::27f\"}],\"startTime\":\"2025-02-15T08:12:52Z\",\"initContainerStatuses\":[{\"name\":\"init-volume-configmap-h8zg\",\"state\":{\"terminated\":{\"exitCode\":0,\"reason\":\"Completed\",\"startedAt\":\"2025-02-15T08:12:55Z\",\"finishedAt\":\"2025-02-15T08:12:55Z\",\"containerID\":\"containerd://caf4b3bafcaec61cd9836078209eb64b0f17b158ae2b7aea8bbca9038d50a330\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\",\"imageID\":\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\",\"containerID\":\"containerd://caf4b3bafcaec61cd9836078209eb64b0f17b158ae2b7aea8bbca9038d50a330\",\"started\":false,\"volumeMounts\":[{\"name\":\"test-volume\",\"mountPath\":\"/test-volume\"},{\"name\":\"liveness-probe-volume\",\"mountPath\":\"/probe-volume\"},{\"name\":\"kube-api-access-d7bjr\",\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]}],\"containerStatuses\":[{\"name\":\"test-container-subpath-configmap-h8zg\",\"state\":{\"terminated\":{\"exitCode\":0,\"reason\":\"Completed\",\"startedAt\":\"2025-02-15T08:12:57Z\",\"finishedAt\":\"2025-02-15T08:13:03Z\",\"containerID\":\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\"}},\"lastState\":{},\"ready\":false,\"restartCount\":0,\"image\":\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\",\"imageID\":\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\",\"containerID\":\"containerd://89cdb4348fa177380e1188f0393d958858209128b3e2aa56308c1ea0c2b4bac1\",\"started\":false,\"volumeMounts\":[{\"name\":\"test-volume\",\"mountPath\":\"/test-volume\"},{\"name\":\"liveness-probe-volume\",\"mountPath\":\"/probe-volume\"},{\"name\":\"kube-api-access-d7bjr\",\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]},{\"name\":\"test-container-volume-configmap-h8zg\",\"state\":{\"running\":{\"startedAt\":\"2025-02-15T08:12:58Z\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\",\"imageID\":\"registry.k8s.io/e2e-test-images/busybox@sha256:a9155b13325b2abef48e71de77bb8ac015412a566829f621d06bfae5c699b1b9\",\"containerID\":\"containerd://b83d07f353e6c10960e144839d90c658f1f40aea77ea78a9483620e33f98b67b\",\"started\":true,\"volumeMounts\":[{\"name\":\"test-volume\",\"mountPath\":\"/test-volume\"},{\"name\":\"liveness-probe-volume\",\"mountPath\":\"/probe-volume\"},{\"name\":\"kube-api-access-d7bjr\",\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]}],\"qosClass\":\"BestEffort\"}\r\n\r\n// The ContainersToStart of the computed PodActions is empty, so no new container is created to replace the previous one.\r\nFeb 15 08:14:31 kind-worker kubelet[285]: I0215 08:14:31.576812     285 helpers.go:104] \"Already successfully ran container, do nothing\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\" containerName=\"test-container-subpath-configmap-h8zg\"\r\nFeb 15 08:14:31 kind-worker kubelet[285]: I0215 08:14:31.576881     285 kuberuntime_manager.go:1148] \"computePodActions got for pod\" podActions=\"KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]\" pod=\"subpath-2059/pod-subpath-test-configmap-h8zg\"\r\n```\r\n\r\n**Root cause:**\r\n\r\nThe RestartPolicy of the pod is `OnFailure` and the container exited successfully.\r\n\r\n- https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/test/e2e/storage/testsuites/subpath.go#L793\r\n- https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/pkg/kubelet/container/helpers.go#L103\r\n- https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/test/e2e/framework/pod/utils.go#L34\r\n\r\n**Fix it:**\r\n\r\nDrop `trap exit TERM` from the command to make the container exit with a non-zero exit code.\r\n\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #130268\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n"}
{"id": 372, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test checks if loading an app in the background will catch and re-raise an exception. It occasionally fails on pypy3. Possibly, the background loading thread does not finish before the assert line, and so there is no exception yet. Try to address this by waiting for the lock before asserting. This could still fail if the thread starts *very* slowly.\r\n\r\ncloses #4292 \n"}
{"id": 225, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nPreviously, ZooKeeperPersistenceEngine would crash the whole Master process if\nthere was stored data from a prior Spark version. Now, we just delete these files.\n\n"}
{"id": 282, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nAPI Key expiration value has millisecond precision as we use\r\n{@link Instant#toEpoqueMilli()} when creating the API key\r\ndocument.\r\nIt could often happen that `Instant.now()` Instant in the testCreateApiKey\r\nwas close enough to the ApiKeyService's `clock.instant()` Instant,\r\nwhen the nanos were removed from the latter ( due to the call\r\nto `toEpoqueMilli()` ) the result of comparing these two Instants\r\nwas a few nanos short of a 7 days.\r\n\r\nResolves: #47958\n"}
{"id": 268, "ground_truth": "Concurrency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nReally love this repo, I've been using it to finetune CodeGen models with >2k context windows.\r\n\r\nIt's way faster than hugging face (3x) and slightly faster than Megatron for the 350M and 2.7b parameter CodeGen models but doesn't work for the 6.1B and 16B parameter models as they have a head dimension of 256.\r\n\r\n<img width=\"1006\" alt=\"Screen Shot 2022-11-01 at 5 32 47 PM\" src=\"https://user-images.githubusercontent.com/17725268/199345886-f8b7531e-9918-4fba-ab37-4ae980ec2796.png\">\r\n\r\nI would imagine CodeGen finetuning will be a solid use-case for flash attention since coding models can really benefit from long context windows. And CodeGen is basically SOTA for coding (competitive with Codex).\r\n\r\nIs this something that is even possible with flash attention?\n"}
{"id": 72, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Thank you for submitting a Pull Request. Please:\n* Read our Pull Request guidelines:\n  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests\n* Associate an issue with the Pull Request.\n* Ensure that the code is up-to-date with the `main` branch.\n* Include a description of the proposed changes and how to test them.\n-->\n\n"}
{"id": 336, "ground_truth": "Randomness", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nLet's test CI..\n"}
{"id": 342, "ground_truth": "Time", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n\r\n(cherry picked from commit 28278debebabdd44bf69609f17d2c7c9c8a39695)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n"}
{"id": 127, "ground_truth": "Randomness", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test `Win32ProgramRepositoryMustCallOnAppRenamedForLnkAppsWhenRenamedEventIsRaised` was experiencing random failures due to object identity mismatches in the repository's hash-based storage system.\r\n\r\n## Root Cause\r\n\r\nThe test was manually creating `Win32Program` objects:\r\n\r\n```csharp\r\nWin32Program olditem = new Win32Program\r\n{\r\n    Name = \"oldpath\",\r\n    ExecutableName = oldpath,\r\n    FullPath = linkingTo,\r\n};\r\n```\r\n\r\nHowever, the `DoOnAppRenamedAsync` method creates the `oldApp` object for removal using a different approach for .lnk files:\r\n\r\n```csharp\r\noldApp = new Win32Program() { \r\n    Name = Path.GetFileNameWithoutExtension(e.OldName), \r\n    ExecutableName = Path.GetFileName(e.OldName), \r\n    FullPath = newApp?.FullPath ?? oldPath \r\n};\r\n```\r\n\r\nSince the repository uses `GetHashCode()` (based on `Name`, `ExecutableName`, and `FullPath`) to identify objects for removal, any subtle differences in these properties would cause the `Remove()` operation to fail, leading to test assertion failures.\r\n\r\n## Fix\r\n\r\nChanged the test to use `Win32Program.GetAppFromPath()` instead of manual object creation:\r\n\r\n```csharp\r\nWin32Program olditem = Win32Program.GetAppFromPath(oldFullPath);\r\nWin32Program newitem = Win32Program.GetAppFromPath(newFullPath);\r\n```\r\n\r\nThis mirrors the approach used in the working `Win32ProgramRepositoryMustCallOnAppRenamedForUrlAppsWhenRenamedEventIsRaised` test and ensures that test objects are created using the same code path as the production code, eliminating hash code mismatches.\r\n\r\n## Why This Was Random\r\n\r\nThe test failure appeared random because it depended on subtle differences in object creation that could vary based on timing, mock setup, or other environmental factors. By using the same object creation method as the production code, the test becomes deterministic.\n"}
{"id": 192, "ground_truth": "Resource Leak", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nhttps://github.com/square/okhttp/actions/runs/7593827185/job/20684688664\r\n\r\n\r\n```\r\nHttpOverHttp2Test > concurrentRequestWithEmptyFlowControlWindow(Protocol, MockWebServer) > [1] h2_prior_knowledge FAILED\r\n    java.lang.AssertionError: Timed out waiting for log message.\r\n        at okhttp3.TestLogHandler.take(TestLogHandler.kt:92)\r\n        at okhttp3.internal.http2.HttpOverHttp2Test.waitForDataFrames(HttpOverHttp2Test.kt:425)\r\n        at okhttp3.internal.http2.HttpOverHttp2Test.concurrentRequestWithEmptyFlowControlWindow(HttpOverHttp2Test.kt:481)\r\n```        \r\n\r\nSteps \r\n1. FakeFileSystem cache to remove that issue\r\n2. Close requests cleanly\r\n3. Verify in CI \n"}
{"id": 152, "ground_truth": "Unordered Data", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe problem is that in the yoga example's training data there are 5 classes but when using any other amount of classes as training data the training fails. Set the output layer to the actual number of classes instead.\r\n\r\nWithout this fix, a ValueError is raised (e.g. `ValueError: Shapes (None, 7) and (None, 5) are incompatible`).\n"}
{"id": 147, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test failed in\r\n<https://lab.llvm.org/buildbot/#/builders/162/builds/5785> due to frame variable not being in stop commands, even though the DAP log shows the command being present there. I'm pretty sure this is a race in the test the collection of the test log. I fix that by making sure we wait for the expected output, and also by increasing the timeout (1s is cutting it very close).\r\n\r\nThe arm failure link is no longer functional, but I'm fairly certain that this was the cause of those flakes as well.\n"}
{"id": 228, "ground_truth": "Time", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nI have written integration code which would allow conditionExpressions to be xpath and evaluator for it\n\nalso test case is added to illustrate the use of xpath\n\n"}
{"id": 365, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\n    Keep PR title verbose enough and add prefix telling\r\n    about what components it touches e.g \"query:\" or \".*:\"\r\n-->\r\n\r\n<!--\r\n    Don't forget about CHANGELOG!\r\n\r\n    Changelog entry format:\r\n    - [#<PR-id>](<PR-URL>) Thanos <Component> ...\r\n\r\n    <PR-id> Id of your pull request.\r\n    <PR-URL> URL of your PR such as https://github.com/thanos-io/thanos/pull/<PR-id>\r\n    <Component> Component affected by your changes such as Query, Store, Receive.\r\n-->\r\n\r\n* [x] I added CHANGELOG entry for this change.\r\n* [x] Change is not relevant to the end user.\r\n\r\n## Changes\r\n\r\nFixes: #8115\r\n\r\n<!-- Enumerate changes you made -->\r\n\r\n## Verification\r\n\r\n<!-- How you tested it? How do you know it works? -->\r\n\n"}
{"id": 106, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n# Which issue does this PR close?\r\n\r\n<!--\r\nWe generally require a GitHub issue to be filed for all bug fixes and enhancements and this helps us generate change logs for our releases. You can link an issue to this PR using the GitHub syntax. For example `Closes #123` indicates that this PR will close issue #123.\r\n-->\r\n\r\nCloses #.\r\n\r\n# Rationale for this change\r\n \r\n<!--\r\nWhy are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\nExplaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.\r\n-->\r\n\r\nWe occasionally see this test fail as it takes more than a millisecond on very contended CI runners for the asynchronous cleanup to be performed - https://github.com/apache/arrow-rs/actions/runs/8690327829/job/23830012773\r\n\r\n# What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n# Are there any user-facing changes?\r\n\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!---\r\nIf there are any breaking changes to public APIs, please add the `breaking change` label.\r\n-->\r\n\n"}
{"id": 68, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Summary\r\n\r\nThis is my guess as to the source of the resolver flake, based on information and extensive debugging from @zanieb. In short, if we rely on `self.index.packages` as a source of truth during error reporting, we open ourselves up to a source of non-determinism, because we fetch package metadata asynchronously in the background while we solve -- so packages _could_ be included in or excluded from the index depending on the order in which those requests are returned.\r\n\r\nSo, instead, we now track the set of packages that _were_ visited by the solver. Visiting a package _requires_ that we wait for its metadata to be available. By limiting analysis to those packages that were visited during solving, we are faithfully representing the state of the solver at the time of failure.\r\n\r\nCloses #863 \n"}
{"id": 238, "ground_truth": "Concurrency", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nCI test **darwin://python/ray/tests:test_multi_node** is consistently_failing. Recent failures: \n\t- https://buildkite.com/ray-project/postmerge-macos/builds/7162#01987fcf-d2dd-40d7-ab47-675bc7df8452\n\t- https://buildkite.com/ray-project/postmerge-macos/builds/7162#01987f0b-3872-4031-a057-406c38641c46\n\nDataCaseName-darwin://python/ray/tests:test_multi_node-END\nManaged by OSS Test Policy\n"}
{"id": 420, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #13526\n"}
{"id": 394, "ground_truth": "Async wait", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nRefs #56190\r\n\r\nThis PR does two things in the specific `test` case that is flaky:\r\n\r\n- Make the `composedSignal` a weak ref so the test won't hold any reference that could keep that variable from being GCed.\r\n- Add `gcUntil` so it will make more calls to `global.gc`. Precisely, until that condition is reached or a maximum number of attempts is hit.\n"}
{"id": 135, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR increases the max time we wait for a connect server to come up for testing. The current threshold is too low, and is causing flakyness.\r\n\r\n### Why are the changes needed?\r\nIt makes connect tests less flaky.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nIt is test infra code.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\n"}
{"id": 33, "ground_truth": "Concurrency", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #143878\n* #143865\n\nSummary:\nTest erroneously assumed that input/output sizes are same and that all\nstates are matchable.\n\nFixes issue #143798\n\nTest Plan:\nTest passes\n\nReviewers\nTest passes\n\ncc @H-Huang @awgu @kwen2501 @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k\n"}
{"id": 380, "ground_truth": "Floating point operations", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFix flaky TFRT tensor utility unit test.\n\n"}
{"id": 114, "ground_truth": "Concurrency", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n6.0.0\r\n\r\nRelease version:\r\n\r\n## Summary\r\n\r\nFix some flaky tests found on master:\r\n\r\n- persistence_enabled.mqttv5.t_takeover_clean_session_with_delayed_willmsg\r\n- emqx_prometheus_data_SUITE\r\n\r\n\r\n## PR Checklist\r\n<!--\r\nPlease convert the PR to a draft if any of the following conditions are not met.\r\n-->\r\n- [ ] For internal contributor: there is a jira ticket to track this change\r\n- [x] The changes are covered with new or existing tests\r\n- [ ] n/a Change log for changes visible by users has been added to `changes/ee/(feat|perf|fix|breaking)-<PR-id>.en.md` files\r\n- [x] Schema changes are backward compatible or intentionally breaking (describe the changes and the reasoning in the summary)\r\n\r\n<!--\r\nPlease, take in account the following guidelines while working on PR:\r\n* Try to achieve reasonable coverage of the new code\r\n* Add property-based tests for code that performs complex user input validation or implements a complex algorithm\r\n* Create a PR to [emqx-docs](https://github.com/emqx/emqx-docs) if documentation update is required, or make a follow-up jira ticket\r\n* Do not squash large PRs into a single commit, try to keep comprehensive history of incremental changes\r\n* Do not squash any significant amount of review fixes into the previous commits\r\n-->\r\n\r\n<!--\r\n## Checklist for CI (.github/workflows) changes\r\n- [ ] If changed package build workflow, pass [this action](https://github.com/emqx/emqx/actions/workflows/build_packages.yaml) (manual trigger)\r\n- [ ] Change log has been added to `changes/` dir for user-facing artifacts update\r\n-->\r\n\n"}
{"id": 205, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## TLDR\r\n\r\nfixes a few flaky tests\r\n\r\n## Dive Deeper\r\n\r\n- Mock file size check in fileUtils test to avoid creating large files\r\n- Add retry options to directory cleanup in memoryDiscovery test\r\n- Create test directories in parallel for better performance\r\n\r\n## Reviewer Test Plan\r\n\r\nrun the tests and they shouldn't flake\r\n\r\n## Testing Matrix\r\n\r\n|          | üçè  | ü™ü  | üêß  |\r\n| -------- | --- | --- | --- |\r\n| npm run  | ‚ùì  | ‚ùì  | ‚úÖ  |\r\n| npx      | ‚ùì  | ‚ùì  | ‚ùì  |\r\n| Docker   | ‚ùì  | ‚ùì  | ‚ùì  |\r\n| Podman   | ‚ùì  | -   | -   |\r\n| Seatbelt | ‚ùì  | -   | -   |\r\n\r\n## Linked issues / bugs\r\n\r\n- Fixes #8345\r\n- Fixes #8351\n"}
{"id": 290, "ground_truth": "Test order dependency", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFollow-up of #10811 to ignore other types of `FileSystemException` like `DirectoryNotEmptyException` as well\r\n\r\n```\r\nTestDataFrameWrites > testFaultToleranceOnWrite() > format = parquet FAILED\r\n    java.nio.file.DirectoryNotEmptyException: /tmp/junit-7768099913831474039/parquet/test\r\n        at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:289)\r\n        at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:104)\r\n        at java.base/java.nio.file.Files.delete(Files.java:1152)\r\n        at org.apache.commons.io.FileUtils.delete(FileUtils.java:1222)\r\n        at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1242)\r\n        at org.apache.iceberg.spark.source.TestDataFrameWrites.testFaultToleranceOnWrite(TestDataFrameWrites.java:427)\r\n```\r\n\r\ncc @Fokko @nastra \n"}
{"id": 353, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis issue can be caused by a non-existing path but also a misunderstanding from the config file. A short example will help the user.\n\n"}
{"id": 326, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nSet `getNumHomes: 1` to enable the 567 ms or 2 seconds of wait for the server to be ready in `TestServerPathEncodingIssues/Unicode_paths` and `TestServerPathEncodingIssues/Windows_multilingual_404`.\r\n\r\nFixes #10332\r\n\r\n---\r\n\r\nHi @bep, I hope this is the right fix.  Besides waiting for the GitHub Actions CI test result, I'll be uploading 0.104.2-2 to Debian shortly and will report back if these tests pass the very first time on all architectures.\n"}
{"id": 129, "ground_truth": "Time", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Enter a brief description/summary of your PR here. What does it fix/what does it change/how was it tested (even manually, if necessary)? -->\r\n## Summary of the Pull Request\r\n\r\nAnother attempt for trying to fix flaky WebView2 CI ü§û \r\n\r\n<!-- Please review the items on the PR checklist before submitting-->\r\n## PR Checklist\r\n\r\n- [ ] **Closes:** #xxx\r\n- [ ] **Communication:** I've discussed this with core contributors already. If work hasn't been agreed, this work might be rejected\r\n- [ ] **Tests:** Added/updated and all pass\r\n- [ ] **Localization:** All end user facing strings can be localized\r\n- [ ] **Dev docs:** Added/updated\r\n- [ ] **New binaries:** Added on the required places\r\n   - [ ] [JSON for signing](https://github.com/microsoft/PowerToys/blob/main/.pipelines/ESRPSigning_core.json) for new binaries\r\n   - [ ] [WXS for installer](https://github.com/microsoft/PowerToys/blob/main/installer/PowerToysSetup/Product.wxs) for new binaries and localization folder\r\n   - [ ] [YML for CI pipeline](https://github.com/microsoft/PowerToys/blob/main/.pipelines/ci/templates/build-powertoys-steps.yml) for new test projects\r\n   - [ ] [YML for signed pipeline](https://github.com/microsoft/PowerToys/blob/main/.pipelines/release.yml)\r\n- [ ] **Documentation updated:** If checked, please file a pull request on [our docs repo](https://github.com/MicrosoftDocs/windows-uwp/tree/docs/hub/powertoys) and link it here: #xxx\r\n\r\n<!-- Provide a more detailed description of the PR, other things fixed or any additional comments/features here -->\r\n## Detailed Description of the Pull Request / Additional comments\r\n\r\n<!-- Describe how you validated the behavior. Add automated tests wherever possible, but list manual validation steps taken as well -->\r\n## Validation Steps Performed\r\n\r\n\n"}
{"id": 250, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nProperly shutdown broker for each test and speed up tests by sending less messages\n"}
{"id": 38, "ground_truth": "Environment", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n```\r\nFalsifying example: test_gpu_data_iterator(\r\n    n_samples_per_batch=57, n_features=1, n_batches=9, subsample=False,\r\n)\r\n```\r\n\r\nIt's caused by one different element in the generated cuts.  We need to change the test to adapt to the differences in the models.\n"}
{"id": 203, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\ntrust center tests need to create their own users because of top level restrictions on number of trust centers\n"}
{"id": 16, "ground_truth": "OS", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nGiven my home-directory is `/Users/rocko` - and I have a file named\n`npm.json` in it and also a repository with name `npm`, which is a\nfolder for the node-module.\n\nWhen try to require the `/Users/rocko/npm/index.js` two direcotry\nlevels down in the npm folder (e.g. `/Users/rocko/npm/test/tap`)\nwith require(\"../../\") node will load `/Users/rocko/npm/index.json`.\n\nWhen I use require(\"../..\") node will load `/Users/rocko/npm.json`\nwhich is fixed by this commit.\n\nOriginal PR: https://github.com/joyent/node/pull/7094\n\n"}
{"id": 142, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\nRemove hardcoded `start_time` with relative calculation to prevent test\r\nfailures in CI. The old pattern used `start_time=16000`\r\nwhich could result in negative durations when we calculate durations time.monotonic() < 16000,\r\ncausing the timeout logic to fail.\r\n\r\nI repkace that with populating start_time relative to current monotonic time,\r\nto guarantee the processor always runs for exactly\r\n(timeout + 1) seconds, to ensure consistent timeout behaviour across\r\nall environments.\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n"}
{"id": 103, "ground_truth": "Async Wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n**JIRA**: [JBPM-9895](https://issues.redhat.com/browse/JBPM-9895)\r\n\r\nCherry-pick from adc8aaf884bb967b2d25bb0072b4063b3ddcdf96\n"}
{"id": 187, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\n!!!ATTENTION!!!\r\n\r\nIf you are fixing *any* crash or *any* potential security issue, *do not*\r\nopen a pull request in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.\r\nThank you in advance for helping to keep Envoy secure.\r\n\r\n!!!ATTENTION!!!\r\n\r\nFor an explanation of how to fill out the fields, please see the relevant section\r\nin [PULL_REQUESTS.md](https://github.com/envoyproxy/envoy/blob/main/PULL_REQUESTS.md)\r\n-->\r\n\r\nFix #29595 \r\n\r\nIt is possible that we will get data + rst from the wrapped transport socket since there is a `do {} while` loop. We should trigger the `onRead` if there is any processed data.\r\n\r\nDo not flake anymore.\r\n```\r\nbazel test test/integration:protocol_integration_test --test_filter=\"*LargeRequestMethod*\" --test_arg=\"-l trace\" -c opt --runs_per_test=1000\r\n```\r\n\r\nCommit Message:\r\nAdditional Description:\r\nRisk Level:\r\nTesting:\r\nDocs Changes:\r\nRelease Notes:\r\nPlatform Specific Features:\r\n[Optional Runtime guard:]\r\n[Optional Fixes #Issue]\r\n[Optional Fixes commit #PR or SHA]\r\n[Optional Deprecated:]\r\n[Optional [API Considerations](https://github.com/envoyproxy/envoy/blob/main/api/review_checklist.md):]\r\n\n"}
{"id": 164, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe URL www.squirrelchasers.com is actually existing, so we should not access it in our tests. Hope this make the test more stable. \n"}
{"id": 362, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n/kind bug\r\n<!--\r\nAdd one of the following kinds:\r\n/kind bug\r\n/kind cleanup\r\n/kind documentation\r\n/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n/kind api-change\r\n/kind deprecation\r\n/kind failing-test\r\n/kind flake\r\n/kind regression\r\n-->\r\n\r\n#### What this PR does / why we need it:\r\nfix flaky test on dra manager_test.go\r\n#### Which issue(s) this PR fixes:\r\n<!--\r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #119701\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\n\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n"}
{"id": 375, "ground_truth": "Randomness", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n<!-- ‚ö†Ô∏è‚ö†Ô∏è Do Not Delete This! bug_report_template ‚ö†Ô∏è‚ö†Ô∏è -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- üïÆ Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->\n<!-- üîé Search existing issues to avoid creating duplicates. -->\n<!-- üß™ Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->\n<!-- üí° Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->\n<!-- üîß Launch with `code --disable-extensions` to check. -->\nDoes this issue occur when all extensions are disabled?: Yes/No\n\n<!-- ü™ì If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->\n<!-- üì£ Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->\n- VS Code Version: \n- OS Version: \n\nMy PR is failing to be merged due to windows unit tests failing surrounding composite tokens\n\nhttps://dev.azure.com/vscode/VSCode/_build/results?buildId=152385&view=logs&j=e4714b0f-e6ff-5e04-bac1-2dcbc509f151&t=5d351641-7442-59b2-8726-6d905a792029\n\n```\n  4442 passing (37s)\n  96 pending\n  1 failing\n\n  1) Composite***\n       ‚Ä¢ equals\n         ‚Ä¢ true\n           ‚Ä¢ composite tokens:\n     Error: Method not implemented.\n      at Test***.toString (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:20:13)\n      at Base***.fullRange (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/base***.js:87:54)\n      at new Composite*** (file:///D:/a/_work/1/s/out/vs/editor/common/codecs/composite***.js:4:21)\n      at new Test*** (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:17:7)\n      at Context.<anonymous> (file:///D:/a/_work/1/s/out/vs/editor/test/common/codecs/tokens/composite***.test.js:126:24)\n      at process.processImmediate (node:internal/timers:483:21)\n\n```\n"}
{"id": 186, "ground_truth": "Test Order Dependency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n‚Ä¶ot specifying topic\n\n"}
{"id": 180, "ground_truth": "Resource Leak", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe `render_template_fields` method of mapped operator needs to use database session object to render mapped fields, but it cannot get the session passed by @provide_session decorator, because it is used in derived classes and we cannot change the signature without impacting those classes.\r\n\r\nSo far it was done by creating new session in mapped_operator, but it has the drawback of creating an extra session while one is already created (remnder_template_fields is always run in the context of task run and it always has a session created already in _run_raw_task). It also causes problems in our tests where two opened database session accessed database at the same time and it cases sqlite exception on concurrent access and mysql error on running operations out of sync - likely when the same object was modified in both sessions.\r\n\r\nThis PR changes the approach - rather than creating a new session in the mapped_operator, we are retrieving the session from one stored by the _run_raw_task. It is done by context manager and adequate protection has been added to make sure that:\r\n\r\na) the call is made within the context manager\r\nb) context manageer is never initialized twice in the same\r\n   call stack\r\n\r\nAfter this change, resources used by running task will be smaller, and mapped tasks will not always open 2 DB sesions.\r\n\r\nFixes: #33178\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n"}
{"id": 71, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nI hereby agree to the terms of the [RisingWave Labs, Inc. Contributor License Agreement](https://raw.githubusercontent.com/risingwavelabs/risingwave/17af8a747593ebdbfa826691daf75bdab7d14fa0/.github/contributor-license-agreement.txt).\r\n\r\nissue: resolve #22872 \r\n\r\n## What's changed and what's your intention?\r\n\r\nTests under src/storage/src/vector/distance.rs used a signed difference to compare floating-point values:\r\n`($first - $second) < FLOAT_ALLOWED_BIAS`\r\n\r\nThis is asymmetric and can let large negative differences pass. After upgrading CMake I also observed a drift (~1.2e-5) that slightly exceeds the current 1e-5 epsilon causing the test to fail locally.\r\n\r\nObserved failure: `Expected: 31.531513, Actual: 31.531525  (|Œî| ‚âà 1.2e-5)`\r\n\r\n**This PR:**\r\n- Fixes the logic to compare absolute difference.\r\n- Uses a combined absolute + relative tolerance to remain strict yet robust across SIMD/FMA/BLAS/FAISS code paths.\r\n\r\n**Changes:**\r\n- Replace the signed comparison with abs() on the difference, this measures the magnitude of the difference, so both positive and negative deviations are caught.\r\n- Introduce FLOAT_ABS_EPS = 2e-5 and FLOAT_REL_EPS = 1e-6 to allow for expected noise while still detecting genuine errors.\r\n- The tolerance is computed as the larger of the absolute floor (FLOAT_ABS_EPS) and a scaled relative term (FLOAT_REL_EPS * max(|a|, |b|)). This relative component adjusts the allowed difference based on the magnitude of the values, ensuring the comparison remains fair and consistent across both small and large numbers.\r\n\r\n<!--\r\n\r\n**Please do not leave this empty!**\r\n\r\nPlease explain **IN DETAIL** what the changes are in this PR and why they are needed:\r\n\r\n- Summarize your change (**mandatory**)\r\n- How does this PR work? Need a brief introduction for the changed logic (optional)\r\n- Describe clearly one logical change and avoid lazy messages (optional)\r\n- Describe any limitations of the current code (optional)\r\n- Refer to a related PR or issue link (optional)\r\n\r\n-->\r\n\r\n## Checklist\r\n\r\n- [ ] I have written necessary rustdoc comments.\r\n- [ ] <!-- OPTIONAL --> I have added necessary unit tests and integration tests.\r\n- [ ] <!-- OPTIONAL --> I have added test labels as necessary. <!-- See https://github.com/risingwavelabs/risingwave/blob/main/docs/developer-guide.md#ci-labels-guide) -->\r\n- [ ] <!-- OPTIONAL --> I have added fuzzing tests or opened an issue to track them. <!-- Recommended for new SQL features, see #7934 -->\r\n- [ ] <!-- OPTIONAL --> My PR contains breaking changes. <!-- If it deprecates some features, please create a tracking issue to remove them in the future -->\r\n- [ ] <!-- OPTIONAL --> My PR changes performance-critical code, so I will run (micro) benchmarks and present the results. <!-- To manually trigger a benchmark, please check out [Notion](https://www.notion.so/risingwave-labs/Manually-trigger-nexmark-performance-dashboard-test-b784f1eae1cf48889b2645d020b6b7d3). -->\r\n- [ ] <!-- OPTIONAL --> I have checked the [Release Timeline](https://github.com/risingwavelabs/rw-commits-history/blob/main/release_timeline.md) and [Currently Supported Versions](https://docs.risingwave.com/changelog/release-support-policy#support-end-dates-for-recent-releases) to determine which release branches I need to cherry-pick this PR into. <!-- Please check out the [details](https://github.com/risingwavelabs/risingwave/blob/main/CONTRIBUTING.md) -->\r\n\r\n\r\n## Documentation\r\n\r\n- [ ] <!-- OPTIONAL --> My PR needs documentation updates. <!-- Please use the **Release note** section below to summarize the impact on users -->\r\n\r\n<details>\r\n<summary><b>Release note</b></summary>\r\n\r\n<!--\r\nIf this PR includes changes that directly affect users or other significant modifications relevant to the community, kindly draft a release note to provide a concise summary of these changes.\r\n\r\nPlease prioritize highlighting the impact these changes will have on users.\r\nDiscuss technical details in the \"What's changed\" section, and focus on the impact on users in the release note.\r\n\r\nYou should also mention the environment or conditions where the impact may occur.\r\n-->\r\n\r\n</details>\r\n\n"}
{"id": 254, "ground_truth": "OS", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nNoticed `test_basic_series_frame_alignment` and `test_raw_roundtrip` can occasitionally fail in the CI. Marking as xfail strict=False and clearing the clipboard respectively\n"}
{"id": 48, "ground_truth": "Unordered Data", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\nHey! You're PRing? Cool!\r\nPlease be sure to check out our contribution guide (https://github.com/python-telegram-bot/python-telegram-bot/blob/master/.github/CONTRIBUTING.rst).\r\nEspecially, please have a look at the check list for PRs (https://github.com/python-telegram-bot/python-telegram-bot/blob/master/.github/CONTRIBUTING.rst#check-list-for-prs). Feel free to copy (parts of) the checklist to the PR description to remind you or the maintainers of open points or if you have questions on anything.\r\n-->\r\n\r\n\n"}
{"id": 320, "ground_truth": "Time", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n"}
{"id": 312, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR aims to fix flaky tests in `SparkConnectServiceSuite` which are caused by `executorHolder` [undefined](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L908).\r\nThe conditions to reproduce this issue are:\r\n\r\n(1) The operation finishes before its `executeHolder` is set in [MockSparkListener#onOtherEvent](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L961).\r\n(2) `executeHolder` is accessed through calling `verifyEvents.onComplete` after the operation finishes. \r\n\r\n`SparkListenerConnectOperationStarted` is posted asynchronously with the corresponding operation so the condition (1) can be met. After an operation finishes, `executeHolder` is [removed from a map](https://github.com/apache/spark/blob/af16aa8e11c223642f928b0b9893854a851d70bb/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectExecutionManager.scala#L153) so if the condition (1) is met, `executeHolder` is never set because `SparkConnectService.executionManager.getExecuteHolder` consistently returns `None`.\r\n\r\nOne example of the test affected by this issue is `SPARK-43923: commands send events - get_resources_command`.\r\nYou can easily reproduce this issue by inserting sleep into `MockSparkListener#onOtherEvent` like as follows.\r\n\r\n```\r\n   val executeKey =\r\n     ExecuteKey(sessionHolder.userId, sessionHolder.sessionId, e.operationId)\r\n+  Thread.sleep(1000)\r\n   executeHolder = SparkConnectService.executionManager.getExecuteHolder(executeKey)\r\n```\r\n\r\nAnd then, run test.\r\n```\r\n$ build/sbt 'connect/testOnly org.apache.spark.sql.connect.planner.SparkConnectServiceSuite -- -z \"get_resources_command\"'\r\n```\r\nTo resolve this issue, this PR proposes:\r\n\r\n* Change `VerifyEvents#onCompleted` just to assert `executeHolder.eventsManager.getProducedRowCount == producedRowCount`\r\n* Call `VerifyEvents#onCompleted` from `StreamObserver#onCompleted`\r\n* Add `VerifyEvents#assertClosed` to check if the status is `Closed`\r\n\r\n### Why are the changes needed?\r\nFor test stability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nInserting `Thread.sleep(1000)` like mentioned above and then run `SparkConnectServiceSuite`.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\n"}
{"id": 295, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n**Build Scans:**\n- [elasticsearch-intake #27235 / part4](https://gradle-enterprise.elastic.co/s/gf3w65zgpsugy)\n- [elasticsearch-pull-request #87067 / part-4](https://gradle-enterprise.elastic.co/s/ptir5haak4lju)\n- [elasticsearch-pull-request #87067 / part-4-fips](https://gradle-enterprise.elastic.co/s/2inhl3fgi7sae)\n\n**Reproduction Line:**\n```\n./gradlew \":x-pack:plugin:security:test\" --tests \"org.elasticsearch.xpack.security.authc.AuthenticationServiceTests.testInvalidToken\" -Dtests.seed=3801949404B71D88 -Dtests.locale=nl-SX -Dtests.timezone=Etc/Universal -Druntime.java=24\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nN/A\n\n**Failure History:**\n[See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.xpack.security.authc.AuthenticationServiceTests),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!('testInvalidToken'),title:'Test',type:optionsListControl)))))\n\n**Failure Message:**\n```\njava.lang.Exception: Test abandoned because suite timeout was reached.\n```\n\n**Issue Reasons:**\n- [main] 3 failures in test testInvalidToken (1.3% fail rate in 228 executions)\n\n**Note:**\nThis issue was created using new test triage automation. Please report issues or feedback to es-delivery.\n"}
{"id": 208, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test was flaking because instead of receiving an ENTER, we received a PRESENT. This happens when the presence entry arrives whilst a SYNC is in progress.\r\n\r\nWe wait for presence.get() to return, ensuring that the sync is complete prior to proceeding.\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n* **Tests**\n  * Improved reliability of presence-related integration tests by ensuring deterministic synchronization before component mount.\n  * Added detailed debug logging around presence events to aid diagnosis when tests fail.\n  * Reduced test flakiness by capturing and validating events consistently.\n\n* **Chores**\n  * Minor test harness adjustments to streamline debugging and maintainability without affecting user-facing behavior.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->\n"}
{"id": 10, "ground_truth": "Async Wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis PR allows SparkQA (i.e. Jenkins) to report in its posts to GitHub what category of test failed, if one can be determined.\n\nThe failure categories are:\n- general failure\n- RAT checks failed\n- Scala style checks failed\n- Python style checks failed\n- Build failed\n- Spark unit tests failed\n- PySpark unit tests failed\n- MiMa checks failed\n\nThis PR also fixes the diffing logic used to determine if a patch introduces new classes.\n\n"}
{"id": 232, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nWhen playing around with it, I noticed some flakiness in this test across sessions.\r\n \r\nAfter debugging, turns out the heavy sync primitives that I was calling (like `nvshmem_quiet()` or `nvshmem_fence()`) from inside Triton kernels was causing deadlocks. The original test tried to guarantee ordering: `put(data) -> fence/quiet -> put(flag)`. But the GPU thread got stuck in `quiet()` waiting for network confirmation while holding the SM, creating a deadlock.\r\n\r\nThe fix was realizing `wait_until` already provides all the sync you need. Just do:\r\n- PE A: `nvshmem_wait_until(&ivar, ...)`  \r\n- PE B: `nvshmem_put(&ivar_on_PE_A, ...)`\r\n\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #159788\n* #159756\n* #159755\n* #159734\n* #159701\n* __->__ #159215\n* #159136\n* #158718\n* #158515\n\r\n\r\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta\n"}
{"id": 119, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Description:\r\nI observed a previous suggested fix injected some delay to fix the flaky test, but I believe that fix might be unstable in a CI environment or when run on different machines, given the dependency on some constant wait time. I suggest a new way to fix the test by adding some synchronization for the test execution only. I at first identify the source code location whose slow execution (from Line 436 to 442) leads to the flaky test failure. Hence, I introduce one variable in this test class  that is only there to provide some synchronization. Basically, until these statements are executed, I force the thread that shuts down the services. The waiting location is at Line 447 of this test class.\r\n\r\n### Failure:\r\nRunning org.fluentd.logger.TestFluentLogger\r\n2023-03-31 10:11:42,340 DEBUG [pool-2-thread-1] Started MockFluentd port:33845\r\nException in thread \"pool-3-thread-15\" java.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\njava.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\nException in thread \"pool-3-thread-7\" java.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\nException in thread \"pool-3-thread-10\" java.lang.NullPointerException\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\r\nat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\r\nat org.fluentd.logger.TestFluentLogger$7.run(TestFluentLogger.java:436)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\n2023-03-31 10:16:50,774 ERROR [main] Timed out\r\n2023-03-31 10:16:50,774 DEBUG [pool-2-thread-1] Terminated MockFluentd port:33845\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 308.585 sec <<< FAILURE!\r\n\r\n### Results :\r\n\r\nFailed tests:\r\ntestInMultiThreading(org.fluentd.logger.TestFluentLogger): expected:<210000> but was:<209591>\r\n\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0\n"}
{"id": 125, "ground_truth": "I/O", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Description\r\n\r\nThis PR addresses the issue of flaky test results when executing the `run` command test (`verifies that a melos script can call another script containing steps, and ensures all commands in those steps are executed successfully`) in Linux CI/CD environments, as reported in issue #670. \r\n\r\nCloses #670\r\n\r\n## Type of Change\r\n\r\n<!--- Put an `x` in all the boxes that apply: -->\r\n\r\n- [ ] ‚ú® `feat` -- New feature (non-breaking change which adds functionality)\r\n- [x] üõ†Ô∏è `fix` -- Bug fix (non-breaking change which fixes an issue)\r\n- [ ] ‚ùå `!` -- Breaking change (fix or feature that would cause existing functionality to change)\r\n- [ ] üßπ `refactor` -- Code refactor\r\n- [ ] ‚úÖ `ci` -- Build configuration change\r\n- [ ] üìù `docs` -- Documentation\r\n- [ ] üóëÔ∏è `chore` -- Chore\r\n\n"}
{"id": 390, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\nparallel/test-http-keep-alive-empty-line\n\n### Platform\n\nLinux ARM64\n\n### Console output\n\n```console\nnot ok 1876 parallel/test-http-keep-alive-empty-line\n  ---\n  duration_ms: 3582.76000\n  severity: fail\n  exitcode: 1\n  stack: |-\n    node:events:486\n          throw er; // Unhandled 'error' event\n          ^\n    \n    Error [ERR_STREAM_WRITE_AFTER_END]: write after end\n        at _write (node:internal/streams/writable:487:11)\n        at Writable.write (node:internal/streams/writable:508:10)\n        at Timeout._onTimeout (file:///home/iojs/build/workspace/node-test-commit-arm-debug/test/parallel/test-http-keep-alive-empty-line.mjs:32:14)\n        at listOnTimeout (node:internal/timers:608:17)\n        at process.processTimers (node:internal/timers:543:7)\n    Emitted 'error' event on Socket instance at:\n        at emitErrorNT (node:internal/streams/destroy:170:8)\n        at emitErrorCloseNT (node:internal/streams/destroy:129:3)\n        at process.processTicksAndRejections (node:internal/process/task_queues:90:21) {\n      code: 'ERR_STREAM_WRITE_AFTER_END'\n    }\n    \n    Node.js v25.0.0-pre\n  ...\n```\n\n### Build links\n\n| Reason | <code>parallel/test-http-keep-alive-empty-line</code> |\n| - | :- |\n| Type | JS_TEST_FAILURE |\n| Failed PR | 7 ([https://github.com/nodejs/node/pull/59507/](https://ci.nodejs.org/job/node-test-pull-request/68737/), [https://github.com/nodejs/node/pull/59350/](https://ci.nodejs.org/job/node-test-pull-request/68767/), [https://github.com/nodejs/node/pull/59550/](https://ci.nodejs.org/job/node-test-pull-request/68771/), [https://github.com/nodejs/node/pull/59464/](https://ci.nodejs.org/job/node-test-pull-request/68774/), [https://github.com/nodejs/node/pull/59527/](https://ci.nodejs.org/job/node-test-pull-request/68779/), [https://github.com/nodejs/node/pull/59570/](https://ci.nodejs.org/job/node-test-pull-request/68790/), [https://github.com/nodejs/node/pull/59558/](https://ci.nodejs.org/job/node-test-pull-request/68798/)) |\n| Appeared | [test-equinix-ubuntu2204_container-arm64-4](https://ci.nodejs.org/job/node-test-commit-arm-debug/nodes=ubuntu2204_debug-arm64/20010/console), [test-equinix-rhel8_container-arm64-2](https://ci.nodejs.org/job/node-test-commit-arm/nodes=rhel8-arm64/60027/console), [test-equinix-ubuntu2204_container-arm64-6](https://ci.nodejs.org/job/node-test-commit-arm/nodes=ubuntu2204-arm64/60019/console) |\n| First CI | https://ci.nodejs.org/job/node-test-pull-request/68737/ |\n| Last CI | https://ci.nodejs.org/job/node-test-pull-request/68798/ |\n\n### Additional information\n\nThis test was added in https://github.com/nodejs/node/pull/58178 and has been flaky some time after that.\n"}
{"id": 105, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## What does this PR do?\r\n\r\n`ExpressionVisitorTest#testTraverseExpression` relies on ordered traversal. However, the traversal depends on the order of `getDeclaredFields()` to be deterministic (see below). The Java specification explicitly marks `getDeclaredFields` as a function that can return field names in any order. In the wild, this is most likely to manifest on different architectures with different JVM versions.\r\n\r\nhttps://github.com/apache/fury/blob/54b62fb6ab5d7e557131efe07c7402c885f6e7c4/java/fury-core/src/main/java/org/apache/fury/reflect/ReflectionUtils.java#L368-L381\r\n\r\nUsing [NonDex](https://github.com/TestingResearchIllinois/NonDex), we can replicate the flaky behavior with the following command:\r\n\r\n```\r\nmvn edu.illinois:nondex-maven-plugin:2.1.7:nondex -pl fury-core/ -Dcheckstyle.skip -Dmaven.javadoc.skip -Dtest=org.apache.fury.codegen.ExpressionVisitorTest\r\n```\r\n\r\nThe fix, in this case, is to simply use HashSet equality instead of an ordered List equality. The above NonDex command should succeed after applying this patch.\r\n\r\nI do, however, note that there are other flaky tests (found by running NonDex on the entire `fury-core` project) that fail due to reliance on e.g. `PriorityQueue#toArray`, which is also explicity marked to return nondeterministically ordered arrays.\r\n\r\n## Does this PR introduce any user-facing change?\r\n\r\nNo\n"}
{"id": 231, "ground_truth": "Time", "predicted": "OS", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best-practices-for-faster-reviews\r\n3. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/devel/pull-requests.md#write-release-notes-if-needed\r\n-->\r\n\r\n**What this PR does / why we need it**:\r\nDelete \"hugetlb\" from whitelistControllers\r\n\r\n**Which issue this PR fixes** *(optional, in `fixes #<issue number>(, fixes #<issue_number>, ...)` format, will close that issue when PR gets merged)*: fixes #50770\r\n\r\n**Special notes for your reviewer**:\r\n\r\n**Release note**:\r\n<!--  Steps to write your release note:\r\n1. Use the release-note-* labels to set the release note state (if you have access)\r\n2. Enter your extended release note in the below block; leaving it blank means using the PR title as the release note. If no release note is required, just write `NONE`.\r\n-->\r\n```\r\nNONE\r\n```\r\n\n"}
{"id": 112, "ground_truth": "Async Wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nI moved this in a previous PR, but I think firstly it's only actually required by one of the tests, and more importantly it might be reloading the object before the page has finished submitting.\n"}
{"id": 364, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Changes\r\n\r\n- Increased timeout delay for the flaky test to reduce flakiness.\r\n- Adjusted the waiting logic in `e2e_test.go` to ensure proper container health check completion.\r\n\r\n## Verification\r\n\r\n- Ran the test suite multiple times locally in Gitpod and observed no flakiness.\r\n- Confirmed that the timeout handles the observed race condition.\r\n\r\n* [x] I added CHANGELOG entry for this change.\r\n\r\n## Changelog\r\n\r\n- [#8114](https://github.com/thanos-io/thanos/pull/8114) E2E: Increased test timeout delay to reduce flakiness in container startup checks.\r\n\r\n\n"}
{"id": 159, "ground_truth": "Test Order Dependency", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes random failures in the killSkipmeYesNo test by asserting that at least one client is killed, instead of exactly one.\r\n\r\nCloses #4165 \r\n\n"}
{"id": 281, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Failure cluster [9ddf28290cbb0b1d7335](https://go.k8s.io/triage#9ddf28290cbb0b1d7335)\n\n##### Error text:\n```\nFailed\n=== RUN   TestApply/optional-list-map-key\napply_test.go:221: ERROR: \n        \tError Trace:\t/home/prow/go/src/k8s.io/kubernetes/test/integration/apimachinery/apply/apply_test.go:93\n        \t            \t\t\t\t/home/prow/go/src/k8s.io/kubernetes/test/integration/apimachinery/apply/apply_test.go:127\n        \t            \t\t\t\t/home/prow/go/src/k8s.io/kubernetes/test/integration/apimachinery/apply/apply_test.go:221\n        \t            \t\t\t\t/home/prow/go/src/k8s.io/kubernetes/test/utils/ktesting/tcontext.go:383\n        \tError:      \tNot equal: \n        \t            \texpected: \"- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"x\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status1\\n  operation: Apply\\n  subresource: status\\n- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n        k:{\\\"a\\\":2,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status2\\n  operation: Apply\\n  subresource: status\\n\"\n        \t            \tactual  : \"- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n        k:{\\\"a\\\":2,\\\"b\\\":\\\"y\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status2\\n  operation: Apply\\n  subresource: status\\n- apiVersion: testapigroup.apimachinery.k8s.io/v1\\n  fieldsType: FieldsV1\\n  fieldsV1:\\n    f:status:\\n      f:infos:\\n        k:{\\\"a\\\":1,\\\"b\\\":\\\"x\\\"}:\\n          .: {}\\n          f:a: {}\\n          f:b: {}\\n          f:data: {}\\n  manager: status1\\n  operation: Apply\\n  subresource: status\\n\"\n        \t            \t\n        \t            \tDiff:\n        \t            \t--- Expected\n        \t            \t+++ Actual\n        \t            \t@@ -1,14 +1 @@\n        \t            \t-- apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t-  fieldsType: FieldsV1\n        \t            \t-  fieldsV1:\n        \t            \t-    f:status:\n        \t            \t-      f:infos:\n        \t            \t-        k:{\"a\":1,\"b\":\"x\"}:\n        \t            \t-          .: {}\n        \t            \t-          f:a: {}\n        \t            \t-          f:b: {}\n        \t            \t-          f:data: {}\n        \t            \t-  manager: status1\n        \t            \t-  operation: Apply\n        \t            \t-  subresource: status\n        \t            \t - apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t@@ -31,2 +18,15 @@\n        \t            \t   subresource: status\n        \t            \t+- apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t+  fieldsType: FieldsV1\n        \t            \t+  fieldsV1:\n        \t            \t+    f:status:\n        \t            \t+      f:infos:\n        \t            \t+        k:{\"a\":1,\"b\":\"x\"}:\n        \t            \t+          .: {}\n        \t            \t+          f:a: {}\n        \t            \t+          f:b: {}\n        \t            \t+          f:data: {}\n        \t            \t+  manager: status1\n        \t            \t+  operation: Apply\n        \t            \t+  subresource: status\n        \t            \t \n        \tTest:       \tTestApply/optional-list-map-key\n        \tMessages:   \tremove status #1:\n        \t            \tapiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \tkind: Carp\n        \t            \tmetadata:\n        \t            \t  creationTimestamp: \"2025-08-29T07:24:13Z\"\n        \t            \t  managedFields:\n        \t            \t  - apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t    fieldsType: FieldsV1\n        \t            \t    fieldsV1:\n        \t            \t      f:status:\n        \t            \t        f:infos:\n        \t            \t          k:{\"a\":1,\"b\":\"y\"}:\n        \t            \t            .: {}\n        \t            \t            f:a: {}\n        \t            \t            f:b: {}\n        \t            \t            f:data: {}\n        \t            \t          k:{\"a\":2,\"b\":\"y\"}:\n        \t            \t            .: {}\n        \t            \t            f:a: {}\n        \t            \t            f:b: {}\n        \t            \t            f:data: {}\n        \t            \t    manager: status2\n        \t            \t    operation: Apply\n        \t            \t    subresource: status\n        \t            \t    time: \"2025-08-29T07:24:13Z\"\n        \t            \t  - apiVersion: testapigroup.apimachinery.k8s.io/v1\n        \t            \t    fieldsType: FieldsV1\n        \t            \t    fieldsV1:\n        \t            \t      f:status:\n        \t            \t        f:infos:\n        \t            \t          k:{\"a\":1,\"b\":\"x\"}:\n        \t            \t            .: {}\n        \t            \t            f:a: {}\n        \t            \t            f:b: {}\n        \t            \t            f:data: {}\n        \t            \t    manager: status1\n        \t            \t    operation: Apply\n        \t            \t    subresource: status\n        \t            \t    time: \"2025-08-29T07:24:14Z\"\n        \t            \t  name: test-carp\n        \t            \t  namespace: testapply-optional-list-map-key-tvqfh\n        \t            \t  resourceVersion: \"605\"\n        \t            \t  uid: a34ec7bb-a425-468a-b298-9f8ab731f2de\n        \t            \tspec: {}\n        \t            \tstatus:\n        \t            \t  infos:\n        \t            \t  - a: 1\n        \t            \t    b: x\n        \t            \t    data: status1_a1_bx\n        \t            \t  - a: 1\n        \t            \t    b: \"y\"\n        \t            \t    data: status2_a1_by\n        \t            \t  - a: 2\n        \t            \t    b: \"y\"\n        \t            \t    data: status2_a2_by\n        \n--- FAIL: TestApply/optional-list-map-key (0.75s)\n```\n\nIt looks like the order of the list entries is non-deterministic, causing a flake when the order is different than expected.\n\n\n#### Recent failures:\n[8/27/2025, 7:46:41 PM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1960760579014529024)\n[8/26/2025, 7:11:13 AM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1960208069166108672)\n[8/23/2025, 1:44:09 AM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1959038602079899648)\n[8/21/2025, 1:19:07 AM ci-kubernetes-integration-master](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-master/1958307527783354368)\n\n\n/kind failing-test\n/kind flake\n/sig api-machinery\n\n\n"}
{"id": 154, "ground_truth": "Unordered Data", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n- Added default blurb for help text to show the 3 kinds of requests possible\n- Generate request-type specific help text (e.g., trying 'cloudcfg run' will provide usage helptext on the run command.)\n\n```\n$ ./output/go/cloudcfg\nusage: cloudcfg -h <host> [-c config/file.json] [-p <hostPort>:<containerPort>,..., <hostPort-n>:<containerPort-n>] <method> <methodArgs>\n\n  Kubernetes REST API:\n  cloudcfg [OPTIONS] get|list|create|delete|update <url>\n\n  Run an image:\n  cloudcfg [OPTIONS] run <image> <replicas> <name>\n\n  Manage replication controllers:\n  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>\n\n  Options:\n  -auth=\"/Users/tlow/.kubernetes_auth\": Path to the auth info file.  If missing, prompt the user\n  -c=\"\": Path to the config file.\n  -h=\"\": The host to connect to.\n  -json=false: If true, print raw JSON for responses\n  -l=\"\": Label query to use for listing\n  -p=\"\": The port spec, comma-separated list of <external>:<internal>,...\n  -s=-1: If positive, create and run a corresponding service on this port, only used with 'run'\n  -u=1m0s: Update interarrival period\n  -v=false: Print the version number.\n  -yaml=false: If true, print raw YAML for responses\n\n$ ./output/go/cloudcfg rollingupdate\n2014/06/12 11:13:18 usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>\n$ ./output/go/cloudcfg get\n2014/06/12 11:13:22 usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>\n$ ./output/go/cloudcfg run\n2014/06/12 11:13:25 usage: cloudcfg [OPTIONS] run <image> <replicas> <name>\n```\n\n"}
{"id": 413, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--- Note to EXTERNAL Contributors -->\r\n<!-- Thanks for opening a PR! \r\nIf it is a significant code change, please **make sure there is an open issue** for this. \r\nWe work best with you when we have accepted the idea first before you code. -->\r\n\r\n<!--- For ALL Contributors üëá -->\r\n\r\n## What was changed\r\n<!-- Describe what has changed in this PR -->\r\n\r\nBumped to Go 1.25 to use the new `synctest.Test` primitive to run deterministic timing tests.\r\n\r\nTip: use `Hide Whitespace` option for reviewing.\r\n\r\n## Why?\r\n<!-- Tell your future self why have you made these changes -->\r\n\r\n`TestRunDurationWithoutTimeout` is flaky - example:\r\nhttps://github.com/temporalio/omes/actions/runs/17052148233/job/48342124492?pr=179#step:4:125\r\n\r\n## Checklist\r\n<!--- add/delete as needed --->\r\n\r\n1. Closes <!-- add issue number here -->\r\n\r\n2. How was this tested:\r\n<!--- Please describe how you tested your changes/how we can test them -->\r\n\r\n3. Any docs updates needed?\r\n<!--- update README if applicable\r\n      or point out where to update docs.temporal.io -->\r\n\n"}
{"id": 109, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n````\nFAILED providers/microsoft/azure/tests/unit/microsoft/azure/triggers/test_wasb.py::TestWasbBlobSensorTrigger::test_success - AssertionError: assert False is True\n +  where False = <built-in method done of _asyncio.Task object at 0xffe8138b6b00>()\n +    where <built-in method done of _asyncio.Task object at 0xffe8138b6b00> = <Task pending name='Task-2479' coro=<<async_generator_asend without __name__>()> wait_for=<Future pending cb=[shield.<locals>._outer_done_callback() at /usr/python/lib/python3.12/asyncio/tasks.py:922, Task.task_wakeup()]>>.done\n```\n\nFor example https://github.com/apache/airflow/actions/runs/17534123692/job/49795194833#step:8:2322\n"}
{"id": 293, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Rationale for this change\nFixing flaky bloom filter memory check test\n\n### What changes are included in this PR?\nAdded an additional runtime.GC() call to ensure the releases are called.\n\n\n"}
{"id": 120, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nCurrently, there's a clause that sends no response when the connection\r\nstaet in the channel is `idle`, the initial state.\r\n\r\nhttps://github.com/emqx/emqx/blob/e861f3963e9eee6b16e685aa4c7b57ed428d041b/apps/emqx/src/emqx_channel.erl#L490\r\n\r\nHere, we attempt to publish a simple message before sending the large\r\none to ensure that the channel's connection state is connected.\n"}
{"id": 31, "ground_truth": "Async wait", "predicted": "Floating point operations", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe StringComparator now works on serialized data.\n\nTo this end new string read/write/copy/compare methods were introduced, which use a variable-length encoding for the characters.\n\n~~key-points:~~\n  ~~\\- The most significant bits are written/read first.~~\n  ~~\\- The first 2 bits of the character are used to encode the size of the character.~~\n ~~\\- A character is at most 3 Bytes big.~~\n\nAdditionally, the StringSerializer now has full unicode support. ~~i couldn't find a unicode character that ~~uses more than 22 bits, as such 3 Bytes should be sufficient.~~\n\n"}
{"id": 243, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nRefs #943\r\n\r\n* We've already got a test that ensures `response.elapsed > datetime.timedelta(0)`, so let's not try to enforce a \"this should end up as more than one second\" case.\r\n* Let's bump the slow response default to a more reliably noticable 1 second delay.\r\n\n"}
{"id": 311, "ground_truth": "Unordered collections", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n* remove casts\r\n* generalize `is_valid_sample_rate` to return the validated float\r\n* added a `validate_scopes` helper method to validate the scopes entry on the context and return the narrowed type\r\n* added a `get_typed_attribute` helper method to narrow the type on getting attributes from otel\r\n* fix a flaky test on gevent\n"}
{"id": 288, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #983 \r\n\r\n### Motivation\r\n\r\nOld TestMaxPendingChunkMessages() uses the concurrent message publish to make the consumer discard unavailable chunk. And it's flaky.\r\n\r\nSo the `sendSingeChunk()` is introduced to manual create scenarios where old chunks should be discarded.\r\n\r\n### Modifications\r\n\r\n- Fix `TestMaxPendingChunkMessages()`\r\n\r\n### Verifying this change\r\n\r\n- [x] Make sure that the change passes the CI checks.\r\n\n"}
{"id": 189, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\n!!!ATTENTION!!!\r\n\r\nIf you are fixing *any* crash or *any* potential security issue, *do not*\r\nopen a pull request in this repo. Please report the issue via emailing\r\nenvoy-security@googlegroups.com where the issue will be triaged appropriately.\r\nThank you in advance for helping to keep Envoy secure.\r\n\r\n!!!ATTENTION!!!\r\n\r\nFor an explanation of how to fill out the fields, please see the relevant section\r\nin [PULL_REQUESTS.md](https://github.com/envoyproxy/envoy/blob/main/PULL_REQUESTS.md)\r\n-->\r\n\r\nCommit Message: aws: fix test flake when no IMDS can be found\r\nAdditional Description: Lack of IMDS (169.254.169.254 address) can cause a race condition and crash during testing due to the shutdown of cluster manager. This scenario should not occur normally, as cluster manager will still exist and the lack of IMDS is handled gracefully.\r\nRisk Level: Low\r\nTesting: N/A\r\nDocs Changes:\r\nRelease Notes:\r\nPlatform Specific Features:\r\n[Optional Runtime guard:]\r\n[Optional Fixes #Issue]\r\n[Optional Fixes commit #PR or SHA]\r\n[Optional Deprecated:]\r\n[Optional [API Considerations](https://github.com/envoyproxy/envoy/blob/main/api/review_checklist.md):]\r\n\n"}
{"id": 431, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThere's a TOCTOU issue that can happen when selecting unused ports for the server to use (we get assigned an unused port by the OS, and between then and when the server actually binds to the port another test steals it). Improve this by checking if the server existed soon after setup, and if so we retry starting it. Client connection can also fail spuriously (in local testing) so added a retry mechanism.\r\n\r\nThis also fixes a hang, where if the server exited (almost always due to the issue described above) before we connected to it, attempting to connect our client ZMQ sockets to it would just hang. To resolve this, I added a timeout so we can't wait forever. \n"}
{"id": 20, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThere was a couple of errors on the dev script preventing it to work properly. I also merged this with the open pr I had for adding `next/css`\n\n"}
{"id": 271, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nIntroduced by 3d707be950b387552585451071928e7b39cdfa53.\r\n\r\nFrom docs buildbot: http://buildbot.python.org/all/builders/Docs%203.x/builds/399/steps/lint/logs/stdio\r\n\r\n```\r\npython3 tools/rstlint.py -i tools -i venv\r\n[1] faq/windows.rst:303: trailing whitespace\r\n[1] faq/windows.rst:305: trailing whitespace\r\n2 problems with severity 1 found.\r\n```\n"}
{"id": 74, "ground_truth": "Network", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nhttps://dev.azure.com/monacotools/Monaco/_build/results?buildId=314193&view=logs&j=9833cdaa-2e92-5f8d-56a8-239fd18fd628&t=efbaf268-cf98-50c2-bb3c-7950770fd6bc&l=749\n\n```\n  1) vscode API - terminal\n       Terminal\n         environmentVariableCollection\n           should have collection variables apply to terminals immediately after setting:\n     Error: Timeout: b1~b2~ should be printed after 20 seconds.\nDid not pass accept function\n      at poll (/Users/runner/work/1/s/extensions/vscode-api-tests/src/utils.ts:176:10)\n      at Context.<anonymous> (/Users/runner/work/1/s/extensions/vscode-api-tests/src/singlefolder-tests/terminal.test.ts:775:5)\n\n  2) vscode API - terminal\n       Terminal\n         environmentVariableCollection\n           should have collection variables apply to environment variables that don't exist:\n     Error: Timeout: ~b2~ should be printed after 20 seconds.\nDid not pass accept function\n      at poll (/Users/runner/work/1/s/extensions/vscode-api-tests/src/utils.ts:176:10)\n      at Context.<anonymous> (/Users/runner/work/1/s/extensions/vscode-api-tests/src/singlefolder-tests/terminal.test.ts:820:5)\n\n  3) vscode API - terminal\n       Terminal\n         environmentVariableCollection\n           should respect clearing entries:\n     Error: Timeout: ~b1~ should be printed after 20 seconds.\nDid not pass accept function\n      at poll (/Users/runner/work/1/s/extensions/vscode-api-tests/src/utils.ts:176:10)\n      at Context.<anonymous> (/Users/runner/work/1/s/extensions/vscode-api-tests/src/singlefolder-tests/terminal.test.ts:862:5)\n\n\n```\n"}
{"id": 302, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## PR summary\r\n\r\nFor labelcolor={linecolor,markeredgecolor,markerfacecolor}, text will match the specified attribute if consistent, but fall back to black if they differ within a single labeled artist.\r\n\r\nThese tests use 10 random colours out of the ['r', 'g', 'b'] set, so 3 (all red, all green, all blue) out of 3**10 will result in the text _not_ being black. This is rare (0.0051%), but does happen once in a while (e.g., https://github.com/matplotlib/matplotlib/actions/runs/10967975776/job/30458622005).\r\n\r\nInstead, just hard-code some different colours in the test.\r\n\r\n## PR checklist\r\n\r\n- [n/a] \"closes #0000\" is in the body of the PR description to [link the related issue](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue)\r\n- [x] new and changed code is [tested](https://matplotlib.org/devdocs/devel/testing.html)\r\n- [n/a] *Plotting related* features are demonstrated in an [example](https://matplotlib.org/devdocs/devel/document.html#write-examples-and-tutorials)\r\n- [n/a] *New Features* and *API Changes* are noted with a [directive and release note](https://matplotlib.org/devdocs/devel/api_changes.html#announce-changes-deprecations-and-new-features)\r\n- [n/a] Documentation complies with [general](https://matplotlib.org/devdocs/devel/document.html#write-rest-pages) and [docstring](https://matplotlib.org/devdocs/devel/document.html#write-docstrings) guidelines\n"}
{"id": 294, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Rationale for this change\nAdditional checks for `io.EOF` to avoid flaky tests\n\n"}
{"id": 319, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis issue can be caused by a non-existing path but also a misunderstanding from the config file. A short example will help the user.\n\n"}
{"id": 66, "ground_truth": "Network", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nxref #14626\n"}
{"id": 35, "ground_truth": "Logic", "predicted": "Floating point operations", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nPlatforms: linux\n\n  This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_grad_with_manual_interleaved_ScheduleClass0_use_new_runtime_True&suite=ScheduleTest&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/42919034022).\n\n  Over the past 3 hours, it has been determined flaky in 7 workflow(s) with 7 failures and 7 successes.\n\n  **Debugging instructions (after clicking on the recent samples link):**\n  DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\n  To find relevant log snippets:\n  1. Click on the workflow logs linked above\n  2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n  3. Grep for `test_grad_with_manual_interleaved_ScheduleClass0_use_new_runtime_True`\n  4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n  \n  \n\n<details><summary>Sample error message</summary>\n\n```\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 1721, in wrapper\n    raise rv\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 158 / 512 (30.9%)\nGreatest absolute difference: 0.0002696812152862549 at index (331,) (up to 4e-05 allowed)\nGreatest relative difference: 0.09954891353845596 at index (176,) (up to 1e-05 allowed)\n\nTo execute this test, run the following from the base repo dir:\n    python test/distributed/pipelining/test_schedule_multiproc.py ScheduleTest.test_grad_with_manual_interleaved_ScheduleClass0_use_new_runtime_True\n\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n```\n\n</details>\n\n\n  Test file path: `distributed/pipelining/test_schedule_multiproc.py`\n\n  For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.\n\ncc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @clee2000\n"}
{"id": 89, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://scans.gradle.com/s/bfgjiqwioneey/tests/task/:profiler:test/details/com.splunk.opentelemetry.profiler.snapshot.LongRunningBackgroundTaskTest/traceBackgroundThreadProfilingContinuesAfterEntrySpanEnds()?top-execution=1\n"}
{"id": 223, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThanks for Diana Carroll to report this issue (https://spark-project.atlassian.net/browse/SPARK-1100)\n\nthe current saveAsTextFile/SequenceFile will overwrite the output directory silently if the directory already exists, this behaviour is not desirable because\n\noverwriting the data silently is not user-friendly\n\nif the partition number of two writing operation changed, then the output directory will contain the results generated by two runnings\n\nMy fix includes:\n\nadd some new APIs with a flag for users to define whether he/she wants to overwrite the directory:\nif the flag is set to true, then the output directory is deleted first and then written into the new data to prevent the output directory contains results from multiple rounds of running;\n\nif the flag is set to false, Spark will throw an exception if the output directory already exists\n\nchanged JavaAPI part\n\ndefault behaviour is overwriting\n\nTwo questions\n\nshould we deprecate the old APIs without such a flag?\n\nI noticed that Spark Streaming also called these APIs, I thought we don't need to change the related part in streaming? @tdas\n\n"}
{"id": 157, "ground_truth": "Unordered Data", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/emqx/emqx/actions/runs/17472259934/job/49624282534?pr=15846#step:4:381\r\n\n"}
{"id": 347, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nEven with Kahan summation, we were occasionally getting floating point differences at the 14th decimal point, well beyond anything a GIS use case would care about.\r\n\r\nFixes #112461\r\nFixes #112463\n"}
{"id": 132, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR removes an assertion from `SparkConnectServiceSuite.scala` to eliminate flakiness which affects the following tests in the test suite.\r\n\r\n* `SPARK-44776: LocalTableScanExec`\r\n* `SPARK-41224: collect data using arrow`\r\n\r\nIn those tests, [VerifyEvents#onNext](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L913) is called and checked `assert(executeHolder.eventsManager.status == ExecuteStatus.Analyzed)`, which can fail and this PR proposes to remove this assertion.\r\nIf this assertion fails, an exception will be thrown and [VerifyEvents#onError](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L921) is called but the reason this `onError` is called is not due to a query/command fails. So `executeHolder.eventsManager.hasError.isDefined` never changes to `true`.\r\n\r\n`VerifyEvents#onNext` is indirectly called from [ExecuteGrpcResponseSender#sendResponse](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/execution/ExecuteGrpcResponseSender.scala#L398), which runs on a sender thread.\r\nOn the other hand, an operation status is changed on another thread. Especially, transition to `Analyzed` status is done [here](https://github.com/apache/spark/blob/9d27db940673196f64b1568d73c459f83eb6f788/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/execution/SparkConnectPlanExecution.scala#L75). But the status can transition to `ReadyForExecution` or `Finished` before `VerifyEvents#onNext` is called.\r\nSo, those tests can occasionally fail.\r\n\r\nYou can easily reproduce this issue by inserting sleep like as follows and then run those tests.\r\n```\r\n   def onNext(v: proto.ExecutePlanResponse): Unit = {\r\n     if (v.hasSchema) {\r\n+      Thread.sleep(5000)\r\n       assert(executeHolder.eventsManager.status == ExecuteStatus.Analyzed)\r\n     }\r\n```\r\n\r\nThe solution this PR proposes is just removing the assertion.\r\nIn `VerifyEvents#onNext`, there is another assertion `assert(executeHolder.eventsManager.status == ExecuteStatus.Finished)` and I think having only this assertion is enough because the status changes to `Finished` through `Analyzed`.\r\n\r\n\r\n### Why are the changes needed?\r\nFor test stability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nInserting sleep into `onNext` like explained above and run the problematic tests.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\r\n\n"}
{"id": 386, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Body\n\nThe test_listener_logs_failed_serialization is flaky - likely due to a race condition. This should be improve.\r\n\r\nExample failure: \r\n\r\nhttps://github.com/apache/airflow/actions/runs/10714765649/job/29709338209?pr=41331#step:7:5256\r\n\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n___________________ test_listener_logs_failed_serialization ____________________\r\n\r\nself = <MagicMock name='mock.warning' id='139678763968448'>\r\n\r\n    def assert_called_once(self):\r\n        \"\"\"assert that the mock was called only once.\r\n        \"\"\"\r\n        if not self.call_count == 1:\r\n            msg = (\"Expected '%s' to have been called once. Called %s times.%s\"\r\n                   % (self._mock_name or 'mock',\r\n                      self.call_count,\r\n                      self._calls_repr()))\r\n>           raise AssertionError(msg)\r\nE           AssertionError: Expected 'warning' to have been called once. Called 0 times.\r\n\r\n/usr/local/lib/python3.8/unittest/mock.py:892: AssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test_listener_logs_failed_serialization():\r\n        listener = OpenLineageListener()\r\n        listener.log = MagicMock()\r\n        listener.adapter = OpenLineageAdapter(\r\n            client=OpenLineageClient(transport=ConsoleTransport(config=ConsoleConfig()))\r\n        )\r\n        event_time = dt.datetime.now()\r\n    \r\n        fut = listener.submit_callable(\r\n            listener.adapter.dag_failed,\r\n            dag_id=\"\",\r\n            run_id=\"\",\r\n            end_date=event_time,\r\n            execution_date=threading.Thread(),\r\n            dag_run_state=DagRunState.FAILED,\r\n            task_ids=[\"task_id\"],\r\n            msg=\"\",\r\n        )\r\n        assert fut.exception(10)\r\n>       listener.log.warning.assert_called_once()\r\nE       AssertionError: Expected 'warning' to have been called once. Called 0 times.\r\n\r\ntests/providers/openlineage/plugins/test_listener.py:628: AssertionError\r\n----------------------------- Captured stdout call -----------------------------\r\n```\n\n### Committer\n\n- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n"}
{"id": 340, "ground_truth": "Unordered collections", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nI am aware of a case where the same set of plugins would lead to one controller starting successfully and one controller failing to start, depending on the order in which the plugins were listed. We currently use file system iteration order, which is undefined. This PR sorts the list by filename before passing the result to the topological sorting function, so every controller with the same set of plugins will now load them in the same order. This should make it easier to debug such problems, because they will either always occur, or they will never occur ‚Äî but at least the behavior will be deterministic.\r\n\r\n### Testing done\r\n\r\nI added a log statement to print the list of active plugins as consumed by the `UberClassLoader` and verified that at each level of the topological sort the entries were in lexicographical order.\r\n\r\n### Proposed changelog entries\r\n\r\nList plugins in deterministic order to improve diagnosability of plugin linkage errors.\r\n\r\n### Proposed upgrade guidelines\r\n\r\nN/A\r\n\r\n```[tasklist]\r\n### Submitter checklist\r\n- [ ] The Jira issue, if it exists, is well-described.\r\n- [ ] The changelog entries and upgrade guidelines are appropriate for the audience affected by the change (users or developers, depending on the change) and are in the imperative mood (see [examples](https://github.com/jenkins-infra/jenkins.io/blob/master/content/_data/changelogs/weekly.yml)). Fill in the **Proposed upgrade guidelines** section only if there are breaking changes or changes that may require extra steps from users during upgrade.\r\n- [ ] There is automated testing or an explanation as to why this change has no tests.\r\n- [ ] New public classes, fields, and methods are annotated with `@Restricted` or have `@since TODO` Javadocs, as appropriate.\r\n- [ ] New deprecations are annotated with `@Deprecated(since = \"TODO\")` or `@Deprecated(forRemoval = true, since = \"TODO\")`, if applicable.\r\n- [ ] New or substantially changed JavaScript is not defined inline and does not call `eval` to ease future introduction of Content Security Policy (CSP) directives (see [documentation](https://www.jenkins.io/doc/developer/security/csp/)).\r\n- [ ] For dependency updates, there are links to external changelogs and, if possible, full differentials.\r\n- [ ] For new APIs and extension points, there is a link to at least one consumer.\r\n```\r\n\r\n### Desired reviewers\r\n\r\n@mention\r\n\r\n<!-- Comment:\r\nIf you need an accelerated review process by the community (e.g., for critical bugs), mention @jenkinsci/core-pr-reviewers.\r\n-->\r\n\r\nBefore the changes are marked as `ready-for-merge`:\r\n\r\n```[tasklist]\r\n### Maintainer checklist\r\n- [ ] There are at least two (2) approvals for the pull request and no outstanding requests for change.\r\n- [ ] Conversations in the pull request are over, or it is explicit that a reviewer is not blocking the change.\r\n- [ ] Changelog entries in the pull request title and/or **Proposed changelog entries** are accurate, human-readable, and in the imperative mood.\r\n- [ ] Proper changelog labels are set so that the changelog can be generated automatically.\r\n- [ ] If the change needs additional upgrade steps from users, the `upgrade-guide-needed` label is set and there is a **Proposed upgrade guidelines** section in the pull request title (see [example](https://github.com/jenkinsci/jenkins/pull/4387)).\r\n- [ ] If it would make sense to backport the change to LTS, a Jira issue must exist, be a _Bug_ or _Improvement_, and be labeled as `lts-candidate` to be considered (see [query](https://issues.jenkins.io/issues/?filter=12146)).\r\n```\r\n\n\n<a href=\"https://gitpod.io/#https://github.com/jenkinsci/jenkins/pull/8453\"><img src=\"https://gitpod.io/button/open-in-gitpod.svg\"/></a>\n\n\n"}
{"id": 239, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #14294\r\n\r\nI ran the test with 1 thread and saw it consistently yielded 5 files, so this should be the proper lower bound.\n"}
{"id": 27, "ground_truth": "Async wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n@StephanEwen, @aljoscha: Can someone please confirm that this is the place to disable the POJO types?\n\nThere are various issues when working with POJOs in the Java API, for example [1].\n\n[1] https://mail-archives.apache.org/mod_mbox/incubator-flink-dev/201407.mbox/%3C53D96049.1060509%40cse.uta.edu%3E\n\n"}
{"id": 337, "ground_truth": "Async wait", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nRemoved white supremacist language.\n"}
{"id": 206, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFixes #145 -  the issue was caused by the slightly variable results returned by the GitHub API.\r\n\r\nIn all the tests in `FunctionalSpec`, we create several test fixtures with the GitHub API, each created to be used once and thrown away:\r\n\r\n* A brand-new GitHub repository, populated with whatever Git history we require\r\n* A brand new PR in the repository, which we also merge\r\n\r\nBefore continuing on to the next step, which would be having Prout scan the repo for recently-merged PRs, we perform several checks to ensure that the GitHub API confirms the status of the merged PR:\r\n\r\nhttps://github.com/guardian/prout/blob/a710e347afb94ca719e11b9c5b6b3312f1245e98/test/lib/Helpers.scala#L180-L195\r\n\r\nHowever, looking a the differing logs produced by the failure & success cases in #145, it became apparent that even though we've checked the GitHub API.\n"}
{"id": 351, "ground_truth": "Concurrency", "predicted": "Unordered collections", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nCorrected Spelling errors.\n\n"}
{"id": 249, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--\r\n  Thanks for sending a pull request!\r\n    1. If this is your first time, please read our contributor guidelines: https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute\r\n    2. Make sure your PR title starts with JIRA issue id, e.g., 'HADOOP-17799. Your PR title ...'.\r\n-->\r\n\r\n### Description of PR\r\n\r\nJIRA: YARN-11642. Fix Flaky Test TestTimelineAuthFilterForV2#testPutTimelineEntities.\r\n\r\nWe can find the following unit test failure report:\r\n\r\n[Report1\r\n](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6410/1/testReport/org.apache.hadoop.yarn.server.timelineservice.security/TestTimelineAuthFilterForV2/testPutTimelineEntities_boolean__boolean__3_/)\r\n\r\n```\r\norg.opentest4j.AssertionFailedError: expected: <2> but was: <1>\r\n...\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.publishAndVerifyEntity(TestTimelineAuthFilterForV2.java:324)\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.publishWithRetries(TestTimelineAuthFilterForV2.java:337)\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.testPutTimelineEntities(TestTimelineAuthFilterForV2.java:383)\r\n```\r\n\r\n[Report2\r\n](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-6396/2/testReport/org.apache.hadoop.yarn.server.timelineservice.security/TestTimelineAuthFilterForV2/testPutTimelineEntities_boolean__boolean__3_/)\r\n\r\n```\r\nexpected: <true> but was: <false>\r\n...\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.verifyEntity(TestTimelineAuthFilterForV2.java:294)\r\nat org.apache.hadoop.yarn.server.timelineservice.security.TestTimelineAuthFilterForV2.testPutTimelineEntities(TestTimelineAuthFilterForV2.java:441)\r\n```\r\n\r\nThe reason for this error is that `PerNodeTimelineCollectorsAuxService` was not initialized normally.\r\n\r\n```\r\n2024-01-05 09:34:21,511 WARN  [main] collector.PerNodeTimelineCollectorsAuxService (StringUtils.java:startupShutdownMessage(755)) - failed to register any UNIX signal loggers: \r\njava.lang.IllegalStateException: Can't re-install the signal handlers.\r\nat org.apache.hadoop.util.SignalLogger.register(SignalLogger.java:73)\r\nat org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:753)\r\n... \r\n```\r\n\r\n`PerNodeTimelineCollectorsAuxService` is initialized using a static method and is initialized in parallel by multiple threads(because our unit tests are parallel.), resulting in an error. \r\n\r\n```\r\n/usr/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-6396/yetus-m2/hadoop-trunk-patch-0 -Dsurefire.rerunFailingTestsCount=2 -Pparallel-tests -Pshelltest -Pnative -Drequire.fuse -Drequire.openssl -Drequire.snappy -Drequire.valgrind -Drequire.zstd -Drequire.test.libhadoop -Pyarn-ui clean test -fae ...\r\n```\r\n\r\nWe can change the static `initialization` to `new PerNodeTimelineCollectorsAuxService()` to avoid this problem, which has been verified in the local environment.\r\n\r\n### How was this patch tested?\r\n\r\nJunit Test.\r\n\r\n### For code changes:\r\n\r\n- [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?\r\n- [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?\r\n- [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?\r\n- [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?\r\n\r\n\n"}
{"id": 166, "ground_truth": "OS", "predicted": "OS", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThere's a test in `test_utils.py` that flakes very often, but only on Python 3.8 and only in CI (locally it's all fine). I've tried a couple of ways to fix it but at this point it's not worth the effort, so just skipping it on 3.8.\n"}
{"id": 417, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFix attendees emails test which was trying to match two strings together when order can change and cause flakiness.  Resolved by expecting string to include emails so order shouldn't matter\n"}
{"id": 15, "ground_truth": "Async Wait", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nAdd address and/or port to errors where applicable for better reporting.\n\nSee joyent/node#7005 and #16 \n\n"}
{"id": 374, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test was flaking because before the suite runs a bunch of settings are added which are critical to making the test reliable. Inside the settings part it opens the editor, edits it and saves the file via ctrl/cmd+s. This is all fine, but it doesn't verify anything so the editor may end up closing before ctrl/cmd+s actually gets handled. We disable the modal in smoke tests since it needs to run headlessly, so it's difficult to see that the file never actually saves and a dirty file is closed and discarded.\r\n\r\nThe fix is to verify settings.json actually does save by changing the shared Editors.saveOpenedFile mechanism to ensure the dirty indicator isn't present on the active tab.\r\n\r\nFixes #254893\r\nPart of #246731\r\n\r\nFYI @aiday-mar \r\n\r\n<!-- Thank you for submitting a Pull Request. Please:\r\n* Read our Pull Request guidelines:\r\n  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests\r\n* Associate an issue with the Pull Request.\r\n* Ensure that the code is up-to-date with the `main` branch.\r\n* Include a description of the proposed changes and how to test them.\r\n-->\r\n\n"}
{"id": 344, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nAs documented in #33323, we have frequent failures of the flaky triggerer job tests.\r\n\r\nThe flaky failures are about some errors when we close all the sessions in teardown of the test. It turns out that the tests had side-effect - they have not waited for the TriggererJob thread to complete, they merely marked them to be stopped, but they have not waited for those to complete - this is quite plausible explanation of the flaky test failures - since those threads have 1 second sleep, it's more than likely that the session has been created and used by the thread while the teardown has been attempting to close all the sessions.\r\n\r\nThis side effect could also have an effect for other tests that were run after - because in a busy test run machine, the side effects could propagate further than just to the teardown, so it could also explain why sometimes (very rarely) other job tests failed with similar errors.\r\n\r\nThe fix is to join the runner after marking it to be stopped.\r\n\r\nFixes: #33323 (Hopefully)\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).\r\n\n"}
{"id": 39, "ground_truth": "Async Wait", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n```\r\n[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-801)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-802)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-803)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-804)[](https://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline#step-202-log-805)[2022-03-24T11:45:03.974Z] tests/python/test_data_iterator.py:111: AssertionError\r\n\r\n[2022-03-24T11:45:03.974Z] ---------------------------------- Hypothesis ----------------------------------\r\n\r\n[2022-03-24T11:45:03.974Z] Falsifying example: test_data_iterator(\r\n\r\n[2022-03-24T11:45:03.974Z]     n_samples_per_batch=4, n_features=2, n_batches=1, subsample=True,\r\n\r\n[2022-03-24T11:45:03.974Z] )\r\n```\r\n\r\nhttps://xgboost-ci.net/blue/organizations/jenkins/xgboost/detail/PR-7687/4/pipeline\n"}
{"id": 339, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nSometimes the execution last exactly SLEEP milliseconds so it becomes a failure.\r\n\r\nSee [JENKINS-54459](https://issues.jenkins-ci.org/browse/JENKINS-54459).\r\n\r\n<!-- Comment: \r\nIf the issue is not fully described in the ticket, add more information here (justification, pull request links, etc.).\r\n\r\n * We do not require JIRA issues for minor improvements.\r\n * Bugfixes should have a JIRA issue (backporting process).\r\n * Major new features should have a JIRA issue reference.\r\n-->\r\n\r\n### Proposed changelog entries\r\n\r\nN/A\r\n\r\n<!-- Comment: \r\nThe changelogs will be integrated by the core maintainers after the merge.  See the changelog examples here: https://jenkins.io/changelog/ -->\r\n\r\n### Submitter checklist\r\n\r\n- [X] JIRA issue is well described\r\n- [X] Changelog entry appropriate for the audience affected by the change (users or developer, depending on the change). [Examples](https://github.com/jenkins-infra/jenkins.io/blob/master/content/_data/changelogs/weekly.yml)\r\n      * Use the `Internal: ` prefix if the change has no user-visible impact (API, test frameworks, etc.)\r\n- [X] Appropriate autotests or explanation to why this change has no tests\r\n- [ ] N/A For dependency updates: links to external changelogs and, if possible, full diffs\r\n\r\n<!-- For new API and extension points: Link to the reference implementation in open-source (or example in Javadoc) -->\r\n\r\n### Desired reviewers\r\n\r\n@reviewbybees \r\n\r\n<!-- Comment:\r\nIf you need an accelerated review process by the community (e.g., for critical bugs), mention @jenkinsci/code-reviewers\r\n-->\r\n\n"}
{"id": 73, "ground_truth": "Logic", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nBuild: https://dev.azure.com/monacotools/a6d41577-0fa3-498e-af22-257312ff0545/_build/results?buildId=317618\nChanges: https://github.com/Microsoft/vscode/compare/a58d48f...30dbfbb\n\nThe RandomInt, \"should include min and max\" unit test failed, which, if it were truly random and I if understand probability well enough, has a 1 in 20 trillion chance of happening (0.8 ^ 100).\n\n```\nrandomInt positive numbers max: 5, min: 0 should include min and max (reason: Expected 2,1,1,3,2,2,1,4,4,5,4,2,4,5,3,2,3,2,3,1,5,1,2,4,3,1,4,3,3,2,1,1,1,4,2,3,5,4,3,2,1,1,1,1,2,3,1,1,3,3,2,3,3,5,1,3,3,2,4,2,2,2,4,4,1,4,1,1,5,4,5,3,3,2,3,4,3,1,3,2,2,4,2,4,2,2,3,5,3,2,4,3,4,1,1,2,5,2,5,4 to include 0.\n```\n"}
{"id": 300, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test checks if loading an app in the background will catch and re-raise an exception. It occasionally fails on pypy3. Possibly, the background loading thread does not finish before the assert line, and so there is no exception yet. Try to address this by waiting for the lock before asserting. This could still fail if the thread starts *very* slowly.\r\n\r\ncloses #4292 \n"}
{"id": 393, "ground_truth": "OS", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\n`test-blob-slice-with-large-size`\n\n### Platform\n\nSmartOS\n\n### Console output\n\n```console\n---\nduration_ms: 290.507\nexitcode: 1\nseverity: fail\nstack: |-\n  /home/iojs/build/workspace/node-test-commit-smartos/nodes/smartos23-x64/test/pummel/test-blob-slice-with-large-size.js:18\n      throw e;\n      ^\n\n  RangeError: Array buffer allocation failed\n      at new ArrayBuffer (<anonymous>)\n      at new Uint8Array (<anonymous>)\n      at new FastBuffer (node:internal/buffer:961:5)\n      at createUnsafeBuffer (node:internal/buffer:1097:12)\n      at allocate (node:buffer:445:10)\n      at Function.allocUnsafe (node:buffer:410:10)\n      at Object.<anonymous> (/home/iojs/build/workspace/node-test-commit-smartos/nodes/smartos23-x64/test/pummel/test-blob-slice-with-large-size.js:12:22)\n      at Module._compile (node:internal/modules/cjs/loader:1734:14)\n      at Object..js (node:internal/modules/cjs/loader:1899:10)\n      at Module.load (node:internal/modules/cjs/loader:1469:32)\n\n  Node.js v24.0.0-pre\n```\n\n### Build links\n\n- https://ci.nodejs.org/job/node-test-commit-smartos/59405/nodes=smartos23-x64/testReport/junit/(root)/pummel/test_blob_slice_with_large_size/\n\n### Additional information\n\nIt seems to reproduce quite rarely\n"}
{"id": 287, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis enables golangci-lint via build tags for integration tests (this\r\nshould have been done long ago!), and fixes the linting errors.\r\n\r\nTwo tests were updated to reduce flakiness:\r\n\r\n* apply config: wait for nodes to issue \"boot done\" sequence event\r\nbefore proceeding\r\n* recover: kill pods even if they appear after the initial set gets\r\nkilled (potential race condition with previous test).\r\n\r\nSigned-off-by: Andrey Smirnov <smirnov.andrey@gmail.com>\r\n\r\n\r\n\n"}
{"id": 314, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis PR moves the topic creation before consumer creations in\n`PlaintextAdminIntegrationTest.testListGroups`, to avoid potential\nerrors if consumer creates topic due to metadata update.\n\nSee discussion\nhttps://github.com/apache/kafka/pull/20244#discussion_r2325557949\n\nReviewers: @chia7712, bbejeck@apache.org\n\n"}
{"id": 298, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe PR at https://github.com/elastic/elasticsearch/pull/114334 disallows functions from returning TEXT, and this affected error messages also. This Analyzer test was flaky because it randomly selected from a list of type combinations, only occasionally hitting the case where it asserted incorrectly on a TEXT error message.\r\n\r\nFixes #115636\r\n\n"}
{"id": 102, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Licensed to the Apache Software Foundation (ASF) under one or more -->\r\n<!-- contributor license agreements.  See the NOTICE file distributed with -->\r\n<!-- this work for additional information regarding copyright ownership. -->\r\n<!-- The ASF licenses this file to You under the Apache License, Version 2.0 -->\r\n<!-- (the \"License\"); you may not use this file except in compliance with -->\r\n<!-- the License.  You may obtain a copy of the License at -->\r\n<!--     http://www.apache.org/licenses/LICENSE-2.0 -->\r\n<!-- Unless required by applicable law or agreed to in writing, software -->\r\n<!-- distributed under the License is distributed on an \"AS IS\" BASIS, -->\r\n<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->\r\n<!-- See the License for the specific language governing permissions and -->\r\n<!-- limitations under the License. -->\r\n\r\n# Summary\r\n\r\nTests in the testSchemaWithCoreAttribuesToAttribute(), testSchemaToContent(), testSchemaWithCoreAttribuesToContent() and testSchemaToAttribute() under TestAttributesToCSV express non-deterministic behavior and change the order of the attributes. The fix is changing HashMap to LinkedHashMap in the test and Set to LinkedHashSet to ensure deterministic behavior.\r\n\r\n[NIFI-10928](https://issues.apache.org/jira/browse/NIFI-10928)\r\n\r\n# Tracking\r\n\r\nPlease complete the following tracking steps prior to pull request creation.\r\n\r\n### Issue Tracking\r\n\r\n- [x] [Apache NiFi Jira](https://issues.apache.org/jira/browse/NIFI) issue created\r\n\r\n### Pull Request Tracking\r\n\r\n- [x] Pull Request title starts with Apache NiFi Jira issue number, such as `NIFI-00000`\r\n- [x] Pull Request commit message starts with Apache NiFi Jira issue number, as such `NIFI-00000`\r\n\r\n### Pull Request Formatting\r\n\r\n- [x] Pull Request based on current revision of the `main` branch\r\n- [x] Pull Request refers to a feature branch with one commit containing changes\r\n\r\n# Verification\r\n\r\nPlease indicate the verification steps performed prior to pull request creation.\r\n\r\n### Build\r\n\r\n- [ ] Build completed using `mvn clean install -P contrib-check`\r\n  - [ ] JDK 8\r\n  - [x] JDK 11\r\n  - [ ] JDK 17\r\n\r\n### Licensing\r\n\r\n- [ ] New dependencies are compatible with the [Apache License 2.0](https://apache.org/licenses/LICENSE-2.0) according to the [License Policy](https://www.apache.org/legal/resolved.html)\r\n- [ ] New dependencies are documented in applicable `LICENSE` and `NOTICE` files\r\n\r\n### Documentation\r\n\r\n- [ ] Documentation formatting appears as expected in rendered files\r\n\n"}
{"id": 262, "ground_truth": "Test order dependency", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis PR sets up the sidebar and sidebar links\r\n\r\n<img width=\"1576\" alt=\"Screenshot 2024-08-12 at 15 08 01\" src=\"https://github.com/user-attachments/assets/4f732c78-c199-4ce8-b4a6-37b33e8287e9\">\r\n\n"}
{"id": 244, "ground_truth": "Floating point operations", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nAfter https://github.com/python-pillow/Pillow/actions/runs/4256800436/jobs/7406140055, this is a further PR for #6875\n"}
{"id": 138, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Problem\nThe\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutTopic()`,\n`ShareGroupCommandTest.testDeleteShareGroupOffsetsArgsWithoutGroup()`,\n`ResetStreamsGroupOffsetTest.testResetOffsetsWithoutGroupOption()`,\n`DeleteStreamsGroupTest.testDeleteWithoutGroupOption()`,\n`DescribeStreamsGroupTest.testDescribeWithoutGroupOption()` tests were\nflaky due to a dependency on Set iteration order in error message\ngeneration.\n\n### Root Cause\nThe cleanup [commit](https://github.com/apache/kafka/pull/20091) that\nreplaced `new HashSet<>(Arrays.asList(...))` with `Set.of(...)` in\nShareGroupCommandOptions and StreamsGroupCommandOptions changed the\niteration characteristics of collections used for error message\ngeneration:\n\nThis produces different orders like `[topic], [group]` vs `[group],\n[topic]`, but the tests expected a specific order, causing intermittent\nfailures.\n\n### Solution\nFix the root cause by ensuring deterministic error message generation\nthrough alphabetical sorting of option names.\n\nReviewers: ShivsundarR <shr@confluent.io>, Ken Huang\n <s7133700@gmail.com>, TengYao Chi <frankvicky@apache.org>\n\n"}
{"id": 55, "ground_truth": "Randomness", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nUsed the `render` shortcut in the \"using a form in a view\" example to ensure that the CSRF is properly enabled.\n\n"}
{"id": 381, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test fails regularly so trying to make it a bit more robust.\n"}
{"id": 422, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Problem Description**\nIn the `RemoteIndexCache.cleanup()` method, the asynchronous invocation\nof `index.deleteIfExists()` may cause a conflict. When the\n`getIndexFileFromRemoteCacheDir()` method is executed, it utilizes\n`Files.walk()` to traverse all files in the directory path. If\n`index.deleteIfExists()` is triggered during this traversal, a\n`NoSuchFileException` will be thrown.\n\n**Solution**\nTo resolve this issue, ensure that `index.deleteIfExists()` has been\nfully executed before invoking `getIndexFileFromRemoteCacheDir()`.\n\n"}
{"id": 111, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nMotivation:\r\n\r\nI thought the previous modification #6354 would fix the flakiness, but it turned out it didn't.\r\n\r\nThe first attempt may not be cancelled by `res.cancel(true)` because of the race between an event loop and the main thread. https://github.com/line/armeria/blob/a0e2225cb7aac6b229b86e25e9b2f34633a9f45b/thrift/thrift0.13/src/test/java/com/linecorp/armeria/it/client/retry/RetryingRpcClientTest.java#L354-L356 Therefore, I propose removing the flaky assertions and adding a new clear assertion that verifies retry occurs only once.\r\n\r\nModifications:\r\n\r\n- Remove flaky assertions in `RetryingRpcClientTest.doNotRetryWhenResponseIsCancelled()`\r\n\r\nResult:\r\n\r\nMake CI stable\r\n\r\n\n"}
{"id": 429, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis PR tries to fix flakiness of `specs::coverage::data_url`.\r\n\r\n`foo` is async function, but is not awaited\r\n\r\ncloses #29759 \n"}
{"id": 200, "ground_truth": "Async Wait", "predicted": "Randomness", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n$1K per flakey test\n"}
{"id": 233, "ground_truth": "Test order dependency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #159443\r\n\r\nunder dynamo, the libraries couldn't properly be cleared unless we manually did `gc.collect()`, but that's slow. it also worked if we just used the _destroy() method to tear down \r\n\r\nFIXES \r\n#159398\r\n#159349\r\n#159254\r\n#159237\r\n#159153\r\n#159114\r\n#159040\r\n#158910\r\n#158841\r\n#158763\r\n#158735\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela\n"}
{"id": 391, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Test\n\nclient-proxy/test-https-proxy-request-handshake-failure\n\n### Platform\n\nLinux ARM64\n\n### Console output\n\n```console\nnot ok 27 client-proxy/test-https-proxy-request-handshake-failure\n  ---\n  duration_ms: 10890.37000\n  severity: fail\n  exitcode: 1\n  stack: |-\n    [CLOSE] 0 null\n    [STDERR] Request timed out\n    Request Error Error [ERR_PROXY_TUNNEL]: Connection to establish proxy tunnel timed out after 1000ms\n        at Socket.onTimeout (node:https:373:19)\n        at Object.onceWrapper (node:events:621:28)\n        at Socket.emit (node:events:519:35)\n        at Socket._onTimeout (node:net:604:8)\n        at listOnTimeout (node:internal/timers:608:17)\n        at process.processTimers (node:internal/timers:543:7) {\n      code: 'ERR_PROXY_TUNNEL',\n      proxyTunnelTimeout: 1000\n    }\n    \n    node:internal/modules/run_main:107\n        triggerUncaughtException(\n        ^\n    \n    AssertionError [ERR_ASSERTION]: The input did not match the regular expression /UNABLE_TO_VERIFY_LEAF_SIGNATURE/. Input:\n    \n    'Request timed out\\n' +\n      'Request Error Error [ERR_PROXY_TUNNEL]: Connection to establish proxy tunnel timed out after\n```\n\n### Build links\n\nhttps://github.com/nodejs/reliability/blob/main/reports/2025-07-22.md\n\n### Additional information\n\nIt's likely caused by the machine being too slow and they cannot finish the request in 1 second - the timeout is unnecessary in the test anyway. Fix in https://github.com/nodejs/node/pull/59165\n"}
{"id": 252, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nMy theory is that since 0ebea6e5c07485a36862e9b6e2be18d1694ad2c5, the saving of objects has become slightly slower\r\nThis caused some failures in the selenium test suite: https://github.com/django/django/actions/runs/13826117388/job/38698352291\r\n\n"}
{"id": 242, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nSummary: The test has been [flaky](https://github.com/facebook/rocksdb/actions/runs/12220443012/job/34088263578?fbclid=IwZXh0bgNhZW0CMTEAAR3iDUK20Z4kdFkYZOT_PgQMYuj3Ebmpf4O-OOLLyeFQs4HAb8pRTWpFnUo_aem_09A_yiv7cwoD5lKjxFKimA). The cause for flakiness is that background threads may not be immediately available after calling env_->SetBackgroundThreads() while the test expects all background threads to be available for compaction. There's no way to get the number of available threads and I don't want to update threadpool implementation just for this test. So I added a fix to wait until background threads being available that relies on sync point. \r\n\r\n\r\nTest plan: monitor future test failure\r\n\n"}
{"id": 307, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nA test failed on a tracked branch\n\n```\nStaleElementReferenceError: stale element reference: stale element not found in the current frame\n  (Session info: chrome=123.0.6312.105)\n    at Object.throwDecodedError (node_modules/selenium-webdriver/lib/error.js:521:15)\n    at parseHttpResponse (node_modules/selenium-webdriver/lib/http.js:510:13)\n    at Executor.execute (node_modules/selenium-webdriver/lib/http.js:443:28)\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at Task.exec (prevent_parallel_calls.ts:28:20) {\n  remoteStacktrace: '#0 0x562f61afa873 <unknown>\\n' +\n    '#1 0x562f617f08c6 <unknown>\\n' +\n    '#2 0x562f617f5e75 <unknown>\\n' +\n    '#3 0x562f617f7bce <unknown>\\n' +\n    '#4 0x562f617f7c5c <unknown>\\n' +\n    '#5 0x562f61835ae2 <unknown>\\n' +\n    '#6 0x562f6185d5a2 <unknown>\\n' +\n    '#7 0x562f6182fe98 <unknown>\\n' +\n    '#8 0x562f6185d76e <unknown>\\n' +\n    '#9 0x562f6187bc19 <unknown>\\n' +\n    '#10 0x562f6185d343 <unknown>\\n' +\n    '#11 0x562f6182e593 <unknown>\\n' +\n    '#12 0x562f6182ef5e <unknown>\\n' +\n    '#13 0x562f61abe85b <unknown>\\n' +\n    '#14 0x562f61ac27b5 <unknown>\\n' +\n    '#15 0x562f61aac581 <unknown>\\n' +\n    '#16 0x562f61ac3342 <unknown>\\n' +\n    '#17 0x562f61a9188f <unknown>\\n' +\n    '#18 0x562f61ae9738 <unknown>\\n' +\n    '#19 0x562f61ae990b <unknown>\\n' +\n    '#20 0x562f61af99c4 <unknown>\\n' +\n    '#21 0x7fbdefe8b609 start_thread\\n'\n}\n```\n\nFirst failure: [CI Build - main](https://buildkite.com/elastic/kibana-on-merge/builds/43375#018eb290-51a2-49ec-a9c7-a34017c17388)\n\n<!-- kibanaCiData = {\"failed-test\":{\"test.class\":\"Chrome X-Pack UI Functional Tests.x-pack/test/functional/apps/lens/group4/show_underlying_data_dashboard¬∑ts\",\"test.name\":\"lens app - group 4 lens show underlying data from dashboard should bring both dashboard context and visualization context to discover\",\"test.failCount\":2}} -->\n"}
{"id": 162, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Description\r\nThere are 2 tests in the `DocumentTest.java` class which call the `DocumentTest.toString` method, this method converts a `Map` object to a string, which is not in any particular order, the tests may pass or fail even though the code under test may be working as expected.\r\nhttps://github.com/219sansim/jedis/blob/c1cc657e8b3a984aab8779f84aa0ff55c2b29118/src/main/java/redis/clients/jedis/search/Document.java#L23\r\nTo resolve the bug, I changed the code to check for both possible orderings. \r\n\r\nAnother possible fix could be to check if the string contains both the fields.\r\n\r\n## Reproduction of error\r\nRun maven tests with [NonDex](https://github.com/TestingResearchIllinois/NonDex) tool\r\n```\r\nmvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=redis.clients.jedis.modules.search.DocumentTest#toStringTest\r\nmvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=redis.clients.jedis.modules.search.DocumentTest#serialize\r\n```\r\n**Error Output**\r\nShowing error output of test `serialize`\r\n```\r\n[ERROR] Failures: \r\n[ERROR]   DocumentTest.serialize:45 expected:<...: 10.0, properties:[[string=c, float=12.0]]> but was:<...: 10.0, properties:[[float=12.0, string=c]]>\r\n```\r\nAfter making the changes, the tests pass with [NonDex](https://github.com/TestingResearchIllinois/NonDex) tool \r\n\n"}
{"id": 124, "ground_truth": "Async Wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nTerminal Extension pty terminals should respect dimension overrides\r\n\r\nFixes #90064\r\n\n"}
{"id": 255, "ground_truth": "OS", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n- [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).\r\n\r\nExample: https://app.circleci.com/pipelines/github/pandas-dev/pandas/8766/workflows/4a9ce4f4-0e1c-49aa-b035-ffcb6334be45/jobs/39610\r\n\n"}
{"id": 201, "ground_truth": "Network", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Description\r\nThis is a PR for SEISMIC. I found there would be flakey tests within previous `warmUp` and `clearCache` related ITs. I fixed them in this PR. In addition, I refactored some codes to be cleaner.\r\n\r\n### Related Issues\r\nPrevious IT: #1559 \r\n\r\n### Check List\r\n- [ ] New functionality includes testing.\r\n- [ ] New functionality has been documented.\r\n- [ ] API changes companion pull request [created](https://github.com/opensearch-project/opensearch-api-specification/blob/main/DEVELOPER_GUIDE.md).\r\n- [x] Commits are signed per the DCO using `--signoff`.\r\n- [x] Public documentation issue/PR [created](https://github.com/opensearch-project/documentation-website/issues/new/choose).\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/neural-search/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n\n"}
{"id": 54, "ground_truth": "Logic", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nResolves a couple of issues with running Flatpages tests by...\n- Creating an example_site fixture\n- Overriding project SITE_ID setting to 1\n- Normalizing the use of the hardcoded (1) site_id to settings.SITE_ID\n\n"}
{"id": 133, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### What changes were proposed in this pull request?\r\nThis PR aims to fix flaky tests in `SparkConnectServiceSuite` which are caused by `executorHolder` [undefined](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L908).\r\nThe conditions to reproduce this issue are:\r\n\r\n(1) The operation finishes before its `executeHolder` is set in [MockSparkListener#onOtherEvent](https://github.com/apache/spark/blob/ab9a63626018156b3e0f267f14409c30031692b7/sql/connect/server/src/test/scala/org/apache/spark/sql/connect/planner/SparkConnectServiceSuite.scala#L961).\r\n(2) `executeHolder` is accessed through calling `verifyEvents.onComplete` after the operation finishes. \r\n\r\n`SparkListenerConnectOperationStarted` is posted asynchronously with the corresponding operation so the condition (1) can be met. After an operation finishes, `executeHolder` is [removed from a map](https://github.com/apache/spark/blob/af16aa8e11c223642f928b0b9893854a851d70bb/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectExecutionManager.scala#L153) so if the condition (1) is met, `executeHolder` is never set because `SparkConnectService.executionManager.getExecuteHolder` consistently returns `None`.\r\n\r\nOne example of the test affected by this issue is `SPARK-43923: commands send events - get_resources_command`.\r\nYou can easily reproduce this issue by inserting sleep into `MockSparkListener#onOtherEvent` like as follows.\r\n\r\n```\r\n   val executeKey =\r\n     ExecuteKey(sessionHolder.userId, sessionHolder.sessionId, e.operationId)\r\n+  Thread.sleep(1000)\r\n   executeHolder = SparkConnectService.executionManager.getExecuteHolder(executeKey)\r\n```\r\n\r\nAnd then, run test.\r\n```\r\n$ build/sbt 'connect/testOnly org.apache.spark.sql.connect.planner.SparkConnectServiceSuite -- -z \"get_resources_command\"'\r\n```\r\nTo resolve this issue, this PR proposes:\r\n\r\n* Change `VerifyEvents#onCompleted` just to assert `executeHolder.eventsManager.getProducedRowCount == producedRowCount`\r\n* Call `VerifyEvents#onCompleted` from `StreamObserver#onCompleted`\r\n* Add `VerifyEvents#assertClosed` to check if the status is `Closed`\r\n\r\n### Why are the changes needed?\r\nFor test stability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nInserting `Thread.sleep(1000)` like mentioned above and then run `SparkConnectServiceSuite`.\r\n\r\n### Was this patch authored or co-authored using generative AI tooling?\r\nNo.\n"}
{"id": 305, "ground_truth": "Time", "predicted": "Floating point operations", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFurther lower threshold for F64 in //xla/service/gpu/model:hlo_op_profiler_test\n\nThis was originally proposed in https://github.com/openxla/xla/pull/16102, but I still ran into issue where it failed by slight margin:\n\n```\nExpected: (profiler.MeasureClockCyclesPerOp(HloOpcode::kDivide, F64) .value() .clock_cycles()) > (300), actual: 296 vs 300\n```\n\nThat said, I ran 1000 tests and did not encounter this issue. Reducing the threshold to 280 since the bound seems very close and flaky test is no good either way.\n\n"}
{"id": 245, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Feature or Bugfix\r\n- Workaround\r\n\r\n### Purpose\r\n- Reduce distration due to failed continuous integration pipelines.\r\n\r\n### Detail\r\n- Increase the timeout duration in `linkcheck`-related test roots by a factor of 5x, to a quarter-second.\r\n\r\n### Relates\r\n- May resolve #12159.\n"}
{"id": 338, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis PR implements the Fullscreen API. \r\n\r\nThe final commit also adds some Qt UI (an \"exit fullscreen\" button that animates from the top down, sort of like how Chrome does it, and also Firefox). For Qt-backends, the escape key also exits out of fullscreen, fully.\r\n\r\nThe spec can be found [here](https://fullscreen.spec.whatwg.org/).\r\n\r\nNew web platform test results (fullscreen/api) with this patch series applied:\r\n\r\n```\r\nRan 57 tests finished in 13.7 seconds.\r\n  ‚Ä¢ 36 ran as expected. 0 tests skipped.\r\n  ‚Ä¢ 1 tests had errors unexpectedly\r\n  ‚Ä¢ 1 tests timed out unexpectedly\r\n  ‚Ä¢ 21 tests had unexpected subtest results\r\n```\r\n(up from 0 expected to 36. Another test succeeds when #4329 is applied)\r\n\r\nAdditional work that needs to happen:\r\n- https://fullscreen.spec.whatwg.org/#dom-document-fullscreenenabled needs to be implemented fully, where a document's \"allowed to use\" can be deterimined by the `allowFullscreen` attribute set on an iframe. This attribute can not be changed dynamically and once set for a document, is set for the rest of it's life time.\r\n\r\nAdditional work that's required can be determined by the WPT suite.\n"}
{"id": 128, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Enter a brief description/summary of your PR here. What does it fix/what does it change/how was it tested (even manually, if necessary)? -->\r\n## Summary of the Pull Request\r\n\r\nSometimes the Interop Tests hung in CI. This seems to be caused by the sending of a message before the OS has set up the pipes correctly. We add a little wait to the test to reduce flakiness.\r\n\r\n<!-- Describe how you validated the behavior. Add automated tests wherever possible, but list manual validation steps taken as well -->\r\n## Validation Steps Performed\r\n![image](https://github.com/microsoft/PowerToys/assets/26118718/fa326de5-89fa-411a-b678-e36878c6cbb6)\r\nRan the \"TestSend\" with \"Run Until Failure\" and it ran 1000 times without hanging.\r\n\n"}
{"id": 246, "ground_truth": "Concurrency", "predicted": "Async wait", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nAs a follow-up to HDFS-16935, we should provide utility to trigger heartbeat and wait until BP thread queue is fully processed. This would ensure 100% consistency w.r.t active namenode being able to receive bad block reports from the given datanode. This utility would resolve flakes for the tests that rely on namenode's awareness of the reported bad blocks by datanodes.\n"}
{"id": 376, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nBuild: https://dev.azure.com/monacotools/a6d41577-0fa3-498e-af22-257312ff0545/_build/results?buildId=317618\nChanges: https://github.com/Microsoft/vscode/compare/a58d48f...30dbfbb\n\nThe RandomInt, \"should include min and max\" unit test failed, which, if it were truly random and I if understand probability well enough, has a 1 in 20 trillion chance of happening (0.8 ^ 100).\n\n```\nrandomInt positive numbers max: 5, min: 0 should include min and max (reason: Expected 2,1,1,3,2,2,1,4,4,5,4,2,4,5,3,2,3,2,3,1,5,1,2,4,3,1,4,3,3,2,1,1,1,4,2,3,5,4,3,2,1,1,1,1,2,3,1,1,3,3,2,3,3,5,1,3,3,2,4,2,2,2,4,4,1,4,1,1,5,4,5,3,3,2,3,4,3,1,3,2,2,4,2,4,2,2,3,5,3,2,4,3,4,1,1,2,5,2,5,4 to include 0.\n```\n"}
{"id": 40, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n\n| Reason | <code>client-proxy/test-https-proxy-request-invalid-char-in-url</code> |\n| - | :- |\n| Type | JS_TEST_FAILURE |\n| Failed PR | 3 ([https://github.com/nodejs/node/pull/59587/](https://ci.nodejs.org/job/node-test-pull-request/68946/), [https://github.com/nodejs/node/pull/59734/](https://ci.nodejs.org/job/node-test-pull-request/69013/), [https://github.com/nodejs/node/pull/59717/](https://ci.nodejs.org/job/node-test-pull-request/69014/)) |\n| Appeared | [vm-efv4j](https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66595/console), [vm-a49f1](https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66594/console), [vm-yy37e](https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66523/console) |\n| First CI | https://ci.nodejs.org/job/node-test-pull-request/68946/ |\n| Last CI | https://ci.nodejs.org/job/node-test-pull-request/69014/ |\n\n<details>\n<summary><a href=\"https://ci.nodejs.org/job/node-test-commit-osx/nodes=osx13-x64/66595/console\">Example</a></summary>\n\n```\nnot ok 4333 client-proxy/test-https-proxy-request-invalid-char-in-url\n  ---\n  duration_ms: 413.52500\n  severity: fail\n  exitcode: 1\n  stack: |-\n    #6 eneded response for: 'https://local\\rhost:55386/carriage-return-in-host'\n    #5 eneded response for: 'https://local\\r\\nhost:55386/crlf-in-host'\n    #4 eneded response for: 'https://local\\nhost:55386/newline-in-host'\n    #3 eneded response for: 'https://localhost:5\\r5386/carriage-return-in-port'\n    #2 eneded response for: 'https://localhost:5\\n5386/newline-in-port'\n    #1 eneded response for: 'https://localhost:5\\r\\n5386/crlf-in-port'\n    node:assert:147\n      throw new AssertionError(obj);\n      ^\n    \n    AssertionError [ERR_ASSERTION]: Expected values to be strictly deep-equal:\n    + actual - expected\n    \n    + Set(8) {\n    - Set(6) {\n        {\n    +     error: Error [ERR_STREAM_WRITE_AFTER_END]: write after end\n    +         at _write (node:internal/streams/writable:487:11)\n    +         at Writable.write (node:internal/streams/writable:508:10)\n    +      ...\n```\n</details>\n\nFrom https://github.com/nodejs/reliability/blob/main/reports/2025-09-03.md\n"}
{"id": 369, "ground_truth": "Concurrency", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Please make sure the target branch is right. In most case, the target branch should be `develop`. -->\r\n\r\n### Which Issue(s) This PR Fixes\r\n\r\n<!-- Please ensure that the related issue has already been created, and [link this pull request to that issue using keywords](<https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword>) to ensure automatic closure. -->\r\n\r\nFixes #7431\r\n\r\n### Brief Description\r\n\r\nFix flaky test of DLedgerControllerTest#testBrokerLifecycleListener\r\n\r\n### How Did You Test This Change?\r\n\r\n<!-- In order to ensure the code quality of Apache RocketMQ, we expect every pull request to have undergone thorough testing. -->\r\n\n"}
{"id": 416, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Description**\r\n\r\nCloses #1142 \n"}
{"id": 301, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## PR Summary\r\n\r\nSince parametrizing the test allows it to run in parallel, this makes it flaky, as one process can overwrite the test result image of another.\r\n\r\nOur standard way for dealing with tests that use the same baseline image is to pass duplicate filenames to `image_comparison`, because that is serialized.\r\n\r\nThis is basically the same as #16656 for `test_stem`.\r\n\r\n## PR Checklist\r\n\r\n- [x] Has Pytest style unit tests\r\n- [x] Code is [Flake 8](http://flake8.pycqa.org/en/latest/) compliant\r\n- [N/A] New features are documented, with examples if plot related\r\n- [N/A] Documentation is sphinx and numpydoc compliant\r\n- [N/A] Added an entry to doc/users/next_whats_new/ if major new feature (follow instructions in README.rst there)\r\n- [N/A] Documented in doc/api/api_changes.rst if API changed in a backward-incompatible way\n"}
{"id": 115, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nIn conn_test.go, \"TestStartupTimeout\" at the very last step requires the log to contain \"no response to connection startup within timeout\".\r\n\r\nHowever, during my test, it fails sometimes, not always, with log containing \"no response received from cassandra within timeout period\" instead.\r\n\r\nFixing the flaky test by allowing log to contain either message.\n"}
{"id": 285, "ground_truth": "Unordered collections", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nPRs backported:\r\n\r\n- #9909 \r\n- #9918\r\n- #9919 \r\n- #9920\r\n- #9922 \r\n- #9937 \r\n- #9938 \r\n- #9939 \r\n- #9940 \r\n- #9941 \r\n- #9948 \r\n- #9950 \r\n- #9953 \r\n- #9954 \r\n- #9961 \r\n- #9962 \r\n- #9963 \r\n- #9966 \r\n- #9967 \r\n- #9969\r\n- #9971\r\n- #9972 \r\n- #9934 \r\n- #9968 \r\n- use extension ref `release-1.9`\n"}
{"id": 303, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe tests that style.use() and imread() support urls can just use local\r\nurls (`file://`); this avoids failing the tests when downloading the\r\ngist fails due to a flaky connection.\r\n\r\nSince RFC8089 file: urls don't necessarily start with two slashes\r\nanymore (https://tools.ietf.org/html/rfc8089#appendix-A).\r\n\r\n(If we really want to test https we could use http.server to spin up a local server but that seems overkill.)\r\n\r\n\r\n## PR Summary\r\n\r\n## PR Checklist\r\n\r\n- [ ] Has Pytest style unit tests\r\n- [ ] Code is [Flake 8](http://flake8.pycqa.org/en/latest/) compliant\r\n- [ ] New features are documented, with examples if plot related\r\n- [ ] Documentation is sphinx and numpydoc compliant\r\n- [ ] Added an entry to doc/users/next_whats_new/ if major new feature (follow instructions in README.rst there)\r\n- [ ] Documented in doc/api/api_changes.rst if API changed in a backward-incompatible way\r\n\r\n<!--\r\nThank you so much for your PR!  To help us review your contribution, please\r\nconsider the following points:\r\n\r\n- A development guide is available at https://matplotlib.org/devdocs/devel/index.html.\r\n\r\n- Help with git and github is available at\r\n  https://matplotlib.org/devel/gitwash/development_workflow.html.\r\n\r\n- Do not create the PR out of master, but out of a separate branch.\r\n\r\n- The PR title should summarize the changes, for example \"Raise ValueError on\r\n  non-numeric input to set_xlim\".  Avoid non-descriptive titles such as\r\n  \"Addresses issue #8576\".\r\n\r\n- The summary should provide at least 1-2 sentences describing the pull request\r\n  in detail (Why is this change required?  What problem does it solve?) and\r\n  link to any relevant issues.\r\n\r\n- If you are contributing fixes to docstrings, please pay attention to\r\n  http://matplotlib.org/devel/documenting_mpl.html#formatting.  In particular,\r\n  note the difference between using single backquotes, double backquotes, and\r\n  asterisks in the markup.\r\n\r\nWe understand that PRs can sometimes be overwhelming, especially as the\r\nreviews start coming in.  Please let us know if the reviews are unclear or\r\nthe recommended next step seems overly demanding, if you would like help in\r\naddressing a reviewer's comments, or if you have been waiting too long to hear\r\nback on your PR.\r\n-->\r\n\n"}
{"id": 280, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Failure cluster [46c1b1ee59dffad82fad](https://storage.googleapis.com/k8s-triage/index.html?pr=1#46c1b1ee59dffad82fad)\r\n\r\n##### Error text:\r\n```\r\n=== RUN   TestFrameworkHandler_IterateOverWaitingPods/pods_with_different_profiles_are_waiting_on_permit_stage\r\n    framework.go:381: I0229 10:48:13.714983] the scheduler starts to work with those plugins Plugins={\"PreEnqueue\":{\"Enabled\":null,\"Disabled\":null},\"QueueSort\":{\"Enabled\":[{\"Name\":\"PrioritySort\",\"Weight\":0}],\"Disabled\":null},\"PreFilter\":{\"Enabled\":null,\"Disabled\":null},\"Filter\":{\"Enabled\":null,\"Disabled\":null},\"PostFilter\":{\"Enabled\":null,\"Disabled\":null},\"PreScore\":{\"Enabled\":null,\"Disabled\":null},\"Score\":{\"Enabled\":null,\"Disabled\":null},\"Reserve\":{\"Enabled\":null,\"Disabled\":null},\"Permit\":{\"Enabled\":[{\"Name\":\"fakePermit\",\"Weight\":0}],\"Disabled\":null},\"PreBind\":{\"Enabled\":null,\"Disabled\":null},\"Bind\":{\"Enabled\":[{\"Name\":\"DefaultBinder\",\"Weight\":0}],\"Disabled\":null},\"PostBind\":{\"Enabled\":null,\"Disabled\":null},\"MultiPoint\":{\"Enabled\":null,\"Disabled\":null}}\r\n    framework.go:381: I0229 10:48:13.715309] the scheduler starts to work with those plugins Plugins={\"PreEnqueue\":{\"Enabled\":null,\"Disabled\":null},\"QueueSort\":{\"Enabled\":[{\"Name\":\"PrioritySort\",\"Weight\":0}],\"Disabled\":null},\"PreFilter\":{\"Enabled\":null,\"Disabled\":null},\"Filter\":{\"Enabled\":null,\"Disabled\":null},\"PostFilter\":{\"Enabled\":null,\"Disabled\":null},\"PreScore\":{\"Enabled\":null,\"Disabled\":null},\"Score\":{\"Enabled\":null,\"Disabled\":null},\"Reserve\":{\"Enabled\":null,\"Disabled\":null},\"Permit\":{\"Enabled\":[{\"Name\":\"fakePermit\",\"Weight\":0}],\"Disabled\":null},\"PreBind\":{\"Enabled\":null,\"Disabled\":null},\"Bind\":{\"Enabled\":[{\"Name\":\"DefaultBinder\",\"Weight\":0}],\"Disabled\":null},\"PostBind\":{\"Enabled\":null,\"Disabled\":null},\"MultiPoint\":{\"Enabled\":null,\"Disabled\":null}}\r\n    framework.go:381: I0229 10:48:13.715630] the scheduler starts to work with those plugins Plugins={\"PreEnqueue\":{\"Enabled\":null,\"Disabled\":null},\"QueueSort\":{\"Enabled\":[{\"Name\":\"PrioritySort\",\"Weight\":0}],\"Disabled\":null},\"PreFilter\":{\"Enabled\":null,\"Disabled\":null},\"Filter\":{\"Enabled\":null,\"Disabled\":null},\"PostFilter\":{\"Enabled\":null,\"Disabled\":null},\"PreScore\":{\"Enabled\":null,\"Disabled\":null},\"Score\":{\"Enabled\":null,\"Disabled\":null},\"Reserve\":{\"Enabled\":null,\"Disabled\":null},\"Permit\":{\"Enabled\":[{\"Name\":\"fakePermit\",\"Weight\":0}],\"Disabled\":null},\"PreBind\":{\"Enabled\":null,\"Disabled\":null},\"Bind\":{\"Enabled\":[{\"Name\":\"DefaultBinder\",\"Weight\":0}],\"Disabled\":null},\"PostBind\":{\"Enabled\":null,\"Disabled\":null},\"MultiPoint\":{\"Enabled\":null,\"Disabled\":null}}\r\n    eventhandlers.go:75: I0229 10:48:13.717736] Add event for node node=\"node1\"\r\n    node_tree.go:65: I0229 10:48:13.717988] Added node in listed group to NodeTree node=\"node1\" zone=\"\"\r\n    eventhandlers.go:75: I0229 10:48:13.718196] Add event for node node=\"node2\"\r\n    node_tree.go:65: I0229 10:48:13.718385] Added node in listed group to NodeTree node=\"node2\" zone=\"\"\r\n    eventhandlers.go:75: I0229 10:48:13.718588] Add event for node node=\"node3\"\r\n    node_tree.go:65: I0229 10:48:13.718793] Added node in listed group to NodeTree node=\"node3\" zone=\"\"\r\n    eventhandlers.go:128: I0229 10:48:13.718980] Add event for unscheduled pod pod=\"pod1\"\r\n    scheduling_queue.go:595: I0229 10:48:13.719138] Pod moved to an internal scheduling queue pod=\"pod1\" event=\"PodAdd\" queue=\"Active\"\r\n    schedule_one.go:84: I0229 10:48:13.719250] About to try and schedule pod pod=\"pod1\"\r\n    schedule_one.go:97: I0229 10:48:13.719321] Attempting to schedule pod pod=\"pod1\"\r\n    framework.go:1475: I0229 10:48:13.719623] Permit: One or more plugins asked to wait and no plugin rejected pod node=\"node1\" pod=\"pod1\"\r\n    framework.go:1500: I0229 10:48:13.719889] Pod waiting on permit pod=\"pod1\"\r\n    eventhandlers.go:128: I0229 10:48:13.720049] Add event for unscheduled pod pod=\"pod2\"\r\n    scheduling_queue.go:595: I0229 10:48:13.720184] Pod moved to an internal scheduling queue pod=\"pod2\" event=\"PodAdd\" queue=\"Active\"\r\n    schedule_one.go:84: I0229 10:48:13.720358] About to try and schedule pod pod=\"pod2\"\r\n    schedule_one.go:97: I0229 10:48:13.720451] Attempting to schedule pod pod=\"pod2\"\r\n    framework.go:1475: I0229 10:48:13.720791] Permit: One or more plugins asked to wait and no plugin rejected pod node=\"node2\" pod=\"pod2\"\r\n    framework.go:1500: I0229 10:48:13.721010] Pod waiting on permit pod=\"pod2\"\r\n    eventhandlers.go:128: I0229 10:48:13.721439] Add event for unscheduled pod pod=\"pod3\"\r\n    scheduling_queue.go:595: I0229 10:48:13.721732] Pod moved to an internal scheduling queue pod=\"pod3\" event=\"PodAdd\" queue=\"Active\"\r\n    eventhandlers.go:128: I0229 10:48:13.721917] Add event for unscheduled pod pod=\"pod4\"\r\n    schedule_one.go:84: I0229 10:48:13.722078] About to try and schedule pod pod=\"pod3\"\r\n    schedule_one.go:97: I0229 10:48:13.722236] Attempting to schedule pod pod=\"pod3\"\r\n    scheduling_queue.go:595: I0229 10:48:13.722449] Pod moved to an internal scheduling queue pod=\"pod4\" event=\"PodAdd\" queue=\"Active\"\r\n    framework.go:1475: I0229 10:48:13.722729] Permit: One or more plugins asked to wait and no plugin rejected pod node=\"node3\" pod=\"pod3\"\r\n    framework.go:1500: I0229 10:48:13.723032] Pod waiting on permit pod=\"pod3\"\r\n    schedule_one.go:84: I0229 10:48:13.723270] About to try and schedule pod pod=\"pod4\"\r\n    schedule_one.go:97: I0229 10:48:13.723413] Attempting to schedule pod pod=\"pod4\"\r\n    scheduler_test.go:1044: Unexpected waitingPods in scheduler profile test-scheduler-profile-1, expect: []string{\"pod1\", \"pod2\", \"pod3\", \"pod4\"}, got: []string{\"pod1\", \"pod2\", \"pod3\"}\r\n    --- FAIL: TestFrameworkHandler_IterateOverWaitingPods/pods_with_different_profiles_are_waiting_on_permit_stage (0.01s)\r\n```\r\n\r\n#### Recent failures:\r\n[3/1/2024, 7:14:16 AM pr:pull-kubernetes-unit](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/123614/pull-kubernetes-unit/1763538031354056704)\r\n[2/29/2024, 5:28:43 AM pr:pull-kubernetes-unit](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/123555/pull-kubernetes-unit/1763149087500144640)\r\n[2/28/2024, 12:17:55 PM pr:pull-kubernetes-unit](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/123562/pull-kubernetes-unit/1762889666685571072)\r\n\r\n\r\n/kind failing-test\r\n<!-- If this is a flake, please add: /kind flake -->\r\n/kind flake\r\n\r\n<!-- Please assign a SIG using: /sig SIG-NAME -->\r\n/sig scheduling\n"}
{"id": 425, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### What type of PR is this?\r\n/kind bug\r\n/kind flake\r\n\r\n#### What this PR does / why we need it:\r\nFixes flakiness in `[sig-network] Proxy version v1 should proxy through a service and a pod [Conformance]` caused by the changes in #129837.\r\n\r\nThe test is kind of badly written. (It's old.) It required that as soon as it became aware that the Service and its Endpoints/EndpointSlice had been updated, that the apiserver proxying codepath must _also_ be aware of them, and must _immediately_ be able to proxy connections to the service, and if not, then the test fails. Apparently it was flaky even before the changes. But #129837 made things much worse because [now the proxying codepath uses an informer and a lister rather than using a client directly](https://github.com/kubernetes/kubernetes/pull/129837#discussion_r2001237237), so it's much more possible for the apiserver proxy code to have not noticed the Service/EndpointSlice yet at the point when e2e.test has noticed it.\r\n\r\nAnyway, fix is just to poll for the apiserver to be ready before we start.\r\n\r\nOh, also, the conformance description of the test was _completely_ wrong. It looks like when the conformance comments were added, someone copied the description of the wrong test? So I fixed that too.\r\n\r\n(Also, update to use the new `e2eendpointslice.WaitForEndpointCount`.)\r\n\r\n#### Which issue(s) this PR is related to:\r\nFixes #133257\r\n\r\n#### Does this PR introduce a user-facing change?\r\n```release-note\r\nNONE\r\n```\r\n\r\n/sig network\r\n/priority important-soon\r\n/triage accepted\r\n/milestone v1.34\n"}
{"id": 427, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n### Failure cluster [fb94bb199721f2524f81](https://go.k8s.io/triage#fb94bb199721f2524f81)\r\n\r\n##### Error text:\r\n```\r\n[FAILED] deploying csi-hostpath driver: create ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"psp-csi-hostpath-role-ephemeral-9424\" already exists\r\nIn [It] at: test/e2e/storage/drivers/csi.go:280 @ 05/03/23 07:02:07.131\r\n\r\n```\r\n#### Recent failures:\r\n[5/10/2023, 11:15:07 PM ci-kubernetes-e2e-gce-cos-k8sstable1-default](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-cos-k8sstable1-default/1656407525098000384)\r\n[5/3/2023, 8:41:51 AM ci-kubernetes-e2e-gce-cos-k8sstable1-default](https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gce-cos-k8sstable1-default/1653651043134738432)\r\n\r\n\r\n/kind failing-test\r\n/kind flake\r\n/sig storage\n"}
{"id": 113, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Description**\r\nSteve pointed this out when he was running into flaky tests. Looking at videos of the failures, it looks like the previous dialog had not finished closing before proceeding, which threw off the tests. Fix has worked for me a few times.\r\n\r\n**Review Instructions**\r\nNot really reviewable; tests should pass.\r\n\r\n**Issue**\r\nNone\r\n\r\n**Security**\r\nIf there are any concerns that require extra attention from the security team, highlight them here.\r\n\r\nPlease make sure that you've checked the following before submitting your pull request. Thanks!\r\n\r\n- [x] Check that your code compiles by running `npm run build`\r\n- [x] Ensure that the PR targets the correct branch. Check the milestone or fix version of the ticket.\r\n- [x] If this is the first time you're submitting a PR or even if you just need a refresher, consider reviewing our [style guide](https://github.com/dockstore/dockstore/wiki/Dockstore-Frontend-Opinionated-Style-Guide#pr-checklist)\r\n- [x] Do not bypass Angular sanitization (bypassSecurityTrustHtml, etc.), or justify why you need to do so\r\n- [x] If displaying markdown, use the `markdown-wrapper` component, which does extra sanitization\r\n- [x] Do not use cookies, although this may change in the future\r\n- [x] Run `npm audit` and ensure you are not introducing new vulnerabilities\r\n- [x] Do due diligence on new 3rd party libraries, checking for CVEs\r\n- [x] Don't allow user-uploaded images to be served from the Dockstore domain\r\n- [x] If this PR is for a user-facing feature, create and link a documentation ticket for this feature (usually in the same milestone as the linked issue). Style points if you create a documentation PR directly and link that instead.\r\n- [x] Check whether this PR disables tests. If it legitimately needs to disable a test, create a new ticket to re-enable it in a specific milestone. \r\n\n"}
{"id": 118, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe root cause was that `andIUnlinkTheDefendant` was navigating to the defendant details screen as part of its implementation. But it was only called after `andIClickThroughToTheDefendantDetailsScreen`, which _also_ navigates to the defendant details screen. For some reason most of the time playwright wasn't complaining about the double-click, but just occasionally, in Safari, it was. Keeping `andIUnlinkTheDefendant` to just filling out the unlink form fixes the flakiness.\n"}
{"id": 366, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Please make sure the target branch is right. In most case, the target branch should be `develop`. -->\r\n\r\n### Which Issue(s) This PR Fixes\r\n\r\n<!-- Please ensure that the related issue has already been created, and [link this pull request to that issue using keywords](<https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword>) to ensure automatic closure. -->\r\n\r\nFixes #7614 \r\n\r\n### Brief Description\r\n\r\n<!-- Write a brief description for your pull request to help the maintainer understand the reasons behind your changes. -->\r\n\r\n### How Did You Test This Change?\r\n\r\n```\r\nbazel test --config=remote  //store:src/test/java/org/apache/rocketmq/store/RocksDBMessageStoreTest --runs_per_test=1024\r\n```\r\nVerify there is no failures anymore.\r\n\r\n\n"}
{"id": 378, "ground_truth": "Concurrency", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nfix flaky test directory call\n\nsometimes multiple runners share the same temp directory. When this happens there's a chance that the check for number of files output is (2) instead of (1). Fix this by creating a unique directory for this test.\n\n"}
{"id": 384, "ground_truth": "Unordered collections", "predicted": "Unordered collections", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Description\r\n\r\n<img width=\"2048\" height=\"675\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a035d725-79a5-4da5-962e-79cf4bfee447\" />\r\nhttps://github.com/apache/airflow/actions/runs/16537752338/job/46774859176?pr=53597\r\n\r\n\r\nSee failure example: https://github.com/apache/airflow/actions/runs/16537752338/job/46774859176?pr=53597\r\n\r\nThis happened because `DagRun.get_task_instances()` returns TaskInstances in a non-deterministic order (usually depending on the database), and the test incorrectly assumed that the last TI in the returned list would be the one with the lexicographically largest `task_id`.\r\n\r\n### Solution\r\n\r\nTo ensure consistent behavior, we now explicitly sort TaskInstances by `task_id` in natural (human-friendly) order using `natsort`, so that:\r\n\r\n- `\"task2\"` comes before `\"task10\"`\r\n- `tis[-1]` now reliably refers to the correct \"last\" task when sorted by `task_id`\r\n\r\nThis ensures `DagRun.get_last_ti()` behaves deterministically and the associated test is no longer flaky.\r\n\r\n### Changes\r\n\r\n- Use `natsorted` in `get_last_ti()` to sort TIs by `task_id` in a stable natural order\r\n- Document rationale in the code\r\n\r\n\r\n\r\n\r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n"}
{"id": 32, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nPlatforms: linux, rocm, slow\n\n\n\n  This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_aoti_fx_add&suite=AOTFxirTestCase&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/49795173420).\n\n  Over the past 6 hours, it has been determined flaky in 10 workflow(s) with 20 failures and 10 successes.\n\n  **Debugging instructions (after clicking on the recent samples link):**\n  DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\n  To find relevant log snippets:\n  1. Click on the workflow logs linked above\n  2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n  3. Grep for `test_aoti_fx_add`\n  4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n  \n  \n  Test file path: `inductor/test_fxir_backend.py`\n\n  For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.\n\ncc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben\n"}
{"id": 328, "ground_truth": "Time", "predicted": "Network", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n`contentTracing captured events include V8 samples from the main process` has been flaking a fair amount on Appveyor as of late.\r\n\r\nSee [here](https://ci.appveyor.com/project/electron-bot/electron-x64-testing/builds/47184539/job/6ycj2f45a7flb0dc/tests) for example.\n"}
{"id": 424, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[Jira: KAFKA-18537](https://issues.apache.org/jira/browse/KAFKA-18537)\nAdd a wait for cleaner thread shutdown in `testCleanerThreadShutdown` to\neliminate flakiness. After calling `cache.close()`, the test now uses\n`TestUtils.waitForCondition` to poll until the background\n‚Äúremote-log-index-cleaner‚Äù thread has fully exited before asserting that\nno cleaner threads remain. This ensures the asynchronous shutdown always\ncompletes before the final assertions.\n\nReviewers: TengYao Chi <kitingiao@gmail.com>, Chia-Ping Tsai\n <chia7712@gmail.com>\n\n"}
{"id": 131, "ground_truth": "I/O", "predicted": "I/O", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Summary of the Pull Request\r\n\r\n**What is this about:**\r\nThe pdf unit tests use a mock of the IStream interface which fails if it can't copy the contents of the stream in a single pass.\r\n\r\n**What is include in the PR:** \r\nFixes for the mock to run correctly even when it receives a smaller buffer side.\r\nAlso run Pdf Thumbnail tests as part of the automated tests.\r\nTook the chance to also add PDF modules to the bug report template.\r\n\r\n**How does someone test / validate:** \r\nTests run successfully.\r\n\r\n## Quality Checklist\r\n\r\n- [x] **Linked issue:** #13247\r\n- [ ] **Communication:** I've discussed this with core contributors in the issue. \r\n- [ ] **Tests:** Added/updated and all pass\r\n- [ ] **Installer:** Added/updated and all pass\r\n- [ ] **Localization:** All end user facing strings can be localized\r\n- [ ] **Docs:** Added/ updated\r\n- [x] **Binaries:** Any new files are added to WXS / YML\r\n   - [x] No new binaries\r\n   - [ ] [YML for signing](https://github.com/microsoft/PowerToys/blob/master/.pipelines/pipeline.user.windows.yml#L68) for new binaries\r\n   - [ ] [WXS for installer](https://github.com/microsoft/PowerToys/blob/master/installer/PowerToysSetup/Product.wxs) for new binaries\r\n\n"}
{"id": 45, "ground_truth": "Logic", "predicted": "Resource leak", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Motivation\r\n\r\nRunning ClusterMigrationTest could fail with OOME.\r\nWhen investigating the problem, it showed up that ClusterMigrationTest's TestBroker inner class doesn't clean up resources properly. The admin clients weren't closed.\r\n\r\n### Modifications\r\n\r\n* Call `super.internalCleanup()` in the `cleanup` method.\r\n* Close admin clients in the test cleanup method\r\n\r\n### Documentation\r\n\r\n<!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->\r\n\r\n- [ ] `doc` <!-- Your PR contains doc changes. -->\r\n- [ ] `doc-required` <!-- Your PR changes impact docs and you will update later -->\r\n- [x] `doc-not-needed` <!-- Your PR changes do not impact docs -->\r\n- [ ] `doc-complete` <!-- Docs have been already added -->\n"}
{"id": 297, "ground_truth": "Concurrency", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n**Build Scans:**\n- [elasticsearch-periodic-platform-support #9255 / windows-2025_checkpart1_platform-support-windows](https://gradle-enterprise.elastic.co/s/wrqmjkr2q5tv2)\n- [elasticsearch-periodic-platform-support #9227 / amazonlinux-2023_platform-support-aws](https://gradle-enterprise.elastic.co/s/lnzqxbgk5pjyi)\n- [elasticsearch-periodic-platform-support #9227 / oraclelinux-9_platform-support-unix](https://gradle-enterprise.elastic.co/s/bc573kel6n2ae)\n\n**Reproduction Line:**\n```\ngradlew \":server:internalClusterTest\" --tests \"org.elasticsearch.search.aggregations.bucket.FiltersCancellationIT.testFiltersSubAggsCancellation\" -Dtests.seed=E591B05A93049B8B -Dtests.locale=fr-TG -Dtests.timezone=US/Eastern -Druntime.java=24\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nN/A\n\n**Failure History:**\n[See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.search.aggregations.bucket.FiltersCancellationIT),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!('testFiltersSubAggsCancellation'),title:'Test',type:optionsListControl)))))\n\n**Failure Message:**\n```\njava.lang.AssertionError: null\n```\n\n**Issue Reasons:**\n- [main] 3 failures in test testFiltersSubAggsCancellation (1.1% fail rate in 263 executions)\n- [main] 2 failures in pipeline elasticsearch-periodic-platform-support (40.0% fail rate in 5 executions)\n\n**Note:**\nThis issue was created using new test triage automation. Please report issues or feedback to es-delivery.\n"}
{"id": 11, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nSince python does not have a library for max heap and usual tricks like inverting values etc.. does not work for all cases. \n\nWe have our own implementation of max heap. \n\n"}
{"id": 198, "ground_truth": "Network", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nIdeas and help of Alexander Motin ;)\r\n\r\nTicket: #28198\r\n(cherry picked from commit ef3eb51d9b80715c45401827316ab1ef7fc6933b)\n"}
{"id": 304, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nGhostscript doesn't obey SOURCE_DATE_EPOCH\r\n(https://bugs.ghostscript.com/show_bug.cgi?id=696765) so locally\r\ntest_savefig_to_stringio is a bit flaky -- sometimes the string\r\ncomparisons fail due to differences in timestamps between two\r\nghostscript invocations (I'm actually a bit curious why this never fails\r\non CI).\r\nFix that by stripping out the timestamp.\r\n\r\n## PR Summary\r\n\r\n## PR Checklist\r\n\r\n- [ ] Has Pytest style unit tests\r\n- [ ] Code is [Flake 8](http://flake8.pycqa.org/en/latest/) compliant\r\n- [ ] New features are documented, with examples if plot related\r\n- [ ] Documentation is sphinx and numpydoc compliant\r\n- [ ] Added an entry to doc/users/next_whats_new/ if major new feature (follow instructions in README.rst there)\r\n- [ ] Documented in doc/api/api_changes.rst if API changed in a backward-incompatible way\r\n\r\n<!--\r\nThank you so much for your PR!  To help us review your contribution, please\r\nconsider the following points:\r\n\r\n- A development guide is available at https://matplotlib.org/devdocs/devel/index.html.\r\n\r\n- Help with git and github is available at\r\n  https://matplotlib.org/devel/gitwash/development_workflow.html.\r\n\r\n- Do not create the PR out of master, but out of a separate branch.\r\n\r\n- The PR title should summarize the changes, for example \"Raise ValueError on\r\n  non-numeric input to set_xlim\".  Avoid non-descriptive titles such as\r\n  \"Addresses issue #8576\".\r\n\r\n- The summary should provide at least 1-2 sentences describing the pull request\r\n  in detail (Why is this change required?  What problem does it solve?) and\r\n  link to any relevant issues.\r\n\r\n- If you are contributing fixes to docstrings, please pay attention to\r\n  http://matplotlib.org/devel/documenting_mpl.html#formatting.  In particular,\r\n  note the difference between using single backquotes, double backquotes, and\r\n  asterisks in the markup.\r\n\r\nWe understand that PRs can sometimes be overwhelming, especially as the\r\nreviews start coming in.  Please let us know if the reviews are unclear or\r\nthe recommended next step seems overly demanding, if you would like help in\r\naddressing a reviewer's comments, or if you have been waiting too long to hear\r\nback on your PR.\r\n-->\r\n\n"}
{"id": 99, "ground_truth": "Async wait", "predicted": "Async wait", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nTry fixing a UI test that has been flaky for a while recently.\n"}
{"id": 37, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nPlatforms: asan, linux, mac, macos, rocm, slow, win, windows, dynamo\n\n\n\n\n\nThis test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_get_mode_stack&suite=TestTorchFunctionMode&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/28989307006).\n\nOver the past 3 hours, it has been determined flaky in 69 workflow(s) with 210 failures and 69 successes.\n\n**Debugging instructions (after clicking on the recent samples link):**\nDO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.\nTo find relevant log snippets:\n1. Click on the workflow logs linked above\n2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.\n3. Grep for `test_get_mode_stack`\n4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.\n\n\n\n<details><summary>Sample error message</summary>\n\n```\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/test/test_overrides.py\", line 1328, in test_get_mode_stack\n    self.assertEqual(_get_current_function_mode_stack(), [])\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py\", line 3785, in assertEqual\n    error_metas = not_close_error_metas(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/testing/_comparison.py\", line 1227, in not_close_error_metas\n    raise error_meta.to_error() from None  # noqa: RSE102\nAssertionError: The length of the sequences mismatch: 1 != 0\n\nTo execute this test, run the following from the base repo dir:\n    python test/test_overrides.py TestTorchFunctionMode.test_get_mode_stack\n\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n```\n\n</details>\n\n\nTest file path: `test_overrides.py`\n\ncc @clee2000 @hameerabbasi @rgommers @ezyang\n"}
{"id": 67, "ground_truth": "Logic", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nThese tests are the same ones that run against other analytics DBs, but for some reason when we run them against Presto/Hive we get false positives (or are they false negatives? ü§î) at a much greater rate. These tests also seem to take much longer to run, which may be related.\r\n\r\nIf we can get to the bottom of this it will massively improve the experience of contributing to Superset.\r\n\r\n#### How to reproduce the bug\r\n\r\nRun CI a few times.\r\n\r\n### Expected results\r\n\r\nCI should pass if the PR hasn't introduced any bugs.\r\n\r\n### Actual results\r\n\r\nIt fails a lot. Often multiple times in a row, with different tests.\r\n\r\n### Additional context\r\n\r\nI want to start putting together a list of the tests that have failed, to see if there is some sort of pattern. Feel free to add new entries to this list (only if you are certain that it really is a false positive!). Please also link to a relevant line of the GitHub Action where the test failed.\r\n\r\n- [ERROR tests/integration_tests/dashboards/filter_state/api_tests.py::test_delete_not_owner](https://github.com/apache/superset/runs/4524150250?check_suite_focus=true#step:8:2363)\r\n- [ERROR tests/integration_tests/security_tests.py::TestRolePermission::test_admin_permissions](https://github.com/apache/superset/runs/4525283552?check_suite_focus=true#step:8:2420)\r\n\n"}
{"id": 190, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\nFew of recent ci runs have failures for `//test/extensions/filters/http/alternate_protocols_cache:filter_integration_test`:\nhttps://github.com/envoyproxy/envoy/actions/runs/15444829526/job/43471494796\nhttps://github.com/envoyproxy/envoy/actions/runs/15439887326/job/43454992923\nci log:\n```\n[ RUN      ] Protocols/MixedUpstreamIntegrationTest.BasicRequestAutoWithHttp3/IPv4_Http2Downstream_Http3UpstreamHttpParserNghttp2Legacy\ntest/extensions/filters/http/alternate_protocols_cache/filter_integration_test.cc:536: Failure\nExpected: (getSrtt(alt_svc, timeSystem())) != (0), actual: 0 vs 0\nAlt-svc entry :''\nStack trace:\n  0xaaaad262aa0c: Envoy::(anonymous namespace)::MixedUpstreamIntegrationTest_BasicRequestAutoWithHttp3_Test::TestBody()\n  0xaaaad3ad4224: testing::internal::HandleExceptionsInMethodIfSupported<>()\n  0xaaaad3ad40c0: testing::Test::Run()\n  0xaaaad3ad528c: testing::TestInfo::Run()\n... Google Test internal frames ...\n\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_quic_always_support_server_preferred_address to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_enable_universal_header_validator to: false\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http1_use_balsa_parser to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http2_use_oghttp2 to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_no_extension_lookup_by_name to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_runtime_initialized to: false\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_quic_always_support_server_preferred_address to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_enable_universal_header_validator to: false\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http1_use_balsa_parser to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_http2_use_oghttp2 to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_no_extension_lookup_by_name to: true\n[external/com_google_absl/absl/flags/internal/flag.cc : 147] RAW: Restore saved value of envoy_reloadable_features_runtime_initialized to: false\n[  FAILED  ] Protocols/MixedUpstreamIntegrationTest.BasicRequestAutoWithHttp3/IPv4_Http2Downstream_Http3UpstreamHttpParserNghttp2Legacy, where GetParam() = 24-byte object <00-00 00-00 01-00 00-00 02-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00> (571 ms)\n```\n"}
{"id": 104, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe changes in this pull request address issue SUREFIRE-756. Namely, the changes include\r\n\r\n- Output of the random seed used to generate a particular random test order when -Dsurefire.runOrder=random or -Dfailsafe.runOrder=random is set\r\n- Ability to replay a previously observed random test order by setting -Dsurefire.seed and -Dfailsafe.seed to the seed that observed the random test order\r\n- Tests to ensure that the setting of the _same_ random seeds do create the _same_ test orders and _different_ random seeds do create _different_ test orders. Note that the inherent randomness of the orders does mean that the tests can be flaky (nondeterministically pass or fail without changes to the code). The current tests have a rate of 0.4% ```(1/3)^5``` of failing. Increasing the number of tests (```3```) or the number of times to loop (```5```) would decrease the odds of the tests failing.\n"}
{"id": 165, "ground_truth": "Async Wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test `test_installed_modules` appears to not be all that useful.\n\nThe test exists to verify the behavior of the [`_generate_installed_modules` function](https://github.com/getsentry/sentry-python/blob/9b66f3b51502ca600554c711bc3f599c18f8f18b/sentry_sdk/utils.py#L1689). However, all the test does is essentially check the output of `_generate_installed_modules` against a refactored version of the function call itself.\n\nIn short, in its current form, the test appears to not make too much sense. As the test recently started failing, let's just delete it.\n\n<!-- Describe your PR here -->\n\n---\n\nThank you for contributing to `sentry-python`! Please add tests to validate your changes, and lint your code using `tox -e linters`.\n\nRunning the test suite on your PR might require maintainer approval.\n"}
{"id": 117, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nSome tests may randomly fail du to debounce timer on field validation.\n"}
{"id": 100, "ground_truth": "Test order dependency", "predicted": "Test order dependency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/apache/airflow/actions/runs/16658216543/job/47149178223#step:10:696\r\n\r\nSometimes the test failing with cause of other test stored cache.\r\nWhen we run entire test file its working if we run only `task-sdk/tests/task_sdk/io/test_path.py::TestAttach::test_alias` , this was failing due to there is no previously cached object and its asserting for two fields.\r\n \r\n<!--\r\n Licensed to the Apache Software Foundation (ASF) under one\r\n or more contributor license agreements.  See the NOTICE file\r\n distributed with this work for additional information\r\n regarding copyright ownership.  The ASF licenses this file\r\n to you under the Apache License, Version 2.0 (the\r\n \"License\"); you may not use this file except in compliance\r\n with the License.  You may obtain a copy of the License at\r\n\r\n   http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n Unless required by applicable law or agreed to in writing,\r\n software distributed under the License is distributed on an\r\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\n KIND, either express or implied.  See the License for the\r\n specific language governing permissions and limitations\r\n under the License.\r\n -->\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of an existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n\r\n\r\n<!-- Please keep an empty line above the dashes. -->\r\n---\r\n**^ Add meaningful description above**\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [airflow-core/newsfragments](https://github.com/apache/airflow/tree/main/airflow-core/newsfragments).\r\n\n"}
{"id": 283, "ground_truth": "Async wait", "predicted": "I/O", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nResolves : https://issues.apache.org/jira/browse/KAFKA-16518\r\n\r\n- Adds a new argument \"standalone\" to kafka-storage.sh\r\n- If standalone mode, creates a checkpoint file in metadata dir ${kafkaConfig.metadataLogDir}/__cluster_metadata-0/\r\n\r\n\r\n### Committer Checklist (excluded from commit message)\r\n- [ ] Verify design and implementation \r\n- [ ] Verify test coverage and CI build status\r\n- [ ] Verify documentation (including upgrade notes)\r\n\n"}
{"id": 306, "ground_truth": "I/O", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[IFRT] Remove the error message mentioning an invalid output target\n\nA log message was being generated when an invalid output target was specified\nand being ignored. This caused some OSS tests to fail in a flaky manner, so\nlogging is removed in this change.\n\n"}
{"id": 236, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Why are these changes needed?\r\n\r\nTSAN failure is a data race only in the test's `FakeShutdownExecutor`, not production code. Fake was writing shared `std::string` fields from two threads without synchronization -- https://buildkite.com/ray-project/postmerge/builds/12666#019907c0-3bdd-4401-9aa8-6f13215ce819/176-796\r\n\r\n- Added a `std::mutex` to `FakeShutdownExecutor` and guarded assignments to `last_exit_type` and `last_detail` in all `Execute*` methods. No production code changes.\r\n- Added mutex-guarded getters in `FakeShutdownExecutor` and used them in the assertion to eliminate remaining unsynchronized reads in the TSAN test.\r\n\r\n## Related issue number\r\n\r\nCloses #55801\r\n\n"}
{"id": 234, "ground_truth": "Floating point operations", "predicted": "Floating point operations", "match": true, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nTunableOp will sometimes find a less precise solution due to the small input vectors used in this UT. Bumping op tolerance to eliminate flakiness.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang\n"}
{"id": 428, "ground_truth": "Async wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n#### What type of PR is this?\r\n/kind flake\r\n<!--\r\nAdd one of the following kinds:\r\n/kind bug\r\n/kind cleanup\r\n/kind documentation\r\n/kind feature\r\n\r\nOptionally add one or more of the following kinds if applicable:\r\n/kind api-change\r\n/kind deprecation\r\n/kind failing-test\r\n/kind flake\r\n/kind regression\r\n-->\r\n\r\n#### What this PR does / why we need it:\r\nThis pr fixes a race problem introduced by #128642\r\n\r\nWhen using `exit 0` as the command of a container. After the container is terminated,`finishedAt - startedAt` can be -1 second, which violates the validation after.\r\n\r\nRelated job: https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/129680/pull-kubernetes-e2e-gce/1933002039273459712\r\n#### Which issue(s) this PR is related to:\r\n<!--\r\nPlease link relevant issues to help with tracking.\r\n\r\nTo automatically close the linked issue(s) when this PR is merged,\r\nadd the word \"Fixes\" before the issue number or link.\r\nDo not use \"Fixes\" if the PR is of kind `failing-test` or `flake`.\r\n\r\nReference KEPs when applicable in addition to specific issues.\r\n\r\nExamples:\r\nFixes #<issue number>\r\n<issue link> (issue in a different repository)\r\nKEP: https://github.com/kubernetes/enhancements/issues/<kep-issue-number>\r\n\r\nIf there is no associated issue, then write \"N/A\".\r\n-->\r\n\r\n#### Special notes for your reviewer:\r\n\r\n#### Does this PR introduce a user-facing change?\r\n<!--\r\nIf no, just write \"NONE\" in the release-note block below.\r\nIf yes, a release note is required:\r\nEnter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\".\r\n\r\nFor more information on release notes see: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n-->\r\n```release-note\r\nNONE\r\n```\r\n\r\n#### Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.:\r\n\r\n<!--\r\nThis section can be blank if this pull request does not require a release note.\r\n\r\nWhen adding links which point to resources within git repositories, like\r\nKEPs or supporting documentation, please reference a specific commit and avoid\r\nlinking directly to the master branch. This ensures that links reference a\r\nspecific point in time, rather than a document that may change over time.\r\n\r\nSee here for guidance on getting permanent links to files: https://help.github.com/en/articles/getting-permanent-links-to-files\r\n\r\nPlease use the following format for linking documentation:\r\n- [KEP]: <link>\r\n- [Usage]: <link>\r\n- [Other doc]: <link>\r\n-->\r\n```docs\r\n\r\n```\r\n\n"}
{"id": 316, "ground_truth": "OS", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Motivation\n\nI'm making this change for clarification in the Getting Started guide which is the gateway for new users to Pulsar.\n### Modifications\n\nUpdated documentation.\n### Result\n\nJust the documentation.\n\n"}
{"id": 367, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n<!-- Please make sure the target branch is right. In most case, the target branch should be `develop`. -->\r\n\r\n### Which Issue(s) This PR Fixes\r\n\r\n<!-- Please ensure that the related issue has already been created, and [link this pull request to that issue using keywords](<https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword>) to ensure automatic closure. -->\r\n\r\nFixes #7627\r\n\r\n### Brief Description\r\n\r\n<!-- Write a brief description for your pull request to help the maintainer understand the reasons behind your changes. -->\r\n\r\n### How Did You Test This Change?\r\n\r\n<!-- In order to ensure the code quality of Apache RocketMQ, we expect every pull request to have undergone thorough testing. -->\r\n\n"}
{"id": 260, "ground_truth": "Async wait", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nFor example this test fails randomly and needs to be re-run multiple times: https://github.com/mastra-ai/mastra/actions/runs/15888293944/job/44806477266?pr=5482\r\n\r\nSeems to be flawed test logic (fixed here)\n"}
{"id": 296, "ground_truth": "Randomness", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n[PR Linked Issue]\n**Build Scans:**\n- [elasticsearch-intake #27158 / part3](https://gradle-enterprise.elastic.co/s/akaigagd47vke)\n- [elasticsearch-periodic-platform-support #10137 / rocky-9_platform-support-unix](https://gradle-enterprise.elastic.co/s/7266jgkv5xaqy)\n\n**Reproduction Line:**\n```\n./gradlew \":x-pack:plugin:esql:internalClusterTest\" --tests \"org.elasticsearch.xpack.esql.action.RandomizedTimeSeriesIT.testGroupByNothing\" -Dtests.seed=3050CE9CE9791881 -Dtests.locale=sa -Dtests.timezone=Australia/LHI -Druntime.java=24\n```\n\n**Applicable branches:**\nmain\n\n**Reproduces locally?:**\nN/A\n\n**Failure History:**\n[See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.xpack.esql.action.RandomizedTimeSeriesIT),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!('testGroupByNothing'),title:'Test',type:optionsListControl)))))\n\n**Failure Message:**\n```\norg.elasticsearch.action.bulk.IndexDocFailureStoreStatus$ExceptionWithFailureStoreStatus: [.ds-tsit_ds-2025.08.20-000001/TEfC-UbzS46u74MtBQWqXA][[.ds-tsit_ds-2025.08.20-000001][0]] org.elasticsearch.index.engine.VersionConflictEngineException: [C49S8X6g-QjW9uynAAABmMd3qH4][KBkJUGhjrYx0kKOigCXrLeK0RcVVTL_itQLi69ku_AIJqqsrijofpOU@2025-08-20T12:32:44.670Z]: version conflict, document already exists (current version [1])\n```\n\n**Issue Reasons:**\n- [main] 2 failures in test testGroupByNothing (1.7% fail rate in 118 executions)\n\n**Note:**\nThis issue was created using new test triage automation. Please report issues or feedback to es-delivery.\n"}
{"id": 289, "ground_truth": "Async wait", "predicted": "Time", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\n\r\nFixes #1291\r\n\r\n### Motivation\r\n\r\nThe `MaxTime` setting for AckGroupingTracker is too short at only 10,000 nanoseconds.\r\nThis causes the test very flaky.\r\n\r\n### Modifications\r\n\r\n- Change the `MaxTime` to 10 seconds\r\n\r\n### Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n\r\n### Does this pull request potentially affect one of the following parts:\r\n\r\n*If `yes` was chosen, please highlight the changes*\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / no)\r\n  - The public API: (yes / no)\r\n  - The schema: (yes / no / don't know)\r\n  - The default values of configurations: (yes / no)\r\n  - The wire protocol: (yes / no)\r\n\r\n### Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / GoDocs / not documented)\r\n  - If a feature is not applicable for documentation, explain why?\r\n  - If a feature is not documented yet in this PR, please create a followup issue for adding the documentation\r\n\n"}
{"id": 126, "ground_truth": "Resource Leak", "predicted": "Test order dependency", "match": false, "prompt": "You are an expert in flaky tests. Given the issue description below, classify the root cause of flaky tests into one of the following categories:\n\nTest order dependency, Time, Resource leak, Network, Unordered collections, Async wait, Concurrency, Floating point operations, Randomness, OS, I/O\n\nRespond only with the exact category name.\n\n### Issue Description:\nfixes #36695\r\n\n"}
